,case_title_x,Description,url,_index,_type,body,case_title_y
0,"[ARR] [Sev B] SR-2205300030001910 title: DBFS API 2.0, 1 MB upload limitation","The cx wants to upload files using DBFS API 2.0, but since the size limitation is 1 MB, they can't upload file bigger than 1 MB. How can make it possible?

The cx is using PowerShell scripts.
Here are parts of their scripts:
#uploads the libraries
    Get-ChildItem -Path $LibraryFilePath |
 Foreach-Object {

        $filename = $_

        write-host $filename.FullName

        $file = $_.Name

        write-host $file
        $base64file = [Convert]::ToBase64String([IO.File]::ReadAllBytes($filename.FullName))

        $path = ""/FileStore/jars/init-libs/$file""

        write-host $path
 write-host $path

        $jsonbody = @{
            path      = $path
            contents  = $base64file
            overwrite = ""true""
        }


        $Uri = ""$ADB_URL/api/2.0/dbfs/put""
        $Headers = @{'Authorization' = ""Bearer $ADB_MRL_Token"" }
        $Method = 'POST'
        $Body = $jsonbody | ConvertTo-Json

        write-host ""Uploading $file...""
        $Result = Invoke-RestMethod -Uri $Uri -Header $Headers -Body $Body -Method $Method

        if ($Result -ne $null) {
            write-host ""$file Uploaded to DBFS""
 
        }
}",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/dbfs,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents DBFS API 2.0 Article 01/27/2022 9 minutes to read 6 contributors In this article Limitations Add block Close Create Delete Get status List Mkdirs Move Put Read Data structures The DBFS API is a Databricks API that makes it simple to interact with various data sources without having to include your credentials every time you read a file. See Databricks File System (DBFS) for more information. For an easy to use command line client of the DBFS API, see Databricks CLI. Note To ensure high quality of service under heavy load, Azure Databricks is now enforcing API rate limits for DBFS API calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version. Important To access Databricks REST APIs, you must authenticate. Limitations Using the DBFS API with firewall enabled storage containers is not supported. Databricks recommends you use Databricks Connect or az storage. Add block Endpoint HTTP Method 2.0/dbfs/add-block POST Append a block of data to the stream specified by the input handle. If the handle does not exist, this call will throw an exception with RESOURCE_DOES_NOT_EXIST. If the block of data exceeds 1 MB, this call will throw an exception with MAX_BLOCK_SIZE_EXCEEDED. A typical workflow for file upload would be: Call create and get a handle. Make one or more add-block calls with the handle you have. Call close with the handle you have. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/add-block \
--data '{ ""data"": ""SGVsbG8sIFdvcmxkIQ=="", ""handle"": 1234567890123456 }'
 JSON Copy {}
 Request structure Field Name Type Description handle INT64 The handle on an open stream. This field is required. data BYTES The base64-encoded data to append to the stream. This has a limit of 1 MB. This field is required. Close Endpoint HTTP Method 2.0/dbfs/close POST Close the stream specified by the input handle. If the handle does not exist, this call throws an exception with RESOURCE_DOES_NOT_EXIST. A typical workflow for file upload would be: Call create and get a handle. Make one or more add-block calls with the handle you have. Call close with the handle you have. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/close \
--data '{ ""handle"": 1234567890123456 }'
 If the call succeeds, no output displays. Request structure Field Name Type Description handle INT64 The handle on an open stream. This field is required. Create Endpoint HTTP Method 2.0/dbfs/create POST Open a stream to write to a file and returns a handle to this stream. There is a 10 minute idle timeout on this handle. If a file or directory already exists on the given path and overwrite is set to false, this call throws an exception with RESOURCE_ALREADY_EXISTS. A typical workflow for file upload would be: Call create and get a handle. Make one or more add-block calls with the handle you have. Call close with the handle you have. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/create \
--data '{ ""path"": ""/tmp/HelloWorld.txt"", ""overwrite"": true }'
 JSON Copy { ""handle"": 1234567890123456 }
 Request structure Field Name Type Description path STRING The path of the new file. The path should be the absolute DBFS path (for example /mnt/my-file.txt). This field is required. overwrite BOOL The flag that specifies whether to overwrite existing file or files. Response structure Field Name Type Description handle INT64 Handle which should subsequently be passed into the add-block and close calls when writing to a file through a stream. Delete Endpoint HTTP Method 2.0/dbfs/delete POST Delete the file or directory (optionally recursively delete all files in the directory). This call throws an exception with IO_ERROR if the path is a non-empty directory and recursive is set to false or on other similar errors. When you delete a large number of files, the delete operation is done in increments. The call returns a response after approximately 45 seconds with an error message (503 Service Unavailable) asking you to re-invoke the delete operation until the directory structure is fully deleted. For example: JSON Copy {
  ""error_code"": ""PARTIAL_DELETE"",
  ""message"": ""The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.""
}
 For operations that delete more than 10K files, we discourage using the DBFS REST API, but advise you to perform such operations in the context of a cluster, using the File system utility (dbutils.fs). dbutils.fs covers the functional scope of the DBFS REST API, but from notebooks. Running such operations using notebooks provides better control and manageability, such as selective deletes, and the possibility to automate periodic delete jobs. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/delete \
--data '{ ""path"": ""/tmp/HelloWorld.txt"" }'
 JSON Copy {}
 Request structure Field Name Type Description path STRING The path of the file or directory to delete. The path should be the absolute DBFS path (e.g. /mnt/foo/). This field is required. recursive BOOL Whether or not to recursively delete the directory’s contents. Deleting empty directories can be done without providing the recursive flag. Get status Endpoint HTTP Method 2.0/dbfs/get-status GET Get the file information of a file or directory. If the file or directory does not exist, this call throws an exception with RESOURCE_DOES_NOT_EXIST. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/get-status \
--data '{ ""path"": ""/tmp/HelloWorld.txt"" }' \
| jq .
 JSON Copy {
  ""path"": ""/tmp/HelloWorld.txt"",
  ""is_dir"": false,
  ""file_size"": 13,
  ""modification_time"": 1622054945000
}
 Request structure Field Name Type Description path STRING The path of the file or directory. The path should be the absolute DBFS path (for example, /mnt/my-folder/). This field is required. Response structure Field Name Type Description path STRING The path of the file or directory. is_dir BOOL Whether the path is a directory. file_size INT64 The length of the file in bytes or zero if the path is a directory. modification_time INT64 The last time, in epoch milliseconds, the file or directory was modified. List Endpoint HTTP Method 2.0/dbfs/list GET List the contents of a directory, or details of the file. If the file or directory does not exist, this call throws an exception with RESOURCE_DOES_NOT_EXIST. When calling list on a large directory, the list operation will time out after approximately 60 seconds. We strongly recommend using list only on directories containing less than 10K files and discourage using the DBFS REST API for operations that list more than 10K files. Instead, we recommend that you perform such operations in the context of a cluster, using the File system utility (dbutils.fs), which provides the same functionality without timing out. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/list \
--data '{ ""path"": ""/tmp"" }' \
| jq .
 JSON Copy {
  ""files"": [
    {
      ""path"": ""/tmp/HelloWorld.txt"",
      ""is_dir"": false,
      ""file_size"": 13,
      ""modification_time"": 1622054945000
    },
    {
      ""...""
    }
  ]
}
 Request structure Field Name Type Description path STRING The path of the file or directory. The path should be the absolute DBFS path (e.g. /mnt/foo/). This field is required. Response structure Field Name Type Description files An array of FileInfo A list of FileInfo that describe contents of directory or file. Mkdirs Endpoint HTTP Method 2.0/dbfs/mkdirs POST Create the given directory and necessary parent directories if they do not exist. If there exists a file (not a directory) at any prefix of the input path, this call throws an exception with RESOURCE_ALREADY_EXISTS. If this operation fails it may have succeeded in creating some of the necessary parent directories. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/mkdirs \
--data '{ ""path"": ""/tmp/my-new-dir"" }'
 JSON Copy {}
 Request structure Field Name Type Description path STRING The path of the new directory. The path should be the absolute DBFS path (for example, /mnt/my-folder/). This field is required. Move Endpoint HTTP Method 2.0/dbfs/move POST Move a file from one location to another location within DBFS. If the source file does not exist, this call throws an exception with RESOURCE_DOES_NOT_EXIST. If there already exists a file in the destination path, this call throws an exception with RESOURCE_ALREADY_EXISTS. If the given source path is a directory, this call always recursively moves all files. When moving a large number of files, the API call will time out after approximately 60 seconds, potentially resulting in partially moved data. Therefore, for operations that move more than 10K files, we strongly discourage using the DBFS REST API. Instead, we recommend that you perform such operations in the context of a cluster, using the File system utility (dbutils.fs) from a notebook, which provides the same functionality without timing out. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/move \
--data '{ ""source_path"": ""/tmp/HelloWorld.txt"", ""destination_path"": ""/tmp/my-new-dir/HelloWorld.txt"" }'
 JSON Copy {}
 Request structure Field Name Type Description source_path STRING The source path of the file or directory. The path should be the absolute DBFS path (for example, /mnt/my-source-folder/). This field is required. destination_path STRING The destination path of the file or directory. The path should be the absolute DBFS path (for example, /mnt/my-destination-folder/). This field is required. Put Endpoint HTTP Method 2.0/dbfs/put POST Upload a file through the use of multipart form post. It is mainly used for streaming uploads, but can also be used as a convenient single call for data upload. The amount of data that can be passed using the contents parameter is limited to 1 MB if specified as a string (MAX_BLOCK_SIZE_EXCEEDED is thrown if exceeded) and 2 GB as a file. Example To upload a local file named HelloWorld.txt in the current directory: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/put \
--form contents=@HelloWorld.txt \
--form path=""/tmp/HelloWorld.txt"" \
--form overwrite=true
 To upload content Hello, World! as a base64 encoded string: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/put \
--data '{ ""path"": ""/tmp/HelloWorld.txt"", ""contents"": ""SGVsbG8sIFdvcmxkIQ=="", ""overwrite"": true }'
 JSON Copy {}
 Request structure Field Name Type Description path STRING The path of the new file. The path should be the absolute DBFS path (e.g. /mnt/foo/). This field is required. contents BYTES This parameter might be absent, and instead a posted file will be used. overwrite BOOL The flag that specifies whether to overwrite existing files. Read Endpoint HTTP Method 2.0/dbfs/read GET Return the contents of a file. If the file does not exist, this call throws an exception with RESOURCE_DOES_NOT_EXIST. If the path is a directory, the read length is negative, or if the offset is negative, this call throws an exception with INVALID_PARAMETER_VALUE. If the read length exceeds 1 MB, this call throws an exception with MAX_READ_SIZE_EXCEEDED. If offset + length exceeds the number of bytes in a file, reads contents until the end of file. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/dbfs/read \
--data '{ ""path"": ""/tmp/HelloWorld.txt"", ""offset"": 1, ""length"": 8 }' \
| jq .
 JSON Copy {
  ""bytes_read"": 8,
  ""data"": ""ZWxsbywgV28=""
}
 Request structure Field Name Type Description path STRING The path of the file to read. The path should be the absolute DBFS path (e.g. /mnt/foo/). This field is required. offset INT64 The offset to read from in bytes. length INT64 The number of bytes to read starting from the offset. This has a limit of 1 MB, and a default value of 0.5 MB. Response structure Field Name Type Description bytes_read INT64 The number of bytes read (could be less than length if we hit end of file). This refers to number of bytes read in unencoded version (response data is base64-encoded). data BYTES The base64-encoded contents of the file read. Data structures In this section: FileInfo FileInfo The attributes of a file or directory. Field Name Type Description path STRING The path of the file or directory. is_dir BOOL Whether the path is a directory. file_size INT64 The length of the file in bytes or zero if the path is a directory. modification_time INT64 The last time, in epoch milliseconds, the file or directory was modified.",DBFS API 2.0
1,[ARR] [Sev B] SR-2206010030001463 NoSuchMethoderrorComesWhileinstallingdifferent libraries on databricks cluster,"[Issue]
 No such method erorr for installed libs.
the customer need to know the reason why this error happens and how to solve it.

Error
Caused by: NoSuchMethodError: reactor.netty.http.client.HttpClient.resolver(Lio/netty/resolver/AddressResolverGroup;)Lreactor/netty/transport/ClientTransport;

-New problem, worked before

When this issue started:
Wed, Jun 1, 2022, 12:00 AM (UTC+05:30) Chennai, Kolkata, Mumbai, New Delhi

Notebook URL
https://adb-4359460891691173.13.azuredatabricks.net/?o=4359460891691173#notebook/2152658368282677/command/4361637730630314

Cluster URL:
https://adb-4359460891691173.13.azuredatabricks.net/?o=4359460891691173#setting/clusters/1026-114902-flue504/configuration

[What I checked]
I found in previous case there are similar errors happening on DBR 10.X.
Actually I found init script as workaround below.

%python
dbutils.fs.put(""/databricks/script-test/private-pypi-install.sh"",""""""
rm -rf /databricks/jars/common--encryption--io.projectreactor.netty__reactor-netty-http__1.0.9_shaded.jar
#rm -rf /databricks/jars/*io.netty__netty-all*
#rm -rf /databricks/jars/*io.netty_netty-resolver_io.netty__netty-resolver__*
rm -rf /databricks/jars/common--encryption--io.projectreactor.netty__reactor-netty-core__1.0.9_shaded.jar
wget -P ""/databricks/jars/"" ""https://repo1.maven.org/maven2/io/netty/netty-all/4.1.63.Final/netty-all-4.1.63.Final.jar""
wget -P ""/databricks/jars/"" ""https://repo1.maven.org/maven2/io/netty/netty-resolver/4.1.69.Final/netty-resolver-4.1.69.Final.jar""
"""""", True) 


[Ask]
#1
Could you kindly let us know the cause of the issue?

#2
Could you tell us if above init script works fine for this case?",https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/10.4,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Databricks Runtime 10.4 LTS Article 07/12/2022 19 minutes to read 5 contributors In this article New features and improvements Library upgrades Apache Spark Maintenance updates System environment The following release notes provide information about Databricks Runtime 10.4 and Databricks Runtime 10.4 Photon, powered by Apache Spark 3.2.1. Photon is in Public Preview. Databricks released these images in March 2022. New features and improvements Iceberg to Delta table converter (Public Preview) Auto Compaction rollbacks are now enabled by default Low Shuffle Merge is now enabled by default Insertion order tags are now preserved for UPDATEs and DELETEs HikariCP is now the default Hive metastore connection pool Azure Synapse connector now enables the maximum number of allowed reject rows to be set Asynchronous state checkpointing is now generally available Parameter defaults can now be specified for SQL user-defined functions New Spark SQL functions New working directory for High Concurrency clusters Identity columns support in Delta tables is now generally available Iceberg to Delta table converter (Public Preview) Convert to Delta now supports converting an Iceberg table to a Delta table in place. It does this by using Iceberg native metadata and file manifests. See Convert an Iceberg table to a Delta table. Auto Compaction rollbacks are now enabled by default This release improves the behavior for Delta Lake writes that commit when there are concurrent Auto Compaction transactions. Before this release, such writes would often quit, due to concurrent modifications to a table. Writes will now succeed even if there are concurrent Auto Compaction transactions. Low Shuffle Merge is now enabled by default The MERGE INTO command now always uses the new low-shuffle implementation. This behavior improves the performance of the MERGE INTO command significantly for most workloads. The configuration setting that was previously used to enable this feature has been removed. See Low Shuffle Merge. Insertion order tags are now preserved for UPDATEs and DELETEs The UPDATE and DELETE commands now preserve existing clustering information (including Z-ordering) for files that are updated or deleted. This behavior is a best-effort approach, and this approach does not apply to cases when files are so small that these files are combined during the update or delete. HikariCP is now the default Hive metastore connection pool HikariCP brings many stability improvements for Hive metastore access while maintaining fewer connections compared to the previous BoneCP connection pool implementation. HikariCP is enabled by default on any Databricks Runtime cluster that uses the Databricks Hive metastore (for example, when spark.sql.hive.metastore.jars is not set). You can also explicitly switch to other connection pool implementations, for example BoneCP, by setting spark.databricks.hive.metastore.client.pool.type. Azure Synapse connector now enables the maximum number of allowed reject rows to be set The Azure Synapse connector now supports a maxErrors DataFrame option. This update enables you to configure the maximum number of rejected rows that are allowed during reads and writes before the load operation is cancelled. All rejected rows are ignored. For example, if two out of ten records have errors, only eight records are processed. This option maps directly to the REJECT_VALUE option for the CREATE EXTERNAL TABLE statement in PolyBase and to the MAXERRORS option for the Azure Synapse connector’s COPY command. By default, maxErrors value is set to 0: all records are expected to be valid. Asynchronous state checkpointing is now generally available You can enable asynchronous state checkpointing in stateful streaming queries with large state updates. This can reduce the end-to-end micro-batch latency. This feature is now generally available. See Enable asynchronous state checkpointing for Structured Streaming. Parameter defaults can now be specified for SQL user-defined functions When you create a SQL user-defined function (SQL UDF), you can now specify default expressions for the SQL UDF’s parameters. You can then call the SQL UDF without providing arguments for those parameters, and Databricks will fill in the default values for those parameters. See CREATE FUNCTION (SQL). New Spark SQL functions The following Spark SQL functions are now available with this release: try_multiply: Returns multiplier multiplied by multiplicand, or NULL on overflow. try_subtract: Returns the subtraction of expr2 from expr1, or NULL on overflow. New working directory for High Concurrency clusters On High Concurrency clusters with either table access control or credential passthrough enabled, the current working directory of notebooks is now the user’s home directory. Previously, the working directory was /databricks/driver. Identity columns support in Delta tables is now generally available Delta Lake now supports identity columns. When you write to a Delta table that defines an identity column, and you do not provide values for that column, Delta now automatically assigns a unique and statistically increasing or decreasing value. See CREATE TABLE [USING]. Library upgrades Upgraded Python libraries: filelock from 3.4.2 to 3.6.0 Upgraded R libraries: brew from 1.0-6 to 1.0-7 broom from 0.7.11 to 0.7.12 cli from 3.1.0 to 3.2.0 clipr from 0.7.1 to 0.8.0 colorspace from 2.0-2 to 2.0-3 crayon from 1.4.2 to 1.5.0 dplyr from 1.0.7 to 1.0.8 dtplyr from 1.2.0 to 1.2.1 evaluate from 0.14 to 0.15 foreach from 1.5.1 to 1.5.2 future from 1.23.0 to 1.24.0 generics from 0.1.1 to 0.1.2 glue from 1.6.0 to 1.6.1 gower from 0.2.2 to 1.0.0 iterators from 1.0.13 to 1.0.14 jsonlite from 1.7.3 to 1.8.0 magrittr from 2.0.1 to 2.0.2 mgcv from 1.8-38 to 1.8-39 pillar from 1.6.4 to 1.7.0 randomForest from 4.6-14 to 4.7-1 readr from 2.1.1 to 2.1.2 recipes from 0.1.17 to 0.2.0 rlang from 0.4.12 to 1.0.1 rpart from 4.1-15 to 4.1.16 RSQLite from 2.2.9 to 2.2.10 sparklyr from 1.7.4 to 1.7.5 testthat from 3.1.1 to 3.1.2 tidyr from 1.1.4 to 1.2.0 tidyselect from 1.1.1 to 1.1.2 tinytex from 0.36 to 0.37 yaml from 2.2.1 to 2.3.5 Upgraded Java libraries: io.delta.delta-sharing-spark_2.12 from 0.3.0 to 0.4.0 Apache Spark Databricks Runtime 10.4 includes Apache Spark 3.2.1. This release includes all Spark fixes and improvements included in Databricks Runtime 10.3, as well as the following additional bug fixes and improvements made to Spark: [SPARK-38322] [SQL] Support query stage show runtime statistics in formatted explain mode [SPARK-38162] [SQL] Optimize one row plan in normal and AQE Optimizer [SPARK-38229] [SQL] Should’t check temp/external/ifNotExists with visitReplaceTable when parser [SPARK-34183] [SS] DataSource V2: Required distribution and ordering in micro-batch execution [SPARK-37932] [SQL]Wait to resolve missing attributes before applying DeduplicateRelations [SPARK-37904] [SQL] Improve RebalancePartitions in rules of Optimizer [SPARK-38236] [SQL][3.2][3.1] Check if table location is absolute by “new Path(locationUri).isAbsolute” in create/alter table [SPARK-38035] [SQL] Add docker tests for build-in JDBC dialect [SPARK-38042] [SQL] Ensure that ScalaReflection.dataTypeFor works on aliased array types [SPARK-38273] [SQL] decodeUnsafeRows’s iterators should close underlying input streams [SPARK-38311] [SQL] Fix DynamicPartitionPruning/BucketedReadSuite/ExpressionInfoSuite under ANSI mode [SPARK-38305] [CORE] Explicitly check if source exists in unpack() before calling FileUtil methods [SPARK-38275] [SS] Include the writeBatch’s memory usage as the total memory usage of RocksDB state store [SPARK-38132] [SQL] Remove NotPropagation rule [SPARK-38286] [SQL] Union’s maxRows and maxRowsPerPartition may overflow [SPARK-38306] [SQL] Fix ExplainSuite,StatisticsCollectionSuite and StringFunctionsSuite under ANSI mode [SPARK-38281] [SQL][Tests] Fix AnalysisSuite under ANSI mode [SPARK-38307] [SQL][Tests] Fix ExpressionTypeCheckingSuite and CollectionExpressionsSuite under ANSI mode [SPARK-38300] [SQL] Use ByteStreams.toByteArray to simplify fileToString and resourceToBytes in catalyst.util [SPARK-38304] [SQL] Elt() should return null if index is null under ANSI mode [SPARK-38271] PoissonSampler may output more rows than MaxRows [SPARK-38297] [PYTHON] Explicitly cast the return value at DataFrame.to_numpy in POS [SPARK-38295] [SQL][Tests] Fix ArithmeticExpressionSuite under ANSI mode [SPARK-38290] [SQL] Fix JsonSuite and ParquetIOSuite under ANSI mode [SPARK-38299] [SQL] Clean up deprecated usage of StringBuilder.newBuilder [SPARK-38060] [SQL] Respect allowNonNumericNumbers when parsing quoted NaN and Infinity values in JSON reader [SPARK-38276] [SQL] Add approved TPCDS plans under ANSI mode [SPARK-38206] [SS] Ignore nullability on comparing the data type of join keys on stream-stream join [SPARK-37290] [SQL] - Exponential planning time in case of non-deterministic function [SPARK-38232] [SQL] Explain formatted does not collect subqueries under query stage in AQE [SPARK-38283] [SQL] Test invalid datetime parsing under ANSI mode [SPARK-38140] [SQL] Desc column stats (min, max) for timestamp type is not consistent with the values due to time zone difference [SPARK-38227] [SQL][SS] Apply strict nullability of nested column in time window / session window [SPARK-38221] [SQL] Eagerly iterate over groupingExpressions when moving complex grouping expressions out of an Aggregate node [SPARK-38216] [SQL] Fail early if all the columns are partitioned columns when creating a Hive table [SPARK-38214] [SS]No need to filter windows when windowDuration is multiple of slideDuration [SPARK-38182] [SQL] Fix NoSuchElementException if pushed filter does not contain any references [SPARK-38159] [SQL] Add a new FileSourceMetadataAttribute for the Hidden File Metadata [SPARK-38123] [SQL] Unified use DataType as targetType of QueryExecutionErrors#castingCauseOverflowError [SPARK-38118] [SQL] Func(wrong data type) in HAVING clause should throw data mismatch error [SPARK-35173] [SQL][PYTHON] Add multiple columns adding support [SPARK-38177] [SQL] Fix wrong transformExpressions in Optimizer [SPARK-38228] [SQL] Legacy store assignment should not fail on error under ANSI mode [SPARK-38173] [SQL] Quoted column cannot be recognized correctly when quotedRegexColumnNa… [SPARK-38130] [SQL] Remove array_sort orderable entries check [SPARK-38199] [SQL] Delete the unused dataType specified in the definition of IntervalColumnAccessor [SPARK-38203] [SQL] Fix SQLInsertTestSuite and SchemaPruningSuite under ANSI mode [SPARK-38163] [SQL] Preserve the error class of SparkThrowable while constructing of function builder [SPARK-38157] [SQL] Explicitly set ANSI to false in test timestampNTZ/timestamp.sql and SQLQueryTestSuite to match the expected golden results [SPARK-38069] [SQL][SS] Improve the calculation of time window [SPARK-38164] [SQL] New SQL functions: try_subtract and try_multiply [SPARK-38176] [SQL] ANSI mode: allow implicitly casting String to other simple types [SPARK-37498] [PYTHON] Add eventually for test_reuse_worker_of_parallelize_range [SPARK-38198] [SQL][3.2] Fix QueryExecution.debug#toFile use the passed in maxFields when explainMode is CodegenMode [SPARK-38131] [SQL] Use error classes in user-facing exceptions only [SPARK-37652] [SQL] Add test for optimize skewed join through union [SPARK-37585] [SQL] Update InputMetric in DataSourceRDD with TaskCompletionListener [SPARK-38113] [SQL] Use error classes in the execution errors of pivoting [SPARK-38178] [SS] Correct the logic to measure the memory usage of RocksDB [SPARK-37969] [SQL] HiveFileFormat should check field name [SPARK-37652] Revert “[SQL]Add test for optimize skewed join through union” [SPARK-38124] [SQL][SS] Introduce StatefulOpClusteredDistribution and apply to stream-stream join [SPARK-38030] [SQL] Canonicalization should not remove nullability of AttributeReference dataType [SPARK-37907] [SQL] InvokeLike support ConstantFolding [SPARK-37891] [CORE] Add scalastyle check to disable scala.concurrent.ExecutionContext.Implicits.global [SPARK-38150] [SQL] Update comment of RelationConversions [SPARK-37943] [SQL] Use error classes in the compilation errors of grouping [SPARK-37652] [SQL]Add test for optimize skewed join through union [SPARK-38056] [Web UI][3.2] Fix issue of Structured streaming not working in history server when using LevelDB [SPARK-38144] [CORE] Remove unused spark.storage.safetyFraction config [SPARK-38120] [SQL] Fix HiveExternalCatalog.listPartitions when partition column name is upper case and dot in partition value [SPARK-38122] [Docs] Update the App Key of DocSearch [SPARK-37479] [SQL] Migrate DROP NAMESPACE to use V2 command by default [SPARK-35703] [SQL] Relax constraint for bucket join and remove HashClusteredDistribution [SPARK-37983] [SQL] Back out agg build time metrics from sort aggregate [SPARK-37915] [SQL] Combine unions if there is a project between them [SPARK-38105] [SQL] Use error classes in the parsing errors of joins [SPARK-38073] [PYTHON] Update atexit function to avoid issues with late binding [SPARK-37941] [SQL] Use error classes in the compilation errors of casting [SPARK-37937] [SQL] Use error classes in the parsing errors of lateral join [SPARK-38100] [SQL] Remove unused private method in Decimal [SPARK-37987] [SS] Fix flaky test StreamingAggregationSuite.changing schema of state when restarting query [SPARK-38003] [SQL] LookupFunctions rule should only look up functions from the scalar function registry [SPARK-38075] [SQL] Fix hasNext in HiveScriptTransformationExec’s process output iterator [SPARK-37965] [SQL] Remove check field name when reading/writing existing data in Orc [SPARK-37922] [SQL] Combine to one cast if we can safely up-cast two casts (for dbr-branch-10.x) [SPARK-37675] [SPARK-37793] Prevent overwriting of push shuffle merged files once the shuffle is finalized [SPARK-38011] [SQL] Remove duplicated and useless configuration in ParquetFileFormat [SPARK-37929] [SQL] Support cascade mode for dropNamespace API [SPARK-37931] [SQL] Quote the column name if needed [SPARK-37990] [SQL] Support TimestampNTZ in RowToColumnConverter [SPARK-38001] [SQL] Replace the error classes related to unsupported features by UNSUPPORTED_FEATURE [SPARK-37839] [SQL] DS V2 supports partial aggregate push-down AVG [SPARK-37878] [SQL] Migrate SHOW CREATE TABLE to use v2 command by default [SPARK-37731] [SQL] Refactor and cleanup function lookup in Analyzer [SPARK-37979] [SQL] Switch to more generic error classes in AES functions [SPARK-37867] [SQL] Compile aggregate functions of build-in JDBC dialect [SPARK-38028] [SQL] Expose Arrow Vector from ArrowColumnVector [SPARK-30062] [SQL] Add the IMMEDIATE statement to the DB2 dialect truncate implementation [SPARK-36649] [SQL] Support Trigger.AvailableNow on Kafka data source [SPARK-38018] [SQL] Fix ColumnVectorUtils.populate to handle CalendarIntervalType correctly [SPARK-38023] [CORE] ExecutorMonitor.onExecutorRemoved should handle ExecutorDecommission as finished [SPARK-38019] [CORE] Make ExecutorMonitor.timedOutExecutors deterministic [SPARK-37957] [SQL] Correctly pass deterministic flag for V2 scalar functions [SPARK-37985] [SQL] Fix flaky test for SPARK-37578 [SPARK-37986] [SQL] Support TimestampNTZ in radix sort [SPARK-37967] [SQL] Literal.create support ObjectType [SPARK-37827] [SQL] Put the some built-in table properties into V1Table.propertie to adapt to V2 command [SPARK-37963] [SQL] Need to update Partition URI after renaming table in InMemoryCatalog [SPARK-35442] [SQL] Support propagate empty relation through aggregate/union [SPARK-37933] [SQL] Change the traversal method of V2ScanRelationPushDown push down rules [SPARK-37917] [SQL] Push down limit 1 for right side of left semi/anti join if join condition is empty [SPARK-37959] [ML] Fix the UT of checking norm in KMeans & BiKMeans [SPARK-37906] [SQL] spark-sql should not pass last comment to backend [SPARK-37627] [SQL] Add sorted column in BucketTransform Maintenance updates See Databricks Runtime 10.4 maintenance updates. System environment Operating System: Ubuntu 20.04.4 LTS Java: Zulu 8.56.0.21-CA-linux64 Scala: 2.12.14 Python: 3.8.10 R: 4.1.2 Delta Lake: 1.1.0 Installed Python libraries Library Version Library Version Library Version Antergos Linux 2015.10 (ISO-Rolling) appdirs 1.4.4 argon2-cffi 20.1.0 async-generator 1.10 attrs 20.3.0 backcall 0.2.0 bidict 0.21.4 bleach 3.3.0 boto3 1.16.7 botocore 1.19.7 certifi 2020.12.5 cffi 1.14.5 chardet 4.0.0 cycler 0.10.0 Cython 0.29.23 dbus-python 1.2.16 decorator 5.0.6 defusedxml 0.7.1 distlib 0.3.4 distro-info 0.23ubuntu1 entrypoints 0.3 facets-overview 1.0.0 filelock 3.6.0 idna 2.10 ipykernel 5.3.4 ipython 7.22.0 ipython-genutils 0.2.0 ipywidgets 7.6.3 jedi 0.17.2 Jinja2 2.11.3 jmespath 0.10.0 joblib 1.0.1 jsonschema 3.2.0 jupyter-client 6.1.12 jupyter-core 4.7.1 jupyterlab-pygments 0.1.2 jupyterlab-widgets 1.0.0 kiwisolver 1.3.1 koalas 1.8.2 MarkupSafe 2.0.1 matplotlib 3.4.2 mistune 0.8.4 nbclient 0.5.3 nbconvert 6.0.7 nbformat 5.1.3 nest-asyncio 1.5.1 notebook 6.3.0 numpy 1.20.1 packaging 20.9 pandas 1.2.4 pandocfilters 1.4.3 parso 0.7.0 patsy 0.5.1 pexpect 4.8.0 pickleshare 0.7.5 Pillow 8.2.0 pip 21.0.1 plotly 5.5.0 prometheus-client 0.10.1 prompt-toolkit 3.0.17 protobuf 3.17.2 psycopg2 2.8.5 ptyprocess 0.7.0 pyarrow 4.0.0 pycparser 2.20 Pygments 2.8.1 PyGObject 3.36.0 pyparsing 2.4.7 pyrsistent 0.17.3 python-apt 2.0.0+ubuntu0.20.4.7 python-dateutil 2.8.1 python-engineio 4.3.0 python-socketio 5.4.1 pytz 2020.5 pyzmq 20.0.0 requests 2.25.1 requests-unixsocket 0.2.0 s3transfer 0.3.7 scikit-learn 0.24.1 scipy 1.6.2 seaborn 0.11.1 Send2Trash 1.5.0 setuptools 52.0.0 six 1.15.0 ssh-import-id 5.10 statsmodels 0.12.2 tenacity 8.0.1 terminado 0.9.4 testpath 0.4.4 threadpoolctl 2.1.0 tornado 6.1 traitlets 5.0.5 unattended-upgrades 0.1 urllib3 1.25.11 virtualenv 20.4.1 wcwidth 0.2.5 webencodings 0.5.1 wheel 0.36.2 widgetsnbextension 3.5.1 Installed R libraries R libraries are installed from the Microsoft CRAN snapshot on 2022-02-24. Library Version Library Version Library Version askpass 1.1 assertthat 0.2.1 backports 1.4.1 base 4.1.2 base64enc 0.1-3 bit 4.0.4 bit64 4.0.5 blob 1.2.2 boot 1.3-28 brew 1.0-7 brio 1.1.3 broom 0.7.12 bslib 0.3.1 cachem 1.0.6 callr 3.7.0 caret 6.0-90 cellranger 1.1.0 chron 2.3-56 class 7.3-20 cli 3.2.0 clipr 0.8.0 cluster 2.1.2 codetools 0.2-18 colorspace 2.0-3 commonmark 1.7 compiler 4.1.2 config 0.3.1 cpp11 0.4.2 crayon 1.5.0 credentials 1.3.2 curl 4.3.2 data.table 1.14.2 datasets 4.1.2 DBI 1.1.2 dbplyr 2.1.1 desc 1.4.0 devtools 2.4.3 diffobj 0.3.5 digest 0.6.29 dplyr 1.0.8 dtplyr 1.2.1 e1071 1.7-9 ellipsis 0.3.2 evaluate 0.15 fansi 1.0.2 farver 2.1.0 fastmap 1.1.0 fontawesome 0.2.2 forcats 0.5.1 foreach 1.5.2 foreign 0.8-82 forge 0.2.0 fs 1.5.2 future 1.24.0 future.apply 1.8.1 gargle 1.2.0 generics 0.1.2 gert 1.5.0 ggplot2 3.3.5 gh 1.3.0 gitcreds 0.1.1 glmnet 4.1-3 globals 0.14.0 glue 1.6.1 googledrive 2.0.0 googlesheets4 1.0.0 gower 1.0.0 graphics 4.1.2 grDevices 4.1.2 grid 4.1.2 gridExtra 2.3 gsubfn 0.7 gtable 0.3.0 hardhat 0.2.0 haven 2.4.3 highr 0.9 hms 1.1.1 htmltools 0.5.2 htmlwidgets 1.5.4 httpuv 1.6.5 httr 1.4.2 hwriter 1.3.2 hwriterPlus 1.0-3 ids 1.0.1 ini 0.3.1 ipred 0.9-12 isoband 0.2.5 iterators 1.0.14 jquerylib 0.1.4 jsonlite 1.8.0 KernSmooth 2.23-20 knitr 1.37 labeling 0.4.2 later 1.3.0 lattice 0.20-45 lava 1.6.10 lifecycle 1.0.1 listenv 0.8.0 lubridate 1.8.0 magrittr 2.0.2 markdown 1.1 MASS 7.3-55 Matrix 1.4-0 memoise 2.0.1 methods 4.1.2 mgcv 1.8-39 mime 0.12 ModelMetrics 1.2.2.2 modelr 0.1.8 munsell 0.5.0 nlme 3.1-155 nnet 7.3-17 numDeriv 2016.8-1.1 openssl 1.4.6 parallel 4.1.2 parallelly 1.30.0 pillar 1.7.0 pkgbuild 1.3.1 pkgconfig 2.0.3 pkgload 1.2.4 plogr 0.2.0 plyr 1.8.6 praise 1.0.0 prettyunits 1.1.1 pROC 1.18.0 processx 3.5.2 prodlim 2019.11.13 progress 1.2.2 progressr 0.10.0 promises 1.2.0.1 proto 1.0.0 proxy 0.4-26 ps 1.6.0 purrr 0.3.4 r2d3 0.2.5 R6 2.5.1 randomForest 4.7-1 rappdirs 0.3.3 rcmdcheck 1.4.0 RColorBrewer 1.1-2 Rcpp 1.0.8 RcppEigen 0.3.3.9.1 readr 2.1.2 readxl 1.3.1 recipes 0.2.0 rematch 1.0.1 rematch2 2.1.2 remotes 2.4.2 reprex 2.0.1 reshape2 1.4.4 rlang 1.0.1 rmarkdown 2.11 RODBC 1.3-19 roxygen2 7.1.2 rpart 4.1.16 rprojroot 2.0.2 Rserve 1.8-10 RSQLite 2.2.10 rstudioapi 0.13 rversions 2.1.1 rvest 1.0.2 sass 0.4.0 scales 1.1.1 selectr 0.4-2 sessioninfo 1.2.2 shape 1.4.6 shiny 1.7.1 sourcetools 0.1.7 sparklyr 1.7.5 SparkR 3.2.0 spatial 7.3-11 splines 4.1.2 sqldf 0.4-11 SQUAREM 2021.1 stats 4.1.2 stats4 4.1.2 stringi 1.7.6 stringr 1.4.0 survival 3.2-13 sys 3.4 tcltk 4.1.2 TeachingDemos 2.10 testthat 3.1.2 tibble 3.1.6 tidyr 1.2.0 tidyselect 1.1.2 tidyverse 1.3.1 timeDate 3043.102 tinytex 0.37 tools 4.1.2 tzdb 0.2.0 usethis 2.1.5 utf8 1.2.2 utils 4.1.2 uuid 1.0-3 vctrs 0.3.8 viridisLite 0.4.0 vroom 1.5.7 waldo 0.3.1 whisker 0.4 withr 2.4.3 xfun 0.29 xml2 1.3.3 xopen 1.0.0 xtable 1.8-4 yaml 2.3.5 zip 2.2.0 Installed Java and Scala libraries (Scala 2.12 cluster version) Group ID Artifact ID Version antlr antlr 2.7.7 com.amazonaws amazon-kinesis-client 1.12.0 com.amazonaws aws-java-sdk-autoscaling 1.11.655 com.amazonaws aws-java-sdk-cloudformation 1.11.655 com.amazonaws aws-java-sdk-cloudfront 1.11.655 com.amazonaws aws-java-sdk-cloudhsm 1.11.655 com.amazonaws aws-java-sdk-cloudsearch 1.11.655 com.amazonaws aws-java-sdk-cloudtrail 1.11.655 com.amazonaws aws-java-sdk-cloudwatch 1.11.655 com.amazonaws aws-java-sdk-cloudwatchmetrics 1.11.655 com.amazonaws aws-java-sdk-codedeploy 1.11.655 com.amazonaws aws-java-sdk-cognitoidentity 1.11.655 com.amazonaws aws-java-sdk-cognitosync 1.11.655 com.amazonaws aws-java-sdk-config 1.11.655 com.amazonaws aws-java-sdk-core 1.11.655 com.amazonaws aws-java-sdk-datapipeline 1.11.655 com.amazonaws aws-java-sdk-directconnect 1.11.655 com.amazonaws aws-java-sdk-directory 1.11.655 com.amazonaws aws-java-sdk-dynamodb 1.11.655 com.amazonaws aws-java-sdk-ec2 1.11.655 com.amazonaws aws-java-sdk-ecs 1.11.655 com.amazonaws aws-java-sdk-efs 1.11.655 com.amazonaws aws-java-sdk-elasticache 1.11.655 com.amazonaws aws-java-sdk-elasticbeanstalk 1.11.655 com.amazonaws aws-java-sdk-elasticloadbalancing 1.11.655 com.amazonaws aws-java-sdk-elastictranscoder 1.11.655 com.amazonaws aws-java-sdk-emr 1.11.655 com.amazonaws aws-java-sdk-glacier 1.11.655 com.amazonaws aws-java-sdk-glue 1.11.655 com.amazonaws aws-java-sdk-iam 1.11.655 com.amazonaws aws-java-sdk-importexport 1.11.655 com.amazonaws aws-java-sdk-kinesis 1.11.655 com.amazonaws aws-java-sdk-kms 1.11.655 com.amazonaws aws-java-sdk-lambda 1.11.655 com.amazonaws aws-java-sdk-logs 1.11.655 com.amazonaws aws-java-sdk-machinelearning 1.11.655 com.amazonaws aws-java-sdk-opsworks 1.11.655 com.amazonaws aws-java-sdk-rds 1.11.655 com.amazonaws aws-java-sdk-redshift 1.11.655 com.amazonaws aws-java-sdk-route53 1.11.655 com.amazonaws aws-java-sdk-s3 1.11.655 com.amazonaws aws-java-sdk-ses 1.11.655 com.amazonaws aws-java-sdk-simpledb 1.11.655 com.amazonaws aws-java-sdk-simpleworkflow 1.11.655 com.amazonaws aws-java-sdk-sns 1.11.655 com.amazonaws aws-java-sdk-sqs 1.11.655 com.amazonaws aws-java-sdk-ssm 1.11.655 com.amazonaws aws-java-sdk-storagegateway 1.11.655 com.amazonaws aws-java-sdk-sts 1.11.655 com.amazonaws aws-java-sdk-support 1.11.655 com.amazonaws aws-java-sdk-swf-libraries 1.11.22 com.amazonaws aws-java-sdk-workspaces 1.11.655 com.amazonaws jmespath-java 1.11.655 com.chuusai shapeless_2.12 2.3.3 com.clearspring.analytics stream 2.9.6 com.databricks Rserve 1.8-3 com.databricks jets3t 0.7.1-0 com.databricks.scalapb compilerplugin_2.12 0.4.15-10 com.databricks.scalapb scalapb-runtime_2.12 0.4.15-10 com.esotericsoftware kryo-shaded 4.0.2 com.esotericsoftware minlog 1.3.0 com.fasterxml classmate 1.3.4 com.fasterxml.jackson.core jackson-annotations 2.12.3 com.fasterxml.jackson.core jackson-core 2.12.3 com.fasterxml.jackson.core jackson-databind 2.12.3 com.fasterxml.jackson.dataformat jackson-dataformat-cbor 2.12.3 com.fasterxml.jackson.datatype jackson-datatype-joda 2.12.3 com.fasterxml.jackson.module jackson-module-paranamer 2.12.3 com.fasterxml.jackson.module jackson-module-scala_2.12 2.12.3 com.github.ben-manes.caffeine caffeine 2.3.4 com.github.fommil jniloader 1.1 com.github.fommil.netlib core 1.1.2 com.github.fommil.netlib native_ref-java 1.1 com.github.fommil.netlib native_ref-java-natives 1.1 com.github.fommil.netlib native_system-java 1.1 com.github.fommil.netlib native_system-java-natives 1.1 com.github.fommil.netlib netlib-native_ref-linux-x86_64-natives 1.1 com.github.fommil.netlib netlib-native_system-linux-x86_64-natives 1.1 com.github.luben zstd-jni 1.5.0-4 com.github.wendykierp JTransforms 3.1 com.google.code.findbugs jsr305 3.0.0 com.google.code.gson gson 2.8.6 com.google.crypto.tink tink 1.6.0 com.google.flatbuffers flatbuffers-java 1.9.0 com.google.guava guava 15.0 com.google.protobuf protobuf-java 2.6.1 com.h2database h2 1.4.195 com.helger profiler 1.1.1 com.jcraft jsch 0.1.50 com.jolbox bonecp 0.8.0.RELEASE com.lihaoyi sourcecode_2.12 0.1.9 com.microsoft.azure azure-data-lake-store-sdk 2.3.9 com.ning compress-lzf 1.0.3 com.sun.istack istack-commons-runtime 3.0.8 com.sun.mail javax.mail 1.5.2 com.tdunning json 1.8 com.thoughtworks.paranamer paranamer 2.8 com.trueaccord.lenses lenses_2.12 0.4.12 com.twitter chill-java 0.10.0 com.twitter chill_2.12 0.10.0 com.twitter util-app_2.12 7.1.0 com.twitter util-core_2.12 7.1.0 com.twitter util-function_2.12 7.1.0 com.twitter util-jvm_2.12 7.1.0 com.twitter util-lint_2.12 7.1.0 com.twitter util-registry_2.12 7.1.0 com.twitter util-stats_2.12 7.1.0 com.typesafe config 1.2.1 com.typesafe.scala-logging scala-logging_2.12 3.7.2 com.univocity univocity-parsers 2.9.1 com.zaxxer HikariCP 4.0.3 commons-cli commons-cli 1.2 commons-codec commons-codec 1.15 commons-collections commons-collections 3.2.2 commons-dbcp commons-dbcp 1.4 commons-fileupload commons-fileupload 1.3.3 commons-httpclient commons-httpclient 3.1 commons-io commons-io 2.8.0 commons-lang commons-lang 2.6 commons-logging commons-logging 1.1.3 commons-net commons-net 3.1 commons-pool commons-pool 1.5.4 dev.ludovic.netlib arpack 2.2.1 dev.ludovic.netlib blas 2.2.1 dev.ludovic.netlib lapack 2.2.1 hive-2.3__hadoop-3.2 jets3t-0.7 liball_deps_2.12 info.ganglia.gmetric4j gmetric4j 1.0.10 io.airlift aircompressor 0.21 io.delta delta-sharing-spark_2.12 0.4.0 io.dropwizard.metrics metrics-core 4.1.1 io.dropwizard.metrics metrics-graphite 4.1.1 io.dropwizard.metrics metrics-healthchecks 4.1.1 io.dropwizard.metrics metrics-jetty9 4.1.1 io.dropwizard.metrics metrics-jmx 4.1.1 io.dropwizard.metrics metrics-json 4.1.1 io.dropwizard.metrics metrics-jvm 4.1.1 io.dropwizard.metrics metrics-servlets 4.1.1 io.netty netty-all 4.1.68.Final io.prometheus simpleclient 0.7.0 io.prometheus simpleclient_common 0.7.0 io.prometheus simpleclient_dropwizard 0.7.0 io.prometheus simpleclient_pushgateway 0.7.0 io.prometheus simpleclient_servlet 0.7.0 io.prometheus.jmx collector 0.12.0 jakarta.annotation jakarta.annotation-api 1.3.5 jakarta.servlet jakarta.servlet-api 4.0.3 jakarta.validation jakarta.validation-api 2.0.2 jakarta.ws.rs jakarta.ws.rs-api 2.1.6 javax.activation activation 1.1.1 javax.annotation javax.annotation-api 1.3.2 javax.el javax.el-api 2.2.4 javax.jdo jdo-api 3.0.1 javax.transaction jta 1.1 javax.transaction transaction-api 1.1 javax.xml.bind jaxb-api 2.2.2 javax.xml.stream stax-api 1.0-2 javolution javolution 5.5.1 jline jline 2.14.6 joda-time joda-time 2.10.10 log4j apache-log4j-extras 1.2.17 log4j log4j 1.2.17 maven-trees hive-2.3__hadoop-3.2 liball_deps_2.12 net.java.dev.jna jna 5.8.0 net.razorvine pyrolite 4.30 net.sf.jpam jpam 1.1 net.sf.opencsv opencsv 2.3 net.sf.supercsv super-csv 2.2.0 net.snowflake snowflake-ingest-sdk 0.9.6 net.snowflake snowflake-jdbc 3.13.3 net.snowflake spark-snowflake_2.12 2.9.0-spark_3.1 net.sourceforge.f2j arpack_combined_all 0.1 org.acplt.remotetea remotetea-oncrpc 1.1.2 org.antlr ST4 4.0.4 org.antlr antlr-runtime 3.5.2 org.antlr antlr4-runtime 4.8 org.antlr stringtemplate 3.2.1 org.apache.ant ant 1.9.2 org.apache.ant ant-jsch 1.9.2 org.apache.ant ant-launcher 1.9.2 org.apache.arrow arrow-format 2.0.0 org.apache.arrow arrow-memory-core 2.0.0 org.apache.arrow arrow-memory-netty 2.0.0 org.apache.arrow arrow-vector 2.0.0 org.apache.avro avro 1.10.2 org.apache.avro avro-ipc 1.10.2 org.apache.avro avro-mapred 1.10.2 org.apache.commons commons-compress 1.21 org.apache.commons commons-crypto 1.1.0 org.apache.commons commons-lang3 3.12.0 org.apache.commons commons-math3 3.4.1 org.apache.commons commons-text 1.6 org.apache.curator curator-client 2.13.0 org.apache.curator curator-framework 2.13.0 org.apache.curator curator-recipes 2.13.0 org.apache.derby derby 10.14.2.0 org.apache.hadoop hadoop-client-api 3.3.1-databricks org.apache.hadoop hadoop-client-runtime 3.3.1 org.apache.hive hive-beeline 2.3.9 org.apache.hive hive-cli 2.3.9 org.apache.hive hive-jdbc 2.3.9 org.apache.hive hive-llap-client 2.3.9 org.apache.hive hive-llap-common 2.3.9 org.apache.hive hive-serde 2.3.9 org.apache.hive hive-shims 2.3.9 org.apache.hive hive-storage-api 2.7.2 org.apache.hive.shims hive-shims-0.23 2.3.9 org.apache.hive.shims hive-shims-common 2.3.9 org.apache.hive.shims hive-shims-scheduler 2.3.9 org.apache.htrace htrace-core4 4.1.0-incubating org.apache.httpcomponents httpclient 4.5.13 org.apache.httpcomponents httpcore 4.4.12 org.apache.ivy ivy 2.5.0 org.apache.mesos mesos-shaded-protobuf 1.4.0 org.apache.orc orc-core 1.6.12 org.apache.orc orc-mapreduce 1.6.12 org.apache.orc orc-shims 1.6.12 org.apache.parquet parquet-column 1.12.0-databricks-0003 org.apache.parquet parquet-common 1.12.0-databricks-0003 org.apache.parquet parquet-encoding 1.12.0-databricks-0003 org.apache.parquet parquet-format-structures 1.12.0-databricks-0003 org.apache.parquet parquet-hadoop 1.12.0-databricks-0003 org.apache.parquet parquet-jackson 1.12.0-databricks-0003 org.apache.thrift libfb303 0.9.3 org.apache.thrift libthrift 0.12.0 org.apache.xbean xbean-asm9-shaded 4.20 org.apache.yetus audience-annotations 0.5.0 org.apache.zookeeper zookeeper 3.6.2 org.apache.zookeeper zookeeper-jute 3.6.2 org.checkerframework checker-qual 3.5.0 org.codehaus.jackson jackson-core-asl 1.9.13 org.codehaus.jackson jackson-mapper-asl 1.9.13 org.codehaus.janino commons-compiler 3.0.16 org.codehaus.janino janino 3.0.16 org.datanucleus datanucleus-api-jdo 4.2.4 org.datanucleus datanucleus-core 4.1.17 org.datanucleus datanucleus-rdbms 4.1.19 org.datanucleus javax.jdo 3.2.0-m3 org.eclipse.jetty jetty-client 9.4.43.v20210629 org.eclipse.jetty jetty-continuation 9.4.43.v20210629 org.eclipse.jetty jetty-http 9.4.43.v20210629 org.eclipse.jetty jetty-io 9.4.43.v20210629 org.eclipse.jetty jetty-jndi 9.4.43.v20210629 org.eclipse.jetty jetty-plus 9.4.43.v20210629 org.eclipse.jetty jetty-proxy 9.4.43.v20210629 org.eclipse.jetty jetty-security 9.4.43.v20210629 org.eclipse.jetty jetty-server 9.4.43.v20210629 org.eclipse.jetty jetty-servlet 9.4.43.v20210629 org.eclipse.jetty jetty-servlets 9.4.43.v20210629 org.eclipse.jetty jetty-util 9.4.43.v20210629 org.eclipse.jetty jetty-util-ajax 9.4.43.v20210629 org.eclipse.jetty jetty-webapp 9.4.43.v20210629 org.eclipse.jetty jetty-xml 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-api 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-client 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-common 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-server 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-servlet 9.4.43.v20210629 org.fusesource.leveldbjni leveldbjni-all 1.8 org.glassfish.hk2 hk2-api 2.6.1 org.glassfish.hk2 hk2-locator 2.6.1 org.glassfish.hk2 hk2-utils 2.6.1 org.glassfish.hk2 osgi-resource-locator 1.0.3 org.glassfish.hk2.external aopalliance-repackaged 2.6.1 org.glassfish.hk2.external jakarta.inject 2.6.1 org.glassfish.jaxb jaxb-runtime 2.3.2 org.glassfish.jersey.containers jersey-container-servlet 2.34 org.glassfish.jersey.containers jersey-container-servlet-core 2.34 org.glassfish.jersey.core jersey-client 2.34 org.glassfish.jersey.core jersey-common 2.34 org.glassfish.jersey.core jersey-server 2.34 org.glassfish.jersey.inject jersey-hk2 2.34 org.hibernate.validator hibernate-validator 6.1.0.Final org.javassist javassist 3.25.0-GA org.jboss.logging jboss-logging 3.3.2.Final org.jdbi jdbi 2.63.1 org.jetbrains annotations 17.0.0 org.joda joda-convert 1.7 org.jodd jodd-core 3.5.2 org.json4s json4s-ast_2.12 3.7.0-M11 org.json4s json4s-core_2.12 3.7.0-M11 org.json4s json4s-jackson_2.12 3.7.0-M11 org.json4s json4s-scalap_2.12 3.7.0-M11 org.lz4 lz4-java 1.7.1 org.mariadb.jdbc mariadb-java-client 2.2.5 org.objenesis objenesis 2.5.1 org.postgresql postgresql 42.2.19 org.roaringbitmap RoaringBitmap 0.9.14 org.roaringbitmap shims 0.9.14 org.rocksdb rocksdbjni 6.20.3 org.rosuda.REngine REngine 2.1.0 org.scala-lang scala-compiler_2.12 2.12.14 org.scala-lang scala-library_2.12 2.12.14 org.scala-lang scala-reflect_2.12 2.12.14 org.scala-lang.modules scala-collection-compat_2.12 2.4.3 org.scala-lang.modules scala-parser-combinators_2.12 1.1.2 org.scala-lang.modules scala-xml_2.12 1.2.0 org.scala-sbt test-interface 1.0 org.scalacheck scalacheck_2.12 1.14.2 org.scalactic scalactic_2.12 3.0.8 org.scalanlp breeze-macros_2.12 1.2 org.scalanlp breeze_2.12 1.2 org.scalatest scalatest_2.12 3.0.8 org.slf4j jcl-over-slf4j 1.7.30 org.slf4j jul-to-slf4j 1.7.30 org.slf4j slf4j-api 1.7.30 org.slf4j slf4j-log4j12 1.7.30 org.spark-project.spark unused 1.0.0 org.threeten threeten-extra 1.5.0 org.tukaani xz 1.8 org.typelevel algebra_2.12 2.0.1 org.typelevel cats-kernel_2.12 2.1.1 org.typelevel macro-compat_2.12 1.1.1 org.typelevel spire-macros_2.12 0.17.0 org.typelevel spire-platform_2.12 0.17.0 org.typelevel spire-util_2.12 0.17.0 org.typelevel spire_2.12 0.17.0 org.wildfly.openssl wildfly-openssl 1.0.7.Final org.xerial sqlite-jdbc 3.8.11.2 org.xerial.snappy snappy-java 1.1.8.4 org.yaml snakeyaml 1.24 oro oro 2.0.8 pl.edu.icm JLargeArrays 1.5 software.amazon.ion ion-java 1.0.2 stax stax-api 1.0.1",Databricks Runtime 10.4 LTS
2,[ARR] [Sev B] SR-2206010030001463 NoSuchMethoderrorComesWhileinstallingdifferent libraries on databricks cluster,"[Issue]
 No such method erorr for installed libs.
the customer need to know the reason why this error happens and how to solve it.

Error
Caused by: NoSuchMethodError: reactor.netty.http.client.HttpClient.resolver(Lio/netty/resolver/AddressResolverGroup;)Lreactor/netty/transport/ClientTransport;

-New problem, worked before

When this issue started:
Wed, Jun 1, 2022, 12:00 AM (UTC+05:30) Chennai, Kolkata, Mumbai, New Delhi

Notebook URL
https://adb-4359460891691173.13.azuredatabricks.net/?o=4359460891691173#notebook/2152658368282677/command/4361637730630314

Cluster URL:
https://adb-4359460891691173.13.azuredatabricks.net/?o=4359460891691173#setting/clusters/1026-114902-flue504/configuration

[What I checked]
I found in previous case there are similar errors happening on DBR 10.X.
Actually I found init script as workaround below.

%python
dbutils.fs.put(""/databricks/script-test/private-pypi-install.sh"",""""""
rm -rf /databricks/jars/common--encryption--io.projectreactor.netty__reactor-netty-http__1.0.9_shaded.jar
#rm -rf /databricks/jars/*io.netty__netty-all*
#rm -rf /databricks/jars/*io.netty_netty-resolver_io.netty__netty-resolver__*
rm -rf /databricks/jars/common--encryption--io.projectreactor.netty__reactor-netty-core__1.0.9_shaded.jar
wget -P ""/databricks/jars/"" ""https://repo1.maven.org/maven2/io/netty/netty-all/4.1.63.Final/netty-all-4.1.63.Final.jar""
wget -P ""/databricks/jars/"" ""https://repo1.maven.org/maven2/io/netty/netty-resolver/4.1.69.Final/netty-resolver-4.1.69.Final.jar""
"""""", True) 


[Ask]
#1
Could you kindly let us know the cause of the issue?

#2
Could you tell us if above init script works fine for this case?",https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/10.4,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Databricks Runtime 10.4 LTS Article 07/12/2022 19 minutes to read 5 contributors In this article New features and improvements Library upgrades Apache Spark Maintenance updates System environment The following release notes provide information about Databricks Runtime 10.4 and Databricks Runtime 10.4 Photon, powered by Apache Spark 3.2.1. Photon is in Public Preview. Databricks released these images in March 2022. New features and improvements Iceberg to Delta table converter (Public Preview) Auto Compaction rollbacks are now enabled by default Low Shuffle Merge is now enabled by default Insertion order tags are now preserved for UPDATEs and DELETEs HikariCP is now the default Hive metastore connection pool Azure Synapse connector now enables the maximum number of allowed reject rows to be set Asynchronous state checkpointing is now generally available Parameter defaults can now be specified for SQL user-defined functions New Spark SQL functions New working directory for High Concurrency clusters Identity columns support in Delta tables is now generally available Iceberg to Delta table converter (Public Preview) Convert to Delta now supports converting an Iceberg table to a Delta table in place. It does this by using Iceberg native metadata and file manifests. See Convert an Iceberg table to a Delta table. Auto Compaction rollbacks are now enabled by default This release improves the behavior for Delta Lake writes that commit when there are concurrent Auto Compaction transactions. Before this release, such writes would often quit, due to concurrent modifications to a table. Writes will now succeed even if there are concurrent Auto Compaction transactions. Low Shuffle Merge is now enabled by default The MERGE INTO command now always uses the new low-shuffle implementation. This behavior improves the performance of the MERGE INTO command significantly for most workloads. The configuration setting that was previously used to enable this feature has been removed. See Low Shuffle Merge. Insertion order tags are now preserved for UPDATEs and DELETEs The UPDATE and DELETE commands now preserve existing clustering information (including Z-ordering) for files that are updated or deleted. This behavior is a best-effort approach, and this approach does not apply to cases when files are so small that these files are combined during the update or delete. HikariCP is now the default Hive metastore connection pool HikariCP brings many stability improvements for Hive metastore access while maintaining fewer connections compared to the previous BoneCP connection pool implementation. HikariCP is enabled by default on any Databricks Runtime cluster that uses the Databricks Hive metastore (for example, when spark.sql.hive.metastore.jars is not set). You can also explicitly switch to other connection pool implementations, for example BoneCP, by setting spark.databricks.hive.metastore.client.pool.type. Azure Synapse connector now enables the maximum number of allowed reject rows to be set The Azure Synapse connector now supports a maxErrors DataFrame option. This update enables you to configure the maximum number of rejected rows that are allowed during reads and writes before the load operation is cancelled. All rejected rows are ignored. For example, if two out of ten records have errors, only eight records are processed. This option maps directly to the REJECT_VALUE option for the CREATE EXTERNAL TABLE statement in PolyBase and to the MAXERRORS option for the Azure Synapse connector’s COPY command. By default, maxErrors value is set to 0: all records are expected to be valid. Asynchronous state checkpointing is now generally available You can enable asynchronous state checkpointing in stateful streaming queries with large state updates. This can reduce the end-to-end micro-batch latency. This feature is now generally available. See Enable asynchronous state checkpointing for Structured Streaming. Parameter defaults can now be specified for SQL user-defined functions When you create a SQL user-defined function (SQL UDF), you can now specify default expressions for the SQL UDF’s parameters. You can then call the SQL UDF without providing arguments for those parameters, and Databricks will fill in the default values for those parameters. See CREATE FUNCTION (SQL). New Spark SQL functions The following Spark SQL functions are now available with this release: try_multiply: Returns multiplier multiplied by multiplicand, or NULL on overflow. try_subtract: Returns the subtraction of expr2 from expr1, or NULL on overflow. New working directory for High Concurrency clusters On High Concurrency clusters with either table access control or credential passthrough enabled, the current working directory of notebooks is now the user’s home directory. Previously, the working directory was /databricks/driver. Identity columns support in Delta tables is now generally available Delta Lake now supports identity columns. When you write to a Delta table that defines an identity column, and you do not provide values for that column, Delta now automatically assigns a unique and statistically increasing or decreasing value. See CREATE TABLE [USING]. Library upgrades Upgraded Python libraries: filelock from 3.4.2 to 3.6.0 Upgraded R libraries: brew from 1.0-6 to 1.0-7 broom from 0.7.11 to 0.7.12 cli from 3.1.0 to 3.2.0 clipr from 0.7.1 to 0.8.0 colorspace from 2.0-2 to 2.0-3 crayon from 1.4.2 to 1.5.0 dplyr from 1.0.7 to 1.0.8 dtplyr from 1.2.0 to 1.2.1 evaluate from 0.14 to 0.15 foreach from 1.5.1 to 1.5.2 future from 1.23.0 to 1.24.0 generics from 0.1.1 to 0.1.2 glue from 1.6.0 to 1.6.1 gower from 0.2.2 to 1.0.0 iterators from 1.0.13 to 1.0.14 jsonlite from 1.7.3 to 1.8.0 magrittr from 2.0.1 to 2.0.2 mgcv from 1.8-38 to 1.8-39 pillar from 1.6.4 to 1.7.0 randomForest from 4.6-14 to 4.7-1 readr from 2.1.1 to 2.1.2 recipes from 0.1.17 to 0.2.0 rlang from 0.4.12 to 1.0.1 rpart from 4.1-15 to 4.1.16 RSQLite from 2.2.9 to 2.2.10 sparklyr from 1.7.4 to 1.7.5 testthat from 3.1.1 to 3.1.2 tidyr from 1.1.4 to 1.2.0 tidyselect from 1.1.1 to 1.1.2 tinytex from 0.36 to 0.37 yaml from 2.2.1 to 2.3.5 Upgraded Java libraries: io.delta.delta-sharing-spark_2.12 from 0.3.0 to 0.4.0 Apache Spark Databricks Runtime 10.4 includes Apache Spark 3.2.1. This release includes all Spark fixes and improvements included in Databricks Runtime 10.3, as well as the following additional bug fixes and improvements made to Spark: [SPARK-38322] [SQL] Support query stage show runtime statistics in formatted explain mode [SPARK-38162] [SQL] Optimize one row plan in normal and AQE Optimizer [SPARK-38229] [SQL] Should’t check temp/external/ifNotExists with visitReplaceTable when parser [SPARK-34183] [SS] DataSource V2: Required distribution and ordering in micro-batch execution [SPARK-37932] [SQL]Wait to resolve missing attributes before applying DeduplicateRelations [SPARK-37904] [SQL] Improve RebalancePartitions in rules of Optimizer [SPARK-38236] [SQL][3.2][3.1] Check if table location is absolute by “new Path(locationUri).isAbsolute” in create/alter table [SPARK-38035] [SQL] Add docker tests for build-in JDBC dialect [SPARK-38042] [SQL] Ensure that ScalaReflection.dataTypeFor works on aliased array types [SPARK-38273] [SQL] decodeUnsafeRows’s iterators should close underlying input streams [SPARK-38311] [SQL] Fix DynamicPartitionPruning/BucketedReadSuite/ExpressionInfoSuite under ANSI mode [SPARK-38305] [CORE] Explicitly check if source exists in unpack() before calling FileUtil methods [SPARK-38275] [SS] Include the writeBatch’s memory usage as the total memory usage of RocksDB state store [SPARK-38132] [SQL] Remove NotPropagation rule [SPARK-38286] [SQL] Union’s maxRows and maxRowsPerPartition may overflow [SPARK-38306] [SQL] Fix ExplainSuite,StatisticsCollectionSuite and StringFunctionsSuite under ANSI mode [SPARK-38281] [SQL][Tests] Fix AnalysisSuite under ANSI mode [SPARK-38307] [SQL][Tests] Fix ExpressionTypeCheckingSuite and CollectionExpressionsSuite under ANSI mode [SPARK-38300] [SQL] Use ByteStreams.toByteArray to simplify fileToString and resourceToBytes in catalyst.util [SPARK-38304] [SQL] Elt() should return null if index is null under ANSI mode [SPARK-38271] PoissonSampler may output more rows than MaxRows [SPARK-38297] [PYTHON] Explicitly cast the return value at DataFrame.to_numpy in POS [SPARK-38295] [SQL][Tests] Fix ArithmeticExpressionSuite under ANSI mode [SPARK-38290] [SQL] Fix JsonSuite and ParquetIOSuite under ANSI mode [SPARK-38299] [SQL] Clean up deprecated usage of StringBuilder.newBuilder [SPARK-38060] [SQL] Respect allowNonNumericNumbers when parsing quoted NaN and Infinity values in JSON reader [SPARK-38276] [SQL] Add approved TPCDS plans under ANSI mode [SPARK-38206] [SS] Ignore nullability on comparing the data type of join keys on stream-stream join [SPARK-37290] [SQL] - Exponential planning time in case of non-deterministic function [SPARK-38232] [SQL] Explain formatted does not collect subqueries under query stage in AQE [SPARK-38283] [SQL] Test invalid datetime parsing under ANSI mode [SPARK-38140] [SQL] Desc column stats (min, max) for timestamp type is not consistent with the values due to time zone difference [SPARK-38227] [SQL][SS] Apply strict nullability of nested column in time window / session window [SPARK-38221] [SQL] Eagerly iterate over groupingExpressions when moving complex grouping expressions out of an Aggregate node [SPARK-38216] [SQL] Fail early if all the columns are partitioned columns when creating a Hive table [SPARK-38214] [SS]No need to filter windows when windowDuration is multiple of slideDuration [SPARK-38182] [SQL] Fix NoSuchElementException if pushed filter does not contain any references [SPARK-38159] [SQL] Add a new FileSourceMetadataAttribute for the Hidden File Metadata [SPARK-38123] [SQL] Unified use DataType as targetType of QueryExecutionErrors#castingCauseOverflowError [SPARK-38118] [SQL] Func(wrong data type) in HAVING clause should throw data mismatch error [SPARK-35173] [SQL][PYTHON] Add multiple columns adding support [SPARK-38177] [SQL] Fix wrong transformExpressions in Optimizer [SPARK-38228] [SQL] Legacy store assignment should not fail on error under ANSI mode [SPARK-38173] [SQL] Quoted column cannot be recognized correctly when quotedRegexColumnNa… [SPARK-38130] [SQL] Remove array_sort orderable entries check [SPARK-38199] [SQL] Delete the unused dataType specified in the definition of IntervalColumnAccessor [SPARK-38203] [SQL] Fix SQLInsertTestSuite and SchemaPruningSuite under ANSI mode [SPARK-38163] [SQL] Preserve the error class of SparkThrowable while constructing of function builder [SPARK-38157] [SQL] Explicitly set ANSI to false in test timestampNTZ/timestamp.sql and SQLQueryTestSuite to match the expected golden results [SPARK-38069] [SQL][SS] Improve the calculation of time window [SPARK-38164] [SQL] New SQL functions: try_subtract and try_multiply [SPARK-38176] [SQL] ANSI mode: allow implicitly casting String to other simple types [SPARK-37498] [PYTHON] Add eventually for test_reuse_worker_of_parallelize_range [SPARK-38198] [SQL][3.2] Fix QueryExecution.debug#toFile use the passed in maxFields when explainMode is CodegenMode [SPARK-38131] [SQL] Use error classes in user-facing exceptions only [SPARK-37652] [SQL] Add test for optimize skewed join through union [SPARK-37585] [SQL] Update InputMetric in DataSourceRDD with TaskCompletionListener [SPARK-38113] [SQL] Use error classes in the execution errors of pivoting [SPARK-38178] [SS] Correct the logic to measure the memory usage of RocksDB [SPARK-37969] [SQL] HiveFileFormat should check field name [SPARK-37652] Revert “[SQL]Add test for optimize skewed join through union” [SPARK-38124] [SQL][SS] Introduce StatefulOpClusteredDistribution and apply to stream-stream join [SPARK-38030] [SQL] Canonicalization should not remove nullability of AttributeReference dataType [SPARK-37907] [SQL] InvokeLike support ConstantFolding [SPARK-37891] [CORE] Add scalastyle check to disable scala.concurrent.ExecutionContext.Implicits.global [SPARK-38150] [SQL] Update comment of RelationConversions [SPARK-37943] [SQL] Use error classes in the compilation errors of grouping [SPARK-37652] [SQL]Add test for optimize skewed join through union [SPARK-38056] [Web UI][3.2] Fix issue of Structured streaming not working in history server when using LevelDB [SPARK-38144] [CORE] Remove unused spark.storage.safetyFraction config [SPARK-38120] [SQL] Fix HiveExternalCatalog.listPartitions when partition column name is upper case and dot in partition value [SPARK-38122] [Docs] Update the App Key of DocSearch [SPARK-37479] [SQL] Migrate DROP NAMESPACE to use V2 command by default [SPARK-35703] [SQL] Relax constraint for bucket join and remove HashClusteredDistribution [SPARK-37983] [SQL] Back out agg build time metrics from sort aggregate [SPARK-37915] [SQL] Combine unions if there is a project between them [SPARK-38105] [SQL] Use error classes in the parsing errors of joins [SPARK-38073] [PYTHON] Update atexit function to avoid issues with late binding [SPARK-37941] [SQL] Use error classes in the compilation errors of casting [SPARK-37937] [SQL] Use error classes in the parsing errors of lateral join [SPARK-38100] [SQL] Remove unused private method in Decimal [SPARK-37987] [SS] Fix flaky test StreamingAggregationSuite.changing schema of state when restarting query [SPARK-38003] [SQL] LookupFunctions rule should only look up functions from the scalar function registry [SPARK-38075] [SQL] Fix hasNext in HiveScriptTransformationExec’s process output iterator [SPARK-37965] [SQL] Remove check field name when reading/writing existing data in Orc [SPARK-37922] [SQL] Combine to one cast if we can safely up-cast two casts (for dbr-branch-10.x) [SPARK-37675] [SPARK-37793] Prevent overwriting of push shuffle merged files once the shuffle is finalized [SPARK-38011] [SQL] Remove duplicated and useless configuration in ParquetFileFormat [SPARK-37929] [SQL] Support cascade mode for dropNamespace API [SPARK-37931] [SQL] Quote the column name if needed [SPARK-37990] [SQL] Support TimestampNTZ in RowToColumnConverter [SPARK-38001] [SQL] Replace the error classes related to unsupported features by UNSUPPORTED_FEATURE [SPARK-37839] [SQL] DS V2 supports partial aggregate push-down AVG [SPARK-37878] [SQL] Migrate SHOW CREATE TABLE to use v2 command by default [SPARK-37731] [SQL] Refactor and cleanup function lookup in Analyzer [SPARK-37979] [SQL] Switch to more generic error classes in AES functions [SPARK-37867] [SQL] Compile aggregate functions of build-in JDBC dialect [SPARK-38028] [SQL] Expose Arrow Vector from ArrowColumnVector [SPARK-30062] [SQL] Add the IMMEDIATE statement to the DB2 dialect truncate implementation [SPARK-36649] [SQL] Support Trigger.AvailableNow on Kafka data source [SPARK-38018] [SQL] Fix ColumnVectorUtils.populate to handle CalendarIntervalType correctly [SPARK-38023] [CORE] ExecutorMonitor.onExecutorRemoved should handle ExecutorDecommission as finished [SPARK-38019] [CORE] Make ExecutorMonitor.timedOutExecutors deterministic [SPARK-37957] [SQL] Correctly pass deterministic flag for V2 scalar functions [SPARK-37985] [SQL] Fix flaky test for SPARK-37578 [SPARK-37986] [SQL] Support TimestampNTZ in radix sort [SPARK-37967] [SQL] Literal.create support ObjectType [SPARK-37827] [SQL] Put the some built-in table properties into V1Table.propertie to adapt to V2 command [SPARK-37963] [SQL] Need to update Partition URI after renaming table in InMemoryCatalog [SPARK-35442] [SQL] Support propagate empty relation through aggregate/union [SPARK-37933] [SQL] Change the traversal method of V2ScanRelationPushDown push down rules [SPARK-37917] [SQL] Push down limit 1 for right side of left semi/anti join if join condition is empty [SPARK-37959] [ML] Fix the UT of checking norm in KMeans & BiKMeans [SPARK-37906] [SQL] spark-sql should not pass last comment to backend [SPARK-37627] [SQL] Add sorted column in BucketTransform Maintenance updates See Databricks Runtime 10.4 maintenance updates. System environment Operating System: Ubuntu 20.04.4 LTS Java: Zulu 8.56.0.21-CA-linux64 Scala: 2.12.14 Python: 3.8.10 R: 4.1.2 Delta Lake: 1.1.0 Installed Python libraries Library Version Library Version Library Version Antergos Linux 2015.10 (ISO-Rolling) appdirs 1.4.4 argon2-cffi 20.1.0 async-generator 1.10 attrs 20.3.0 backcall 0.2.0 bidict 0.21.4 bleach 3.3.0 boto3 1.16.7 botocore 1.19.7 certifi 2020.12.5 cffi 1.14.5 chardet 4.0.0 cycler 0.10.0 Cython 0.29.23 dbus-python 1.2.16 decorator 5.0.6 defusedxml 0.7.1 distlib 0.3.4 distro-info 0.23ubuntu1 entrypoints 0.3 facets-overview 1.0.0 filelock 3.6.0 idna 2.10 ipykernel 5.3.4 ipython 7.22.0 ipython-genutils 0.2.0 ipywidgets 7.6.3 jedi 0.17.2 Jinja2 2.11.3 jmespath 0.10.0 joblib 1.0.1 jsonschema 3.2.0 jupyter-client 6.1.12 jupyter-core 4.7.1 jupyterlab-pygments 0.1.2 jupyterlab-widgets 1.0.0 kiwisolver 1.3.1 koalas 1.8.2 MarkupSafe 2.0.1 matplotlib 3.4.2 mistune 0.8.4 nbclient 0.5.3 nbconvert 6.0.7 nbformat 5.1.3 nest-asyncio 1.5.1 notebook 6.3.0 numpy 1.20.1 packaging 20.9 pandas 1.2.4 pandocfilters 1.4.3 parso 0.7.0 patsy 0.5.1 pexpect 4.8.0 pickleshare 0.7.5 Pillow 8.2.0 pip 21.0.1 plotly 5.5.0 prometheus-client 0.10.1 prompt-toolkit 3.0.17 protobuf 3.17.2 psycopg2 2.8.5 ptyprocess 0.7.0 pyarrow 4.0.0 pycparser 2.20 Pygments 2.8.1 PyGObject 3.36.0 pyparsing 2.4.7 pyrsistent 0.17.3 python-apt 2.0.0+ubuntu0.20.4.7 python-dateutil 2.8.1 python-engineio 4.3.0 python-socketio 5.4.1 pytz 2020.5 pyzmq 20.0.0 requests 2.25.1 requests-unixsocket 0.2.0 s3transfer 0.3.7 scikit-learn 0.24.1 scipy 1.6.2 seaborn 0.11.1 Send2Trash 1.5.0 setuptools 52.0.0 six 1.15.0 ssh-import-id 5.10 statsmodels 0.12.2 tenacity 8.0.1 terminado 0.9.4 testpath 0.4.4 threadpoolctl 2.1.0 tornado 6.1 traitlets 5.0.5 unattended-upgrades 0.1 urllib3 1.25.11 virtualenv 20.4.1 wcwidth 0.2.5 webencodings 0.5.1 wheel 0.36.2 widgetsnbextension 3.5.1 Installed R libraries R libraries are installed from the Microsoft CRAN snapshot on 2022-02-24. Library Version Library Version Library Version askpass 1.1 assertthat 0.2.1 backports 1.4.1 base 4.1.2 base64enc 0.1-3 bit 4.0.4 bit64 4.0.5 blob 1.2.2 boot 1.3-28 brew 1.0-7 brio 1.1.3 broom 0.7.12 bslib 0.3.1 cachem 1.0.6 callr 3.7.0 caret 6.0-90 cellranger 1.1.0 chron 2.3-56 class 7.3-20 cli 3.2.0 clipr 0.8.0 cluster 2.1.2 codetools 0.2-18 colorspace 2.0-3 commonmark 1.7 compiler 4.1.2 config 0.3.1 cpp11 0.4.2 crayon 1.5.0 credentials 1.3.2 curl 4.3.2 data.table 1.14.2 datasets 4.1.2 DBI 1.1.2 dbplyr 2.1.1 desc 1.4.0 devtools 2.4.3 diffobj 0.3.5 digest 0.6.29 dplyr 1.0.8 dtplyr 1.2.1 e1071 1.7-9 ellipsis 0.3.2 evaluate 0.15 fansi 1.0.2 farver 2.1.0 fastmap 1.1.0 fontawesome 0.2.2 forcats 0.5.1 foreach 1.5.2 foreign 0.8-82 forge 0.2.0 fs 1.5.2 future 1.24.0 future.apply 1.8.1 gargle 1.2.0 generics 0.1.2 gert 1.5.0 ggplot2 3.3.5 gh 1.3.0 gitcreds 0.1.1 glmnet 4.1-3 globals 0.14.0 glue 1.6.1 googledrive 2.0.0 googlesheets4 1.0.0 gower 1.0.0 graphics 4.1.2 grDevices 4.1.2 grid 4.1.2 gridExtra 2.3 gsubfn 0.7 gtable 0.3.0 hardhat 0.2.0 haven 2.4.3 highr 0.9 hms 1.1.1 htmltools 0.5.2 htmlwidgets 1.5.4 httpuv 1.6.5 httr 1.4.2 hwriter 1.3.2 hwriterPlus 1.0-3 ids 1.0.1 ini 0.3.1 ipred 0.9-12 isoband 0.2.5 iterators 1.0.14 jquerylib 0.1.4 jsonlite 1.8.0 KernSmooth 2.23-20 knitr 1.37 labeling 0.4.2 later 1.3.0 lattice 0.20-45 lava 1.6.10 lifecycle 1.0.1 listenv 0.8.0 lubridate 1.8.0 magrittr 2.0.2 markdown 1.1 MASS 7.3-55 Matrix 1.4-0 memoise 2.0.1 methods 4.1.2 mgcv 1.8-39 mime 0.12 ModelMetrics 1.2.2.2 modelr 0.1.8 munsell 0.5.0 nlme 3.1-155 nnet 7.3-17 numDeriv 2016.8-1.1 openssl 1.4.6 parallel 4.1.2 parallelly 1.30.0 pillar 1.7.0 pkgbuild 1.3.1 pkgconfig 2.0.3 pkgload 1.2.4 plogr 0.2.0 plyr 1.8.6 praise 1.0.0 prettyunits 1.1.1 pROC 1.18.0 processx 3.5.2 prodlim 2019.11.13 progress 1.2.2 progressr 0.10.0 promises 1.2.0.1 proto 1.0.0 proxy 0.4-26 ps 1.6.0 purrr 0.3.4 r2d3 0.2.5 R6 2.5.1 randomForest 4.7-1 rappdirs 0.3.3 rcmdcheck 1.4.0 RColorBrewer 1.1-2 Rcpp 1.0.8 RcppEigen 0.3.3.9.1 readr 2.1.2 readxl 1.3.1 recipes 0.2.0 rematch 1.0.1 rematch2 2.1.2 remotes 2.4.2 reprex 2.0.1 reshape2 1.4.4 rlang 1.0.1 rmarkdown 2.11 RODBC 1.3-19 roxygen2 7.1.2 rpart 4.1.16 rprojroot 2.0.2 Rserve 1.8-10 RSQLite 2.2.10 rstudioapi 0.13 rversions 2.1.1 rvest 1.0.2 sass 0.4.0 scales 1.1.1 selectr 0.4-2 sessioninfo 1.2.2 shape 1.4.6 shiny 1.7.1 sourcetools 0.1.7 sparklyr 1.7.5 SparkR 3.2.0 spatial 7.3-11 splines 4.1.2 sqldf 0.4-11 SQUAREM 2021.1 stats 4.1.2 stats4 4.1.2 stringi 1.7.6 stringr 1.4.0 survival 3.2-13 sys 3.4 tcltk 4.1.2 TeachingDemos 2.10 testthat 3.1.2 tibble 3.1.6 tidyr 1.2.0 tidyselect 1.1.2 tidyverse 1.3.1 timeDate 3043.102 tinytex 0.37 tools 4.1.2 tzdb 0.2.0 usethis 2.1.5 utf8 1.2.2 utils 4.1.2 uuid 1.0-3 vctrs 0.3.8 viridisLite 0.4.0 vroom 1.5.7 waldo 0.3.1 whisker 0.4 withr 2.4.3 xfun 0.29 xml2 1.3.3 xopen 1.0.0 xtable 1.8-4 yaml 2.3.5 zip 2.2.0 Installed Java and Scala libraries (Scala 2.12 cluster version) Group ID Artifact ID Version antlr antlr 2.7.7 com.amazonaws amazon-kinesis-client 1.12.0 com.amazonaws aws-java-sdk-autoscaling 1.11.655 com.amazonaws aws-java-sdk-cloudformation 1.11.655 com.amazonaws aws-java-sdk-cloudfront 1.11.655 com.amazonaws aws-java-sdk-cloudhsm 1.11.655 com.amazonaws aws-java-sdk-cloudsearch 1.11.655 com.amazonaws aws-java-sdk-cloudtrail 1.11.655 com.amazonaws aws-java-sdk-cloudwatch 1.11.655 com.amazonaws aws-java-sdk-cloudwatchmetrics 1.11.655 com.amazonaws aws-java-sdk-codedeploy 1.11.655 com.amazonaws aws-java-sdk-cognitoidentity 1.11.655 com.amazonaws aws-java-sdk-cognitosync 1.11.655 com.amazonaws aws-java-sdk-config 1.11.655 com.amazonaws aws-java-sdk-core 1.11.655 com.amazonaws aws-java-sdk-datapipeline 1.11.655 com.amazonaws aws-java-sdk-directconnect 1.11.655 com.amazonaws aws-java-sdk-directory 1.11.655 com.amazonaws aws-java-sdk-dynamodb 1.11.655 com.amazonaws aws-java-sdk-ec2 1.11.655 com.amazonaws aws-java-sdk-ecs 1.11.655 com.amazonaws aws-java-sdk-efs 1.11.655 com.amazonaws aws-java-sdk-elasticache 1.11.655 com.amazonaws aws-java-sdk-elasticbeanstalk 1.11.655 com.amazonaws aws-java-sdk-elasticloadbalancing 1.11.655 com.amazonaws aws-java-sdk-elastictranscoder 1.11.655 com.amazonaws aws-java-sdk-emr 1.11.655 com.amazonaws aws-java-sdk-glacier 1.11.655 com.amazonaws aws-java-sdk-glue 1.11.655 com.amazonaws aws-java-sdk-iam 1.11.655 com.amazonaws aws-java-sdk-importexport 1.11.655 com.amazonaws aws-java-sdk-kinesis 1.11.655 com.amazonaws aws-java-sdk-kms 1.11.655 com.amazonaws aws-java-sdk-lambda 1.11.655 com.amazonaws aws-java-sdk-logs 1.11.655 com.amazonaws aws-java-sdk-machinelearning 1.11.655 com.amazonaws aws-java-sdk-opsworks 1.11.655 com.amazonaws aws-java-sdk-rds 1.11.655 com.amazonaws aws-java-sdk-redshift 1.11.655 com.amazonaws aws-java-sdk-route53 1.11.655 com.amazonaws aws-java-sdk-s3 1.11.655 com.amazonaws aws-java-sdk-ses 1.11.655 com.amazonaws aws-java-sdk-simpledb 1.11.655 com.amazonaws aws-java-sdk-simpleworkflow 1.11.655 com.amazonaws aws-java-sdk-sns 1.11.655 com.amazonaws aws-java-sdk-sqs 1.11.655 com.amazonaws aws-java-sdk-ssm 1.11.655 com.amazonaws aws-java-sdk-storagegateway 1.11.655 com.amazonaws aws-java-sdk-sts 1.11.655 com.amazonaws aws-java-sdk-support 1.11.655 com.amazonaws aws-java-sdk-swf-libraries 1.11.22 com.amazonaws aws-java-sdk-workspaces 1.11.655 com.amazonaws jmespath-java 1.11.655 com.chuusai shapeless_2.12 2.3.3 com.clearspring.analytics stream 2.9.6 com.databricks Rserve 1.8-3 com.databricks jets3t 0.7.1-0 com.databricks.scalapb compilerplugin_2.12 0.4.15-10 com.databricks.scalapb scalapb-runtime_2.12 0.4.15-10 com.esotericsoftware kryo-shaded 4.0.2 com.esotericsoftware minlog 1.3.0 com.fasterxml classmate 1.3.4 com.fasterxml.jackson.core jackson-annotations 2.12.3 com.fasterxml.jackson.core jackson-core 2.12.3 com.fasterxml.jackson.core jackson-databind 2.12.3 com.fasterxml.jackson.dataformat jackson-dataformat-cbor 2.12.3 com.fasterxml.jackson.datatype jackson-datatype-joda 2.12.3 com.fasterxml.jackson.module jackson-module-paranamer 2.12.3 com.fasterxml.jackson.module jackson-module-scala_2.12 2.12.3 com.github.ben-manes.caffeine caffeine 2.3.4 com.github.fommil jniloader 1.1 com.github.fommil.netlib core 1.1.2 com.github.fommil.netlib native_ref-java 1.1 com.github.fommil.netlib native_ref-java-natives 1.1 com.github.fommil.netlib native_system-java 1.1 com.github.fommil.netlib native_system-java-natives 1.1 com.github.fommil.netlib netlib-native_ref-linux-x86_64-natives 1.1 com.github.fommil.netlib netlib-native_system-linux-x86_64-natives 1.1 com.github.luben zstd-jni 1.5.0-4 com.github.wendykierp JTransforms 3.1 com.google.code.findbugs jsr305 3.0.0 com.google.code.gson gson 2.8.6 com.google.crypto.tink tink 1.6.0 com.google.flatbuffers flatbuffers-java 1.9.0 com.google.guava guava 15.0 com.google.protobuf protobuf-java 2.6.1 com.h2database h2 1.4.195 com.helger profiler 1.1.1 com.jcraft jsch 0.1.50 com.jolbox bonecp 0.8.0.RELEASE com.lihaoyi sourcecode_2.12 0.1.9 com.microsoft.azure azure-data-lake-store-sdk 2.3.9 com.ning compress-lzf 1.0.3 com.sun.istack istack-commons-runtime 3.0.8 com.sun.mail javax.mail 1.5.2 com.tdunning json 1.8 com.thoughtworks.paranamer paranamer 2.8 com.trueaccord.lenses lenses_2.12 0.4.12 com.twitter chill-java 0.10.0 com.twitter chill_2.12 0.10.0 com.twitter util-app_2.12 7.1.0 com.twitter util-core_2.12 7.1.0 com.twitter util-function_2.12 7.1.0 com.twitter util-jvm_2.12 7.1.0 com.twitter util-lint_2.12 7.1.0 com.twitter util-registry_2.12 7.1.0 com.twitter util-stats_2.12 7.1.0 com.typesafe config 1.2.1 com.typesafe.scala-logging scala-logging_2.12 3.7.2 com.univocity univocity-parsers 2.9.1 com.zaxxer HikariCP 4.0.3 commons-cli commons-cli 1.2 commons-codec commons-codec 1.15 commons-collections commons-collections 3.2.2 commons-dbcp commons-dbcp 1.4 commons-fileupload commons-fileupload 1.3.3 commons-httpclient commons-httpclient 3.1 commons-io commons-io 2.8.0 commons-lang commons-lang 2.6 commons-logging commons-logging 1.1.3 commons-net commons-net 3.1 commons-pool commons-pool 1.5.4 dev.ludovic.netlib arpack 2.2.1 dev.ludovic.netlib blas 2.2.1 dev.ludovic.netlib lapack 2.2.1 hive-2.3__hadoop-3.2 jets3t-0.7 liball_deps_2.12 info.ganglia.gmetric4j gmetric4j 1.0.10 io.airlift aircompressor 0.21 io.delta delta-sharing-spark_2.12 0.4.0 io.dropwizard.metrics metrics-core 4.1.1 io.dropwizard.metrics metrics-graphite 4.1.1 io.dropwizard.metrics metrics-healthchecks 4.1.1 io.dropwizard.metrics metrics-jetty9 4.1.1 io.dropwizard.metrics metrics-jmx 4.1.1 io.dropwizard.metrics metrics-json 4.1.1 io.dropwizard.metrics metrics-jvm 4.1.1 io.dropwizard.metrics metrics-servlets 4.1.1 io.netty netty-all 4.1.68.Final io.prometheus simpleclient 0.7.0 io.prometheus simpleclient_common 0.7.0 io.prometheus simpleclient_dropwizard 0.7.0 io.prometheus simpleclient_pushgateway 0.7.0 io.prometheus simpleclient_servlet 0.7.0 io.prometheus.jmx collector 0.12.0 jakarta.annotation jakarta.annotation-api 1.3.5 jakarta.servlet jakarta.servlet-api 4.0.3 jakarta.validation jakarta.validation-api 2.0.2 jakarta.ws.rs jakarta.ws.rs-api 2.1.6 javax.activation activation 1.1.1 javax.annotation javax.annotation-api 1.3.2 javax.el javax.el-api 2.2.4 javax.jdo jdo-api 3.0.1 javax.transaction jta 1.1 javax.transaction transaction-api 1.1 javax.xml.bind jaxb-api 2.2.2 javax.xml.stream stax-api 1.0-2 javolution javolution 5.5.1 jline jline 2.14.6 joda-time joda-time 2.10.10 log4j apache-log4j-extras 1.2.17 log4j log4j 1.2.17 maven-trees hive-2.3__hadoop-3.2 liball_deps_2.12 net.java.dev.jna jna 5.8.0 net.razorvine pyrolite 4.30 net.sf.jpam jpam 1.1 net.sf.opencsv opencsv 2.3 net.sf.supercsv super-csv 2.2.0 net.snowflake snowflake-ingest-sdk 0.9.6 net.snowflake snowflake-jdbc 3.13.3 net.snowflake spark-snowflake_2.12 2.9.0-spark_3.1 net.sourceforge.f2j arpack_combined_all 0.1 org.acplt.remotetea remotetea-oncrpc 1.1.2 org.antlr ST4 4.0.4 org.antlr antlr-runtime 3.5.2 org.antlr antlr4-runtime 4.8 org.antlr stringtemplate 3.2.1 org.apache.ant ant 1.9.2 org.apache.ant ant-jsch 1.9.2 org.apache.ant ant-launcher 1.9.2 org.apache.arrow arrow-format 2.0.0 org.apache.arrow arrow-memory-core 2.0.0 org.apache.arrow arrow-memory-netty 2.0.0 org.apache.arrow arrow-vector 2.0.0 org.apache.avro avro 1.10.2 org.apache.avro avro-ipc 1.10.2 org.apache.avro avro-mapred 1.10.2 org.apache.commons commons-compress 1.21 org.apache.commons commons-crypto 1.1.0 org.apache.commons commons-lang3 3.12.0 org.apache.commons commons-math3 3.4.1 org.apache.commons commons-text 1.6 org.apache.curator curator-client 2.13.0 org.apache.curator curator-framework 2.13.0 org.apache.curator curator-recipes 2.13.0 org.apache.derby derby 10.14.2.0 org.apache.hadoop hadoop-client-api 3.3.1-databricks org.apache.hadoop hadoop-client-runtime 3.3.1 org.apache.hive hive-beeline 2.3.9 org.apache.hive hive-cli 2.3.9 org.apache.hive hive-jdbc 2.3.9 org.apache.hive hive-llap-client 2.3.9 org.apache.hive hive-llap-common 2.3.9 org.apache.hive hive-serde 2.3.9 org.apache.hive hive-shims 2.3.9 org.apache.hive hive-storage-api 2.7.2 org.apache.hive.shims hive-shims-0.23 2.3.9 org.apache.hive.shims hive-shims-common 2.3.9 org.apache.hive.shims hive-shims-scheduler 2.3.9 org.apache.htrace htrace-core4 4.1.0-incubating org.apache.httpcomponents httpclient 4.5.13 org.apache.httpcomponents httpcore 4.4.12 org.apache.ivy ivy 2.5.0 org.apache.mesos mesos-shaded-protobuf 1.4.0 org.apache.orc orc-core 1.6.12 org.apache.orc orc-mapreduce 1.6.12 org.apache.orc orc-shims 1.6.12 org.apache.parquet parquet-column 1.12.0-databricks-0003 org.apache.parquet parquet-common 1.12.0-databricks-0003 org.apache.parquet parquet-encoding 1.12.0-databricks-0003 org.apache.parquet parquet-format-structures 1.12.0-databricks-0003 org.apache.parquet parquet-hadoop 1.12.0-databricks-0003 org.apache.parquet parquet-jackson 1.12.0-databricks-0003 org.apache.thrift libfb303 0.9.3 org.apache.thrift libthrift 0.12.0 org.apache.xbean xbean-asm9-shaded 4.20 org.apache.yetus audience-annotations 0.5.0 org.apache.zookeeper zookeeper 3.6.2 org.apache.zookeeper zookeeper-jute 3.6.2 org.checkerframework checker-qual 3.5.0 org.codehaus.jackson jackson-core-asl 1.9.13 org.codehaus.jackson jackson-mapper-asl 1.9.13 org.codehaus.janino commons-compiler 3.0.16 org.codehaus.janino janino 3.0.16 org.datanucleus datanucleus-api-jdo 4.2.4 org.datanucleus datanucleus-core 4.1.17 org.datanucleus datanucleus-rdbms 4.1.19 org.datanucleus javax.jdo 3.2.0-m3 org.eclipse.jetty jetty-client 9.4.43.v20210629 org.eclipse.jetty jetty-continuation 9.4.43.v20210629 org.eclipse.jetty jetty-http 9.4.43.v20210629 org.eclipse.jetty jetty-io 9.4.43.v20210629 org.eclipse.jetty jetty-jndi 9.4.43.v20210629 org.eclipse.jetty jetty-plus 9.4.43.v20210629 org.eclipse.jetty jetty-proxy 9.4.43.v20210629 org.eclipse.jetty jetty-security 9.4.43.v20210629 org.eclipse.jetty jetty-server 9.4.43.v20210629 org.eclipse.jetty jetty-servlet 9.4.43.v20210629 org.eclipse.jetty jetty-servlets 9.4.43.v20210629 org.eclipse.jetty jetty-util 9.4.43.v20210629 org.eclipse.jetty jetty-util-ajax 9.4.43.v20210629 org.eclipse.jetty jetty-webapp 9.4.43.v20210629 org.eclipse.jetty jetty-xml 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-api 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-client 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-common 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-server 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-servlet 9.4.43.v20210629 org.fusesource.leveldbjni leveldbjni-all 1.8 org.glassfish.hk2 hk2-api 2.6.1 org.glassfish.hk2 hk2-locator 2.6.1 org.glassfish.hk2 hk2-utils 2.6.1 org.glassfish.hk2 osgi-resource-locator 1.0.3 org.glassfish.hk2.external aopalliance-repackaged 2.6.1 org.glassfish.hk2.external jakarta.inject 2.6.1 org.glassfish.jaxb jaxb-runtime 2.3.2 org.glassfish.jersey.containers jersey-container-servlet 2.34 org.glassfish.jersey.containers jersey-container-servlet-core 2.34 org.glassfish.jersey.core jersey-client 2.34 org.glassfish.jersey.core jersey-common 2.34 org.glassfish.jersey.core jersey-server 2.34 org.glassfish.jersey.inject jersey-hk2 2.34 org.hibernate.validator hibernate-validator 6.1.0.Final org.javassist javassist 3.25.0-GA org.jboss.logging jboss-logging 3.3.2.Final org.jdbi jdbi 2.63.1 org.jetbrains annotations 17.0.0 org.joda joda-convert 1.7 org.jodd jodd-core 3.5.2 org.json4s json4s-ast_2.12 3.7.0-M11 org.json4s json4s-core_2.12 3.7.0-M11 org.json4s json4s-jackson_2.12 3.7.0-M11 org.json4s json4s-scalap_2.12 3.7.0-M11 org.lz4 lz4-java 1.7.1 org.mariadb.jdbc mariadb-java-client 2.2.5 org.objenesis objenesis 2.5.1 org.postgresql postgresql 42.2.19 org.roaringbitmap RoaringBitmap 0.9.14 org.roaringbitmap shims 0.9.14 org.rocksdb rocksdbjni 6.20.3 org.rosuda.REngine REngine 2.1.0 org.scala-lang scala-compiler_2.12 2.12.14 org.scala-lang scala-library_2.12 2.12.14 org.scala-lang scala-reflect_2.12 2.12.14 org.scala-lang.modules scala-collection-compat_2.12 2.4.3 org.scala-lang.modules scala-parser-combinators_2.12 1.1.2 org.scala-lang.modules scala-xml_2.12 1.2.0 org.scala-sbt test-interface 1.0 org.scalacheck scalacheck_2.12 1.14.2 org.scalactic scalactic_2.12 3.0.8 org.scalanlp breeze-macros_2.12 1.2 org.scalanlp breeze_2.12 1.2 org.scalatest scalatest_2.12 3.0.8 org.slf4j jcl-over-slf4j 1.7.30 org.slf4j jul-to-slf4j 1.7.30 org.slf4j slf4j-api 1.7.30 org.slf4j slf4j-log4j12 1.7.30 org.spark-project.spark unused 1.0.0 org.threeten threeten-extra 1.5.0 org.tukaani xz 1.8 org.typelevel algebra_2.12 2.0.1 org.typelevel cats-kernel_2.12 2.1.1 org.typelevel macro-compat_2.12 1.1.1 org.typelevel spire-macros_2.12 0.17.0 org.typelevel spire-platform_2.12 0.17.0 org.typelevel spire-util_2.12 0.17.0 org.typelevel spire_2.12 0.17.0 org.wildfly.openssl wildfly-openssl 1.0.7.Final org.xerial sqlite-jdbc 3.8.11.2 org.xerial.snappy snappy-java 1.1.8.4 org.yaml snakeyaml 1.24 oro oro 2.0.8 pl.edu.icm JLargeArrays 1.5 software.amazon.ion ion-java 1.0.2 stax stax-api 1.0.1",Databricks Runtime 10.4 LTS
3,Customer is unable to run a jar job,"Here is one of the latest runs with the issue: https://adb-192708428136412.12.azuredatabricks.net/?o=192708428136412#job/573399566174844/run/6259

Here is the customer's statement:

""data-vacuum-app is a Spark job that lists the containers within a predefined Azure storage account in a datalake.
Then it runs DeltaTable.vacuum() on the tables in the retrieved containers.
The job run successfully on my machine.
 
Trying to upload the Jar to Azure Databricks leads to the following error:
 
“IllegalStateException: A request was made to load the default HttpClient provider but one could not be found on the classpath. If you are using a dependency manager, consider including a dependency on azure-core-http-netty or azure-core-http-okhttp. Depending on your existing dependencies, you have the choice of Netty or OkHttp implementations. Additionally, refer to https://aka.ms/azsdk/java/docs/custom-httpclient to learn about writing your own implementation.”
 
A file with the error stack trace is attached.
The project's sbt + code files are attached.
 
Please note that the line causing the error in line 29 in the AzureHelper.scala file.""

Is there an incompatibility with the DBR runtime? Which workaround do you recommend?",https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/10.4,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Databricks Runtime 10.4 LTS Article 07/12/2022 19 minutes to read 5 contributors In this article New features and improvements Library upgrades Apache Spark Maintenance updates System environment The following release notes provide information about Databricks Runtime 10.4 and Databricks Runtime 10.4 Photon, powered by Apache Spark 3.2.1. Photon is in Public Preview. Databricks released these images in March 2022. New features and improvements Iceberg to Delta table converter (Public Preview) Auto Compaction rollbacks are now enabled by default Low Shuffle Merge is now enabled by default Insertion order tags are now preserved for UPDATEs and DELETEs HikariCP is now the default Hive metastore connection pool Azure Synapse connector now enables the maximum number of allowed reject rows to be set Asynchronous state checkpointing is now generally available Parameter defaults can now be specified for SQL user-defined functions New Spark SQL functions New working directory for High Concurrency clusters Identity columns support in Delta tables is now generally available Iceberg to Delta table converter (Public Preview) Convert to Delta now supports converting an Iceberg table to a Delta table in place. It does this by using Iceberg native metadata and file manifests. See Convert an Iceberg table to a Delta table. Auto Compaction rollbacks are now enabled by default This release improves the behavior for Delta Lake writes that commit when there are concurrent Auto Compaction transactions. Before this release, such writes would often quit, due to concurrent modifications to a table. Writes will now succeed even if there are concurrent Auto Compaction transactions. Low Shuffle Merge is now enabled by default The MERGE INTO command now always uses the new low-shuffle implementation. This behavior improves the performance of the MERGE INTO command significantly for most workloads. The configuration setting that was previously used to enable this feature has been removed. See Low Shuffle Merge. Insertion order tags are now preserved for UPDATEs and DELETEs The UPDATE and DELETE commands now preserve existing clustering information (including Z-ordering) for files that are updated or deleted. This behavior is a best-effort approach, and this approach does not apply to cases when files are so small that these files are combined during the update or delete. HikariCP is now the default Hive metastore connection pool HikariCP brings many stability improvements for Hive metastore access while maintaining fewer connections compared to the previous BoneCP connection pool implementation. HikariCP is enabled by default on any Databricks Runtime cluster that uses the Databricks Hive metastore (for example, when spark.sql.hive.metastore.jars is not set). You can also explicitly switch to other connection pool implementations, for example BoneCP, by setting spark.databricks.hive.metastore.client.pool.type. Azure Synapse connector now enables the maximum number of allowed reject rows to be set The Azure Synapse connector now supports a maxErrors DataFrame option. This update enables you to configure the maximum number of rejected rows that are allowed during reads and writes before the load operation is cancelled. All rejected rows are ignored. For example, if two out of ten records have errors, only eight records are processed. This option maps directly to the REJECT_VALUE option for the CREATE EXTERNAL TABLE statement in PolyBase and to the MAXERRORS option for the Azure Synapse connector’s COPY command. By default, maxErrors value is set to 0: all records are expected to be valid. Asynchronous state checkpointing is now generally available You can enable asynchronous state checkpointing in stateful streaming queries with large state updates. This can reduce the end-to-end micro-batch latency. This feature is now generally available. See Enable asynchronous state checkpointing for Structured Streaming. Parameter defaults can now be specified for SQL user-defined functions When you create a SQL user-defined function (SQL UDF), you can now specify default expressions for the SQL UDF’s parameters. You can then call the SQL UDF without providing arguments for those parameters, and Databricks will fill in the default values for those parameters. See CREATE FUNCTION (SQL). New Spark SQL functions The following Spark SQL functions are now available with this release: try_multiply: Returns multiplier multiplied by multiplicand, or NULL on overflow. try_subtract: Returns the subtraction of expr2 from expr1, or NULL on overflow. New working directory for High Concurrency clusters On High Concurrency clusters with either table access control or credential passthrough enabled, the current working directory of notebooks is now the user’s home directory. Previously, the working directory was /databricks/driver. Identity columns support in Delta tables is now generally available Delta Lake now supports identity columns. When you write to a Delta table that defines an identity column, and you do not provide values for that column, Delta now automatically assigns a unique and statistically increasing or decreasing value. See CREATE TABLE [USING]. Library upgrades Upgraded Python libraries: filelock from 3.4.2 to 3.6.0 Upgraded R libraries: brew from 1.0-6 to 1.0-7 broom from 0.7.11 to 0.7.12 cli from 3.1.0 to 3.2.0 clipr from 0.7.1 to 0.8.0 colorspace from 2.0-2 to 2.0-3 crayon from 1.4.2 to 1.5.0 dplyr from 1.0.7 to 1.0.8 dtplyr from 1.2.0 to 1.2.1 evaluate from 0.14 to 0.15 foreach from 1.5.1 to 1.5.2 future from 1.23.0 to 1.24.0 generics from 0.1.1 to 0.1.2 glue from 1.6.0 to 1.6.1 gower from 0.2.2 to 1.0.0 iterators from 1.0.13 to 1.0.14 jsonlite from 1.7.3 to 1.8.0 magrittr from 2.0.1 to 2.0.2 mgcv from 1.8-38 to 1.8-39 pillar from 1.6.4 to 1.7.0 randomForest from 4.6-14 to 4.7-1 readr from 2.1.1 to 2.1.2 recipes from 0.1.17 to 0.2.0 rlang from 0.4.12 to 1.0.1 rpart from 4.1-15 to 4.1.16 RSQLite from 2.2.9 to 2.2.10 sparklyr from 1.7.4 to 1.7.5 testthat from 3.1.1 to 3.1.2 tidyr from 1.1.4 to 1.2.0 tidyselect from 1.1.1 to 1.1.2 tinytex from 0.36 to 0.37 yaml from 2.2.1 to 2.3.5 Upgraded Java libraries: io.delta.delta-sharing-spark_2.12 from 0.3.0 to 0.4.0 Apache Spark Databricks Runtime 10.4 includes Apache Spark 3.2.1. This release includes all Spark fixes and improvements included in Databricks Runtime 10.3, as well as the following additional bug fixes and improvements made to Spark: [SPARK-38322] [SQL] Support query stage show runtime statistics in formatted explain mode [SPARK-38162] [SQL] Optimize one row plan in normal and AQE Optimizer [SPARK-38229] [SQL] Should’t check temp/external/ifNotExists with visitReplaceTable when parser [SPARK-34183] [SS] DataSource V2: Required distribution and ordering in micro-batch execution [SPARK-37932] [SQL]Wait to resolve missing attributes before applying DeduplicateRelations [SPARK-37904] [SQL] Improve RebalancePartitions in rules of Optimizer [SPARK-38236] [SQL][3.2][3.1] Check if table location is absolute by “new Path(locationUri).isAbsolute” in create/alter table [SPARK-38035] [SQL] Add docker tests for build-in JDBC dialect [SPARK-38042] [SQL] Ensure that ScalaReflection.dataTypeFor works on aliased array types [SPARK-38273] [SQL] decodeUnsafeRows’s iterators should close underlying input streams [SPARK-38311] [SQL] Fix DynamicPartitionPruning/BucketedReadSuite/ExpressionInfoSuite under ANSI mode [SPARK-38305] [CORE] Explicitly check if source exists in unpack() before calling FileUtil methods [SPARK-38275] [SS] Include the writeBatch’s memory usage as the total memory usage of RocksDB state store [SPARK-38132] [SQL] Remove NotPropagation rule [SPARK-38286] [SQL] Union’s maxRows and maxRowsPerPartition may overflow [SPARK-38306] [SQL] Fix ExplainSuite,StatisticsCollectionSuite and StringFunctionsSuite under ANSI mode [SPARK-38281] [SQL][Tests] Fix AnalysisSuite under ANSI mode [SPARK-38307] [SQL][Tests] Fix ExpressionTypeCheckingSuite and CollectionExpressionsSuite under ANSI mode [SPARK-38300] [SQL] Use ByteStreams.toByteArray to simplify fileToString and resourceToBytes in catalyst.util [SPARK-38304] [SQL] Elt() should return null if index is null under ANSI mode [SPARK-38271] PoissonSampler may output more rows than MaxRows [SPARK-38297] [PYTHON] Explicitly cast the return value at DataFrame.to_numpy in POS [SPARK-38295] [SQL][Tests] Fix ArithmeticExpressionSuite under ANSI mode [SPARK-38290] [SQL] Fix JsonSuite and ParquetIOSuite under ANSI mode [SPARK-38299] [SQL] Clean up deprecated usage of StringBuilder.newBuilder [SPARK-38060] [SQL] Respect allowNonNumericNumbers when parsing quoted NaN and Infinity values in JSON reader [SPARK-38276] [SQL] Add approved TPCDS plans under ANSI mode [SPARK-38206] [SS] Ignore nullability on comparing the data type of join keys on stream-stream join [SPARK-37290] [SQL] - Exponential planning time in case of non-deterministic function [SPARK-38232] [SQL] Explain formatted does not collect subqueries under query stage in AQE [SPARK-38283] [SQL] Test invalid datetime parsing under ANSI mode [SPARK-38140] [SQL] Desc column stats (min, max) for timestamp type is not consistent with the values due to time zone difference [SPARK-38227] [SQL][SS] Apply strict nullability of nested column in time window / session window [SPARK-38221] [SQL] Eagerly iterate over groupingExpressions when moving complex grouping expressions out of an Aggregate node [SPARK-38216] [SQL] Fail early if all the columns are partitioned columns when creating a Hive table [SPARK-38214] [SS]No need to filter windows when windowDuration is multiple of slideDuration [SPARK-38182] [SQL] Fix NoSuchElementException if pushed filter does not contain any references [SPARK-38159] [SQL] Add a new FileSourceMetadataAttribute for the Hidden File Metadata [SPARK-38123] [SQL] Unified use DataType as targetType of QueryExecutionErrors#castingCauseOverflowError [SPARK-38118] [SQL] Func(wrong data type) in HAVING clause should throw data mismatch error [SPARK-35173] [SQL][PYTHON] Add multiple columns adding support [SPARK-38177] [SQL] Fix wrong transformExpressions in Optimizer [SPARK-38228] [SQL] Legacy store assignment should not fail on error under ANSI mode [SPARK-38173] [SQL] Quoted column cannot be recognized correctly when quotedRegexColumnNa… [SPARK-38130] [SQL] Remove array_sort orderable entries check [SPARK-38199] [SQL] Delete the unused dataType specified in the definition of IntervalColumnAccessor [SPARK-38203] [SQL] Fix SQLInsertTestSuite and SchemaPruningSuite under ANSI mode [SPARK-38163] [SQL] Preserve the error class of SparkThrowable while constructing of function builder [SPARK-38157] [SQL] Explicitly set ANSI to false in test timestampNTZ/timestamp.sql and SQLQueryTestSuite to match the expected golden results [SPARK-38069] [SQL][SS] Improve the calculation of time window [SPARK-38164] [SQL] New SQL functions: try_subtract and try_multiply [SPARK-38176] [SQL] ANSI mode: allow implicitly casting String to other simple types [SPARK-37498] [PYTHON] Add eventually for test_reuse_worker_of_parallelize_range [SPARK-38198] [SQL][3.2] Fix QueryExecution.debug#toFile use the passed in maxFields when explainMode is CodegenMode [SPARK-38131] [SQL] Use error classes in user-facing exceptions only [SPARK-37652] [SQL] Add test for optimize skewed join through union [SPARK-37585] [SQL] Update InputMetric in DataSourceRDD with TaskCompletionListener [SPARK-38113] [SQL] Use error classes in the execution errors of pivoting [SPARK-38178] [SS] Correct the logic to measure the memory usage of RocksDB [SPARK-37969] [SQL] HiveFileFormat should check field name [SPARK-37652] Revert “[SQL]Add test for optimize skewed join through union” [SPARK-38124] [SQL][SS] Introduce StatefulOpClusteredDistribution and apply to stream-stream join [SPARK-38030] [SQL] Canonicalization should not remove nullability of AttributeReference dataType [SPARK-37907] [SQL] InvokeLike support ConstantFolding [SPARK-37891] [CORE] Add scalastyle check to disable scala.concurrent.ExecutionContext.Implicits.global [SPARK-38150] [SQL] Update comment of RelationConversions [SPARK-37943] [SQL] Use error classes in the compilation errors of grouping [SPARK-37652] [SQL]Add test for optimize skewed join through union [SPARK-38056] [Web UI][3.2] Fix issue of Structured streaming not working in history server when using LevelDB [SPARK-38144] [CORE] Remove unused spark.storage.safetyFraction config [SPARK-38120] [SQL] Fix HiveExternalCatalog.listPartitions when partition column name is upper case and dot in partition value [SPARK-38122] [Docs] Update the App Key of DocSearch [SPARK-37479] [SQL] Migrate DROP NAMESPACE to use V2 command by default [SPARK-35703] [SQL] Relax constraint for bucket join and remove HashClusteredDistribution [SPARK-37983] [SQL] Back out agg build time metrics from sort aggregate [SPARK-37915] [SQL] Combine unions if there is a project between them [SPARK-38105] [SQL] Use error classes in the parsing errors of joins [SPARK-38073] [PYTHON] Update atexit function to avoid issues with late binding [SPARK-37941] [SQL] Use error classes in the compilation errors of casting [SPARK-37937] [SQL] Use error classes in the parsing errors of lateral join [SPARK-38100] [SQL] Remove unused private method in Decimal [SPARK-37987] [SS] Fix flaky test StreamingAggregationSuite.changing schema of state when restarting query [SPARK-38003] [SQL] LookupFunctions rule should only look up functions from the scalar function registry [SPARK-38075] [SQL] Fix hasNext in HiveScriptTransformationExec’s process output iterator [SPARK-37965] [SQL] Remove check field name when reading/writing existing data in Orc [SPARK-37922] [SQL] Combine to one cast if we can safely up-cast two casts (for dbr-branch-10.x) [SPARK-37675] [SPARK-37793] Prevent overwriting of push shuffle merged files once the shuffle is finalized [SPARK-38011] [SQL] Remove duplicated and useless configuration in ParquetFileFormat [SPARK-37929] [SQL] Support cascade mode for dropNamespace API [SPARK-37931] [SQL] Quote the column name if needed [SPARK-37990] [SQL] Support TimestampNTZ in RowToColumnConverter [SPARK-38001] [SQL] Replace the error classes related to unsupported features by UNSUPPORTED_FEATURE [SPARK-37839] [SQL] DS V2 supports partial aggregate push-down AVG [SPARK-37878] [SQL] Migrate SHOW CREATE TABLE to use v2 command by default [SPARK-37731] [SQL] Refactor and cleanup function lookup in Analyzer [SPARK-37979] [SQL] Switch to more generic error classes in AES functions [SPARK-37867] [SQL] Compile aggregate functions of build-in JDBC dialect [SPARK-38028] [SQL] Expose Arrow Vector from ArrowColumnVector [SPARK-30062] [SQL] Add the IMMEDIATE statement to the DB2 dialect truncate implementation [SPARK-36649] [SQL] Support Trigger.AvailableNow on Kafka data source [SPARK-38018] [SQL] Fix ColumnVectorUtils.populate to handle CalendarIntervalType correctly [SPARK-38023] [CORE] ExecutorMonitor.onExecutorRemoved should handle ExecutorDecommission as finished [SPARK-38019] [CORE] Make ExecutorMonitor.timedOutExecutors deterministic [SPARK-37957] [SQL] Correctly pass deterministic flag for V2 scalar functions [SPARK-37985] [SQL] Fix flaky test for SPARK-37578 [SPARK-37986] [SQL] Support TimestampNTZ in radix sort [SPARK-37967] [SQL] Literal.create support ObjectType [SPARK-37827] [SQL] Put the some built-in table properties into V1Table.propertie to adapt to V2 command [SPARK-37963] [SQL] Need to update Partition URI after renaming table in InMemoryCatalog [SPARK-35442] [SQL] Support propagate empty relation through aggregate/union [SPARK-37933] [SQL] Change the traversal method of V2ScanRelationPushDown push down rules [SPARK-37917] [SQL] Push down limit 1 for right side of left semi/anti join if join condition is empty [SPARK-37959] [ML] Fix the UT of checking norm in KMeans & BiKMeans [SPARK-37906] [SQL] spark-sql should not pass last comment to backend [SPARK-37627] [SQL] Add sorted column in BucketTransform Maintenance updates See Databricks Runtime 10.4 maintenance updates. System environment Operating System: Ubuntu 20.04.4 LTS Java: Zulu 8.56.0.21-CA-linux64 Scala: 2.12.14 Python: 3.8.10 R: 4.1.2 Delta Lake: 1.1.0 Installed Python libraries Library Version Library Version Library Version Antergos Linux 2015.10 (ISO-Rolling) appdirs 1.4.4 argon2-cffi 20.1.0 async-generator 1.10 attrs 20.3.0 backcall 0.2.0 bidict 0.21.4 bleach 3.3.0 boto3 1.16.7 botocore 1.19.7 certifi 2020.12.5 cffi 1.14.5 chardet 4.0.0 cycler 0.10.0 Cython 0.29.23 dbus-python 1.2.16 decorator 5.0.6 defusedxml 0.7.1 distlib 0.3.4 distro-info 0.23ubuntu1 entrypoints 0.3 facets-overview 1.0.0 filelock 3.6.0 idna 2.10 ipykernel 5.3.4 ipython 7.22.0 ipython-genutils 0.2.0 ipywidgets 7.6.3 jedi 0.17.2 Jinja2 2.11.3 jmespath 0.10.0 joblib 1.0.1 jsonschema 3.2.0 jupyter-client 6.1.12 jupyter-core 4.7.1 jupyterlab-pygments 0.1.2 jupyterlab-widgets 1.0.0 kiwisolver 1.3.1 koalas 1.8.2 MarkupSafe 2.0.1 matplotlib 3.4.2 mistune 0.8.4 nbclient 0.5.3 nbconvert 6.0.7 nbformat 5.1.3 nest-asyncio 1.5.1 notebook 6.3.0 numpy 1.20.1 packaging 20.9 pandas 1.2.4 pandocfilters 1.4.3 parso 0.7.0 patsy 0.5.1 pexpect 4.8.0 pickleshare 0.7.5 Pillow 8.2.0 pip 21.0.1 plotly 5.5.0 prometheus-client 0.10.1 prompt-toolkit 3.0.17 protobuf 3.17.2 psycopg2 2.8.5 ptyprocess 0.7.0 pyarrow 4.0.0 pycparser 2.20 Pygments 2.8.1 PyGObject 3.36.0 pyparsing 2.4.7 pyrsistent 0.17.3 python-apt 2.0.0+ubuntu0.20.4.7 python-dateutil 2.8.1 python-engineio 4.3.0 python-socketio 5.4.1 pytz 2020.5 pyzmq 20.0.0 requests 2.25.1 requests-unixsocket 0.2.0 s3transfer 0.3.7 scikit-learn 0.24.1 scipy 1.6.2 seaborn 0.11.1 Send2Trash 1.5.0 setuptools 52.0.0 six 1.15.0 ssh-import-id 5.10 statsmodels 0.12.2 tenacity 8.0.1 terminado 0.9.4 testpath 0.4.4 threadpoolctl 2.1.0 tornado 6.1 traitlets 5.0.5 unattended-upgrades 0.1 urllib3 1.25.11 virtualenv 20.4.1 wcwidth 0.2.5 webencodings 0.5.1 wheel 0.36.2 widgetsnbextension 3.5.1 Installed R libraries R libraries are installed from the Microsoft CRAN snapshot on 2022-02-24. Library Version Library Version Library Version askpass 1.1 assertthat 0.2.1 backports 1.4.1 base 4.1.2 base64enc 0.1-3 bit 4.0.4 bit64 4.0.5 blob 1.2.2 boot 1.3-28 brew 1.0-7 brio 1.1.3 broom 0.7.12 bslib 0.3.1 cachem 1.0.6 callr 3.7.0 caret 6.0-90 cellranger 1.1.0 chron 2.3-56 class 7.3-20 cli 3.2.0 clipr 0.8.0 cluster 2.1.2 codetools 0.2-18 colorspace 2.0-3 commonmark 1.7 compiler 4.1.2 config 0.3.1 cpp11 0.4.2 crayon 1.5.0 credentials 1.3.2 curl 4.3.2 data.table 1.14.2 datasets 4.1.2 DBI 1.1.2 dbplyr 2.1.1 desc 1.4.0 devtools 2.4.3 diffobj 0.3.5 digest 0.6.29 dplyr 1.0.8 dtplyr 1.2.1 e1071 1.7-9 ellipsis 0.3.2 evaluate 0.15 fansi 1.0.2 farver 2.1.0 fastmap 1.1.0 fontawesome 0.2.2 forcats 0.5.1 foreach 1.5.2 foreign 0.8-82 forge 0.2.0 fs 1.5.2 future 1.24.0 future.apply 1.8.1 gargle 1.2.0 generics 0.1.2 gert 1.5.0 ggplot2 3.3.5 gh 1.3.0 gitcreds 0.1.1 glmnet 4.1-3 globals 0.14.0 glue 1.6.1 googledrive 2.0.0 googlesheets4 1.0.0 gower 1.0.0 graphics 4.1.2 grDevices 4.1.2 grid 4.1.2 gridExtra 2.3 gsubfn 0.7 gtable 0.3.0 hardhat 0.2.0 haven 2.4.3 highr 0.9 hms 1.1.1 htmltools 0.5.2 htmlwidgets 1.5.4 httpuv 1.6.5 httr 1.4.2 hwriter 1.3.2 hwriterPlus 1.0-3 ids 1.0.1 ini 0.3.1 ipred 0.9-12 isoband 0.2.5 iterators 1.0.14 jquerylib 0.1.4 jsonlite 1.8.0 KernSmooth 2.23-20 knitr 1.37 labeling 0.4.2 later 1.3.0 lattice 0.20-45 lava 1.6.10 lifecycle 1.0.1 listenv 0.8.0 lubridate 1.8.0 magrittr 2.0.2 markdown 1.1 MASS 7.3-55 Matrix 1.4-0 memoise 2.0.1 methods 4.1.2 mgcv 1.8-39 mime 0.12 ModelMetrics 1.2.2.2 modelr 0.1.8 munsell 0.5.0 nlme 3.1-155 nnet 7.3-17 numDeriv 2016.8-1.1 openssl 1.4.6 parallel 4.1.2 parallelly 1.30.0 pillar 1.7.0 pkgbuild 1.3.1 pkgconfig 2.0.3 pkgload 1.2.4 plogr 0.2.0 plyr 1.8.6 praise 1.0.0 prettyunits 1.1.1 pROC 1.18.0 processx 3.5.2 prodlim 2019.11.13 progress 1.2.2 progressr 0.10.0 promises 1.2.0.1 proto 1.0.0 proxy 0.4-26 ps 1.6.0 purrr 0.3.4 r2d3 0.2.5 R6 2.5.1 randomForest 4.7-1 rappdirs 0.3.3 rcmdcheck 1.4.0 RColorBrewer 1.1-2 Rcpp 1.0.8 RcppEigen 0.3.3.9.1 readr 2.1.2 readxl 1.3.1 recipes 0.2.0 rematch 1.0.1 rematch2 2.1.2 remotes 2.4.2 reprex 2.0.1 reshape2 1.4.4 rlang 1.0.1 rmarkdown 2.11 RODBC 1.3-19 roxygen2 7.1.2 rpart 4.1.16 rprojroot 2.0.2 Rserve 1.8-10 RSQLite 2.2.10 rstudioapi 0.13 rversions 2.1.1 rvest 1.0.2 sass 0.4.0 scales 1.1.1 selectr 0.4-2 sessioninfo 1.2.2 shape 1.4.6 shiny 1.7.1 sourcetools 0.1.7 sparklyr 1.7.5 SparkR 3.2.0 spatial 7.3-11 splines 4.1.2 sqldf 0.4-11 SQUAREM 2021.1 stats 4.1.2 stats4 4.1.2 stringi 1.7.6 stringr 1.4.0 survival 3.2-13 sys 3.4 tcltk 4.1.2 TeachingDemos 2.10 testthat 3.1.2 tibble 3.1.6 tidyr 1.2.0 tidyselect 1.1.2 tidyverse 1.3.1 timeDate 3043.102 tinytex 0.37 tools 4.1.2 tzdb 0.2.0 usethis 2.1.5 utf8 1.2.2 utils 4.1.2 uuid 1.0-3 vctrs 0.3.8 viridisLite 0.4.0 vroom 1.5.7 waldo 0.3.1 whisker 0.4 withr 2.4.3 xfun 0.29 xml2 1.3.3 xopen 1.0.0 xtable 1.8-4 yaml 2.3.5 zip 2.2.0 Installed Java and Scala libraries (Scala 2.12 cluster version) Group ID Artifact ID Version antlr antlr 2.7.7 com.amazonaws amazon-kinesis-client 1.12.0 com.amazonaws aws-java-sdk-autoscaling 1.11.655 com.amazonaws aws-java-sdk-cloudformation 1.11.655 com.amazonaws aws-java-sdk-cloudfront 1.11.655 com.amazonaws aws-java-sdk-cloudhsm 1.11.655 com.amazonaws aws-java-sdk-cloudsearch 1.11.655 com.amazonaws aws-java-sdk-cloudtrail 1.11.655 com.amazonaws aws-java-sdk-cloudwatch 1.11.655 com.amazonaws aws-java-sdk-cloudwatchmetrics 1.11.655 com.amazonaws aws-java-sdk-codedeploy 1.11.655 com.amazonaws aws-java-sdk-cognitoidentity 1.11.655 com.amazonaws aws-java-sdk-cognitosync 1.11.655 com.amazonaws aws-java-sdk-config 1.11.655 com.amazonaws aws-java-sdk-core 1.11.655 com.amazonaws aws-java-sdk-datapipeline 1.11.655 com.amazonaws aws-java-sdk-directconnect 1.11.655 com.amazonaws aws-java-sdk-directory 1.11.655 com.amazonaws aws-java-sdk-dynamodb 1.11.655 com.amazonaws aws-java-sdk-ec2 1.11.655 com.amazonaws aws-java-sdk-ecs 1.11.655 com.amazonaws aws-java-sdk-efs 1.11.655 com.amazonaws aws-java-sdk-elasticache 1.11.655 com.amazonaws aws-java-sdk-elasticbeanstalk 1.11.655 com.amazonaws aws-java-sdk-elasticloadbalancing 1.11.655 com.amazonaws aws-java-sdk-elastictranscoder 1.11.655 com.amazonaws aws-java-sdk-emr 1.11.655 com.amazonaws aws-java-sdk-glacier 1.11.655 com.amazonaws aws-java-sdk-glue 1.11.655 com.amazonaws aws-java-sdk-iam 1.11.655 com.amazonaws aws-java-sdk-importexport 1.11.655 com.amazonaws aws-java-sdk-kinesis 1.11.655 com.amazonaws aws-java-sdk-kms 1.11.655 com.amazonaws aws-java-sdk-lambda 1.11.655 com.amazonaws aws-java-sdk-logs 1.11.655 com.amazonaws aws-java-sdk-machinelearning 1.11.655 com.amazonaws aws-java-sdk-opsworks 1.11.655 com.amazonaws aws-java-sdk-rds 1.11.655 com.amazonaws aws-java-sdk-redshift 1.11.655 com.amazonaws aws-java-sdk-route53 1.11.655 com.amazonaws aws-java-sdk-s3 1.11.655 com.amazonaws aws-java-sdk-ses 1.11.655 com.amazonaws aws-java-sdk-simpledb 1.11.655 com.amazonaws aws-java-sdk-simpleworkflow 1.11.655 com.amazonaws aws-java-sdk-sns 1.11.655 com.amazonaws aws-java-sdk-sqs 1.11.655 com.amazonaws aws-java-sdk-ssm 1.11.655 com.amazonaws aws-java-sdk-storagegateway 1.11.655 com.amazonaws aws-java-sdk-sts 1.11.655 com.amazonaws aws-java-sdk-support 1.11.655 com.amazonaws aws-java-sdk-swf-libraries 1.11.22 com.amazonaws aws-java-sdk-workspaces 1.11.655 com.amazonaws jmespath-java 1.11.655 com.chuusai shapeless_2.12 2.3.3 com.clearspring.analytics stream 2.9.6 com.databricks Rserve 1.8-3 com.databricks jets3t 0.7.1-0 com.databricks.scalapb compilerplugin_2.12 0.4.15-10 com.databricks.scalapb scalapb-runtime_2.12 0.4.15-10 com.esotericsoftware kryo-shaded 4.0.2 com.esotericsoftware minlog 1.3.0 com.fasterxml classmate 1.3.4 com.fasterxml.jackson.core jackson-annotations 2.12.3 com.fasterxml.jackson.core jackson-core 2.12.3 com.fasterxml.jackson.core jackson-databind 2.12.3 com.fasterxml.jackson.dataformat jackson-dataformat-cbor 2.12.3 com.fasterxml.jackson.datatype jackson-datatype-joda 2.12.3 com.fasterxml.jackson.module jackson-module-paranamer 2.12.3 com.fasterxml.jackson.module jackson-module-scala_2.12 2.12.3 com.github.ben-manes.caffeine caffeine 2.3.4 com.github.fommil jniloader 1.1 com.github.fommil.netlib core 1.1.2 com.github.fommil.netlib native_ref-java 1.1 com.github.fommil.netlib native_ref-java-natives 1.1 com.github.fommil.netlib native_system-java 1.1 com.github.fommil.netlib native_system-java-natives 1.1 com.github.fommil.netlib netlib-native_ref-linux-x86_64-natives 1.1 com.github.fommil.netlib netlib-native_system-linux-x86_64-natives 1.1 com.github.luben zstd-jni 1.5.0-4 com.github.wendykierp JTransforms 3.1 com.google.code.findbugs jsr305 3.0.0 com.google.code.gson gson 2.8.6 com.google.crypto.tink tink 1.6.0 com.google.flatbuffers flatbuffers-java 1.9.0 com.google.guava guava 15.0 com.google.protobuf protobuf-java 2.6.1 com.h2database h2 1.4.195 com.helger profiler 1.1.1 com.jcraft jsch 0.1.50 com.jolbox bonecp 0.8.0.RELEASE com.lihaoyi sourcecode_2.12 0.1.9 com.microsoft.azure azure-data-lake-store-sdk 2.3.9 com.ning compress-lzf 1.0.3 com.sun.istack istack-commons-runtime 3.0.8 com.sun.mail javax.mail 1.5.2 com.tdunning json 1.8 com.thoughtworks.paranamer paranamer 2.8 com.trueaccord.lenses lenses_2.12 0.4.12 com.twitter chill-java 0.10.0 com.twitter chill_2.12 0.10.0 com.twitter util-app_2.12 7.1.0 com.twitter util-core_2.12 7.1.0 com.twitter util-function_2.12 7.1.0 com.twitter util-jvm_2.12 7.1.0 com.twitter util-lint_2.12 7.1.0 com.twitter util-registry_2.12 7.1.0 com.twitter util-stats_2.12 7.1.0 com.typesafe config 1.2.1 com.typesafe.scala-logging scala-logging_2.12 3.7.2 com.univocity univocity-parsers 2.9.1 com.zaxxer HikariCP 4.0.3 commons-cli commons-cli 1.2 commons-codec commons-codec 1.15 commons-collections commons-collections 3.2.2 commons-dbcp commons-dbcp 1.4 commons-fileupload commons-fileupload 1.3.3 commons-httpclient commons-httpclient 3.1 commons-io commons-io 2.8.0 commons-lang commons-lang 2.6 commons-logging commons-logging 1.1.3 commons-net commons-net 3.1 commons-pool commons-pool 1.5.4 dev.ludovic.netlib arpack 2.2.1 dev.ludovic.netlib blas 2.2.1 dev.ludovic.netlib lapack 2.2.1 hive-2.3__hadoop-3.2 jets3t-0.7 liball_deps_2.12 info.ganglia.gmetric4j gmetric4j 1.0.10 io.airlift aircompressor 0.21 io.delta delta-sharing-spark_2.12 0.4.0 io.dropwizard.metrics metrics-core 4.1.1 io.dropwizard.metrics metrics-graphite 4.1.1 io.dropwizard.metrics metrics-healthchecks 4.1.1 io.dropwizard.metrics metrics-jetty9 4.1.1 io.dropwizard.metrics metrics-jmx 4.1.1 io.dropwizard.metrics metrics-json 4.1.1 io.dropwizard.metrics metrics-jvm 4.1.1 io.dropwizard.metrics metrics-servlets 4.1.1 io.netty netty-all 4.1.68.Final io.prometheus simpleclient 0.7.0 io.prometheus simpleclient_common 0.7.0 io.prometheus simpleclient_dropwizard 0.7.0 io.prometheus simpleclient_pushgateway 0.7.0 io.prometheus simpleclient_servlet 0.7.0 io.prometheus.jmx collector 0.12.0 jakarta.annotation jakarta.annotation-api 1.3.5 jakarta.servlet jakarta.servlet-api 4.0.3 jakarta.validation jakarta.validation-api 2.0.2 jakarta.ws.rs jakarta.ws.rs-api 2.1.6 javax.activation activation 1.1.1 javax.annotation javax.annotation-api 1.3.2 javax.el javax.el-api 2.2.4 javax.jdo jdo-api 3.0.1 javax.transaction jta 1.1 javax.transaction transaction-api 1.1 javax.xml.bind jaxb-api 2.2.2 javax.xml.stream stax-api 1.0-2 javolution javolution 5.5.1 jline jline 2.14.6 joda-time joda-time 2.10.10 log4j apache-log4j-extras 1.2.17 log4j log4j 1.2.17 maven-trees hive-2.3__hadoop-3.2 liball_deps_2.12 net.java.dev.jna jna 5.8.0 net.razorvine pyrolite 4.30 net.sf.jpam jpam 1.1 net.sf.opencsv opencsv 2.3 net.sf.supercsv super-csv 2.2.0 net.snowflake snowflake-ingest-sdk 0.9.6 net.snowflake snowflake-jdbc 3.13.3 net.snowflake spark-snowflake_2.12 2.9.0-spark_3.1 net.sourceforge.f2j arpack_combined_all 0.1 org.acplt.remotetea remotetea-oncrpc 1.1.2 org.antlr ST4 4.0.4 org.antlr antlr-runtime 3.5.2 org.antlr antlr4-runtime 4.8 org.antlr stringtemplate 3.2.1 org.apache.ant ant 1.9.2 org.apache.ant ant-jsch 1.9.2 org.apache.ant ant-launcher 1.9.2 org.apache.arrow arrow-format 2.0.0 org.apache.arrow arrow-memory-core 2.0.0 org.apache.arrow arrow-memory-netty 2.0.0 org.apache.arrow arrow-vector 2.0.0 org.apache.avro avro 1.10.2 org.apache.avro avro-ipc 1.10.2 org.apache.avro avro-mapred 1.10.2 org.apache.commons commons-compress 1.21 org.apache.commons commons-crypto 1.1.0 org.apache.commons commons-lang3 3.12.0 org.apache.commons commons-math3 3.4.1 org.apache.commons commons-text 1.6 org.apache.curator curator-client 2.13.0 org.apache.curator curator-framework 2.13.0 org.apache.curator curator-recipes 2.13.0 org.apache.derby derby 10.14.2.0 org.apache.hadoop hadoop-client-api 3.3.1-databricks org.apache.hadoop hadoop-client-runtime 3.3.1 org.apache.hive hive-beeline 2.3.9 org.apache.hive hive-cli 2.3.9 org.apache.hive hive-jdbc 2.3.9 org.apache.hive hive-llap-client 2.3.9 org.apache.hive hive-llap-common 2.3.9 org.apache.hive hive-serde 2.3.9 org.apache.hive hive-shims 2.3.9 org.apache.hive hive-storage-api 2.7.2 org.apache.hive.shims hive-shims-0.23 2.3.9 org.apache.hive.shims hive-shims-common 2.3.9 org.apache.hive.shims hive-shims-scheduler 2.3.9 org.apache.htrace htrace-core4 4.1.0-incubating org.apache.httpcomponents httpclient 4.5.13 org.apache.httpcomponents httpcore 4.4.12 org.apache.ivy ivy 2.5.0 org.apache.mesos mesos-shaded-protobuf 1.4.0 org.apache.orc orc-core 1.6.12 org.apache.orc orc-mapreduce 1.6.12 org.apache.orc orc-shims 1.6.12 org.apache.parquet parquet-column 1.12.0-databricks-0003 org.apache.parquet parquet-common 1.12.0-databricks-0003 org.apache.parquet parquet-encoding 1.12.0-databricks-0003 org.apache.parquet parquet-format-structures 1.12.0-databricks-0003 org.apache.parquet parquet-hadoop 1.12.0-databricks-0003 org.apache.parquet parquet-jackson 1.12.0-databricks-0003 org.apache.thrift libfb303 0.9.3 org.apache.thrift libthrift 0.12.0 org.apache.xbean xbean-asm9-shaded 4.20 org.apache.yetus audience-annotations 0.5.0 org.apache.zookeeper zookeeper 3.6.2 org.apache.zookeeper zookeeper-jute 3.6.2 org.checkerframework checker-qual 3.5.0 org.codehaus.jackson jackson-core-asl 1.9.13 org.codehaus.jackson jackson-mapper-asl 1.9.13 org.codehaus.janino commons-compiler 3.0.16 org.codehaus.janino janino 3.0.16 org.datanucleus datanucleus-api-jdo 4.2.4 org.datanucleus datanucleus-core 4.1.17 org.datanucleus datanucleus-rdbms 4.1.19 org.datanucleus javax.jdo 3.2.0-m3 org.eclipse.jetty jetty-client 9.4.43.v20210629 org.eclipse.jetty jetty-continuation 9.4.43.v20210629 org.eclipse.jetty jetty-http 9.4.43.v20210629 org.eclipse.jetty jetty-io 9.4.43.v20210629 org.eclipse.jetty jetty-jndi 9.4.43.v20210629 org.eclipse.jetty jetty-plus 9.4.43.v20210629 org.eclipse.jetty jetty-proxy 9.4.43.v20210629 org.eclipse.jetty jetty-security 9.4.43.v20210629 org.eclipse.jetty jetty-server 9.4.43.v20210629 org.eclipse.jetty jetty-servlet 9.4.43.v20210629 org.eclipse.jetty jetty-servlets 9.4.43.v20210629 org.eclipse.jetty jetty-util 9.4.43.v20210629 org.eclipse.jetty jetty-util-ajax 9.4.43.v20210629 org.eclipse.jetty jetty-webapp 9.4.43.v20210629 org.eclipse.jetty jetty-xml 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-api 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-client 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-common 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-server 9.4.43.v20210629 org.eclipse.jetty.websocket websocket-servlet 9.4.43.v20210629 org.fusesource.leveldbjni leveldbjni-all 1.8 org.glassfish.hk2 hk2-api 2.6.1 org.glassfish.hk2 hk2-locator 2.6.1 org.glassfish.hk2 hk2-utils 2.6.1 org.glassfish.hk2 osgi-resource-locator 1.0.3 org.glassfish.hk2.external aopalliance-repackaged 2.6.1 org.glassfish.hk2.external jakarta.inject 2.6.1 org.glassfish.jaxb jaxb-runtime 2.3.2 org.glassfish.jersey.containers jersey-container-servlet 2.34 org.glassfish.jersey.containers jersey-container-servlet-core 2.34 org.glassfish.jersey.core jersey-client 2.34 org.glassfish.jersey.core jersey-common 2.34 org.glassfish.jersey.core jersey-server 2.34 org.glassfish.jersey.inject jersey-hk2 2.34 org.hibernate.validator hibernate-validator 6.1.0.Final org.javassist javassist 3.25.0-GA org.jboss.logging jboss-logging 3.3.2.Final org.jdbi jdbi 2.63.1 org.jetbrains annotations 17.0.0 org.joda joda-convert 1.7 org.jodd jodd-core 3.5.2 org.json4s json4s-ast_2.12 3.7.0-M11 org.json4s json4s-core_2.12 3.7.0-M11 org.json4s json4s-jackson_2.12 3.7.0-M11 org.json4s json4s-scalap_2.12 3.7.0-M11 org.lz4 lz4-java 1.7.1 org.mariadb.jdbc mariadb-java-client 2.2.5 org.objenesis objenesis 2.5.1 org.postgresql postgresql 42.2.19 org.roaringbitmap RoaringBitmap 0.9.14 org.roaringbitmap shims 0.9.14 org.rocksdb rocksdbjni 6.20.3 org.rosuda.REngine REngine 2.1.0 org.scala-lang scala-compiler_2.12 2.12.14 org.scala-lang scala-library_2.12 2.12.14 org.scala-lang scala-reflect_2.12 2.12.14 org.scala-lang.modules scala-collection-compat_2.12 2.4.3 org.scala-lang.modules scala-parser-combinators_2.12 1.1.2 org.scala-lang.modules scala-xml_2.12 1.2.0 org.scala-sbt test-interface 1.0 org.scalacheck scalacheck_2.12 1.14.2 org.scalactic scalactic_2.12 3.0.8 org.scalanlp breeze-macros_2.12 1.2 org.scalanlp breeze_2.12 1.2 org.scalatest scalatest_2.12 3.0.8 org.slf4j jcl-over-slf4j 1.7.30 org.slf4j jul-to-slf4j 1.7.30 org.slf4j slf4j-api 1.7.30 org.slf4j slf4j-log4j12 1.7.30 org.spark-project.spark unused 1.0.0 org.threeten threeten-extra 1.5.0 org.tukaani xz 1.8 org.typelevel algebra_2.12 2.0.1 org.typelevel cats-kernel_2.12 2.1.1 org.typelevel macro-compat_2.12 1.1.1 org.typelevel spire-macros_2.12 0.17.0 org.typelevel spire-platform_2.12 0.17.0 org.typelevel spire-util_2.12 0.17.0 org.typelevel spire_2.12 0.17.0 org.wildfly.openssl wildfly-openssl 1.0.7.Final org.xerial sqlite-jdbc 3.8.11.2 org.xerial.snappy snappy-java 1.1.8.4 org.yaml snakeyaml 1.24 oro oro 2.0.8 pl.edu.icm JLargeArrays 1.5 software.amazon.ion ion-java 1.0.2 stax stax-api 1.0.1",Databricks Runtime 10.4 LTS
4,Databricks Enable Table access control has py4j.security.Py4JSecurityException issue,"Dear DBR Colleague,

We enabled table access control in workspace and grant any file privilege to Data Scientist users, but when we running some python packages like pandas and others spark ML package used RDD, it returns py4j.security.Py4JSecurityException .... is not whitelisted on class class.

I would like to ask if there is some solution can add it into whitelist.

Thanks,

Fan",https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Access Azure Data Lake Storage using Azure Active Directory credential passthrough Article 07/12/2022 11 minutes to read 6 contributors In this article Requirements Logging recommendations Enable Azure Data Lake Storage credential passthrough for a High Concurrency cluster Enable Azure Data Lake Storage credential passthrough for a Standard cluster Create a container Access Azure Data Lake Storage directly using credential passthrough Mount Azure Data Lake Storage to DBFS using credential passthrough Security Supported features Limitations Example notebooks Troubleshooting Note This article contains references to the term whitelisted, a term that Azure Databricks does not use. When the term is removed from the software, we’ll remove it from this article. You can authenticate automatically to Accessing Azure Data Lake Storage Gen1 from Azure Databricks (ADLS Gen1) and ADLS Gen2 from Azure Databricks clusters using the same Azure Active Directory (Azure AD) identity that you use to log into Azure Databricks. When you enable Azure Data Lake Storage credential passthrough for your cluster, commands that you run on that cluster can read and write data in Azure Data Lake Storage without requiring you to configure service principal credentials for access to storage. Azure Data Lake Storage credential passthrough is supported with Azure Data Lake Storage Gen1 and Gen2 only. Azure Blob storage does not support credential passthrough. This article covers: Enabling credential passthrough for standard and high-concurrency clusters. Configuring credential passthrough and initializing storage resources in ADLS accounts. Accessing ADLS resources directly when credential passthrough is enabled. Accessing ADLS resources through a mount point when credential passthrough is enabled. Supported features and limitations when using credential passthrough. Notebooks are included to provide examples of using credential passthrough with ADLS Gen1 and ADLS Gen2 storage accounts. Requirements Premium Plan. See Upgrade or Downgrade an Azure Databricks Workspace for details on upgrading a standard plan to a premium plan. An Azure Data Lake Storage Gen1 or Gen2 storage account. Azure Data Lake Storage Gen2 storage accounts must use the hierarchical namespace to work with Azure Data Lake Storage credential passthrough. See Create a storage account for instructions on creating a new ADLS Gen2 account, including how to enable the hierarchical namespace. Properly configured user permissions to Azure Data Lake Storage. An Azure Databricks administrator needs to ensure that users have the correct roles, for example, Storage Blob Data Contributor, to read and write data stored in Azure Data Lake Storage. See Use the Azure portal to assign an Azure role for access to blob and queue data. You cannot use a cluster configured with ADLS credentials, for example, service principal credentials, with credential passthrough. Important You cannot authenticate to Azure Data Lake Storage with your Azure Active Directory credentials if you are behind a firewall that has not been configured to allow traffic to Azure Active Directory. Azure Firewall blocks Active Directory access by default. To allow access, configure the AzureActiveDirectory service tag. You can find equivalent information for network virtual appliances under the AzureActiveDirectory tag in the Azure IP Ranges and Service Tags JSON file. For more information, see Azure Firewall service tags and Azure IP Addresses for Public Cloud. Logging recommendations You can log identities passed through to ADLS storage in the Azure storage diagnostic logs. Logging identities allows ADLS requests to be tied to individual users from Azure Databricks clusters. Turn on diagnostic logging on your storage account to start receiving these logs: Azure Data Lake Storage Gen1: Follow the instructions in Enable diagnostic logging for your Data Lake Storage Gen1 account. Azure Data Lake Storage Gen2: Configure using PowerShell with the Set-AzStorageServiceLoggingProperty command. Specify 2.0 as the version, because log entry format 2.0 includes the user principal name in the request. Enable Azure Data Lake Storage credential passthrough for a High Concurrency cluster High concurrency clusters can be shared by multiple users. They support only Python and SQL with Azure Data Lake Storage credential passthrough. Important Enabling Azure Data Lake Storage credential passthrough for a High Concurrency cluster blocks all ports on the cluster except for ports 44, 53, and 80. When you create a cluster, set Cluster Mode to High Concurrency. Under Advanced Options, select Enable credential passthrough for user-level data access and only allow Python and SQL commands. Enable Azure Data Lake Storage credential passthrough for a Standard cluster Standard clusters with credential passthrough are limited to a single user. Standard clusters support Python, SQL, Scala, and R. On Databricks Runtime 6.0 and above, SparkR is supported; on Databricks Runtime 10.1 and above, sparklyr is supported. You must assign a user at cluster creation, but the cluster can be edited by a user with Can Manage permissions at any time to replace the original user. Important The user assigned to the cluster must have at least Can Attach To permission for the cluster in order to run commands on the cluster. Admins and the cluster creator have Can Manage permissions, but cannot run commands on the cluster unless they are the designated cluster user. When you create a cluster, set the Cluster Mode to Standard. Under Advanced Options, select Enable credential passthrough for user-level data access and select the user name from the Single User Access drop-down. Create a container Containers provide a way to organize objects in an Azure storage account. Access Azure Data Lake Storage directly using credential passthrough After configuring Azure Data Lake Storage credential passthrough and creating storage containers, you can access data directly in Azure Data Lake Storage Gen1 using an adl:// path and Azure Data Lake Storage Gen2 using an abfss:// path. Azure Data Lake Storage Gen1 Python Python Copy spark.read.format(""csv"").load(""adl://<storage-account-name>.azuredatalakestore.net/MyData.csv"").collect()
 R R Copy # SparkR
library(SparkR)
sparkR.session()
collect(read.df(""adl://<storage-account-name>.azuredatalakestore.net/MyData.csv"", source = ""csv""))

# sparklyr
library(sparklyr)
sc <- spark_connect(method = ""databricks"")
sc %>% spark_read_csv(""adl://<storage-account-name>.azuredatalakestore.net/MyData.csv"") %>% sdf_collect()
 Replace <storage-account-name> with the ADLS Gen1 storage account name. Azure Data Lake Storage Gen2 Python Python Copy spark.read.format(""csv"").load(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/MyData.csv"").collect()
 R R Copy # SparkR
library(SparkR)
sparkR.session()
collect(read.df(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/MyData.csv"", source = ""csv""))

# sparklyr
library(sparklyr)
sc <- spark_connect(method = ""databricks"")
sc %>% spark_read_csv(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/MyData.csv"") %>% sdf_collect()
 Replace <container-name> with the name of a container in the ADLS Gen2 storage account. Replace <storage-account-name> with the ADLS Gen2 storage account name. Mount Azure Data Lake Storage to DBFS using credential passthrough You can mount an Azure Data Lake Storage account or a folder inside it to Databricks File System (DBFS). The mount is a pointer to a data lake store, so the data is never synced locally. When you mount data using a cluster enabled with Azure Data Lake Storage credential passthrough, any read or write to the mount point uses your Azure AD credentials. This mount point will be visible to other users, but the only users that will have read and write access are those who: Have access to the underlying Azure Data Lake Storage storage account Are using a cluster enabled for Azure Data Lake Storage credential passthrough Azure Data Lake Storage Gen1 To mount an Azure Data Lake Storage Gen1 resource or a folder inside it, use the following commands: Python Python Copy configs = {
   ""fs.adl.oauth2.access.token.provider.type"": ""CustomAccessTokenProvider"",
   ""fs.adl.oauth2.access.token.custom.provider"": spark.conf.get(""spark.databricks.passthrough.adls.tokenProviderClassName"")
 }

 # Optionally, you can add <directory-name> to the source URI of your mount point.
 dbutils.fs.mount(
   source = ""adl://<storage-account-name>.azuredatalakestore.net/<directory-name>"",
   mount_point = ""/mnt/<mount-name>"",
   extra_configs = configs)
 Scala Scala Copy  val configs = Map(
   ""fs.adl.oauth2.access.token.provider.type"" -> ""CustomAccessTokenProvider"",
   ""fs.adl.oauth2.access.token.custom.provider"" -> spark.conf.get(""spark.databricks.passthrough.adls.tokenProviderClassName"")
 )

 // Optionally, you can add <directory-name> to the source URI of your mount point.
 dbutils.fs.mount(
   source = ""adl://<storage-account-name>.azuredatalakestore.net/<directory-name>"",
   mountPoint = ""/mnt/<mount-name>"",
   extraConfigs = configs)
 Replace <storage-account-name> with the ADLS Gen2 storage account name. Replace <mount-name> with the name of the intended mount point in DBFS. Azure Data Lake Storage Gen2 To mount an Azure Data Lake Storage Gen2 filesystem or a folder inside it, use the following commands: Python Python Copy configs = {
  ""fs.azure.account.auth.type"": ""CustomAccessToken"",
  ""fs.azure.account.custom.token.provider.class"": spark.conf.get(""spark.databricks.passthrough.adls.gen2.tokenProviderClassName"")
}

# Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = ""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/"",
  mount_point = ""/mnt/<mount-name>"",
  extra_configs = configs)
 Scala Scala Copy val configs = Map(
  ""fs.azure.account.auth.type"" -> ""CustomAccessToken"",
  ""fs.azure.account.custom.token.provider.class"" -> spark.conf.get(""spark.databricks.passthrough.adls.gen2.tokenProviderClassName"")
)

// Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = ""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/"",
  mountPoint = ""/mnt/<mount-name>"",
  extraConfigs = configs)
 Replace <container-name> with the name of a container in the ADLS Gen2 storage account. Replace <storage-account-name> with the ADLS Gen2 storage account name. Replace <mount-name> with the name of the intended mount point in DBFS. Warning Do not provide your storage account access keys or service principal credentials to authenticate to the mount point. That would give other users access to the filesystem using those credentials. The purpose of Azure Data Lake Storage credential passthrough is to prevent you from having to use those credentials and to ensure that access to the filesystem is restricted to users who have access to the underlying Azure Data Lake Storage account. Security It is safe to share Azure Data Lake Storage credential passthrough clusters with other users. You will be isolated from each other and will not be able to read or use each other’s credentials. Supported features Feature Minimum Databricks Runtime Version Notes Python and SQL 5.5 Azure Data Lake Storage Gen1 5.5 %run 5.5 DBFS 5.5 Credentials are passed through only if the DBFS path resolves to a location in Azure Data Lake Storage Gen1 or Gen2. For DBFS paths that resolve to other storage systems, use a different method to specify your credentials. Azure Data Lake Storage Gen2 5.5 Delta caching 5.5 PySpark ML API 5.5 The following ML classes are not supported: * org/apache/spark/ml/classification/RandomForestClassifier * org/apache/spark/ml/clustering/BisectingKMeans * org/apache/spark/ml/clustering/GaussianMixture * org/spark/ml/clustering/KMeans * org/spark/ml/clustering/LDA * org/spark/ml/evaluation/ClusteringEvaluator * org/spark/ml/feature/HashingTF * org/spark/ml/feature/OneHotEncoder * org/spark/ml/feature/StopWordsRemover * org/spark/ml/feature/VectorIndexer * org/spark/ml/feature/VectorSizeHint * org/spark/ml/regression/IsotonicRegression * org/spark/ml/regression/RandomForestRegressor * org/spark/ml/util/DatasetUtils Broadcast variables 5.5 Within PySpark, there is a limit on the size of the Python UDFs you can construct, since large UDFs are sent as broadcast variables. Notebook-scoped libraries 5.5 Scala 5.5 SparkR 6.0 sparklyr 10.1 Modularize or link code in notebooks 6.1 PySpark ML API 6.1 All PySpark ML classes supported. Ganglia UI 6.1 Databricks Connect 7.3 Passthrough is supported on Standard clusters. Limitations The following features are not supported with Azure Data Lake Storage credential passthrough: %fs (use the equivalent dbutils.fs command instead). The Databricks REST API. Table access control. The permissions granted by Azure Data Lake Storage credential passthrough could be used to bypass the fine-grained permissions of table ACLs, while the extra restrictions of table ACLs will constrain some of the benefits you get from credential passthrough. In particular: If you have Azure AD permission to access the data files that underlie a particular table you will have full permissions on that table via the RDD API, regardless of the restrictions placed on them via table ACLs. You will be constrained by table ACLs permissions only when using the DataFrame API. You will see warnings about not having permission SELECT on any file if you try to read files directly with the DataFrame API, even though you could read those files directly via the RDD API. You will be unable to read from tables backed by filesystems other than Azure Data Lake Storage, even if you have table ACL permission to read the tables. The following methods on SparkContext (sc) and SparkSession (spark) objects: Deprecated methods. Methods such as addFile() and addJar() that would allow non-admin users to call Scala code. Any method that accesses a filesystem other than Azure Data Lake Storage Gen1 or Gen2 (to access other filesystems on a cluster with Azure Data Lake Storage credential passthrough enabled, use a different method to specify your credentials and see the section on trusted filesystems under Troubleshooting). The old Hadoop APIs (hadoopFile() and hadoopRDD()). Streaming APIs, since the passed-through credentials would expire while the stream was still running. The FUSE mount (/dbfs) is available only in Databricks Runtime 7.3 LTS and above. Mount points with credential passthrough configured are not supported through the FUSE mount. Azure Data Factory. MLflow on high concurrency clusters. azureml-sdk Python package on high concurrency clusters. You cannot extend the lifetime of Azure Active Directory passthrough tokens using Azure Active Directory token lifetime policies. As a consequence, if you send a command to the cluster that takes longer than an hour, it will fail if an Azure Data Lake Storage resource is accessed after the 1 hour mark. When using Hive 2.3 and above you can’t add a partition on a cluster with credential passthrough enabled. For more information, see the relevant troubleshooting section. Example notebooks The following notebooks demonstrate Azure Data Lake Storage credential passthrough for Azure Data Lake Storage Gen1 and Gen2. Azure Data Lake Storage Gen1 passthrough notebook Get notebook Azure Data Lake Storage Gen2 passthrough notebook Get notebook Troubleshooting py4j.security.Py4JSecurityException: … is not whitelisted This exception is thrown when you have accessed a method that Azure Databricks has not explicitly marked as safe for Azure Data Lake Storage credential passthrough clusters. In most cases, this means that the method could allow a user on a Azure Data Lake Storage credential passthrough cluster to access another user’s credentials. org.apache.spark.api.python.PythonSecurityException: Path … uses an untrusted filesystem This exception is thrown when you have tried to access a filesystem that is not known by the Azure Data Lake Storage credential passthrough cluster to be safe. Using an untrusted filesystem might allow a user on a Azure Data Lake Storage credential passthrough cluster to access another user’s credentials, so we disallow all filesystems that we are not confident are being used safely. To configure the set of trusted filesystems on a Azure Data Lake Storage credential passthrough cluster, set the Spark conf key spark.databricks.pyspark.trustedFilesystems on that cluster to be a comma-separated list of the class names that are trusted implementations of org.apache.hadoop.fs.FileSystem. Adding a partition fails with AzureCredentialNotFoundException when credential passthrough is enabled When using Hive 2.3-3.1, if you try to add a partition on a cluster with credential passthrough enabled, the following exception occurs: Console Copy org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:com.databricks.backend.daemon.data.client.adl.AzureCredentialNotFoundException: Could not find ADLS Gen2 Token
 To work around this issue, add partitions on a cluster without credential passthrough enabled.",Access Azure Data Lake Storage using Azure Active Directory credential passthrough
5,Databricks Enable Table access control has py4j.security.Py4JSecurityException issue,"Dear DBR Colleague,

We enabled table access control in workspace and grant any file privilege to Data Scientist users, but when we running some python packages like pandas and others spark ML package used RDD, it returns py4j.security.Py4JSecurityException .... is not whitelisted on class class.

I would like to ask if there is some solution can add it into whitelist.

Thanks,

Fan",https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Access Azure Data Lake Storage using Azure Active Directory credential passthrough Article 07/12/2022 11 minutes to read 6 contributors In this article Requirements Logging recommendations Enable Azure Data Lake Storage credential passthrough for a High Concurrency cluster Enable Azure Data Lake Storage credential passthrough for a Standard cluster Create a container Access Azure Data Lake Storage directly using credential passthrough Mount Azure Data Lake Storage to DBFS using credential passthrough Security Supported features Limitations Example notebooks Troubleshooting Note This article contains references to the term whitelisted, a term that Azure Databricks does not use. When the term is removed from the software, we’ll remove it from this article. You can authenticate automatically to Accessing Azure Data Lake Storage Gen1 from Azure Databricks (ADLS Gen1) and ADLS Gen2 from Azure Databricks clusters using the same Azure Active Directory (Azure AD) identity that you use to log into Azure Databricks. When you enable Azure Data Lake Storage credential passthrough for your cluster, commands that you run on that cluster can read and write data in Azure Data Lake Storage without requiring you to configure service principal credentials for access to storage. Azure Data Lake Storage credential passthrough is supported with Azure Data Lake Storage Gen1 and Gen2 only. Azure Blob storage does not support credential passthrough. This article covers: Enabling credential passthrough for standard and high-concurrency clusters. Configuring credential passthrough and initializing storage resources in ADLS accounts. Accessing ADLS resources directly when credential passthrough is enabled. Accessing ADLS resources through a mount point when credential passthrough is enabled. Supported features and limitations when using credential passthrough. Notebooks are included to provide examples of using credential passthrough with ADLS Gen1 and ADLS Gen2 storage accounts. Requirements Premium Plan. See Upgrade or Downgrade an Azure Databricks Workspace for details on upgrading a standard plan to a premium plan. An Azure Data Lake Storage Gen1 or Gen2 storage account. Azure Data Lake Storage Gen2 storage accounts must use the hierarchical namespace to work with Azure Data Lake Storage credential passthrough. See Create a storage account for instructions on creating a new ADLS Gen2 account, including how to enable the hierarchical namespace. Properly configured user permissions to Azure Data Lake Storage. An Azure Databricks administrator needs to ensure that users have the correct roles, for example, Storage Blob Data Contributor, to read and write data stored in Azure Data Lake Storage. See Use the Azure portal to assign an Azure role for access to blob and queue data. You cannot use a cluster configured with ADLS credentials, for example, service principal credentials, with credential passthrough. Important You cannot authenticate to Azure Data Lake Storage with your Azure Active Directory credentials if you are behind a firewall that has not been configured to allow traffic to Azure Active Directory. Azure Firewall blocks Active Directory access by default. To allow access, configure the AzureActiveDirectory service tag. You can find equivalent information for network virtual appliances under the AzureActiveDirectory tag in the Azure IP Ranges and Service Tags JSON file. For more information, see Azure Firewall service tags and Azure IP Addresses for Public Cloud. Logging recommendations You can log identities passed through to ADLS storage in the Azure storage diagnostic logs. Logging identities allows ADLS requests to be tied to individual users from Azure Databricks clusters. Turn on diagnostic logging on your storage account to start receiving these logs: Azure Data Lake Storage Gen1: Follow the instructions in Enable diagnostic logging for your Data Lake Storage Gen1 account. Azure Data Lake Storage Gen2: Configure using PowerShell with the Set-AzStorageServiceLoggingProperty command. Specify 2.0 as the version, because log entry format 2.0 includes the user principal name in the request. Enable Azure Data Lake Storage credential passthrough for a High Concurrency cluster High concurrency clusters can be shared by multiple users. They support only Python and SQL with Azure Data Lake Storage credential passthrough. Important Enabling Azure Data Lake Storage credential passthrough for a High Concurrency cluster blocks all ports on the cluster except for ports 44, 53, and 80. When you create a cluster, set Cluster Mode to High Concurrency. Under Advanced Options, select Enable credential passthrough for user-level data access and only allow Python and SQL commands. Enable Azure Data Lake Storage credential passthrough for a Standard cluster Standard clusters with credential passthrough are limited to a single user. Standard clusters support Python, SQL, Scala, and R. On Databricks Runtime 6.0 and above, SparkR is supported; on Databricks Runtime 10.1 and above, sparklyr is supported. You must assign a user at cluster creation, but the cluster can be edited by a user with Can Manage permissions at any time to replace the original user. Important The user assigned to the cluster must have at least Can Attach To permission for the cluster in order to run commands on the cluster. Admins and the cluster creator have Can Manage permissions, but cannot run commands on the cluster unless they are the designated cluster user. When you create a cluster, set the Cluster Mode to Standard. Under Advanced Options, select Enable credential passthrough for user-level data access and select the user name from the Single User Access drop-down. Create a container Containers provide a way to organize objects in an Azure storage account. Access Azure Data Lake Storage directly using credential passthrough After configuring Azure Data Lake Storage credential passthrough and creating storage containers, you can access data directly in Azure Data Lake Storage Gen1 using an adl:// path and Azure Data Lake Storage Gen2 using an abfss:// path. Azure Data Lake Storage Gen1 Python Python Copy spark.read.format(""csv"").load(""adl://<storage-account-name>.azuredatalakestore.net/MyData.csv"").collect()
 R R Copy # SparkR
library(SparkR)
sparkR.session()
collect(read.df(""adl://<storage-account-name>.azuredatalakestore.net/MyData.csv"", source = ""csv""))

# sparklyr
library(sparklyr)
sc <- spark_connect(method = ""databricks"")
sc %>% spark_read_csv(""adl://<storage-account-name>.azuredatalakestore.net/MyData.csv"") %>% sdf_collect()
 Replace <storage-account-name> with the ADLS Gen1 storage account name. Azure Data Lake Storage Gen2 Python Python Copy spark.read.format(""csv"").load(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/MyData.csv"").collect()
 R R Copy # SparkR
library(SparkR)
sparkR.session()
collect(read.df(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/MyData.csv"", source = ""csv""))

# sparklyr
library(sparklyr)
sc <- spark_connect(method = ""databricks"")
sc %>% spark_read_csv(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/MyData.csv"") %>% sdf_collect()
 Replace <container-name> with the name of a container in the ADLS Gen2 storage account. Replace <storage-account-name> with the ADLS Gen2 storage account name. Mount Azure Data Lake Storage to DBFS using credential passthrough You can mount an Azure Data Lake Storage account or a folder inside it to Databricks File System (DBFS). The mount is a pointer to a data lake store, so the data is never synced locally. When you mount data using a cluster enabled with Azure Data Lake Storage credential passthrough, any read or write to the mount point uses your Azure AD credentials. This mount point will be visible to other users, but the only users that will have read and write access are those who: Have access to the underlying Azure Data Lake Storage storage account Are using a cluster enabled for Azure Data Lake Storage credential passthrough Azure Data Lake Storage Gen1 To mount an Azure Data Lake Storage Gen1 resource or a folder inside it, use the following commands: Python Python Copy configs = {
   ""fs.adl.oauth2.access.token.provider.type"": ""CustomAccessTokenProvider"",
   ""fs.adl.oauth2.access.token.custom.provider"": spark.conf.get(""spark.databricks.passthrough.adls.tokenProviderClassName"")
 }

 # Optionally, you can add <directory-name> to the source URI of your mount point.
 dbutils.fs.mount(
   source = ""adl://<storage-account-name>.azuredatalakestore.net/<directory-name>"",
   mount_point = ""/mnt/<mount-name>"",
   extra_configs = configs)
 Scala Scala Copy  val configs = Map(
   ""fs.adl.oauth2.access.token.provider.type"" -> ""CustomAccessTokenProvider"",
   ""fs.adl.oauth2.access.token.custom.provider"" -> spark.conf.get(""spark.databricks.passthrough.adls.tokenProviderClassName"")
 )

 // Optionally, you can add <directory-name> to the source URI of your mount point.
 dbutils.fs.mount(
   source = ""adl://<storage-account-name>.azuredatalakestore.net/<directory-name>"",
   mountPoint = ""/mnt/<mount-name>"",
   extraConfigs = configs)
 Replace <storage-account-name> with the ADLS Gen2 storage account name. Replace <mount-name> with the name of the intended mount point in DBFS. Azure Data Lake Storage Gen2 To mount an Azure Data Lake Storage Gen2 filesystem or a folder inside it, use the following commands: Python Python Copy configs = {
  ""fs.azure.account.auth.type"": ""CustomAccessToken"",
  ""fs.azure.account.custom.token.provider.class"": spark.conf.get(""spark.databricks.passthrough.adls.gen2.tokenProviderClassName"")
}

# Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = ""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/"",
  mount_point = ""/mnt/<mount-name>"",
  extra_configs = configs)
 Scala Scala Copy val configs = Map(
  ""fs.azure.account.auth.type"" -> ""CustomAccessToken"",
  ""fs.azure.account.custom.token.provider.class"" -> spark.conf.get(""spark.databricks.passthrough.adls.gen2.tokenProviderClassName"")
)

// Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = ""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/"",
  mountPoint = ""/mnt/<mount-name>"",
  extraConfigs = configs)
 Replace <container-name> with the name of a container in the ADLS Gen2 storage account. Replace <storage-account-name> with the ADLS Gen2 storage account name. Replace <mount-name> with the name of the intended mount point in DBFS. Warning Do not provide your storage account access keys or service principal credentials to authenticate to the mount point. That would give other users access to the filesystem using those credentials. The purpose of Azure Data Lake Storage credential passthrough is to prevent you from having to use those credentials and to ensure that access to the filesystem is restricted to users who have access to the underlying Azure Data Lake Storage account. Security It is safe to share Azure Data Lake Storage credential passthrough clusters with other users. You will be isolated from each other and will not be able to read or use each other’s credentials. Supported features Feature Minimum Databricks Runtime Version Notes Python and SQL 5.5 Azure Data Lake Storage Gen1 5.5 %run 5.5 DBFS 5.5 Credentials are passed through only if the DBFS path resolves to a location in Azure Data Lake Storage Gen1 or Gen2. For DBFS paths that resolve to other storage systems, use a different method to specify your credentials. Azure Data Lake Storage Gen2 5.5 Delta caching 5.5 PySpark ML API 5.5 The following ML classes are not supported: * org/apache/spark/ml/classification/RandomForestClassifier * org/apache/spark/ml/clustering/BisectingKMeans * org/apache/spark/ml/clustering/GaussianMixture * org/spark/ml/clustering/KMeans * org/spark/ml/clustering/LDA * org/spark/ml/evaluation/ClusteringEvaluator * org/spark/ml/feature/HashingTF * org/spark/ml/feature/OneHotEncoder * org/spark/ml/feature/StopWordsRemover * org/spark/ml/feature/VectorIndexer * org/spark/ml/feature/VectorSizeHint * org/spark/ml/regression/IsotonicRegression * org/spark/ml/regression/RandomForestRegressor * org/spark/ml/util/DatasetUtils Broadcast variables 5.5 Within PySpark, there is a limit on the size of the Python UDFs you can construct, since large UDFs are sent as broadcast variables. Notebook-scoped libraries 5.5 Scala 5.5 SparkR 6.0 sparklyr 10.1 Modularize or link code in notebooks 6.1 PySpark ML API 6.1 All PySpark ML classes supported. Ganglia UI 6.1 Databricks Connect 7.3 Passthrough is supported on Standard clusters. Limitations The following features are not supported with Azure Data Lake Storage credential passthrough: %fs (use the equivalent dbutils.fs command instead). The Databricks REST API. Table access control. The permissions granted by Azure Data Lake Storage credential passthrough could be used to bypass the fine-grained permissions of table ACLs, while the extra restrictions of table ACLs will constrain some of the benefits you get from credential passthrough. In particular: If you have Azure AD permission to access the data files that underlie a particular table you will have full permissions on that table via the RDD API, regardless of the restrictions placed on them via table ACLs. You will be constrained by table ACLs permissions only when using the DataFrame API. You will see warnings about not having permission SELECT on any file if you try to read files directly with the DataFrame API, even though you could read those files directly via the RDD API. You will be unable to read from tables backed by filesystems other than Azure Data Lake Storage, even if you have table ACL permission to read the tables. The following methods on SparkContext (sc) and SparkSession (spark) objects: Deprecated methods. Methods such as addFile() and addJar() that would allow non-admin users to call Scala code. Any method that accesses a filesystem other than Azure Data Lake Storage Gen1 or Gen2 (to access other filesystems on a cluster with Azure Data Lake Storage credential passthrough enabled, use a different method to specify your credentials and see the section on trusted filesystems under Troubleshooting). The old Hadoop APIs (hadoopFile() and hadoopRDD()). Streaming APIs, since the passed-through credentials would expire while the stream was still running. The FUSE mount (/dbfs) is available only in Databricks Runtime 7.3 LTS and above. Mount points with credential passthrough configured are not supported through the FUSE mount. Azure Data Factory. MLflow on high concurrency clusters. azureml-sdk Python package on high concurrency clusters. You cannot extend the lifetime of Azure Active Directory passthrough tokens using Azure Active Directory token lifetime policies. As a consequence, if you send a command to the cluster that takes longer than an hour, it will fail if an Azure Data Lake Storage resource is accessed after the 1 hour mark. When using Hive 2.3 and above you can’t add a partition on a cluster with credential passthrough enabled. For more information, see the relevant troubleshooting section. Example notebooks The following notebooks demonstrate Azure Data Lake Storage credential passthrough for Azure Data Lake Storage Gen1 and Gen2. Azure Data Lake Storage Gen1 passthrough notebook Get notebook Azure Data Lake Storage Gen2 passthrough notebook Get notebook Troubleshooting py4j.security.Py4JSecurityException: … is not whitelisted This exception is thrown when you have accessed a method that Azure Databricks has not explicitly marked as safe for Azure Data Lake Storage credential passthrough clusters. In most cases, this means that the method could allow a user on a Azure Data Lake Storage credential passthrough cluster to access another user’s credentials. org.apache.spark.api.python.PythonSecurityException: Path … uses an untrusted filesystem This exception is thrown when you have tried to access a filesystem that is not known by the Azure Data Lake Storage credential passthrough cluster to be safe. Using an untrusted filesystem might allow a user on a Azure Data Lake Storage credential passthrough cluster to access another user’s credentials, so we disallow all filesystems that we are not confident are being used safely. To configure the set of trusted filesystems on a Azure Data Lake Storage credential passthrough cluster, set the Spark conf key spark.databricks.pyspark.trustedFilesystems on that cluster to be a comma-separated list of the class names that are trusted implementations of org.apache.hadoop.fs.FileSystem. Adding a partition fails with AzureCredentialNotFoundException when credential passthrough is enabled When using Hive 2.3-3.1, if you try to add a partition on a cluster with credential passthrough enabled, the following exception occurs: Console Copy org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:com.databricks.backend.daemon.data.client.adl.AzureCredentialNotFoundException: Could not find ADLS Gen2 Token
 To work around this issue, add partitions on a cluster without credential passthrough enabled.",Access Azure Data Lake Storage using Azure Active Directory credential passthrough
6,Users can create databases poinint to ADLS gen2 storage container on which they do not have permissions in databricks workspace,"Hello team,

Users can create databases pointing to ADLS gen2 storage container on which they do not have permissions in databricks workspace.

This is a security concern.
Consider below example which is shared by cx:
We have not enabled Table Access Control in Data Science workspace as we use credentials passthrough and it supports our needs in a better way.
On SQL workspace we are using Table Access Control as that's the only option available, and we have configured a SP to be able to read and write data to all containers in ADLS gen2.
Now, a user creates a database using data science workspace pointing to a container on which he/she has no access. Databricks allows user to do so.
From SQL workspace user can write data to the database as he/she is owner of the database and SQL endpoint has a SP configured to be able to read/write to all container in ADLS gen2.
Shouldn't databricks validate if a user has access to the specified directory/container while the user creates an external database from Data science workspace?
The business impact is that we are not able to launch SQL Analytics for multiple teams in same workspace if we are not able to set table ACL on the data science and engineering cluster.  

The original issue was that a user can create a database using data science and engg cluster (with table acl disabled) and in SQL analytics, the user can use that database and create table writing to a container that user doesnt have access to. (As SQL analytics uses a workspace level service principal with access to datalake storage)

Below are some scenarios:

1)  The UI shows table ACL and ADLS passthrough are not compatible- refer to file 1 attached
2) In ML run times (DBR 10.5 ML or 10.4LTS ML), if we try to switch to another ML run time, it gives error- - refer to file 2 attached
3) With automation (API/terraform), we are not able to create cluster using same spark config and DBR, so it is not clear whether this is officially supported.

We don't want to have multiple business teams start using SQL Analytics without proper security in place with table ACL.

For reference, this is the spark config which cx is using:

spark.driver.extraJavaOptions ""-Dlog4j2.formatMsgNoLookups=true""
spark.databricks.delta.preview.enabled true
spark.databricks.passthrough.enabled true
spark.databricks.pyspark.iptable.outbound.whitelisted.ports 1521,8089,25,9243
spark.databricks.repl.allowedLanguages sql,python
spark.databricks.acl.dfAclsEnabled true
spark.databricks.cluster.profile serverless
spark.executor.extraJavaOptions ""-Dlog4j2.formatMsgNoLookups=true""
spark.databricks.pyspark.enableProcessIsolation true
spark.databricks.pyspark.dbconnect.enableProcessIsolation true

Below are the cx queries:
1) Is table ACL supported on ML runtimes?
2) Are there any issues with having table ACL and ADLS passthrough together in high concurrency clusters (ML or non-ML runtimes)
3) If yes to above, I would expect we should be able to create databricks clusters with terraform (with the spark configs for table ACL and ADLS passthrough). Please confirm.
https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/cluster

Kindly help us in this.",https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Access Azure Data Lake Storage using Azure Active Directory credential passthrough Article 07/12/2022 11 minutes to read 6 contributors In this article Requirements Logging recommendations Enable Azure Data Lake Storage credential passthrough for a High Concurrency cluster Enable Azure Data Lake Storage credential passthrough for a Standard cluster Create a container Access Azure Data Lake Storage directly using credential passthrough Mount Azure Data Lake Storage to DBFS using credential passthrough Security Supported features Limitations Example notebooks Troubleshooting Note This article contains references to the term whitelisted, a term that Azure Databricks does not use. When the term is removed from the software, we’ll remove it from this article. You can authenticate automatically to Accessing Azure Data Lake Storage Gen1 from Azure Databricks (ADLS Gen1) and ADLS Gen2 from Azure Databricks clusters using the same Azure Active Directory (Azure AD) identity that you use to log into Azure Databricks. When you enable Azure Data Lake Storage credential passthrough for your cluster, commands that you run on that cluster can read and write data in Azure Data Lake Storage without requiring you to configure service principal credentials for access to storage. Azure Data Lake Storage credential passthrough is supported with Azure Data Lake Storage Gen1 and Gen2 only. Azure Blob storage does not support credential passthrough. This article covers: Enabling credential passthrough for standard and high-concurrency clusters. Configuring credential passthrough and initializing storage resources in ADLS accounts. Accessing ADLS resources directly when credential passthrough is enabled. Accessing ADLS resources through a mount point when credential passthrough is enabled. Supported features and limitations when using credential passthrough. Notebooks are included to provide examples of using credential passthrough with ADLS Gen1 and ADLS Gen2 storage accounts. Requirements Premium Plan. See Upgrade or Downgrade an Azure Databricks Workspace for details on upgrading a standard plan to a premium plan. An Azure Data Lake Storage Gen1 or Gen2 storage account. Azure Data Lake Storage Gen2 storage accounts must use the hierarchical namespace to work with Azure Data Lake Storage credential passthrough. See Create a storage account for instructions on creating a new ADLS Gen2 account, including how to enable the hierarchical namespace. Properly configured user permissions to Azure Data Lake Storage. An Azure Databricks administrator needs to ensure that users have the correct roles, for example, Storage Blob Data Contributor, to read and write data stored in Azure Data Lake Storage. See Use the Azure portal to assign an Azure role for access to blob and queue data. You cannot use a cluster configured with ADLS credentials, for example, service principal credentials, with credential passthrough. Important You cannot authenticate to Azure Data Lake Storage with your Azure Active Directory credentials if you are behind a firewall that has not been configured to allow traffic to Azure Active Directory. Azure Firewall blocks Active Directory access by default. To allow access, configure the AzureActiveDirectory service tag. You can find equivalent information for network virtual appliances under the AzureActiveDirectory tag in the Azure IP Ranges and Service Tags JSON file. For more information, see Azure Firewall service tags and Azure IP Addresses for Public Cloud. Logging recommendations You can log identities passed through to ADLS storage in the Azure storage diagnostic logs. Logging identities allows ADLS requests to be tied to individual users from Azure Databricks clusters. Turn on diagnostic logging on your storage account to start receiving these logs: Azure Data Lake Storage Gen1: Follow the instructions in Enable diagnostic logging for your Data Lake Storage Gen1 account. Azure Data Lake Storage Gen2: Configure using PowerShell with the Set-AzStorageServiceLoggingProperty command. Specify 2.0 as the version, because log entry format 2.0 includes the user principal name in the request. Enable Azure Data Lake Storage credential passthrough for a High Concurrency cluster High concurrency clusters can be shared by multiple users. They support only Python and SQL with Azure Data Lake Storage credential passthrough. Important Enabling Azure Data Lake Storage credential passthrough for a High Concurrency cluster blocks all ports on the cluster except for ports 44, 53, and 80. When you create a cluster, set Cluster Mode to High Concurrency. Under Advanced Options, select Enable credential passthrough for user-level data access and only allow Python and SQL commands. Enable Azure Data Lake Storage credential passthrough for a Standard cluster Standard clusters with credential passthrough are limited to a single user. Standard clusters support Python, SQL, Scala, and R. On Databricks Runtime 6.0 and above, SparkR is supported; on Databricks Runtime 10.1 and above, sparklyr is supported. You must assign a user at cluster creation, but the cluster can be edited by a user with Can Manage permissions at any time to replace the original user. Important The user assigned to the cluster must have at least Can Attach To permission for the cluster in order to run commands on the cluster. Admins and the cluster creator have Can Manage permissions, but cannot run commands on the cluster unless they are the designated cluster user. When you create a cluster, set the Cluster Mode to Standard. Under Advanced Options, select Enable credential passthrough for user-level data access and select the user name from the Single User Access drop-down. Create a container Containers provide a way to organize objects in an Azure storage account. Access Azure Data Lake Storage directly using credential passthrough After configuring Azure Data Lake Storage credential passthrough and creating storage containers, you can access data directly in Azure Data Lake Storage Gen1 using an adl:// path and Azure Data Lake Storage Gen2 using an abfss:// path. Azure Data Lake Storage Gen1 Python Python Copy spark.read.format(""csv"").load(""adl://<storage-account-name>.azuredatalakestore.net/MyData.csv"").collect()
 R R Copy # SparkR
library(SparkR)
sparkR.session()
collect(read.df(""adl://<storage-account-name>.azuredatalakestore.net/MyData.csv"", source = ""csv""))

# sparklyr
library(sparklyr)
sc <- spark_connect(method = ""databricks"")
sc %>% spark_read_csv(""adl://<storage-account-name>.azuredatalakestore.net/MyData.csv"") %>% sdf_collect()
 Replace <storage-account-name> with the ADLS Gen1 storage account name. Azure Data Lake Storage Gen2 Python Python Copy spark.read.format(""csv"").load(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/MyData.csv"").collect()
 R R Copy # SparkR
library(SparkR)
sparkR.session()
collect(read.df(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/MyData.csv"", source = ""csv""))

# sparklyr
library(sparklyr)
sc <- spark_connect(method = ""databricks"")
sc %>% spark_read_csv(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/MyData.csv"") %>% sdf_collect()
 Replace <container-name> with the name of a container in the ADLS Gen2 storage account. Replace <storage-account-name> with the ADLS Gen2 storage account name. Mount Azure Data Lake Storage to DBFS using credential passthrough You can mount an Azure Data Lake Storage account or a folder inside it to Databricks File System (DBFS). The mount is a pointer to a data lake store, so the data is never synced locally. When you mount data using a cluster enabled with Azure Data Lake Storage credential passthrough, any read or write to the mount point uses your Azure AD credentials. This mount point will be visible to other users, but the only users that will have read and write access are those who: Have access to the underlying Azure Data Lake Storage storage account Are using a cluster enabled for Azure Data Lake Storage credential passthrough Azure Data Lake Storage Gen1 To mount an Azure Data Lake Storage Gen1 resource or a folder inside it, use the following commands: Python Python Copy configs = {
   ""fs.adl.oauth2.access.token.provider.type"": ""CustomAccessTokenProvider"",
   ""fs.adl.oauth2.access.token.custom.provider"": spark.conf.get(""spark.databricks.passthrough.adls.tokenProviderClassName"")
 }

 # Optionally, you can add <directory-name> to the source URI of your mount point.
 dbutils.fs.mount(
   source = ""adl://<storage-account-name>.azuredatalakestore.net/<directory-name>"",
   mount_point = ""/mnt/<mount-name>"",
   extra_configs = configs)
 Scala Scala Copy  val configs = Map(
   ""fs.adl.oauth2.access.token.provider.type"" -> ""CustomAccessTokenProvider"",
   ""fs.adl.oauth2.access.token.custom.provider"" -> spark.conf.get(""spark.databricks.passthrough.adls.tokenProviderClassName"")
 )

 // Optionally, you can add <directory-name> to the source URI of your mount point.
 dbutils.fs.mount(
   source = ""adl://<storage-account-name>.azuredatalakestore.net/<directory-name>"",
   mountPoint = ""/mnt/<mount-name>"",
   extraConfigs = configs)
 Replace <storage-account-name> with the ADLS Gen2 storage account name. Replace <mount-name> with the name of the intended mount point in DBFS. Azure Data Lake Storage Gen2 To mount an Azure Data Lake Storage Gen2 filesystem or a folder inside it, use the following commands: Python Python Copy configs = {
  ""fs.azure.account.auth.type"": ""CustomAccessToken"",
  ""fs.azure.account.custom.token.provider.class"": spark.conf.get(""spark.databricks.passthrough.adls.gen2.tokenProviderClassName"")
}

# Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = ""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/"",
  mount_point = ""/mnt/<mount-name>"",
  extra_configs = configs)
 Scala Scala Copy val configs = Map(
  ""fs.azure.account.auth.type"" -> ""CustomAccessToken"",
  ""fs.azure.account.custom.token.provider.class"" -> spark.conf.get(""spark.databricks.passthrough.adls.gen2.tokenProviderClassName"")
)

// Optionally, you can add <directory-name> to the source URI of your mount point.
dbutils.fs.mount(
  source = ""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/"",
  mountPoint = ""/mnt/<mount-name>"",
  extraConfigs = configs)
 Replace <container-name> with the name of a container in the ADLS Gen2 storage account. Replace <storage-account-name> with the ADLS Gen2 storage account name. Replace <mount-name> with the name of the intended mount point in DBFS. Warning Do not provide your storage account access keys or service principal credentials to authenticate to the mount point. That would give other users access to the filesystem using those credentials. The purpose of Azure Data Lake Storage credential passthrough is to prevent you from having to use those credentials and to ensure that access to the filesystem is restricted to users who have access to the underlying Azure Data Lake Storage account. Security It is safe to share Azure Data Lake Storage credential passthrough clusters with other users. You will be isolated from each other and will not be able to read or use each other’s credentials. Supported features Feature Minimum Databricks Runtime Version Notes Python and SQL 5.5 Azure Data Lake Storage Gen1 5.5 %run 5.5 DBFS 5.5 Credentials are passed through only if the DBFS path resolves to a location in Azure Data Lake Storage Gen1 or Gen2. For DBFS paths that resolve to other storage systems, use a different method to specify your credentials. Azure Data Lake Storage Gen2 5.5 Delta caching 5.5 PySpark ML API 5.5 The following ML classes are not supported: * org/apache/spark/ml/classification/RandomForestClassifier * org/apache/spark/ml/clustering/BisectingKMeans * org/apache/spark/ml/clustering/GaussianMixture * org/spark/ml/clustering/KMeans * org/spark/ml/clustering/LDA * org/spark/ml/evaluation/ClusteringEvaluator * org/spark/ml/feature/HashingTF * org/spark/ml/feature/OneHotEncoder * org/spark/ml/feature/StopWordsRemover * org/spark/ml/feature/VectorIndexer * org/spark/ml/feature/VectorSizeHint * org/spark/ml/regression/IsotonicRegression * org/spark/ml/regression/RandomForestRegressor * org/spark/ml/util/DatasetUtils Broadcast variables 5.5 Within PySpark, there is a limit on the size of the Python UDFs you can construct, since large UDFs are sent as broadcast variables. Notebook-scoped libraries 5.5 Scala 5.5 SparkR 6.0 sparklyr 10.1 Modularize or link code in notebooks 6.1 PySpark ML API 6.1 All PySpark ML classes supported. Ganglia UI 6.1 Databricks Connect 7.3 Passthrough is supported on Standard clusters. Limitations The following features are not supported with Azure Data Lake Storage credential passthrough: %fs (use the equivalent dbutils.fs command instead). The Databricks REST API. Table access control. The permissions granted by Azure Data Lake Storage credential passthrough could be used to bypass the fine-grained permissions of table ACLs, while the extra restrictions of table ACLs will constrain some of the benefits you get from credential passthrough. In particular: If you have Azure AD permission to access the data files that underlie a particular table you will have full permissions on that table via the RDD API, regardless of the restrictions placed on them via table ACLs. You will be constrained by table ACLs permissions only when using the DataFrame API. You will see warnings about not having permission SELECT on any file if you try to read files directly with the DataFrame API, even though you could read those files directly via the RDD API. You will be unable to read from tables backed by filesystems other than Azure Data Lake Storage, even if you have table ACL permission to read the tables. The following methods on SparkContext (sc) and SparkSession (spark) objects: Deprecated methods. Methods such as addFile() and addJar() that would allow non-admin users to call Scala code. Any method that accesses a filesystem other than Azure Data Lake Storage Gen1 or Gen2 (to access other filesystems on a cluster with Azure Data Lake Storage credential passthrough enabled, use a different method to specify your credentials and see the section on trusted filesystems under Troubleshooting). The old Hadoop APIs (hadoopFile() and hadoopRDD()). Streaming APIs, since the passed-through credentials would expire while the stream was still running. The FUSE mount (/dbfs) is available only in Databricks Runtime 7.3 LTS and above. Mount points with credential passthrough configured are not supported through the FUSE mount. Azure Data Factory. MLflow on high concurrency clusters. azureml-sdk Python package on high concurrency clusters. You cannot extend the lifetime of Azure Active Directory passthrough tokens using Azure Active Directory token lifetime policies. As a consequence, if you send a command to the cluster that takes longer than an hour, it will fail if an Azure Data Lake Storage resource is accessed after the 1 hour mark. When using Hive 2.3 and above you can’t add a partition on a cluster with credential passthrough enabled. For more information, see the relevant troubleshooting section. Example notebooks The following notebooks demonstrate Azure Data Lake Storage credential passthrough for Azure Data Lake Storage Gen1 and Gen2. Azure Data Lake Storage Gen1 passthrough notebook Get notebook Azure Data Lake Storage Gen2 passthrough notebook Get notebook Troubleshooting py4j.security.Py4JSecurityException: … is not whitelisted This exception is thrown when you have accessed a method that Azure Databricks has not explicitly marked as safe for Azure Data Lake Storage credential passthrough clusters. In most cases, this means that the method could allow a user on a Azure Data Lake Storage credential passthrough cluster to access another user’s credentials. org.apache.spark.api.python.PythonSecurityException: Path … uses an untrusted filesystem This exception is thrown when you have tried to access a filesystem that is not known by the Azure Data Lake Storage credential passthrough cluster to be safe. Using an untrusted filesystem might allow a user on a Azure Data Lake Storage credential passthrough cluster to access another user’s credentials, so we disallow all filesystems that we are not confident are being used safely. To configure the set of trusted filesystems on a Azure Data Lake Storage credential passthrough cluster, set the Spark conf key spark.databricks.pyspark.trustedFilesystems on that cluster to be a comma-separated list of the class names that are trusted implementations of org.apache.hadoop.fs.FileSystem. Adding a partition fails with AzureCredentialNotFoundException when credential passthrough is enabled When using Hive 2.3-3.1, if you try to add a partition on a cluster with credential passthrough enabled, the following exception occurs: Console Copy org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:com.databricks.backend.daemon.data.client.adl.AzureCredentialNotFoundException: Could not find ADLS Gen2 Token
 To work around this issue, add partitions on a cluster without credential passthrough enabled.",Access Azure Data Lake Storage using Azure Active Directory credential passthrough
7,Cluster & Worker node spin time,"Customer created a pool of woker nodes with pre-loaded docker image. 

We are just looking a reason why addtional worker node spin up taking 3-4 mins when pool already have pre-loader worker nodes 



https://adb-145468050392161.1.azuredatabricks.net/?o=145468050392161#setting/clusters/0310-114804-ib6akkum/configuration


We are just looking a reason why addtional worker node spin up taking 3-4 mins when pool already have pre-loader worker nodes ;
Is this a new problem, or it has happened before? - Never worked;
Cluster URL if available - https://adb-145468050392161.1.azuredatabricks.net/?o=145468050392161#setting/clusters/0310-114804-ib6akkum/configuration;
Workspace ID if cluster URL is not available - ;
Additional details about the issue - NA;

- ProblemStartTime: 04/07/2022 04:00:00
- Cloud: Azure
- AzureProductSubscriptionID: 2ecc22fb-9c1f-49aa-be87-701d38f92490
- AzureProductSubscriptionName: CONSUMER-CSE-NONPROD-US
- PUID: 10032000D79B9A87
- Tenant Id: 6ac7a1f4-5fb1-4153-bb4f-12d2020a1f7d
- Object Id: 4d84aa40-2f75-4b72-a87a-e15c5c1f57ab
- SubscriptionType: UnifiedEnterprise
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Unified Enterprise
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: False

- ResourceUri: 

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/clusters/instance-pools/pool-best-practices,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Best practices: pools Article 01/26/2022 4 minutes to read 3 contributors In this article Create pools based on workloads Tag pools to manage cost and billing Configure pools to control cost Pre-populate pools Learn more Clusters provide the computation resources and configurations that run your notebooks and jobs. Clusters run on instances provisioned by your cloud provider on demand. The Azure Databricks platform provides an efficient and cost-effective way to manage your analytics infrastructure. This article shows how to address the following challenges when creating new clusters or scaling up existing clusters: The execution time of your Azure Databricks job might be shorter than the time to provision instances and start a new cluster. When autoscaling is enabled on a cluster, it takes time for the cloud provider to provision new instances. This can negatively impact jobs with strict performance requirements or varying workloads. Azure Databricks pools reduce cluster start and scale-up times by maintaining a set of available, ready-to-use instances. You can use a different pool for the driver node and worker nodes. For an introduction to pools and configuration recommendations, view the following video: As shown in the following diagram, when a cluster attached to a pool needs an instance, it first attempts to allocate one of the pool’s available instances. If the pool has no available instances, it expands by allocating a new instance from the cloud provider to accommodate the cluster’s request. When a cluster releases an instance, the instance returns to the pool and is free for use by another cluster. Only clusters attached to a pool can use that pool’s available instances. This article discusses the following best practices to ensure the best performance at the lowest cost when you use pools: Create pools using instance types and Azure Databricks runtimes based on target workloads. When possible, populate pools with spot instances to reduce costs. Populate pools with on-demand instances for jobs with short execution times and strict execution time requirements. Use pool tags and cluster tags to manage billing. Use pool configuration options to minimize cost. Pre-populate pools to make sure instances are available when clusters need them. Create pools based on workloads If your driver node and worker nodes have different requirements, create a different pool for each. You can minimize instance acquisition time by creating a pool for each instance type and Azure Databricks runtime your organization commonly uses. For example, if most data engineering clusters use instance type A, data science clusters use instance type B, and analytics clusters use instance type C, create a pool with each instance type. Configure pools to use on-demand instances for jobs with short execution times and strict execution time requirements. Use on-demand instances to prevent acquired instances from being lost to a higher bidder on the spot market. Configure pools to use spot instances for clusters that support interactive development or jobs that prioritize cost savings over reliability. Tag pools to manage cost and billing Tagging pools to the correct cost center allows you to manage cost and usage chargeback. You can use multiple custom tags to associate multiple cost centers to a pool. However, it’s important to understand how tags are propagated when a cluster is created from pools. As shown in the following diagram, tags from the pools propagate to the underlying cloud provider instances, but the cluster’s tags do not. Apply all custom tags required for managing chargeback of the cloud provider compute cost to the pool. Pool tags and cluster tags both propagate to Azure Databricks billing. You can use the combination of cluster and pool tags to manage chargeback of Azure Databricks Units. To learn more, see Monitor usage using cluster, pool, and workspace tags Configure pools to control cost You can use the following configuration options to help control the cost of pools: Set the Min Idle instances to 0 to avoid paying for running instances that aren’t doing work. The tradeoff is a possible increase in time when a cluster needs to acquire a new instance. Set the Idle Instance Auto Termination time to provide a buffer between when the instance is released from the cluster and when it’s dropped from the pool. Set this to a period that allows you to minimize cost while ensuring the availability of instances for scheduled jobs. For example, job A is scheduled to run at 8:00 AM and takes 40 minutes to complete. Job B is scheduled to run at 9:00 AM and takes 30 minutes to complete. Set the Idle Instance Auto Termination value to 20 minutes to ensure that instances returned to the pool when job A completes are available when job B starts. Unless they are claimed by another cluster, those instances are terminated 20 minutes after job B ends. Set the Max Capacity based on anticipated usage. This sets the ceiling for the maximum number of used and idle instances in the pool. If a job or cluster requests an instance from a pool at its maximum capacity, the request fails, and the cluster doesn’t acquire more instances. Therefore, Databricks recommends that you set the maximum capacity only if there is a strict instance quota or budget constraint. Pre-populate pools To benefit fully from pools, you can pre-populate newly created pools. Set the Min Idle instances greater than zero in the pool configuration. Alternatively, if you’re following the recommendation to set this value to zero, use a starter job to ensure that newly created pools have available instances for clusters to access. With the starter job approach, schedule a job with flexible execution time requirements to run before jobs with more strict performance requirements or before users start using interactive clusters. After the job finishes, the instances used for the job are released back to the pool. Set Min Idle instance setting to 0 and set the Idle Instance Auto Termination time high enough to ensure that idle instances remain available for subsequent jobs. Using a starter job allows the pool instances to spin up, populate the pool, and remain available for downstream job or interactive clusters. Learn more Learn more about Azure Databricks pools.",Best practices: pools
8,2205060040006402  Not able to run the databricks code.,"Question: What time did the problem begin?
Answer: Thu, May 5, 2022, 12:00 AM (UTC-05:00) Eastern Time (US & Canada)

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer: 

Question: What error message you received?
Answer: Waiting for Azure Synapse Analytics to load intermediate data from abfss://dap-de-dls-dev-data@sttecodlseastusdev2.dfs.core.windows.net/data/foundation/meter contract/2022-05-06/20-02-55-276/d826f23a-80ca-40c8-96d2-b8db4550c310/ into 'fnd'.'stg_fmeterContract' using Polybase.



avaError: An error occurred while calling o1573.save.
: com.databricks.spark.sqldw.SqlDWConnectorException: Exception encountered in Azure Synapse Analytics connector code.
at com.databricks.spark.sqldw.Utils$.wrapExceptions(Utils.scala:689)
at com.databricks.spark.sqldw.DefaultSource.createRelation(DefaultSource.scala:89)
at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:96)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:196)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:240)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:236)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)
at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:167)
at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:166)
at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1079)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)
at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)
at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1079)
at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:468)
at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:438)
at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:311)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
a",https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/synapse-analytics,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Azure Synapse Analytics Article 07/12/2022 27 minutes to read 6 contributors In this article Requirements Authentication Streaming support Usage (Batch) Usage (Streaming) Configuration Frequently asked questions (FAQ) Azure Synapse Analytics (formerly SQL Data Warehouse) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data. Use Azure as a key component of a big data solution. Import big data into Azure with simple PolyBase T-SQL queries, or COPY statement and then use the power of MPP to run high-performance analytics. As you integrate and analyze, the data warehouse will become the single version of truth your business can count on for insights. You can access Azure Synapse from Azure Databricks using the Azure Synapse connector, a data source implementation for Apache Spark that uses Azure Blob storage, and PolyBase or the COPY statement in Azure Synapse to transfer large volumes of data efficiently between an Azure Databricks cluster and an Azure Synapse instance. Both the Azure Databricks cluster and the Azure Synapse instance access a common Blob storage container to exchange data between these two systems. In Azure Databricks, Apache Spark jobs are triggered by the Azure Synapse connector to read data from and write data to the Blob storage container. On the Azure Synapse side, data loading and unloading operations performed by PolyBase are triggered by the Azure Synapse connector through JDBC. In Databricks Runtime 7.0 and above, COPY is used by default to load data into Azure Synapse by the Azure Synapse connector through JDBC. Note COPY is available only on Azure Synapse Gen2 instances, which provide better performance. If your database still uses Gen1 instances, we recommend that you migrate the database to Gen2. The Azure Synapse connector is more suited to ETL than to interactive queries, because each query execution can extract large amounts of data to Blob storage. If you plan to perform several queries against the same Azure Synapse table, we recommend that you save the extracted data in a format such as Parquet. Requirements A database master key for the Azure Synapse. Authentication The Azure Synapse connector uses three types of network connections: Spark driver to Azure Synapse Spark driver and executors to Azure storage account Azure Synapse to Azure storage account Copy                                  ┌─────────┐
      ┌─────────────────────────>│ STORAGE │<────────────────────────┐
      │   Storage acc key /      │ ACCOUNT │  Storage acc key /      │
      │   Managed Service ID /   └─────────┘  OAuth 2.0 /            │
      │                               │                              │
      │                               │                              │
      │                               │ Storage acc key /            │
      │                               │ OAuth 2.0 /                  │
      │                               │                              │
      v                               v                       ┌──────v────┐
┌──────────┐                      ┌──────────┐                │┌──────────┴┐
│ Synapse  │                      │  Spark   │                ││ Spark     │
│ Analytics│<────────────────────>│  Driver  │<───────────────>│ Executors │
└──────────┘  JDBC with           └──────────┘    Configured   └───────────┘
              username & password /                in Spark
 The following sections describe each connection’s authentication configuration options. Spark driver to Azure Synapse The Spark driver can connect to Azure Synapse using JDBC with a username and password or OAuth 2.0 with a service principal for authentication. Username and password We recommend that you use the connection strings provided by Azure portal for both authentication types, which enable Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for encrypt=true in the connection string. To allow the Spark driver to reach Azure Synapse, we recommend that you set Allow access to Azure services to ON on the firewall pane of the Azure Synapse server through Azure portal. This setting allows communications from all Azure IP addresses and all Azure subnets, which allows Spark drivers to reach the Azure Synapse instance. OAuth 2.0 with a service principal You can authenticate to Azure Synapse Analytics using a service principal with access to the underlying storage account. For more information on using service principal credentials to access an Azure storage account, see Accessing Azure Data Lake Storage Gen2 and Blob Storage with Azure Databricks. You must set the enableServicePrincipalAuth option to true in the connection configuration Parameters to enable the connector to authenticate with a service principal. You can optionally use a different service principal for the Azure Synapse Analytics connection. An example that configures service principal credentials for the storage account and optional service principal credentials for Synapse: ini ini Copy ; Defining the Service Principal credentials for the Azure storage account
fs.azure.account.auth.type OAuth
fs.azure.account.oauth.provider.type org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider
fs.azure.account.oauth2.client.id <application-id>
fs.azure.account.oauth2.client.secret <service-credential>
fs.azure.account.oauth2.client.endpoint https://login.microsoftonline.com/<directory-id>/oauth2/token

; Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)
spark.databricks.sqldw.jdbc.service.principal.client.id <application-id>
spark.databricks.sqldw.jdbc.service.principal.client.secret <service-credential>
 Scala Scala Copy // Defining the Service Principal credentials for the Azure storage account
spark.conf.set(""fs.azure.account.auth.type"", ""OAuth"")
spark.conf.set(""fs.azure.account.oauth.provider.type"",  ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"")
spark.conf.set(""fs.azure.account.oauth2.client.id"", ""<application-id>"")
spark.conf.set(""fs.azure.account.oauth2.client.secret"", ""<service-credential>"")
spark.conf.set(""fs.azure.account.oauth2.client.endpoint"", ""https://login.microsoftonline.com/<directory-id>/oauth2/token"")

// Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)
spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.id"", ""<application-id>"")
spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.secret"", ""<service-credential>"")
 Python Python Copy # Defining the service principal credentials for the Azure storage account
spark.conf.set(""fs.azure.account.auth.type"", ""OAuth"")
spark.conf.set(""fs.azure.account.oauth.provider.type"",  ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"")
spark.conf.set(""fs.azure.account.oauth2.client.id"", ""<application-id>"")
spark.conf.set(""fs.azure.account.oauth2.client.secret"", ""<service-credential>"")
spark.conf.set(""fs.azure.account.oauth2.client.endpoint"", ""https://login.microsoftonline.com/<directory-id>/oauth2/token"")

# Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)
spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.id"", ""<application-id>"")
spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.secret"", ""<service-credential>"")
 R R Copy # Load SparkR
library(SparkR)
conf <- sparkR.callJMethod(sparkR.session(), ""conf"")

# Defining the service principal credentials for the Azure storage account
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.auth.type"", ""OAuth"")
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.oauth.provider.type"",  ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"")
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.oauth2.client.id"", ""<application-id>"")
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.oauth2.client.secret"", ""<service-credential>"")
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.oauth2.client.endpoint"", ""https://login.microsoftonline.com/<directory-id>/oauth2/token"")

# Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)
sparkR.callJMethod(conf, ""set"", ""spark.databricks.sqldw.jdbc.service.principal.client.id"", ""<application-id>"")
sparkR.callJMethod(conf, ""set"", ""spark.databricks.sqldw.jdbc.service.principal.client.secret"", ""<service-credential>"")
 Spark driver and executors to Azure storage account The Azure storage container acts as an intermediary to store bulk data when reading from or writing to Azure Synapse. Spark connects to ADLS Gen2 or Blob Storage using the abfss driver. The following authentication options are available: Storage account access key and secret OAuth 2.0 authentication. For more information about OAuth 2.0 and Service Principal, see Configure access to Azure storage with an Azure Active Directory service principal. The examples below illustrate these two ways using the storage account access key approach. The same applies to OAuth 2.0 configuration. Notebook session configuration (preferred) Using this approach, the account access key is set in the session configuration associated with the notebook that runs the command. This configuration does not affect other notebooks attached to the same cluster. spark is the SparkSession object provided in the notebook. Python Copy spark.conf.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")
 Global Hadoop configuration This approach updates the global Hadoop configuration associated with the SparkContext object shared by all notebooks. Scala Scala Copy sc.hadoopConfiguration.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")
 Python hadoopConfiguration is not exposed in all versions of PySpark. Although the following command relies on some Spark internals, it should work with all PySpark versions and is unlikely to break or change in the future: Python Copy sc._jsc.hadoopConfiguration().set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")
 Azure Synapse to Azure storage account Azure Synapse also connects to a storage account during loading and unloading of temporary data. In case you have set up an account key and secret for the storage account, you can set forwardSparkAzureStorageCredentials to true, in which case Azure Synapse connector automatically discovers the account access key set in the notebook session configuration or the global Hadoop configuration and forwards the storage account access key to the connected Azure Synapse instance by creating a temporary Azure database scoped credential. Alternatively, if you use ADLS Gen2 + OAuth 2.0 authentication or your Azure Synapse instance is configured to have a Managed Service Identity (typically in conjunction with a VNet + Service Endpoints setup), you must set useAzureMSI to true. In this case the connector will specify IDENTITY = 'Managed Service Identity' for the databased scoped credential and no SECRET. Streaming support The Azure Synapse connector offers efficient and scalable Structured Streaming write support for Azure Synapse that provides consistent user experience with batch writes, and uses PolyBase or COPY for large data transfers between an Azure Databricks cluster and Azure Synapse instance. Similar to the batch writes, streaming is designed largely for ETL, thus providing higher latency that may not be suitable for real-time data processing in some cases. Fault tolerance semantics By default, Azure Synapse Streaming offers end-to-end exactly-once guarantee for writing data into an Azure Synapse table by reliably tracking progress of the query using a combination of checkpoint location in DBFS, checkpoint table in Azure Synapse, and locking mechanism to ensure that streaming can handle any types of failures, retries, and query restarts. Optionally, you can select less restrictive at-least-once semantics for Azure Synapse Streaming by setting spark.databricks.sqldw.streaming.exactlyOnce.enabled option to false, in which case data duplication could occur in the event of intermittent connection failures to Azure Synapse or unexpected query termination. Usage (Batch) You can use this connector via the data source API in Scala, Python, SQL, and R notebooks. Scala Scala Copy 
// Otherwise, set up the Blob storage account access key in the notebook session conf.
spark.conf.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")

// Get some data from an Azure Synapse table.
val df: DataFrame = spark.read
  .format(""com.databricks.spark.sqldw"")
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"")
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
  .option(""forwardSparkAzureStorageCredentials"", ""true"")
  .option(""dbTable"", ""<your-table-name>"")
  .load()

// Load data from an Azure Synapse query.
val df: DataFrame = spark.read
  .format(""com.databricks.spark.sqldw"")
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"")
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
  .option(""forwardSparkAzureStorageCredentials"", ""true"")
  .option(""query"", ""select x, count(*) as cnt from table group by x"")
  .load()

// Apply some transformations to the data, then use the
// Data Source API to write the data back to another table in Azure Synapse.

df.write
  .format(""com.databricks.spark.sqldw"")
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"")
  .option(""forwardSparkAzureStorageCredentials"", ""true"")
  .option(""dbTable"", ""<your-table-name>"")
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
  .save()
 Python Python Copy 
# Otherwise, set up the Blob storage account access key in the notebook session conf.
spark.conf.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")

# Get some data from an Azure Synapse table.
df = spark.read \
  .format(""com.databricks.spark.sqldw"") \
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"") \
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"") \
  .option(""forwardSparkAzureStorageCredentials"", ""true"") \
  .option(""dbTable"", ""<your-table-name>"") \
  .load()

# Load data from an Azure Synapse query.
df = spark.read \
  .format(""com.databricks.spark.sqldw"") \
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"") \
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"") \
  .option(""forwardSparkAzureStorageCredentials"", ""true"") \
  .option(""query"", ""select x, count(*) as cnt from table group by x"") \
  .load()

# Apply some transformations to the data, then use the
# Data Source API to write the data back to another table in Azure Synapse.

df.write \
  .format(""com.databricks.spark.sqldw"") \
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"") \
  .option(""forwardSparkAzureStorageCredentials"", ""true"") \
  .option(""dbTable"", ""<your-table-name>"") \
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"") \
  .save()
 SQL SQL Copy 
-- Otherwise, set up the Blob storage account access key in the notebook session conf.
SET fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net=<your-storage-account-access-key>;

-- Read data using SQL.
CREATE TABLE example_table_in_spark_read
USING com.databricks.spark.sqldw
OPTIONS (
  url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',
  forwardSparkAzureStorageCredentials 'true',
  dbTable '<your-table-name>',
  tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'
);

-- Write data using SQL.
-- Create a new table, throwing an error if a table with the same name already exists:

CREATE TABLE example_table_in_spark_write
USING com.databricks.spark.sqldw
OPTIONS (
  url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',
  forwardSparkAzureStorageCredentials 'true',
  dbTable '<your-table-name>',
  tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'
)
AS SELECT * FROM table_to_save_in_spark;
 R R Copy # Load SparkR
library(SparkR)

# Otherwise, set up the Blob storage account access key in the notebook session conf.
conf <- sparkR.callJMethod(sparkR.session(), ""conf"")
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"", ""<your-storage-account-access-key>"")

# Get some data from an Azure Synapse table.
df <- read.df(
   source = ""com.databricks.spark.sqldw"",
   url = ""jdbc:sqlserver://<the-rest-of-the-connection-string>"",
   forward_spark_azure_storage_credentials = ""true"",
   dbTable = ""<your-table-name>"",
   tempDir = ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")

# Load data from an Azure Synapse query.
df <- read.df(
   source = ""com.databricks.spark.sqldw"",
   url = ""jdbc:sqlserver://<the-rest-of-the-connection-string>"",
   forward_spark_azure_storage_credentials = ""true"",
   query = ""select x, count(*) as cnt from table group by x"",
   tempDir = ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")

# Apply some transformations to the data, then use the
# Data Source API to write the data back to another table in Azure Synapse.

write.df(
  df,
  source = ""com.databricks.spark.sqldw"",
  url = ""jdbc:sqlserver://<the-rest-of-the-connection-string>"",
  forward_spark_azure_storage_credentials = ""true"",
  dbTable = ""<your-table-name>"",
  tempDir = ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
 Usage (Streaming) You can write data using Structured Streaming in Scala and Python notebooks. Scala Scala Copy // Set up the Blob storage account access key in the notebook session conf.
spark.conf.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")

// Prepare streaming source; this could be Kafka or a simple rate stream.
val df: DataFrame = spark.readStream
  .format(""rate"")
  .option(""rowsPerSecond"", ""100000"")
  .option(""numPartitions"", ""16"")
  .load()

// Apply some transformations to the data then use
// Structured Streaming API to continuously write the data to a table in Azure Synapse.

df.writeStream
  .format(""com.databricks.spark.sqldw"")
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"")
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
  .option(""forwardSparkAzureStorageCredentials"", ""true"")
  .option(""dbTable"", ""<your-table-name>"")
  .option(""checkpointLocation"", ""/tmp_checkpoint_location"")
  .start()
 Python Python Copy # Set up the Blob storage account access key in the notebook session conf.
spark.conf.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")

# Prepare streaming source; this could be Kafka or a simple rate stream.
df = spark.readStream \
  .format(""rate"") \
  .option(""rowsPerSecond"", ""100000"") \
  .option(""numPartitions"", ""16"") \
  .load()

# Apply some transformations to the data then use
# Structured Streaming API to continuously write the data to a table in Azure Synapse.

df.writeStream \
  .format(""com.databricks.spark.sqldw"") \
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"") \
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"") \
  .option(""forwardSparkAzureStorageCredentials"", ""true"") \
  .option(""dbTable"", ""<your-table-name>"") \
  .option(""checkpointLocation"", ""/tmp_checkpoint_location"") \
  .start()
 Configuration This section describes how to configure write semantics for the connector, required permissions, and miscellaneous configuration parameters. In this section: Supported save modes for batch writes Supported output modes for streaming writes Write semantics Required Azure Synapse permissions for PolyBase Required Azure Synapse permissions for the COPY statement Parameters Query pushdown into Azure Synapse Temporary data management Temporary object management Streaming checkpoint table management Supported save modes for batch writes The Azure Synapse connector supports ErrorIfExists, Ignore, Append, and Overwrite save modes with the default mode being ErrorIfExists. For more information on supported save modes in Apache Spark, see Spark SQL documentation on Save Modes. Supported output modes for streaming writes The Azure Synapse connector supports Append and Complete output modes for record appends and aggregations. For more details on output modes and compatibility matrix, see the Structured Streaming guide. Write semantics Note COPY is available in Databricks Runtime 7.0 and above. In addition to PolyBase, the Azure Synapse connector supports the COPY statement. The COPY statement offers a more convenient way of loading data into Azure Synapse without the need to create an external table, requires fewer permissions to load data, and improves the performance of data ingestion into Azure Synapse. By default, the connector automatically discovers the best write semantics (COPY when targeting an Azure Synapse Gen2 instance, PolyBase otherwise). You can also specify the write semantics with the following configuration: Scala Scala Copy // Configure the write semantics for Azure Synapse connector in the notebook session conf.
spark.conf.set(""spark.databricks.sqldw.writeSemantics"", ""<write-semantics>"")
 Python Python Copy # Configure the write semantics for Azure Synapse connector in the notebook session conf.
spark.conf.set(""spark.databricks.sqldw.writeSemantics"", ""<write-semantics>"")
 SQL SQL Copy -- Configure the write semantics for Azure Synapse connector in the notebook session conf.
SET spark.databricks.sqldw.writeSemantics=<write-semantics>;
 R R Copy # Load SparkR
library(SparkR)

# Configure the write semantics for Azure Synapse connector in the notebook session conf.
conf <- sparkR.callJMethod(sparkR.session(), ""conf"")
sparkR.callJMethod(conf, ""set"", ""spark.databricks.sqldw.writeSemantics"", ""<write-semantics>"")
 where <write-semantics> is either polybase to use PolyBase, or copy to use the COPY statement. Required Azure Synapse permissions for PolyBase When you use PolyBase, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance: CREATE DATABASE SCOPED CREDENTIAL CREATE EXTERNAL DATA SOURCE CREATE EXTERNAL FILE FORMAT CREATE EXTERNAL TABLE As a prerequisite for the first command, the connector expects that a database master key already exists for the specified Azure Synapse instance. If not, you can create a key using the CREATE MASTER KEY command. Additionally, to read the Azure Synapse table set through dbTable or tables referred in query, the JDBC user must have permission to access needed Azure Synapse tables. To write data back to an Azure Synapse table set through dbTable, the JDBC user must have permission to write to this Azure Synapse table. The following table summarizes the required permissions for all operations with PolyBase: Operation Permissions Permissions when using external data source Batch write CONTROL See Batch write Streaming write CONTROL See Streaming write Read CONTROL See Read Required Azure Synapse permissions for PolyBase with the external data source option Note Available in Databricks Runtime 8.4 and above. You can use PolyBase with a pre-provisioned external data source. See the externalDataSource parameter in Parameters for more information. To use PolyBase with a pre-provisioned external data source, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance: CREATE EXTERNAL FILE FORMAT CREATE EXTERNAL TABLE To create an external data source, you should first create a database scoped credential. The following links describe how to create a scoped credential for service principals and an external data source for an ABFS location: CREATE DATABASE SCOPED CREDENTIAL CREATE EXTERNAL DATA SOURCE Note The external data source location must point to a container. The connector will not work if the location is a directory in a container. The following table summarizes the permissions for PolyBase write operations with the external data source option: Operation Permissions (insert into an existing table) Permissions (insert into a new table) Batch write ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ANY SCHEMA ALTER ANY EXTERNAL DATA SOURCE ALTER ANY EXTERNAL FILE FORMAT ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ANY SCHEMA ALTER ANY EXTERNAL DATA SOURCE ALTER ANY EXTERNAL FILE FORMAT Streaming write ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ANY SCHEMA ALTER ANY EXTERNAL DATA SOURCE ALTER ANY EXTERNAL FILE FORMAT ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ANY SCHEMA ALTER ANY EXTERNAL DATA SOURCE ALTER ANY EXTERNAL FILE FORMAT The following table summarizes the permissions for PolyBase read operations with external data source option: Operation Permissions Read CREATE TABLE ALTER ANY SCHEMA ALTER ANY EXTERNAL DATA SOURCE ALTER ANY EXTERNAL FILE FORMAT You can use this connector to read via the data source API in Scala, Python, SQL, and R notebooks. Scala Scala Copy // Get some data from an Azure Synapse table.
val df: DataFrame = spark.read
  .format(""com.databricks.spark.sqldw"")
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"")
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
  .option(""externalDataSource"", ""<your-pre-provisioned-data-source>"")
  .option(""dbTable"", ""<your-table-name>"")
  .load()
 Python Python Copy # Get some data from an Azure Synapse table.
df = spark.read \
  .format(""com.databricks.spark.sqldw"") \
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"") \
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"") \
  .option(""externalDataSource"", ""<your-pre-provisioned-data-source>"") \
  .option(""dbTable"", ""<your-table-name>"") \
  .load()
 SQL SQL Copy -- Read data using SQL.
CREATE TABLE example_table_in_spark_read
USING com.databricks.spark.sqldw
OPTIONS (
  url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',
  forwardSparkAzureStorageCredentials 'true',
  dbTable '<your-table-name>',
  tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>',
  externalDataSource '<your-pre-provisioned-data-source>'
);
 R R Copy # Get some data from an Azure Synapse table.
df <- read.df(
   source = ""com.databricks.spark.sqldw"",
   url = ""jdbc:sqlserver://<the-rest-of-the-connection-string>"",
   forward_spark_azure_storage_credentials = ""true"",
   dbTable = ""<your-table-name>"",
   tempDir = ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>""
   externalDataSource = ""<your-pre-provisioned-data-source>"")
 Required Azure Synapse permissions for the COPY statement Note Available in Databricks Runtime 7.0 and above. When you use the COPY statement, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance: COPY INTO If the destination table does not exist in Azure Synapse, permission to run the following command is required in addition to the command above: CREATE TABLE The following table summarizes the permissions for batch and streaming writes with COPY: Operation Permissions (insert into an existing table) Permissions (insert into a new table) Batch write ADMINISTER DATABASE BULK OPERATIONS INSERT ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ON SCHEMA :: dbo Streaming write ADMINISTER DATABASE BULK OPERATIONS INSERT ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ON SCHEMA :: dbo Parameters The parameter map or OPTIONS provided in Spark SQL support the following settings: Parameter Required Default Notes dbTable Yes, unless query is specified No default The table to create or read from in Azure Synapse. This parameter is required when saving data back to Azure Synapse. You can also use {SCHEMA NAME}.{TABLE NAME} to access a table in a given schema. If schema name is not provided, the default schema associated with the JDBC user is used. The previously supported dbtable variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. query Yes, unless dbTable is specified No default The query to read from in Azure Synapse. For tables referred in the query, you can also use {SCHEMA NAME}.{TABLE NAME} to access a table in a given schema. If schema name is not provided, the default schema associated with the JDBC user is used. user No No default The Azure Synapse username. Must be used in tandem with password option. Can only be used if the user and password are not passed in the URL. Passing both will result in an error. password No No default The Azure Synapse password. Must be used in tandem with user option. Can only be used if the user and password are not passed in the URL. Passing both will result in an error. url Yes No default A JDBC URL with sqlserver set as the subprotocol. It is recommended to use the connection string provided by Azure portal. Setting encrypt=true is strongly recommended, because it enables SSL encryption of the JDBC connection. If user and password are set separately, you do not need to include them in the URL. jdbcDriver No Determined by the JDBC URL’s subprotocol The class name of the JDBC driver to use. This class must be on the classpath. In most cases, it should not be necessary to specify this option, as the appropriate driver classname should automatically be determined by the JDBC URL’s subprotocol. The previously supported jdbc_driver variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. tempDir Yes No default A abfss URI. We recommend you use a dedicated Blob storage container for the Azure Synapse. The previously supported tempdir variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. tempFormat No PARQUET The format in which to save temporary files to the blob store when writing to Azure Synapse. Defaults to PARQUET; no other values are allowed right now. tempCompression No SNAPPY The compression algorithm to be used to encode/decode temporary by both Spark and Azure Synapse. Currently supported values are: UNCOMPRESSED, SNAPPY and GZIP. forwardSparkAzureStorageCredentials No false If true, the library automatically discovers the credentials that Spark is using to connect to the Blob storage container and forwards those credentials to Azure Synapse over JDBC. These credentials are sent as part of the JDBC query. Therefore it is strongly recommended that you enable SSL encryption of the JDBC connection when you use this option. The current version of Azure Synapse connector requires (exactly) one of forwardSparkAzureStorageCredentials, enableServicePrincipalAuth, or useAzureMSI to be explicitly set to true. The previously supported forward_spark_azure_storage_credentials variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. useAzureMSI No false If true, the library will specify IDENTITY = 'Managed Service Identity' and no SECRET for the database scoped credentials it creates. The current version of Azure Synapse connector requires (exactly) one of forwardSparkAzureStorageCredentials, enableServicePrincipalAuth, or useAzureMSI to be explicitly set to true. enableServicePrincipalAuth No false If true, the library will use the provided service principal credentials to connect to the Azure storage account and Azure Synapse Analytics over JDBC. The current version of Azure Synapse connector requires (exactly) one of forwardSparkAzureStorageCredentials, enableServicePrincipalAuth, or useAzureMSI to be explicitly set to true. tableOptions No CLUSTERED COLUMNSTORE INDEX, DISTRIBUTION = ROUND_ROBIN A string used to specify table options when creating the Azure Synapse table set through dbTable. This string is passed literally to the WITH clause of the CREATE TABLE SQL statement that is issued against Azure Synapse. The previously supported table_options variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. preActions No No default (empty string) A ; separated list of SQL commands to be executed in Azure Synapse before writing data to the Azure Synapse instance. These SQL commands are required to be valid commands accepted by Azure Synapse. If any of these commands fail, it is treated as an error and the write operation is not executed. postActions No No default (empty string) A ; separated list of SQL commands to be executed in Azure Synapse after the connector successfully writes data to the Azure Synapse instance. These SQL commands are required to be valid commands accepted by Azure Synapse. If any of these commands fail, it is treated as an error and you’ll get an exception after the data is successfully written to the Azure Synapse instance. maxStrLength No 256 StringType in Spark is mapped to the NVARCHAR(maxStrLength) type in Azure Synapse. You can use maxStrLength to set the string length for all NVARCHAR(maxStrLength) type columns that are in the table with name dbTable in Azure Synapse. The previously supported maxstrlength variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. checkpointLocation Yes No default Location on DBFS that will be used by Structured Streaming to write metadata and checkpoint information. See Recovering from Failures with Checkpointing in Structured Streaming programming guide. numStreamingTempDirsToKeep No 0 Indicates how many (latest) temporary directories to keep for periodic cleanup of micro batches in streaming. When set to 0, directory deletion is triggered immediately after micro batch is committed, otherwise provided number of latest micro batches is kept and the rest of directories is removed. Use -1 to disable periodic cleanup. applicationName No Databricks-User-Query The tag of the connection for each query. If not specified or the value is an empty string, the default value of the tag is added the JDBC URL. The default value prevents the Azure DB Monitoring tool from raising spurious SQL injection alerts against queries. maxbinlength No No default Control the column length of BinaryType columns. This parameter is translated as VARBINARY(maxbinlength). identityInsert No false Setting to true enables IDENTITY_INSERT mode, which inserts a DataFrame provided value in the identity column of the Azure Synapse table. See Explicitly inserting values into an IDENTITY column. externalDataSource No No default A pre-provisioned external data source to read data from Azure Synapse. An external data source can only be used with PolyBase and removes the CONTROL permission requirement since the connector does not need to create a scoped credential and an external data source to load data. For example usage and the list of permissions required when using an external data source, see Required Azure Synapse permissions for PolyBase with the external data source option. maxErrors No 0 The maximum number of rows that can be rejected during reads and writes before the loading operation (either PolyBase or COPY) is cancelled. The rejected rows will be ignored. For example, if two out of ten records have errors, only eight records will be processed. See REJECT_VALUE documentation in CREATE EXTERNAL TABLE and MAXERRORS documentation in COPY. Note tableOptions, preActions, postActions, and maxStrLength are relevant only when writing data from Azure Databricks to a new table in Azure Synapse. externalDataSource is relevant only when reading data from Azure Synapse and writing data from Azure Databricks to a new table in Azure Synapse with PolyBase semantics. You should not specify other storage authentication types while using externalDataSource such as forwardSparkAzureStorageCredentials or useAzureMSI. checkpointLocation and numStreamingTempDirsToKeep are relevant only for streaming writes from Azure Databricks to a new table in Azure Synapse. Even though all data source option names are case-insensitive, we recommend that you specify them in “camel case” for clarity. Query pushdown into Azure Synapse The Azure Synapse connector implements a set of optimization rules to push the following operators down into Azure Synapse: Filter Project Limit The Project and Filter operators support the following expressions: Most boolean logic operators Comparisons Basic arithmetic operations Numeric and string casts For the Limit operator, pushdown is supported only when there is no ordering specified. For example: SELECT TOP(10) * FROM table, but not SELECT TOP(10) * FROM table ORDER BY col. Note The Azure Synapse connector does not push down expressions operating on strings, dates, or timestamps. Query pushdown built with the Azure Synapse connector is enabled by default. You can disable it by setting spark.databricks.sqldw.pushdown to false. Temporary data management The Azure Synapse connector does not delete the temporary files that it creates in the Blob storage container. Therefore we recommend that you periodically delete temporary files under the user-supplied tempDir location. To facilitate data cleanup, the Azure Synapse connector does not store data files directly under tempDir, but instead creates a subdirectory of the form: <tempDir>/<yyyy-MM-dd>/<HH-mm-ss-SSS>/<randomUUID>/. You can set up periodic jobs (using the Azure Databricks jobs feature or otherwise) to recursively delete any subdirectories that are older than a given threshold (for example, 2 days), with the assumption that there cannot be Spark jobs running longer than that threshold. A simpler alternative is to periodically drop the whole container and create a new one with the same name. This requires that you use a dedicated container for the temporary data produced by the Azure Synapse connector and that you can find a time window in which you can guarantee that no queries involving the connector are running. Temporary object management The Azure Synapse connector automates data transfer between an Azure Databricks cluster and an Azure Synapse instance. For reading data from an Azure Synapse table or query or writing data to an Azure Synapse table, the Azure Synapse connector creates temporary objects, including DATABASE SCOPED CREDENTIAL, EXTERNAL DATA SOURCE, EXTERNAL FILE FORMAT, and EXTERNAL TABLE behind the scenes. These objects live only throughout the duration of the corresponding Spark job and should automatically be dropped thereafter. When a cluster is running a query using the Azure Synapse connector, if the Spark driver process crashes or is forcefully restarted, or if the cluster is forcefully terminated or restarted, temporary objects might not be dropped. To facilitate identification and manual deletion of these objects, the Azure Synapse connector prefixes the names of all intermediate temporary objects created in the Azure Synapse instance with a tag of the form: tmp_databricks_<yyyy_MM_dd_HH_mm_ss_SSS>_<randomUUID>_<internalObject>. We recommend that you periodically look for leaked objects using queries such as the following: SELECT * FROM sys.database_scoped_credentials WHERE name LIKE 'tmp_databricks_%' SELECT * FROM sys.external_data_sources WHERE name LIKE 'tmp_databricks_%' SELECT * FROM sys.external_file_formats WHERE name LIKE 'tmp_databricks_%' SELECT * FROM sys.external_tables WHERE name LIKE 'tmp_databricks_%' Streaming checkpoint table management The Azure Synapse connector does not delete the streaming checkpoint table that is created when new streaming query is started. This behavior is consistent with the checkpointLocation on DBFS. Therefore we recommend that you periodically delete checkpoint tables at the same time as removing checkpoint locations on DBFS for queries that are not going to be run in the future or already have checkpoint location removed. By default, all checkpoint tables have the name <prefix>_<query_id>, where <prefix> is a configurable prefix with default value databricks_streaming_checkpoint and query_id is a streaming query ID with _ characters removed. To find all checkpoint tables for stale or deleted streaming queries, run the query: SQL Copy SELECT * FROM sys.tables WHERE name LIKE 'databricks_streaming_checkpoint%'
 You can configure the prefix with the Spark SQL configuration option spark.databricks.sqldw.streaming.exactlyOnce.checkpointTableNamePrefix. Frequently asked questions (FAQ) I received an error while using the Azure Synapse connector. How can I tell if this error is from Azure Synapse or Azure Databricks? To help you debug errors, any exception thrown by code that is specific to the Azure Synapse connector is wrapped in an exception extending the SqlDWException trait. Exceptions also make the following distinction: SqlDWConnectorException represents an error thrown by the Azure Synapse connector SqlDWSideException represents an error thrown by the connected Azure Synapse instance What should I do if my query failed with the error “No access key found in the session conf or the global Hadoop conf”? This error means that Azure Synapse connector could not find the storage account access key in the notebook session configuration or global Hadoop configuration for the storage account specified in tempDir. See Usage (Batch) for examples of how to configure Storage Account access properly. If a Spark table is created using Azure Synapse connector, you must still provide the storage account access credentials in order to read or write to the Spark table. Can I use a Shared Access Signature (SAS) to access the Blob storage container specified by tempDir? Azure Synapse does not support using SAS to access Blob storage. Therefore the Azure Synapse connector does not support SAS to access the Blob storage container specified by tempDir. I created a Spark table using Azure Synapse connector with the dbTable option, wrote some data to this Spark table, and then dropped this Spark table. Will the table created at the Azure Synapse side be dropped? No. Azure Synapse is considered an external data source. The Azure Synapse table with the name set through dbTable is not dropped when the Spark table is dropped. When writing a DataFrame to Azure Synapse, why do I need to say .option(""dbTable"", tableName).save() instead of just .saveAsTable(tableName)? That is because we want to make the following distinction clear: .option(""dbTable"", tableName) refers to the database (that is, Azure Synapse) table, whereas .saveAsTable(tableName) refers to the Spark table. In fact, you could even combine the two: df.write. ... .option(""dbTable"", tableNameDW).saveAsTable(tableNameSpark) which creates a table in Azure Synapse called tableNameDW and an external table in Spark called tableNameSpark that is backed by the Azure Synapse table. Warning Beware of the following difference between .save() and .saveAsTable(): For df.write. ... .option(""dbTable"", tableNameDW).mode(writeMode).save(), writeMode acts on the Azure Synapse table, as expected. For df.write. ... .option(""dbTable"", tableNameDW).mode(writeMode).saveAsTable(tableNameSpark), writeMode acts on the Spark table, whereas tableNameDW is silently overwritten if it already exists in Azure Synapse. This behavior is no different from writing to any other data source. It is just a caveat of the Spark DataFrameWriter API.",Azure Synapse Analytics
9,2205060040006402  Not able to run the databricks code.,"Question: What time did the problem begin?
Answer: Thu, May 5, 2022, 12:00 AM (UTC-05:00) Eastern Time (US & Canada)

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer: 

Question: What error message you received?
Answer: Waiting for Azure Synapse Analytics to load intermediate data from abfss://dap-de-dls-dev-data@sttecodlseastusdev2.dfs.core.windows.net/data/foundation/meter contract/2022-05-06/20-02-55-276/d826f23a-80ca-40c8-96d2-b8db4550c310/ into 'fnd'.'stg_fmeterContract' using Polybase.



avaError: An error occurred while calling o1573.save.
: com.databricks.spark.sqldw.SqlDWConnectorException: Exception encountered in Azure Synapse Analytics connector code.
at com.databricks.spark.sqldw.Utils$.wrapExceptions(Utils.scala:689)
at com.databricks.spark.sqldw.DefaultSource.createRelation(DefaultSource.scala:89)
at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:96)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:196)
at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:240)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)
at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:236)
at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:192)
at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:167)
at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:166)
at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1079)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)
at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:267)
at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:217)
at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1079)
at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:468)
at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:438)
at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:311)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
a",https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/synapse-analytics,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Azure Synapse Analytics Article 07/12/2022 27 minutes to read 6 contributors In this article Requirements Authentication Streaming support Usage (Batch) Usage (Streaming) Configuration Frequently asked questions (FAQ) Azure Synapse Analytics (formerly SQL Data Warehouse) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data. Use Azure as a key component of a big data solution. Import big data into Azure with simple PolyBase T-SQL queries, or COPY statement and then use the power of MPP to run high-performance analytics. As you integrate and analyze, the data warehouse will become the single version of truth your business can count on for insights. You can access Azure Synapse from Azure Databricks using the Azure Synapse connector, a data source implementation for Apache Spark that uses Azure Blob storage, and PolyBase or the COPY statement in Azure Synapse to transfer large volumes of data efficiently between an Azure Databricks cluster and an Azure Synapse instance. Both the Azure Databricks cluster and the Azure Synapse instance access a common Blob storage container to exchange data between these two systems. In Azure Databricks, Apache Spark jobs are triggered by the Azure Synapse connector to read data from and write data to the Blob storage container. On the Azure Synapse side, data loading and unloading operations performed by PolyBase are triggered by the Azure Synapse connector through JDBC. In Databricks Runtime 7.0 and above, COPY is used by default to load data into Azure Synapse by the Azure Synapse connector through JDBC. Note COPY is available only on Azure Synapse Gen2 instances, which provide better performance. If your database still uses Gen1 instances, we recommend that you migrate the database to Gen2. The Azure Synapse connector is more suited to ETL than to interactive queries, because each query execution can extract large amounts of data to Blob storage. If you plan to perform several queries against the same Azure Synapse table, we recommend that you save the extracted data in a format such as Parquet. Requirements A database master key for the Azure Synapse. Authentication The Azure Synapse connector uses three types of network connections: Spark driver to Azure Synapse Spark driver and executors to Azure storage account Azure Synapse to Azure storage account Copy                                  ┌─────────┐
      ┌─────────────────────────>│ STORAGE │<────────────────────────┐
      │   Storage acc key /      │ ACCOUNT │  Storage acc key /      │
      │   Managed Service ID /   └─────────┘  OAuth 2.0 /            │
      │                               │                              │
      │                               │                              │
      │                               │ Storage acc key /            │
      │                               │ OAuth 2.0 /                  │
      │                               │                              │
      v                               v                       ┌──────v────┐
┌──────────┐                      ┌──────────┐                │┌──────────┴┐
│ Synapse  │                      │  Spark   │                ││ Spark     │
│ Analytics│<────────────────────>│  Driver  │<───────────────>│ Executors │
└──────────┘  JDBC with           └──────────┘    Configured   └───────────┘
              username & password /                in Spark
 The following sections describe each connection’s authentication configuration options. Spark driver to Azure Synapse The Spark driver can connect to Azure Synapse using JDBC with a username and password or OAuth 2.0 with a service principal for authentication. Username and password We recommend that you use the connection strings provided by Azure portal for both authentication types, which enable Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for encrypt=true in the connection string. To allow the Spark driver to reach Azure Synapse, we recommend that you set Allow access to Azure services to ON on the firewall pane of the Azure Synapse server through Azure portal. This setting allows communications from all Azure IP addresses and all Azure subnets, which allows Spark drivers to reach the Azure Synapse instance. OAuth 2.0 with a service principal You can authenticate to Azure Synapse Analytics using a service principal with access to the underlying storage account. For more information on using service principal credentials to access an Azure storage account, see Accessing Azure Data Lake Storage Gen2 and Blob Storage with Azure Databricks. You must set the enableServicePrincipalAuth option to true in the connection configuration Parameters to enable the connector to authenticate with a service principal. You can optionally use a different service principal for the Azure Synapse Analytics connection. An example that configures service principal credentials for the storage account and optional service principal credentials for Synapse: ini ini Copy ; Defining the Service Principal credentials for the Azure storage account
fs.azure.account.auth.type OAuth
fs.azure.account.oauth.provider.type org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider
fs.azure.account.oauth2.client.id <application-id>
fs.azure.account.oauth2.client.secret <service-credential>
fs.azure.account.oauth2.client.endpoint https://login.microsoftonline.com/<directory-id>/oauth2/token

; Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)
spark.databricks.sqldw.jdbc.service.principal.client.id <application-id>
spark.databricks.sqldw.jdbc.service.principal.client.secret <service-credential>
 Scala Scala Copy // Defining the Service Principal credentials for the Azure storage account
spark.conf.set(""fs.azure.account.auth.type"", ""OAuth"")
spark.conf.set(""fs.azure.account.oauth.provider.type"",  ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"")
spark.conf.set(""fs.azure.account.oauth2.client.id"", ""<application-id>"")
spark.conf.set(""fs.azure.account.oauth2.client.secret"", ""<service-credential>"")
spark.conf.set(""fs.azure.account.oauth2.client.endpoint"", ""https://login.microsoftonline.com/<directory-id>/oauth2/token"")

// Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)
spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.id"", ""<application-id>"")
spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.secret"", ""<service-credential>"")
 Python Python Copy # Defining the service principal credentials for the Azure storage account
spark.conf.set(""fs.azure.account.auth.type"", ""OAuth"")
spark.conf.set(""fs.azure.account.oauth.provider.type"",  ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"")
spark.conf.set(""fs.azure.account.oauth2.client.id"", ""<application-id>"")
spark.conf.set(""fs.azure.account.oauth2.client.secret"", ""<service-credential>"")
spark.conf.set(""fs.azure.account.oauth2.client.endpoint"", ""https://login.microsoftonline.com/<directory-id>/oauth2/token"")

# Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)
spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.id"", ""<application-id>"")
spark.conf.set(""spark.databricks.sqldw.jdbc.service.principal.client.secret"", ""<service-credential>"")
 R R Copy # Load SparkR
library(SparkR)
conf <- sparkR.callJMethod(sparkR.session(), ""conf"")

# Defining the service principal credentials for the Azure storage account
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.auth.type"", ""OAuth"")
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.oauth.provider.type"",  ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"")
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.oauth2.client.id"", ""<application-id>"")
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.oauth2.client.secret"", ""<service-credential>"")
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.oauth2.client.endpoint"", ""https://login.microsoftonline.com/<directory-id>/oauth2/token"")

# Defining a separate set of service principal credentials for Azure Synapse Analytics (If not defined, the connector will use the Azure storage account credentials)
sparkR.callJMethod(conf, ""set"", ""spark.databricks.sqldw.jdbc.service.principal.client.id"", ""<application-id>"")
sparkR.callJMethod(conf, ""set"", ""spark.databricks.sqldw.jdbc.service.principal.client.secret"", ""<service-credential>"")
 Spark driver and executors to Azure storage account The Azure storage container acts as an intermediary to store bulk data when reading from or writing to Azure Synapse. Spark connects to ADLS Gen2 or Blob Storage using the abfss driver. The following authentication options are available: Storage account access key and secret OAuth 2.0 authentication. For more information about OAuth 2.0 and Service Principal, see Configure access to Azure storage with an Azure Active Directory service principal. The examples below illustrate these two ways using the storage account access key approach. The same applies to OAuth 2.0 configuration. Notebook session configuration (preferred) Using this approach, the account access key is set in the session configuration associated with the notebook that runs the command. This configuration does not affect other notebooks attached to the same cluster. spark is the SparkSession object provided in the notebook. Python Copy spark.conf.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")
 Global Hadoop configuration This approach updates the global Hadoop configuration associated with the SparkContext object shared by all notebooks. Scala Scala Copy sc.hadoopConfiguration.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")
 Python hadoopConfiguration is not exposed in all versions of PySpark. Although the following command relies on some Spark internals, it should work with all PySpark versions and is unlikely to break or change in the future: Python Copy sc._jsc.hadoopConfiguration().set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")
 Azure Synapse to Azure storage account Azure Synapse also connects to a storage account during loading and unloading of temporary data. In case you have set up an account key and secret for the storage account, you can set forwardSparkAzureStorageCredentials to true, in which case Azure Synapse connector automatically discovers the account access key set in the notebook session configuration or the global Hadoop configuration and forwards the storage account access key to the connected Azure Synapse instance by creating a temporary Azure database scoped credential. Alternatively, if you use ADLS Gen2 + OAuth 2.0 authentication or your Azure Synapse instance is configured to have a Managed Service Identity (typically in conjunction with a VNet + Service Endpoints setup), you must set useAzureMSI to true. In this case the connector will specify IDENTITY = 'Managed Service Identity' for the databased scoped credential and no SECRET. Streaming support The Azure Synapse connector offers efficient and scalable Structured Streaming write support for Azure Synapse that provides consistent user experience with batch writes, and uses PolyBase or COPY for large data transfers between an Azure Databricks cluster and Azure Synapse instance. Similar to the batch writes, streaming is designed largely for ETL, thus providing higher latency that may not be suitable for real-time data processing in some cases. Fault tolerance semantics By default, Azure Synapse Streaming offers end-to-end exactly-once guarantee for writing data into an Azure Synapse table by reliably tracking progress of the query using a combination of checkpoint location in DBFS, checkpoint table in Azure Synapse, and locking mechanism to ensure that streaming can handle any types of failures, retries, and query restarts. Optionally, you can select less restrictive at-least-once semantics for Azure Synapse Streaming by setting spark.databricks.sqldw.streaming.exactlyOnce.enabled option to false, in which case data duplication could occur in the event of intermittent connection failures to Azure Synapse or unexpected query termination. Usage (Batch) You can use this connector via the data source API in Scala, Python, SQL, and R notebooks. Scala Scala Copy 
// Otherwise, set up the Blob storage account access key in the notebook session conf.
spark.conf.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")

// Get some data from an Azure Synapse table.
val df: DataFrame = spark.read
  .format(""com.databricks.spark.sqldw"")
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"")
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
  .option(""forwardSparkAzureStorageCredentials"", ""true"")
  .option(""dbTable"", ""<your-table-name>"")
  .load()

// Load data from an Azure Synapse query.
val df: DataFrame = spark.read
  .format(""com.databricks.spark.sqldw"")
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"")
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
  .option(""forwardSparkAzureStorageCredentials"", ""true"")
  .option(""query"", ""select x, count(*) as cnt from table group by x"")
  .load()

// Apply some transformations to the data, then use the
// Data Source API to write the data back to another table in Azure Synapse.

df.write
  .format(""com.databricks.spark.sqldw"")
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"")
  .option(""forwardSparkAzureStorageCredentials"", ""true"")
  .option(""dbTable"", ""<your-table-name>"")
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
  .save()
 Python Python Copy 
# Otherwise, set up the Blob storage account access key in the notebook session conf.
spark.conf.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")

# Get some data from an Azure Synapse table.
df = spark.read \
  .format(""com.databricks.spark.sqldw"") \
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"") \
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"") \
  .option(""forwardSparkAzureStorageCredentials"", ""true"") \
  .option(""dbTable"", ""<your-table-name>"") \
  .load()

# Load data from an Azure Synapse query.
df = spark.read \
  .format(""com.databricks.spark.sqldw"") \
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"") \
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"") \
  .option(""forwardSparkAzureStorageCredentials"", ""true"") \
  .option(""query"", ""select x, count(*) as cnt from table group by x"") \
  .load()

# Apply some transformations to the data, then use the
# Data Source API to write the data back to another table in Azure Synapse.

df.write \
  .format(""com.databricks.spark.sqldw"") \
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"") \
  .option(""forwardSparkAzureStorageCredentials"", ""true"") \
  .option(""dbTable"", ""<your-table-name>"") \
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"") \
  .save()
 SQL SQL Copy 
-- Otherwise, set up the Blob storage account access key in the notebook session conf.
SET fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net=<your-storage-account-access-key>;

-- Read data using SQL.
CREATE TABLE example_table_in_spark_read
USING com.databricks.spark.sqldw
OPTIONS (
  url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',
  forwardSparkAzureStorageCredentials 'true',
  dbTable '<your-table-name>',
  tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'
);

-- Write data using SQL.
-- Create a new table, throwing an error if a table with the same name already exists:

CREATE TABLE example_table_in_spark_write
USING com.databricks.spark.sqldw
OPTIONS (
  url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',
  forwardSparkAzureStorageCredentials 'true',
  dbTable '<your-table-name>',
  tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'
)
AS SELECT * FROM table_to_save_in_spark;
 R R Copy # Load SparkR
library(SparkR)

# Otherwise, set up the Blob storage account access key in the notebook session conf.
conf <- sparkR.callJMethod(sparkR.session(), ""conf"")
sparkR.callJMethod(conf, ""set"", ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"", ""<your-storage-account-access-key>"")

# Get some data from an Azure Synapse table.
df <- read.df(
   source = ""com.databricks.spark.sqldw"",
   url = ""jdbc:sqlserver://<the-rest-of-the-connection-string>"",
   forward_spark_azure_storage_credentials = ""true"",
   dbTable = ""<your-table-name>"",
   tempDir = ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")

# Load data from an Azure Synapse query.
df <- read.df(
   source = ""com.databricks.spark.sqldw"",
   url = ""jdbc:sqlserver://<the-rest-of-the-connection-string>"",
   forward_spark_azure_storage_credentials = ""true"",
   query = ""select x, count(*) as cnt from table group by x"",
   tempDir = ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")

# Apply some transformations to the data, then use the
# Data Source API to write the data back to another table in Azure Synapse.

write.df(
  df,
  source = ""com.databricks.spark.sqldw"",
  url = ""jdbc:sqlserver://<the-rest-of-the-connection-string>"",
  forward_spark_azure_storage_credentials = ""true"",
  dbTable = ""<your-table-name>"",
  tempDir = ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
 Usage (Streaming) You can write data using Structured Streaming in Scala and Python notebooks. Scala Scala Copy // Set up the Blob storage account access key in the notebook session conf.
spark.conf.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")

// Prepare streaming source; this could be Kafka or a simple rate stream.
val df: DataFrame = spark.readStream
  .format(""rate"")
  .option(""rowsPerSecond"", ""100000"")
  .option(""numPartitions"", ""16"")
  .load()

// Apply some transformations to the data then use
// Structured Streaming API to continuously write the data to a table in Azure Synapse.

df.writeStream
  .format(""com.databricks.spark.sqldw"")
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"")
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
  .option(""forwardSparkAzureStorageCredentials"", ""true"")
  .option(""dbTable"", ""<your-table-name>"")
  .option(""checkpointLocation"", ""/tmp_checkpoint_location"")
  .start()
 Python Python Copy # Set up the Blob storage account access key in the notebook session conf.
spark.conf.set(
  ""fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net"",
  ""<your-storage-account-access-key>"")

# Prepare streaming source; this could be Kafka or a simple rate stream.
df = spark.readStream \
  .format(""rate"") \
  .option(""rowsPerSecond"", ""100000"") \
  .option(""numPartitions"", ""16"") \
  .load()

# Apply some transformations to the data then use
# Structured Streaming API to continuously write the data to a table in Azure Synapse.

df.writeStream \
  .format(""com.databricks.spark.sqldw"") \
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"") \
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"") \
  .option(""forwardSparkAzureStorageCredentials"", ""true"") \
  .option(""dbTable"", ""<your-table-name>"") \
  .option(""checkpointLocation"", ""/tmp_checkpoint_location"") \
  .start()
 Configuration This section describes how to configure write semantics for the connector, required permissions, and miscellaneous configuration parameters. In this section: Supported save modes for batch writes Supported output modes for streaming writes Write semantics Required Azure Synapse permissions for PolyBase Required Azure Synapse permissions for the COPY statement Parameters Query pushdown into Azure Synapse Temporary data management Temporary object management Streaming checkpoint table management Supported save modes for batch writes The Azure Synapse connector supports ErrorIfExists, Ignore, Append, and Overwrite save modes with the default mode being ErrorIfExists. For more information on supported save modes in Apache Spark, see Spark SQL documentation on Save Modes. Supported output modes for streaming writes The Azure Synapse connector supports Append and Complete output modes for record appends and aggregations. For more details on output modes and compatibility matrix, see the Structured Streaming guide. Write semantics Note COPY is available in Databricks Runtime 7.0 and above. In addition to PolyBase, the Azure Synapse connector supports the COPY statement. The COPY statement offers a more convenient way of loading data into Azure Synapse without the need to create an external table, requires fewer permissions to load data, and improves the performance of data ingestion into Azure Synapse. By default, the connector automatically discovers the best write semantics (COPY when targeting an Azure Synapse Gen2 instance, PolyBase otherwise). You can also specify the write semantics with the following configuration: Scala Scala Copy // Configure the write semantics for Azure Synapse connector in the notebook session conf.
spark.conf.set(""spark.databricks.sqldw.writeSemantics"", ""<write-semantics>"")
 Python Python Copy # Configure the write semantics for Azure Synapse connector in the notebook session conf.
spark.conf.set(""spark.databricks.sqldw.writeSemantics"", ""<write-semantics>"")
 SQL SQL Copy -- Configure the write semantics for Azure Synapse connector in the notebook session conf.
SET spark.databricks.sqldw.writeSemantics=<write-semantics>;
 R R Copy # Load SparkR
library(SparkR)

# Configure the write semantics for Azure Synapse connector in the notebook session conf.
conf <- sparkR.callJMethod(sparkR.session(), ""conf"")
sparkR.callJMethod(conf, ""set"", ""spark.databricks.sqldw.writeSemantics"", ""<write-semantics>"")
 where <write-semantics> is either polybase to use PolyBase, or copy to use the COPY statement. Required Azure Synapse permissions for PolyBase When you use PolyBase, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance: CREATE DATABASE SCOPED CREDENTIAL CREATE EXTERNAL DATA SOURCE CREATE EXTERNAL FILE FORMAT CREATE EXTERNAL TABLE As a prerequisite for the first command, the connector expects that a database master key already exists for the specified Azure Synapse instance. If not, you can create a key using the CREATE MASTER KEY command. Additionally, to read the Azure Synapse table set through dbTable or tables referred in query, the JDBC user must have permission to access needed Azure Synapse tables. To write data back to an Azure Synapse table set through dbTable, the JDBC user must have permission to write to this Azure Synapse table. The following table summarizes the required permissions for all operations with PolyBase: Operation Permissions Permissions when using external data source Batch write CONTROL See Batch write Streaming write CONTROL See Streaming write Read CONTROL See Read Required Azure Synapse permissions for PolyBase with the external data source option Note Available in Databricks Runtime 8.4 and above. You can use PolyBase with a pre-provisioned external data source. See the externalDataSource parameter in Parameters for more information. To use PolyBase with a pre-provisioned external data source, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance: CREATE EXTERNAL FILE FORMAT CREATE EXTERNAL TABLE To create an external data source, you should first create a database scoped credential. The following links describe how to create a scoped credential for service principals and an external data source for an ABFS location: CREATE DATABASE SCOPED CREDENTIAL CREATE EXTERNAL DATA SOURCE Note The external data source location must point to a container. The connector will not work if the location is a directory in a container. The following table summarizes the permissions for PolyBase write operations with the external data source option: Operation Permissions (insert into an existing table) Permissions (insert into a new table) Batch write ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ANY SCHEMA ALTER ANY EXTERNAL DATA SOURCE ALTER ANY EXTERNAL FILE FORMAT ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ANY SCHEMA ALTER ANY EXTERNAL DATA SOURCE ALTER ANY EXTERNAL FILE FORMAT Streaming write ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ANY SCHEMA ALTER ANY EXTERNAL DATA SOURCE ALTER ANY EXTERNAL FILE FORMAT ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ANY SCHEMA ALTER ANY EXTERNAL DATA SOURCE ALTER ANY EXTERNAL FILE FORMAT The following table summarizes the permissions for PolyBase read operations with external data source option: Operation Permissions Read CREATE TABLE ALTER ANY SCHEMA ALTER ANY EXTERNAL DATA SOURCE ALTER ANY EXTERNAL FILE FORMAT You can use this connector to read via the data source API in Scala, Python, SQL, and R notebooks. Scala Scala Copy // Get some data from an Azure Synapse table.
val df: DataFrame = spark.read
  .format(""com.databricks.spark.sqldw"")
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"")
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"")
  .option(""externalDataSource"", ""<your-pre-provisioned-data-source>"")
  .option(""dbTable"", ""<your-table-name>"")
  .load()
 Python Python Copy # Get some data from an Azure Synapse table.
df = spark.read \
  .format(""com.databricks.spark.sqldw"") \
  .option(""url"", ""jdbc:sqlserver://<the-rest-of-the-connection-string>"") \
  .option(""tempDir"", ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>"") \
  .option(""externalDataSource"", ""<your-pre-provisioned-data-source>"") \
  .option(""dbTable"", ""<your-table-name>"") \
  .load()
 SQL SQL Copy -- Read data using SQL.
CREATE TABLE example_table_in_spark_read
USING com.databricks.spark.sqldw
OPTIONS (
  url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',
  forwardSparkAzureStorageCredentials 'true',
  dbTable '<your-table-name>',
  tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>',
  externalDataSource '<your-pre-provisioned-data-source>'
);
 R R Copy # Get some data from an Azure Synapse table.
df <- read.df(
   source = ""com.databricks.spark.sqldw"",
   url = ""jdbc:sqlserver://<the-rest-of-the-connection-string>"",
   forward_spark_azure_storage_credentials = ""true"",
   dbTable = ""<your-table-name>"",
   tempDir = ""abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>""
   externalDataSource = ""<your-pre-provisioned-data-source>"")
 Required Azure Synapse permissions for the COPY statement Note Available in Databricks Runtime 7.0 and above. When you use the COPY statement, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance: COPY INTO If the destination table does not exist in Azure Synapse, permission to run the following command is required in addition to the command above: CREATE TABLE The following table summarizes the permissions for batch and streaming writes with COPY: Operation Permissions (insert into an existing table) Permissions (insert into a new table) Batch write ADMINISTER DATABASE BULK OPERATIONS INSERT ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ON SCHEMA :: dbo Streaming write ADMINISTER DATABASE BULK OPERATIONS INSERT ADMINISTER DATABASE BULK OPERATIONS INSERT CREATE TABLE ALTER ON SCHEMA :: dbo Parameters The parameter map or OPTIONS provided in Spark SQL support the following settings: Parameter Required Default Notes dbTable Yes, unless query is specified No default The table to create or read from in Azure Synapse. This parameter is required when saving data back to Azure Synapse. You can also use {SCHEMA NAME}.{TABLE NAME} to access a table in a given schema. If schema name is not provided, the default schema associated with the JDBC user is used. The previously supported dbtable variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. query Yes, unless dbTable is specified No default The query to read from in Azure Synapse. For tables referred in the query, you can also use {SCHEMA NAME}.{TABLE NAME} to access a table in a given schema. If schema name is not provided, the default schema associated with the JDBC user is used. user No No default The Azure Synapse username. Must be used in tandem with password option. Can only be used if the user and password are not passed in the URL. Passing both will result in an error. password No No default The Azure Synapse password. Must be used in tandem with user option. Can only be used if the user and password are not passed in the URL. Passing both will result in an error. url Yes No default A JDBC URL with sqlserver set as the subprotocol. It is recommended to use the connection string provided by Azure portal. Setting encrypt=true is strongly recommended, because it enables SSL encryption of the JDBC connection. If user and password are set separately, you do not need to include them in the URL. jdbcDriver No Determined by the JDBC URL’s subprotocol The class name of the JDBC driver to use. This class must be on the classpath. In most cases, it should not be necessary to specify this option, as the appropriate driver classname should automatically be determined by the JDBC URL’s subprotocol. The previously supported jdbc_driver variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. tempDir Yes No default A abfss URI. We recommend you use a dedicated Blob storage container for the Azure Synapse. The previously supported tempdir variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. tempFormat No PARQUET The format in which to save temporary files to the blob store when writing to Azure Synapse. Defaults to PARQUET; no other values are allowed right now. tempCompression No SNAPPY The compression algorithm to be used to encode/decode temporary by both Spark and Azure Synapse. Currently supported values are: UNCOMPRESSED, SNAPPY and GZIP. forwardSparkAzureStorageCredentials No false If true, the library automatically discovers the credentials that Spark is using to connect to the Blob storage container and forwards those credentials to Azure Synapse over JDBC. These credentials are sent as part of the JDBC query. Therefore it is strongly recommended that you enable SSL encryption of the JDBC connection when you use this option. The current version of Azure Synapse connector requires (exactly) one of forwardSparkAzureStorageCredentials, enableServicePrincipalAuth, or useAzureMSI to be explicitly set to true. The previously supported forward_spark_azure_storage_credentials variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. useAzureMSI No false If true, the library will specify IDENTITY = 'Managed Service Identity' and no SECRET for the database scoped credentials it creates. The current version of Azure Synapse connector requires (exactly) one of forwardSparkAzureStorageCredentials, enableServicePrincipalAuth, or useAzureMSI to be explicitly set to true. enableServicePrincipalAuth No false If true, the library will use the provided service principal credentials to connect to the Azure storage account and Azure Synapse Analytics over JDBC. The current version of Azure Synapse connector requires (exactly) one of forwardSparkAzureStorageCredentials, enableServicePrincipalAuth, or useAzureMSI to be explicitly set to true. tableOptions No CLUSTERED COLUMNSTORE INDEX, DISTRIBUTION = ROUND_ROBIN A string used to specify table options when creating the Azure Synapse table set through dbTable. This string is passed literally to the WITH clause of the CREATE TABLE SQL statement that is issued against Azure Synapse. The previously supported table_options variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. preActions No No default (empty string) A ; separated list of SQL commands to be executed in Azure Synapse before writing data to the Azure Synapse instance. These SQL commands are required to be valid commands accepted by Azure Synapse. If any of these commands fail, it is treated as an error and the write operation is not executed. postActions No No default (empty string) A ; separated list of SQL commands to be executed in Azure Synapse after the connector successfully writes data to the Azure Synapse instance. These SQL commands are required to be valid commands accepted by Azure Synapse. If any of these commands fail, it is treated as an error and you’ll get an exception after the data is successfully written to the Azure Synapse instance. maxStrLength No 256 StringType in Spark is mapped to the NVARCHAR(maxStrLength) type in Azure Synapse. You can use maxStrLength to set the string length for all NVARCHAR(maxStrLength) type columns that are in the table with name dbTable in Azure Synapse. The previously supported maxstrlength variant is deprecated and will be ignored in future releases. Use the “camel case” name instead. checkpointLocation Yes No default Location on DBFS that will be used by Structured Streaming to write metadata and checkpoint information. See Recovering from Failures with Checkpointing in Structured Streaming programming guide. numStreamingTempDirsToKeep No 0 Indicates how many (latest) temporary directories to keep for periodic cleanup of micro batches in streaming. When set to 0, directory deletion is triggered immediately after micro batch is committed, otherwise provided number of latest micro batches is kept and the rest of directories is removed. Use -1 to disable periodic cleanup. applicationName No Databricks-User-Query The tag of the connection for each query. If not specified or the value is an empty string, the default value of the tag is added the JDBC URL. The default value prevents the Azure DB Monitoring tool from raising spurious SQL injection alerts against queries. maxbinlength No No default Control the column length of BinaryType columns. This parameter is translated as VARBINARY(maxbinlength). identityInsert No false Setting to true enables IDENTITY_INSERT mode, which inserts a DataFrame provided value in the identity column of the Azure Synapse table. See Explicitly inserting values into an IDENTITY column. externalDataSource No No default A pre-provisioned external data source to read data from Azure Synapse. An external data source can only be used with PolyBase and removes the CONTROL permission requirement since the connector does not need to create a scoped credential and an external data source to load data. For example usage and the list of permissions required when using an external data source, see Required Azure Synapse permissions for PolyBase with the external data source option. maxErrors No 0 The maximum number of rows that can be rejected during reads and writes before the loading operation (either PolyBase or COPY) is cancelled. The rejected rows will be ignored. For example, if two out of ten records have errors, only eight records will be processed. See REJECT_VALUE documentation in CREATE EXTERNAL TABLE and MAXERRORS documentation in COPY. Note tableOptions, preActions, postActions, and maxStrLength are relevant only when writing data from Azure Databricks to a new table in Azure Synapse. externalDataSource is relevant only when reading data from Azure Synapse and writing data from Azure Databricks to a new table in Azure Synapse with PolyBase semantics. You should not specify other storage authentication types while using externalDataSource such as forwardSparkAzureStorageCredentials or useAzureMSI. checkpointLocation and numStreamingTempDirsToKeep are relevant only for streaming writes from Azure Databricks to a new table in Azure Synapse. Even though all data source option names are case-insensitive, we recommend that you specify them in “camel case” for clarity. Query pushdown into Azure Synapse The Azure Synapse connector implements a set of optimization rules to push the following operators down into Azure Synapse: Filter Project Limit The Project and Filter operators support the following expressions: Most boolean logic operators Comparisons Basic arithmetic operations Numeric and string casts For the Limit operator, pushdown is supported only when there is no ordering specified. For example: SELECT TOP(10) * FROM table, but not SELECT TOP(10) * FROM table ORDER BY col. Note The Azure Synapse connector does not push down expressions operating on strings, dates, or timestamps. Query pushdown built with the Azure Synapse connector is enabled by default. You can disable it by setting spark.databricks.sqldw.pushdown to false. Temporary data management The Azure Synapse connector does not delete the temporary files that it creates in the Blob storage container. Therefore we recommend that you periodically delete temporary files under the user-supplied tempDir location. To facilitate data cleanup, the Azure Synapse connector does not store data files directly under tempDir, but instead creates a subdirectory of the form: <tempDir>/<yyyy-MM-dd>/<HH-mm-ss-SSS>/<randomUUID>/. You can set up periodic jobs (using the Azure Databricks jobs feature or otherwise) to recursively delete any subdirectories that are older than a given threshold (for example, 2 days), with the assumption that there cannot be Spark jobs running longer than that threshold. A simpler alternative is to periodically drop the whole container and create a new one with the same name. This requires that you use a dedicated container for the temporary data produced by the Azure Synapse connector and that you can find a time window in which you can guarantee that no queries involving the connector are running. Temporary object management The Azure Synapse connector automates data transfer between an Azure Databricks cluster and an Azure Synapse instance. For reading data from an Azure Synapse table or query or writing data to an Azure Synapse table, the Azure Synapse connector creates temporary objects, including DATABASE SCOPED CREDENTIAL, EXTERNAL DATA SOURCE, EXTERNAL FILE FORMAT, and EXTERNAL TABLE behind the scenes. These objects live only throughout the duration of the corresponding Spark job and should automatically be dropped thereafter. When a cluster is running a query using the Azure Synapse connector, if the Spark driver process crashes or is forcefully restarted, or if the cluster is forcefully terminated or restarted, temporary objects might not be dropped. To facilitate identification and manual deletion of these objects, the Azure Synapse connector prefixes the names of all intermediate temporary objects created in the Azure Synapse instance with a tag of the form: tmp_databricks_<yyyy_MM_dd_HH_mm_ss_SSS>_<randomUUID>_<internalObject>. We recommend that you periodically look for leaked objects using queries such as the following: SELECT * FROM sys.database_scoped_credentials WHERE name LIKE 'tmp_databricks_%' SELECT * FROM sys.external_data_sources WHERE name LIKE 'tmp_databricks_%' SELECT * FROM sys.external_file_formats WHERE name LIKE 'tmp_databricks_%' SELECT * FROM sys.external_tables WHERE name LIKE 'tmp_databricks_%' Streaming checkpoint table management The Azure Synapse connector does not delete the streaming checkpoint table that is created when new streaming query is started. This behavior is consistent with the checkpointLocation on DBFS. Therefore we recommend that you periodically delete checkpoint tables at the same time as removing checkpoint locations on DBFS for queries that are not going to be run in the future or already have checkpoint location removed. By default, all checkpoint tables have the name <prefix>_<query_id>, where <prefix> is a configurable prefix with default value databricks_streaming_checkpoint and query_id is a streaming query ID with _ characters removed. To find all checkpoint tables for stale or deleted streaming queries, run the query: SQL Copy SELECT * FROM sys.tables WHERE name LIKE 'databricks_streaming_checkpoint%'
 You can configure the prefix with the Spark SQL configuration option spark.databricks.sqldw.streaming.exactlyOnce.checkpointTableNamePrefix. Frequently asked questions (FAQ) I received an error while using the Azure Synapse connector. How can I tell if this error is from Azure Synapse or Azure Databricks? To help you debug errors, any exception thrown by code that is specific to the Azure Synapse connector is wrapped in an exception extending the SqlDWException trait. Exceptions also make the following distinction: SqlDWConnectorException represents an error thrown by the Azure Synapse connector SqlDWSideException represents an error thrown by the connected Azure Synapse instance What should I do if my query failed with the error “No access key found in the session conf or the global Hadoop conf”? This error means that Azure Synapse connector could not find the storage account access key in the notebook session configuration or global Hadoop configuration for the storage account specified in tempDir. See Usage (Batch) for examples of how to configure Storage Account access properly. If a Spark table is created using Azure Synapse connector, you must still provide the storage account access credentials in order to read or write to the Spark table. Can I use a Shared Access Signature (SAS) to access the Blob storage container specified by tempDir? Azure Synapse does not support using SAS to access Blob storage. Therefore the Azure Synapse connector does not support SAS to access the Blob storage container specified by tempDir. I created a Spark table using Azure Synapse connector with the dbTable option, wrote some data to this Spark table, and then dropped this Spark table. Will the table created at the Azure Synapse side be dropped? No. Azure Synapse is considered an external data source. The Azure Synapse table with the name set through dbTable is not dropped when the Spark table is dropped. When writing a DataFrame to Azure Synapse, why do I need to say .option(""dbTable"", tableName).save() instead of just .saveAsTable(tableName)? That is because we want to make the following distinction clear: .option(""dbTable"", tableName) refers to the database (that is, Azure Synapse) table, whereas .saveAsTable(tableName) refers to the Spark table. In fact, you could even combine the two: df.write. ... .option(""dbTable"", tableNameDW).saveAsTable(tableNameSpark) which creates a table in Azure Synapse called tableNameDW and an external table in Spark called tableNameSpark that is backed by the Azure Synapse table. Warning Beware of the following difference between .save() and .saveAsTable(): For df.write. ... .option(""dbTable"", tableNameDW).mode(writeMode).save(), writeMode acts on the Azure Synapse table, as expected. For df.write. ... .option(""dbTable"", tableNameDW).mode(writeMode).saveAsTable(tableNameSpark), writeMode acts on the Spark table, whereas tableNameDW is silently overwritten if it already exists in Azure Synapse. This behavior is no different from writing to any other data source. It is just a caveat of the Spark DataFrameWriter API.",Azure Synapse Analytics
10,2206020060000932 | ARR | RCA on job slowness,"Hello Team,

Greetings! Hope you are doing well.

CX have experienced job slowness issue on May 30, 31. 

Below are the details:

Troubleshooting done so far:
Connected with CX to discuss the issue on teams meeting.
They are having ADF pipeline established and triggering jobs from the same on the Databricks cluster.

CX mentioned they have Interactive cluster on which they are executing these jobs.
The pipeline is scheduled to run every hour on daily basis.
They observed the jobs which were executed on Databricks cluster are running slow from May 30th Evening (around 17:30) and 31st Morning (12:30)

The pipeline is as follows:
Every hour pipeline runs and if it finds new data in SAP then it fetches the new data => place the data in DataLake => perform transformations on the new data using Databricks => place the data in Delta Lake => place the data in Synapse.
CX claims that the stage in Databricks job where the temporary data is being written that stage is taking longer time than expected. This behavior was observed on 30th and 31st May.

The CPU usage on Databricks cluster as per the CX is 95%
CX is requesting for RCA on why the jobs on Databricks cluster were running slow for 30th and 31st May
On next days, they jobs were running fine with higher data load as well.

 
Details shared by CX:
 
Bad Run Databricks URL- https://adb-5124199098898939.19.azuredatabricks.net/?o=5124199098898939#job/1087290199972396/run/36598394

Bad Run ADF Pipeline ID -5e133bb7-7b38-4597-99b5-8d48ee209343

Good Run Databricks URL - https://adb-5124199098898939.19.azuredatabricks.net/?o=5124199098898939#job/634880719087768/run/36689104

Good Run ADF Pipeline ID - c9376cde-89ac-4257-99ed-2a6d6eb06099

Attached the required files.

We tried running DBInsights on this, but it failed twice. One succeeded after 2.5hours but the file had some issue and is not opening. Checked it with the team internally about this but the file did not work. 
 
Please help us :
We have received consent from the CX to access the Databricks workspace to check the job runs and investigate the same.
Kindly help in providing the CX with the RCA of slow job runs.",https://docs.microsoft.com/en-us/azure/databricks/delta/concurrency-control,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Concurrency control Article 05/04/2022 4 minutes to read 4 contributors In this article Optimistic concurrency control Write conflicts Avoid conflicts using partitioning and disjoint command conditions Conflict exceptions Delta Lake provides ACID transaction guarantees between reads and writes. This means that: Multiple writers across multiple clusters can simultaneously modify a table partition and see a consistent snapshot view of the table and there will be a serial order for these writes. Readers continue to see a consistent snapshot view of the table that the Azure Databricks job started with, even when a table is modified during a job. Optimistic concurrency control Delta Lake uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages: Read: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten). Write: Stages all the changes by writing new data files. Validate and commit: Before committing the changes, checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read. If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds. However, if there are conflicts, the write operation fails with a concurrent modification exception rather than corrupting the table as would happen with the write operation on a Parquet table. The isolation level of a table defines the degree to which a transaction must be isolated from modifications made by concurrent operations. For information on the isolation levels supported by Delta Lake on Azure Databricks, see Isolation levels. Write conflicts The following table describes which pairs of write operations can conflict in each isolation level. INSERT UPDATE, DELETE, MERGE INTO OPTIMIZE INSERT Cannot conflict UPDATE, DELETE, MERGE INTO Can conflict in Serializable, cannot conflict in WriteSerializable if it writes to the table without reading first Can conflict in Serializable and WriteSerializable OPTIMIZE Cannot conflict Can conflict in Serializable and WriteSerializable Can conflict in Serializable and WriteSerializable Avoid conflicts using partitioning and disjoint command conditions In all cases marked “can conflict”, whether the two operations will conflict depends on whether they operate on the same set of files. You can make the two sets of files disjoint by partitioning the table by the same columns as those used in the conditions of the operations. For example, the two commands UPDATE table WHERE date > '2010-01-01' ... and DELETE table WHERE date < '2010-01-01' will conflict if the table is not partitioned by date, as both can attempt to modify the same set of files. Partitioning the table by date will avoid the conflict. Hence, partitioning a table according to the conditions commonly used on the command can reduce conflicts significantly. However, partitioning a table by a column that has high cardinality can lead to other performance issues due to large number of subdirectories. Conflict exceptions When a transaction conflict occurs, you will observe one of the following exceptions: ConcurrentAppendException ConcurrentDeleteReadException ConcurrentDeleteDeleteException MetadataChangedException ConcurrentTransactionException ProtocolChangedException ConcurrentAppendException This exception occurs when a concurrent operation adds files in the same partition (or anywhere in an unpartitioned table) that your operation reads. The file additions can be caused by INSERT, DELETE, UPDATE, or MERGE operations. With the default isolation level of WriteSerializable, files added by blind INSERT operations (that is, operations that blindly append data without reading any data) do not conflict with any operation, even if they touch the same partition (or anywhere in an unpartitioned table). If the isolation level is set to Serializable, then blind appends may conflict. This exception is often thrown during concurrent DELETE, UPDATE, or MERGE operations. While the concurrent operations may be physically updating different partition directories, one of them may read the same partition that the other one concurrently updates, thus causing a conflict. You can avoid this by making the separation explicit in the operation condition. Consider the following example. Scala Copy // Target 'deltaTable' is partitioned by date and country
deltaTable.as(""t"").merge(
    source.as(""s""),
    ""s.user_id = t.user_id AND s.date = t.date AND s.country = t.country"")
  .whenMatched().updateAll()
  .whenNotMatched().insertAll()
  .execute()
 Suppose you run the above code concurrently for different dates or countries. Since each job is working on an independent partition on the target Delta table, you don’t expect any conflicts. However, the condition is not explicit enough and can scan the entire table and can conflict with concurrent operations updating any other partitions. Instead, you can rewrite your statement to add specific date and country to the merge condition, as shown in the following example. Scala Copy // Target 'deltaTable' is partitioned by date and country
deltaTable.as(""t"").merge(
    source.as(""s""),
    ""s.user_id = t.user_id AND s.date = t.date AND s.country = t.country AND t.date = '"" + <date> + ""' AND t.country = '"" + <country> + ""'"")
  .whenMatched().updateAll()
  .whenNotMatched().insertAll()
  .execute()
 This operation is now safe to run concurrently on different dates and countries. ConcurrentDeleteReadException This exception occurs when a concurrent operation deleted a file that your operation read. Common causes are a DELETE, UPDATE, or MERGE operation that rewrites files. ConcurrentDeleteDeleteException This exception occurs when a concurrent operation deleted a file that your operation also deletes. This could be caused by two concurrent compaction operations rewriting the same files. MetadataChangedException This exception occurs when a concurrent transaction updates the metadata of a Delta table. Common causes are ALTER TABLE operations or writes to your Delta table that update the schema of the table. ConcurrentTransactionException If a streaming query using the same checkpoint location is started multiple times concurrently and tries to write to the Delta table at the same time. You should never have two streaming queries use the same checkpoint location and run at the same time. ProtocolChangedException This exception can occur in the following cases: When your Delta table is upgraded to a new version. For future operations to succeed you may need to upgrade your Delta Lake version. When multiple writers are creating or replacing a table at the same time. When multiple writers are writing to an empty path at the same time. See Table protocol versioning for more details.",Concurrency control
11,ARR - Approx 400 users are not able to login to  both DEV and SIT ADB workspaces,"Approx 400 users are not able to login to  both DEV and SIT ADB workspaces
https://adb-6563319279822757.17.azuredatabricks.net/",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/scim/scim-users,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents SCIM API 2.0 (Users) Article 06/30/2022 6 minutes to read 4 contributors In this article Requirements Get users Get user by ID Create user Update user by ID (PATCH) Update user by ID (PUT) Delete user by ID Activate and deactivate user by ID Filter active and inactive users Automatically deactivate users Get the maximum user inactivity period of a workspace Important This feature is in Public Preview. An Azure Databricks administrator can invoke all SCIM API endpoints. Non-admin users can invoke the Get users endpoint to read user display names and IDs. Note Each workspace can have a maximum of 10,000 users and 5,000 groups. Service principals count toward the user maximum. SCIM (Users) lets you create users in Azure Databricks and give them the proper level of access, temporarily lock and unlock user accounts, and remove access for users (deprovision them) when they leave your organization or no longer need access to Azure Databricks. For error codes, see SCIM API 2.0 Error Codes. Requirements Your Azure Databricks account must have the Premium Plan. Get users Endpoint HTTP Method 2.0/preview/scim/v2/Users GET Admin users: Retrieve a list of all users in the Azure Databricks workspace. Non-admin users: Retrieve a list of all users in the Azure Databricks workspace, returning username, user display name, and object ID only. Examples This example gets information about all users. Bash Copy curl --netrc -X GET \
https://<databricks-instance>/api/2.0/preview/scim/v2/Users \
| jq .
 This example uses the eq (equals) filter query parameter with userName to get information about a specific user. Bash Copy curl --netrc -X GET \
""https://<databricks-instance>/api/2.0/preview/scim/v2/Users?filter=userName+eq+<username>"" \
| jq .
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <username> with the Azure Databricks workspace username of the user, for example someone@example.com. This example uses a .netrc file and jq. Get user by ID Endpoint HTTP Method 2.0/preview/scim/v2/Users/{id} GET Admin users: Retrieve a single user resource from the Azure Databricks workspace, given their Azure Databricks ID. Example Request Bash Copy curl --netrc -X GET \
https://<databricks-instance>/api/2.0/preview/scim/v2/Users/<user-id> \
| jq .
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <user-id> with the Azure Databricks workspace ID of the user, for example 2345678901234567. To get the user ID, call Get users. This example uses a .netrc file and jq. Response Create user Endpoint HTTP Method 2.0/preview/scim/v2/Users POST Admin users: Create a user in the Azure Databricks workspace. Request parameters follow the standard SCIM 2.0 protocol. Requests must include the following attributes: schemas set to urn:ietf:params:scim:schemas:core:2.0:User userName Example Bash Copy curl --netrc -X POST \
https://<databricks-instance>/api/2.0/preview/scim/v2/Users \
--header 'Content-type: application/scim+json' \
--data @create-user.json \
| jq .
 create-user.json: JSON Copy {
  ""schemas"": [ ""urn:ietf:params:scim:schemas:core:2.0:User"" ],
  ""userName"": ""<username>"",
  ""groups"": [
    {
       ""value"":""123456""
    }
  ],
  ""entitlements"":[
    {
       ""value"":""allow-cluster-create""
    }
  ]
}
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <username> with the Azure Databricks workspace username of the user, for example someone@example.com. This example uses a .netrc file and jq. Update user by ID (PATCH) Endpoint HTTP Method 2.0/preview/scim/v2/Users/{id} PATCH Admin users: Update a user resource with operations on specific attributes, except those that are immutable (userName and userId). The PATCH method is recommended over the PUT method for setting or updating user entitlements. Request parameters follow the standard SCIM 2.0 protocol and depend on the value of the schemas attribute. Example This example adds the allow-cluster-create entitlement to the specified user. Bash Copy curl --netrc -X PATCH \
https://<databricks-instance>/api/2.0/preview/scim/v2/Users/<user-id> \
--header 'Content-type: application/scim+json' \
--data @update-user.json \
| jq .
 update-user.json: JSON Copy {
  ""schemas"": [ ""urn:ietf:params:scim:api:messages:2.0:PatchOp"" ],
  ""Operations"": [
    {
      ""op"": ""add"",
      ""path"": ""entitlements"",
      ""value"": [
        {
           ""value"": ""allow-cluster-create""
        }
      ]
    }
  ]
}
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <user-id> with the Azure Databricks workspace ID of the user, for example 2345678901234567. To get the user ID, call Get users. This example uses a .netrc file and jq. Update user by ID (PUT) Endpoint HTTP Method 2.0/preview/scim/v2/Users/{id} PUT Admin users: Overwrite the user resource across multiple attributes, except those that are immutable (userName and userId). Request must include the schemas attribute, set to urn:ietf:params:scim:schemas:core:2.0:User. Note The PATCH method is recommended over the PUT method for setting or updating user entitlements. Example This example changes the specified user’s previous entitlements to now have only the allow-cluster-create entitlement. Bash Copy curl --netrc -X PUT \
https://<databricks-instance>/api/2.0/preview/scim/v2/Users/<user-id> \
--header 'Content-type: application/scim+json' \
--data @overwrite-user.json \
| jq .
 overwrite-user.json: JSON Copy {
  ""schemas"": [ ""urn:ietf:params:scim:schemas:core:2.0:User"" ],
  ""userName"": ""<username>"",
  ""entitlements"": [
    {
      ""value"": ""allow-cluster-create""
    }
  ]
}
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <user-id> with the Azure Databricks workspace ID of the user, for example 2345678901234567. To get the user ID, call Get users. <username> with the Azure Databricks workspace username of the user, for example someone@example.com. To get the username, call Get users. This example uses a .netrc file and jq. Delete user by ID Endpoint HTTP Method 2.0/preview/scim/v2/Users/{id} DELETE Admin users: Remove a user resource. A user that does not own or belong to a workspace in Azure Databricks is automatically purged after 30 days. Deleting a user from a workspace also removes objects associated with the user. For example, notebooks are archived, clusters are terminated, and jobs become ownerless. The user’s home directory is not automatically deleted. Only an administrator can access or remove a deleted user’s home directory. The access control list (ACL) configuration of a user is preserved even after that user is removed from a workspace. Example request Bash Copy curl --netrc -X DELETE \
https://<databricks-instance>/api/2.0/preview/scim/v2/Users/<user-id>
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <user-id> with the Azure Databricks workspace ID of the user, for example 2345678901234567. To get the user ID, call Get users. This example uses a .netrc file. Activate and deactivate user by ID Important This feature is in Public Preview. Endpoint HTTP Method 2.0/preview/scim/v2/Users/{id} PATCH Admin users: Activate or deactivate a user. Deactivating a user removes all access to a workspace for that user but leaves permissions and objects associated with the user unchanged. Clusters associated with the user keep running, and notebooks remain in their original locations. The user’s tokens are retained but cannot be used to authenticate while the user is deactivated. Scheduled jobs, however, fail unless assigned to a new owner. You can use the Get users and Get user by ID requests to view whether users are active or inactive. Note Allow at least five minutes for the cache to be cleared for deactivation to take effect. Important An Azure Active Directory (Azure AD) user with the Contributor or Owner role on the Azure Databricks subscription can reactivate themselves using the Azure AD login flow. If a user with one of these roles needs to be deactivated, you should also revoke their privileges on the subscription. Set the active value to false to deactivate a user and true to activate a user. Example Request Bash Copy curl --netrc -X PATCH \
https://<databricks-instance>/api/2.0/preview/scim/v2/Users/<user-id> \
--header 'Content-type: application/scim+json' \
--data @toggle-user-activation.json \
| jq .
 toggle-user-activation.json: JSON Copy {
  ""schemas"": [ ""urn:ietf:params:scim:api:messages:2.0:PatchOp"" ],
  ""Operations"": [
    {
      ""op"": ""replace"",
      ""path"": ""active"",
      ""value"": [
        {
          ""value"": ""false""
        }
      ]
    }
  ]
}
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <user-id> with the Azure Databricks workspace ID of the user, for example 2345678901234567. This example uses a .netrc file and jq. Response JSON Copy {
  ""emails"": [
    {
      ""type"": ""work"",
      ""value"": ""someone@example.com"",
      ""primary"": true
    }
  ],
  ""displayName"": ""Someone User"",
  ""schemas"": [
    ""urn:ietf:params:scim:schemas:core:2.0:User"",
    ""urn:ietf:params:scim:schemas:extension:workspace:2.0:User""
  ],
  ""name"": {
    ""familyName"": ""User"",
    ""givenName"": ""Someone""
  },
  ""active"": false,
  ""groups"": [],
  ""id"": ""123456"",
  ""userName"": ""someone@example.com""
}
 Filter active and inactive users Important This feature is in Public Preview. Endpoint HTTP Method 2.0/preview/scim/v2/Users GET Admin users: Retrieve a list of active or inactive users. Example Bash Copy curl --netrc -X GET \
""https://<databricks-instance>/api/2.0/preview/scim/v2/Users?filter=active+eq+false"" \
| jq .
 Replace <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. This example uses a .netrc file and jq. Automatically deactivate users Important This feature is in Public Preview. Admin users: Deactivate users that have not logged in for a customizable period. Scheduled jobs owned by a user are also considered activity. Endpoint HTTP Method 2.0/preview/workspace-conf PATCH The request body is a key-value pair where the value is the time limit for how long a user can be inactive before being automatically deactivated. Example Bash Copy curl --netrc -X PATCH \
https://<databricks-instance>/api/2.0/preview/workspace-conf \
--data @deactivate-users.json \
| jq .
 deactivate-users.json: JSON Copy {
  ""maxUserInactiveDays"": ""90""
}
 Replace <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. This example uses a .netrc file and jq. Get the maximum user inactivity period of a workspace Important This feature is in Public Preview. Admin users: Retrieve the user inactivity limit defined for a workspace. Endpoint HTTP Method 2.0/preview/workspace-conf GET Example request Bash Copy curl --netrc -X GET \
""https://<databricks-instance>/api/2.0/preview/workspace-conf?keys=maxUserInactiveDays"" \
| jq .
 Replace <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. This example uses a .netrc file and jq. Example response JSON Copy {
  ""maxUserInactiveDays"": ""90""
}",SCIM API 2.0 (Users)
12,ARR - Approx 400 users are not able to login to  both DEV and SIT ADB workspaces,"Approx 400 users are not able to login to  both DEV and SIT ADB workspaces
https://adb-6563319279822757.17.azuredatabricks.net/",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/scim/scim-groups,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents SCIM API 2.0 (Groups) Article 06/21/2022 3 minutes to read 4 contributors In this article Requirements Get groups Get group by ID Create group Update group Delete group Important This feature is in Public Preview. Requirements Your Azure Databricks account must have the Premium Plan. Note An Azure Databricks administrator can invoke all SCIM API endpoints. Non-admin users can invoke the Get groups endpoint to read group display names and IDs. You can have no more than 10,000 users and 5,000 groups in a workspace. SCIM (Groups) lets you create users and groups in Azure Databricks and give them the proper level of access and remove access for groups (deprovision them). For error codes, see SCIM API 2.0 Error Codes. Get groups Endpoint HTTP Method 2.0/preview/scim/v2/Groups GET Admin users: Retrieve a list of all groups in the Azure Databricks workspace. Non-admin users: Retrieve a list of all groups in the Azure Databricks workspace, returning group display name and object ID only. Examples Bash Copy curl --netrc -X GET \
https://<databricks-instance>/api/2.0/preview/scim/v2/Groups \
| jq .
 You can use filters to specify subsets of groups. For example, you can apply the sw (starts with) filter parameter to displayName to retrieve a specific group or set of groups. This example retrieves all groups with a displayName field that start with my-. Bash Copy curl --netrc -X GET \
""https://<databricks-instance>/api/2.0/preview/scim/v2/Groups?filter=displayName+sw+my-"" \
| jq .
 Replace <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. This example uses a .netrc file and jq. Get group by ID Endpoint HTTP Method 2.0/preview/scim/v2/Groups/{id} GET Admin users: Retrieve a single group resource. Example request Bash Copy curl --netrc -X GET \
https://<databricks-instance>/api/2.0/preview/scim/v2/Groups/<group-id> \
| jq .
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <group-id> with the ID of the group in the Azure Databricks workspace, for example 2345678901234567. To get the group ID, call Get groups. This example uses a .netrc file and jq. Create group Endpoint HTTP Method 2.0/preview/scim/v2/Groups POST Admin users: Create a group in Azure Databricks. Request parameters follow the standard SCIM 2.0 protocol. Requests must include the following attributes: schemas set to urn:ietf:params:scim:schemas:core:2.0:Group displayName Members list is optional and can include users and other groups. You can also add members to a group using PATCH. Example Bash Copy curl --netrc -X POST \
https://<databricks-instance>/api/2.0/preview/scim/v2/Groups \
--header 'Content-type: application/scim+json' \
--data @create-group.json \
| jq .
 create-group.json: JSON Copy {
  ""schemas"": [ ""urn:ietf:params:scim:schemas:core:2.0:Group"" ],
  ""displayName"": ""<group-name>"",
  ""members"": [
    {
      ""value"":""<user-id>""
    }
  ]
}
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <group-name> with the name of the group in the Azure Databricks workspace, for example my-group. <user-id> with the Azure Databricks workspace ID of the user, for example 2345678901234567. To get the user ID, call Get users. This example uses a .netrc file and jq. Update group Endpoint HTTP Method 2.0/preview/scim/v2/Groups/{id} PATCH Admin users: Update a group in Azure Databricks by adding or removing members. Can add and remove individual members or groups within the group. Request parameters follow the standard SCIM 2.0 protocol and depend on the value of the schemas attribute. Note Azure Databricks does not support updating group names. Example Bash Copy curl --netrc -X PATCH \
https://<databricks-instance>/api/2.0/preview/scim/v2/Groups/<group-id> \
--header 'Content-type: application/scim+json' \
--data @update-group.json \
| jq .
 Add to group update-group.json: JSON Copy {
  ""schemas"": [ ""urn:ietf:params:scim:api:messages:2.0:PatchOp"" ],
  ""Operations"": [
    {
      ""op"":""add"",
      ""value"": {
        ""members"": [
          {
            ""value"":""<user-id>""
          }
        ]
      }
    }
  ]
}
 Remove from group update-group.json: JSON Copy {
  ""schemas"": [ ""urn:ietf:params:scim:api:messages:2.0:PatchOp"" ],
  ""Operations"": [
    {
      ""op"": ""remove"",
      ""path"": ""members[value eq \""<user-id>\""]""
    }
  ]
}
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <group-id> with the ID of the group in the Azure Databricks workspace, for example 2345678901234567. To get the group ID, call Get groups. <user-id> with the Azure Databricks workspace ID of the user, for example 2345678901234567. To get the user ID, call Get users. This example uses a .netrc file and jq. Delete group Endpoint HTTP Method 2.0/preview/scim/v2/Groups/{id} DELETE Admin users: Remove a group from Azure Databricks. Users in the group are not removed. Example Bash Copy curl --netrc -X DELETE \
https://<databricks-instance>/api/2.0/preview/scim/v2/Groups/<group-id>
 Replace: <databricks-instance> with the Azure Databricks workspace instance name, for example adb-1234567890123456.7.azuredatabricks.net. <group-id> with the ID of the group in the Azure Databricks workspace, for example 2345678901234567. To get the group ID, call Get groups. This example uses a .netrc file.",SCIM API 2.0 (Groups)
13,Questions related to databases pointing to azure dalatake storage gen2,"Hi team,

Issue: Questions related to databases pointing to azure dalatake storage gen2.

Cx wants to update the location of a database in Databricks and tried doing so using below command:

alter database 'db-name' set location 'location'; 

but it gives below error:  Error in SQL statement: AnalysisException: Hive 0.13.1 does not support altering database location

As this is the expected behavior on metastore version 0.13.1, do we have any other recommendations to update the location of the database.

Thank you.",https://docs.microsoft.com/en-us/azure/databricks/data/metastores/external-hive-metastore,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents External Apache Hive metastore Article 06/16/2022 8 minutes to read 6 contributors In this article Hive metastore setup Cluster configurations Set up an external metastore using the UI Set up an external metastore using an init script Troubleshooting This article describes how to set up Azure Databricks clusters to connect to existing external Apache Hive metastores. It provides information about recommended metastore setup and cluster configuration requirements, followed by instructions for configuring clusters to connect to an external metastore. The following table summarizes which Hive metastore versions are supported in each version of Databricks Runtime. Databricks Runtime Version 0.13 - 1.2.1 2.1 2.2 2.3 3.1.0 7.x Yes Yes Yes Yes Yes 6.x Yes Yes Yes Yes Yes 5.3 and above Yes Yes Yes Yes Yes 5.1 - 5.2 and 4.x Yes Yes Yes Yes No 3.x Yes Yes No No No Important SQL Server does not work as the underlying metastore database for Hive 2.0 and above; however, Azure SQL Database does work and is used as the example throughout this article. You can use a Hive 1.2.0 or 1.2.1 metastore of an HDInsight cluster as an external metastore. See Use external metadata stores in Azure HDInsight. If you use Azure Database for MySQL as an external metastore, you must change the value of the lower_case_table_names property from 1 (the default) to 2 in the server-side database configuration. For details, see Identifier Case Sensitivity. Hive metastore setup The metastore client running inside a cluster connects to your underlying metastore database directly using JDBC. To test network connectivity from a cluster to the metastore, you can run the following command inside a notebook: Bash Copy %sh
nc -vz <DNS name> <port>
 where <DNS name> is the server name of Azure SQL Database. <port> is the port of the database. Cluster configurations You must set two sets of configuration options to connect a cluster to an external metastore: Spark options configure Spark with the Hive metastore version and the JARs for the metastore client. Hive options configure the metastore client to connect to the external metastore. Spark configuration options Set spark.sql.hive.metastore.version to the version of your Hive metastore and spark.sql.hive.metastore.jars as follows: Hive 0.13: do not set spark.sql.hive.metastore.jars. Hive 1.2.0 or 1.2.1 (Databricks Runtime 6.6 and below): set spark.sql.hive.metastore.jars to builtin. Note Hive 1.2.0 and 1.2.1 are not the built-in metastore on Databricks Runtime 7.0 and above. If you want to use Hive 1.2.0 or 1.2.1 with Databricks Runtime 7.0 and above, follow the procedure described in Download the metastore jars and point to them. Hive 2.3.7 (Databricks Runtime 7.0 - 9.x) or Hive 2.3.9 (Databricks Runtime 10.0 and above): set spark.sql.hive.metastore.jars to builtin. For all other Hive versions, Azure Databricks recommends that you download the metastore JARs and set the configuration spark.sql.hive.metastore.jars to point to the downloaded JARs using the procedure described in Download the metastore jars and point to them. Download the metastore jars and point to them Create a cluster with spark.sql.hive.metastore.jars set to maven and spark.sql.hive.metastore.version to match the version of your metastore. When the cluster is running, search the driver log and find a line like the following: Copy 17/11/18 22:41:19 INFO IsolatedClientLoader: Downloaded metastore jars to <path>
 The directory <path> is the location of downloaded JARs in the driver node of the cluster. Alternatively you can run the following code in a Scala notebook to print the location of the JARs: Scala Copy import com.typesafe.config.ConfigFactory
val path = ConfigFactory.load().getString(""java.io.tmpdir"")

println(s""\nHive JARs are downloaded to the path: $path \n"")
 Run %sh cp -r <path> /dbfs/hive_metastore_jar (replacing <path> with your cluster’s info) to copy this directory to a directory in DBFS called hive_metastore_jar through the FUSE client in the driver node. Create an init script that copies /dbfs/hive_metastore_jar to the local filesystem of the node, making sure to make the init script sleep a few seconds before it accesses the DBFS FUSE client. This ensures that the client is ready. Set spark.sql.hive.metastore.jars to use this directory. If your init script copies /dbfs/hive_metastore_jar to /databricks/hive_metastore_jars/, set spark.sql.hive.metastore.jars to /databricks/hive_metastore_jars/*. The location must include the trailing /*. Restart the cluster. Hive configuration options This section describes options specific to Hive. To connect to an external metastore using local mode, set the following Hive configuration options: ini Copy # JDBC connect string for a JDBC metastore
javax.jdo.option.ConnectionURL <mssql-connection-string>

# Username to use against metastore database
javax.jdo.option.ConnectionUserName <mssql-username>

# Password to use against metastore database
javax.jdo.option.ConnectionPassword <mssql-password>

# Driver class name for a JDBC metastore
javax.jdo.option.ConnectionDriverName com.microsoft.sqlserver.jdbc.SQLServerDriver
 where <mssql-connection-string> is the JDBC connection string (which you can get in the Azure portal). You do not need to include username and password in the connection string, because these will be set by javax.jdo.option.ConnectionUserName and javax.jdo.option.ConnectionDriverName. <mssql-username> and <mssql-password> specify the username and password of your Azure SQL Database account that has read/write access to the database. Note For production environments, we recommend that you set hive.metastore.schema.verification to true. This prevents Hive metastore client from implicitly modifying the metastore database schema when the metastore client version does not match the metastore database version. When enabling this setting for metastore client versions lower than Hive 1.2.0, make sure that the metastore client has the write permission to the metastore database (to prevent the issue described in HIVE-9749). For Hive metastore 1.2.0 and higher, set hive.metastore.schema.verification.record.version to true to enable hive.metastore.schema.verification. For Hive metastore 2.1.1 and higher, set hive.metastore.schema.verification.record.version to true as it is set to false by default. Set up an external metastore using the UI To set up an external metastore using the Azure Databricks UI: Click the Clusters button on the sidebar. Click Create Cluster. Enter the following Spark configuration options: ini Copy # Hive-specific configuration options.
# spark.hadoop prefix is added to make sure these Hive specific options propagate to the metastore client.
# JDBC connect string for a JDBC metastore
spark.hadoop.javax.jdo.option.ConnectionURL <mssql-connection-string>

# Username to use against metastore database
spark.hadoop.javax.jdo.option.ConnectionUserName <mssql-username>

# Password to use against metastore database
spark.hadoop.javax.jdo.option.ConnectionPassword <mssql-password>

# Driver class name for a JDBC metastore
spark.hadoop.javax.jdo.option.ConnectionDriverName com.microsoft.sqlserver.jdbc.SQLServerDriver

# Spark specific configuration options
spark.sql.hive.metastore.version <hive-version>
# Skip this one if <hive-version> is 0.13.x.
spark.sql.hive.metastore.jars <hive-jar-source>
 Continue your cluster configuration, following the instructions in Configure clusters. Click Create Cluster to create the cluster. Set up an external metastore using an init script Init scripts let you connect to an existing Hive metastore without manually setting required configurations. Create the base directory you want to store the init script in if it does not exist. The following example uses dbfs:/databricks/scripts. Run the following snippet in a notebook. The snippet creates the init script /databricks/scripts/external-metastore.sh in Databricks File System (DBFS). Alternatively, you can use the DBFS REST API’s put operation to create the init script. This init script writes required configuration options to a configuration file named 00-custom-spark.conf in a JSON-like format under /databricks/driver/conf/ inside every node of the cluster, whenever a cluster with the name specified as <cluster-name> starts. Azure Databricks provides default Spark configurations in the /databricks/driver/conf/spark-branch.conf file. Configuration files in the /databricks/driver/conf directory apply in reverse alphabetical order. If you want to change the name of the 00-custom-spark.conf file, make sure that it continues to apply before the spark-branch.conf file. Scala Scala Copy dbutils.fs.put(
    ""/databricks/scripts/external-metastore.sh"",
    """"""#!/bin/sh
      |# Loads environment variables to determine the correct JDBC driver to use.
      |source /etc/environment
      |# Quoting the label (i.e. EOF) with single quotes to disable variable interpolation.
      |cat << 'EOF' > /databricks/driver/conf/00-custom-spark.conf
      |[driver] {
      |    # Hive specific configuration options.
      |    # spark.hadoop prefix is added to make sure these Hive specific options will propagate to the metastore client.
      |    # JDBC connect string for a JDBC metastore
      |    ""spark.hadoop.javax.jdo.option.ConnectionURL"" = ""<mssql-connection-string>""
      |
      |    # Username to use against metastore database
      |    ""spark.hadoop.javax.jdo.option.ConnectionUserName"" = ""<mssql-username>""
      |
      |    # Password to use against metastore database
      |    ""spark.hadoop.javax.jdo.option.ConnectionPassword"" = ""<mssql-password>""
      |
      |    # Driver class name for a JDBC metastore
      |    ""spark.hadoop.javax.jdo.option.ConnectionDriverName"" = ""com.microsoft.sqlserver.jdbc.SQLServerDriver""
      |
      |    # Spark specific configuration options
      |    ""spark.sql.hive.metastore.version"" = ""<hive-version>""
      |    # Skip this one if <hive-version> is 0.13.x.
      |    ""spark.sql.hive.metastore.jars"" = ""<hive-jar-source>""
      |}
      |EOF
      |"""""".stripMargin,
    overwrite = true
)
 Python Python Copy contents = """"""#!/bin/sh
# Loads environment variables to determine the correct JDBC driver to use.
source /etc/environment
# Quoting the label (i.e. EOF) with single quotes to disable variable interpolation.
cat << 'EOF' > /databricks/driver/conf/00-custom-spark.conf
[driver] {
    # Hive specific configuration options.
    # spark.hadoop prefix is added to make sure these Hive specific options will propagate to the metastore client.
    # JDBC connect string for a JDBC metastore
    ""spark.hadoop.javax.jdo.option.ConnectionURL"" = ""<mssql-connection-string>""

    # Username to use against metastore database
    ""spark.hadoop.javax.jdo.option.ConnectionUserName"" = ""<mssql-username>""

    # Password to use against metastore database
    ""spark.hadoop.javax.jdo.option.ConnectionPassword"" = ""<mssql-password>""

    # Driver class name for a JDBC metastore
    ""spark.hadoop.javax.jdo.option.ConnectionDriverName"" = ""com.microsoft.sqlserver.jdbc.SQLServerDriver""

    # Spark specific configuration options
    ""spark.sql.hive.metastore.version"" = ""<hive-version>""
    # Skip this one if <hive-version> is 0.13.x.
    ""spark.sql.hive.metastore.jars"" = ""<hive-jar-source>""
    }
EOF
""""""

dbutils.fs.put(
    file = ""/databricks/scripts/external-metastore.sh"",
    contents = contents,
    overwrite = True
)
 Configure your cluster with the init script. Restart the cluster. Troubleshooting Clusters do not start (due to incorrect init script settings) If an init script for setting up the external metastore causes cluster creation failure, configure the init script to log, and debug the init script using the logs. Error in SQL statement: InvocationTargetException Error message pattern in the full exception stack trace: Copy Caused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = [...]
 External metastore JDBC connection information is misconfigured. Verify the configured hostname, port, username, password, and JDBC driver class name. Also, make sure that the username has the right privilege to access the metastore database. Error message pattern in the full exception stack trace: Copy Required table missing : ""`DBS`"" in Catalog """" Schema """". DataNucleus requires this table to perform its persistence operations. [...]
 External metastore database not properly initialized. Verify that you created the metastore database and put the correct database name in the JDBC connection string. Then, start a new cluster with the following two Spark configuration options: ini Copy datanucleus.autoCreateSchema true
datanucleus.fixedDatastore false
 In this way, the Hive client library will try to create and initialize tables in the metastore database automatically when it tries to access them but finds them absent. Error in SQL statement: AnalysisException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetastoreClient Error message in the full exception stacktrace: Copy The specified datastore driver (driver name) was not found in the CLASSPATH
 The cluster is configured to use an incorrect JDBC driver. Setting datanucleus.autoCreateSchema to true doesn’t work as expected By default, Databricks also sets datanucleus.fixedDatastore to true, which prevents any accidental structural changes to the metastore databases. Therefore, the Hive client library cannot create metastore tables even if you set datanucleus.autoCreateSchema to true. This strategy is, in general, safer for production environments since it prevents the metastore database to be accidentally upgraded. If you do want to use datanucleus.autoCreateSchema to help initialize the metastore database, make sure you set datanucleus.fixedDatastore to false. Also, you may want to flip both flags after initializing the metastore database to provide better protection to your production environment.",External Apache Hive metastore
14,Pipelines are failing with an error,">> Pipelines are failing with an error
>> This job is focusing on optimizing and vacuum
>> Taking input and doing some transformations
>> This issue is occurring first time
>> This job run failed 4 times on 27th may with the below error:
>> Timestamps of the Optimize_and_repair_delta pipeline failure on 27th May

  Run Start	           Run End
12:03:35 PM	12:04:57 PM
12:03:35 PM	12:04:56 PM
9:18:06 AM	         9:29:15 AM
8:47:20 AM	         8:48:40 AM

>> Notebook URL - imp_helpers_etl_optimize_and_vacuum_delta_table - Databricks (azuredatabricks.net)
>> Please find the Error message below :-

AnalysisException:

org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient

--------------------------------------------------------------------------- AnalysisException Traceback (most recent call last) <command-2217559976695791> in <module> ----> 1 spark.sql(""DROP TABLE IF EXISTS {0}"".format(dbutils.widgets.get('table_name'))) 2 spark.sql(""CREATE TABLE {0} USING DELTA LOCATION '{1}'"".format(dbutils.widgets.get('table_name'),dbutils.widgets.get('table_path'))) 3 spark.sql(""OPTIMIZE {0} where event_date >= '{1}'"".format(dbutils.widgets.get('table_name'), dt.datetime.strftime(dt.datetime.now() - dt.timedelta(days=4),'%Y%m%d'))) 4 #spark.sql(""OPTIMIZE {0}"".format(dbutils.widgets.get('table_name'))) 5 /databricks/spark/python/pyspark/sql/session.py in sql(self, sqlQuery) 775 [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')] 776 """""" --> 777 return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped) 778 779 def table(self, tableName): /databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py in __call__(self, *args) 1302 1303 answer = self.gateway_client.send_command(command) -> 1304 return_value = get_return_value( 1305 answer, self.gateway_client, self.target_id, self.name) 1306 /databricks/spark/python/pyspark/sql/utils.py in deco(*a, **kw) 121 # Hide where the exception came from that shows a non-Pythonic 122 # JVM exception message. --> 123 raise converted from None 124 else: 125 raise AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient

>> Consent to spin up the cluster if required is provided.
>> Consent to deploy a test cluster is also provided.

Kindly provide some insights on this!",https://docs.microsoft.com/en-us/azure/databricks/data/metastores/external-hive-metastore,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents External Apache Hive metastore Article 06/16/2022 8 minutes to read 6 contributors In this article Hive metastore setup Cluster configurations Set up an external metastore using the UI Set up an external metastore using an init script Troubleshooting This article describes how to set up Azure Databricks clusters to connect to existing external Apache Hive metastores. It provides information about recommended metastore setup and cluster configuration requirements, followed by instructions for configuring clusters to connect to an external metastore. The following table summarizes which Hive metastore versions are supported in each version of Databricks Runtime. Databricks Runtime Version 0.13 - 1.2.1 2.1 2.2 2.3 3.1.0 7.x Yes Yes Yes Yes Yes 6.x Yes Yes Yes Yes Yes 5.3 and above Yes Yes Yes Yes Yes 5.1 - 5.2 and 4.x Yes Yes Yes Yes No 3.x Yes Yes No No No Important SQL Server does not work as the underlying metastore database for Hive 2.0 and above; however, Azure SQL Database does work and is used as the example throughout this article. You can use a Hive 1.2.0 or 1.2.1 metastore of an HDInsight cluster as an external metastore. See Use external metadata stores in Azure HDInsight. If you use Azure Database for MySQL as an external metastore, you must change the value of the lower_case_table_names property from 1 (the default) to 2 in the server-side database configuration. For details, see Identifier Case Sensitivity. Hive metastore setup The metastore client running inside a cluster connects to your underlying metastore database directly using JDBC. To test network connectivity from a cluster to the metastore, you can run the following command inside a notebook: Bash Copy %sh
nc -vz <DNS name> <port>
 where <DNS name> is the server name of Azure SQL Database. <port> is the port of the database. Cluster configurations You must set two sets of configuration options to connect a cluster to an external metastore: Spark options configure Spark with the Hive metastore version and the JARs for the metastore client. Hive options configure the metastore client to connect to the external metastore. Spark configuration options Set spark.sql.hive.metastore.version to the version of your Hive metastore and spark.sql.hive.metastore.jars as follows: Hive 0.13: do not set spark.sql.hive.metastore.jars. Hive 1.2.0 or 1.2.1 (Databricks Runtime 6.6 and below): set spark.sql.hive.metastore.jars to builtin. Note Hive 1.2.0 and 1.2.1 are not the built-in metastore on Databricks Runtime 7.0 and above. If you want to use Hive 1.2.0 or 1.2.1 with Databricks Runtime 7.0 and above, follow the procedure described in Download the metastore jars and point to them. Hive 2.3.7 (Databricks Runtime 7.0 - 9.x) or Hive 2.3.9 (Databricks Runtime 10.0 and above): set spark.sql.hive.metastore.jars to builtin. For all other Hive versions, Azure Databricks recommends that you download the metastore JARs and set the configuration spark.sql.hive.metastore.jars to point to the downloaded JARs using the procedure described in Download the metastore jars and point to them. Download the metastore jars and point to them Create a cluster with spark.sql.hive.metastore.jars set to maven and spark.sql.hive.metastore.version to match the version of your metastore. When the cluster is running, search the driver log and find a line like the following: Copy 17/11/18 22:41:19 INFO IsolatedClientLoader: Downloaded metastore jars to <path>
 The directory <path> is the location of downloaded JARs in the driver node of the cluster. Alternatively you can run the following code in a Scala notebook to print the location of the JARs: Scala Copy import com.typesafe.config.ConfigFactory
val path = ConfigFactory.load().getString(""java.io.tmpdir"")

println(s""\nHive JARs are downloaded to the path: $path \n"")
 Run %sh cp -r <path> /dbfs/hive_metastore_jar (replacing <path> with your cluster’s info) to copy this directory to a directory in DBFS called hive_metastore_jar through the FUSE client in the driver node. Create an init script that copies /dbfs/hive_metastore_jar to the local filesystem of the node, making sure to make the init script sleep a few seconds before it accesses the DBFS FUSE client. This ensures that the client is ready. Set spark.sql.hive.metastore.jars to use this directory. If your init script copies /dbfs/hive_metastore_jar to /databricks/hive_metastore_jars/, set spark.sql.hive.metastore.jars to /databricks/hive_metastore_jars/*. The location must include the trailing /*. Restart the cluster. Hive configuration options This section describes options specific to Hive. To connect to an external metastore using local mode, set the following Hive configuration options: ini Copy # JDBC connect string for a JDBC metastore
javax.jdo.option.ConnectionURL <mssql-connection-string>

# Username to use against metastore database
javax.jdo.option.ConnectionUserName <mssql-username>

# Password to use against metastore database
javax.jdo.option.ConnectionPassword <mssql-password>

# Driver class name for a JDBC metastore
javax.jdo.option.ConnectionDriverName com.microsoft.sqlserver.jdbc.SQLServerDriver
 where <mssql-connection-string> is the JDBC connection string (which you can get in the Azure portal). You do not need to include username and password in the connection string, because these will be set by javax.jdo.option.ConnectionUserName and javax.jdo.option.ConnectionDriverName. <mssql-username> and <mssql-password> specify the username and password of your Azure SQL Database account that has read/write access to the database. Note For production environments, we recommend that you set hive.metastore.schema.verification to true. This prevents Hive metastore client from implicitly modifying the metastore database schema when the metastore client version does not match the metastore database version. When enabling this setting for metastore client versions lower than Hive 1.2.0, make sure that the metastore client has the write permission to the metastore database (to prevent the issue described in HIVE-9749). For Hive metastore 1.2.0 and higher, set hive.metastore.schema.verification.record.version to true to enable hive.metastore.schema.verification. For Hive metastore 2.1.1 and higher, set hive.metastore.schema.verification.record.version to true as it is set to false by default. Set up an external metastore using the UI To set up an external metastore using the Azure Databricks UI: Click the Clusters button on the sidebar. Click Create Cluster. Enter the following Spark configuration options: ini Copy # Hive-specific configuration options.
# spark.hadoop prefix is added to make sure these Hive specific options propagate to the metastore client.
# JDBC connect string for a JDBC metastore
spark.hadoop.javax.jdo.option.ConnectionURL <mssql-connection-string>

# Username to use against metastore database
spark.hadoop.javax.jdo.option.ConnectionUserName <mssql-username>

# Password to use against metastore database
spark.hadoop.javax.jdo.option.ConnectionPassword <mssql-password>

# Driver class name for a JDBC metastore
spark.hadoop.javax.jdo.option.ConnectionDriverName com.microsoft.sqlserver.jdbc.SQLServerDriver

# Spark specific configuration options
spark.sql.hive.metastore.version <hive-version>
# Skip this one if <hive-version> is 0.13.x.
spark.sql.hive.metastore.jars <hive-jar-source>
 Continue your cluster configuration, following the instructions in Configure clusters. Click Create Cluster to create the cluster. Set up an external metastore using an init script Init scripts let you connect to an existing Hive metastore without manually setting required configurations. Create the base directory you want to store the init script in if it does not exist. The following example uses dbfs:/databricks/scripts. Run the following snippet in a notebook. The snippet creates the init script /databricks/scripts/external-metastore.sh in Databricks File System (DBFS). Alternatively, you can use the DBFS REST API’s put operation to create the init script. This init script writes required configuration options to a configuration file named 00-custom-spark.conf in a JSON-like format under /databricks/driver/conf/ inside every node of the cluster, whenever a cluster with the name specified as <cluster-name> starts. Azure Databricks provides default Spark configurations in the /databricks/driver/conf/spark-branch.conf file. Configuration files in the /databricks/driver/conf directory apply in reverse alphabetical order. If you want to change the name of the 00-custom-spark.conf file, make sure that it continues to apply before the spark-branch.conf file. Scala Scala Copy dbutils.fs.put(
    ""/databricks/scripts/external-metastore.sh"",
    """"""#!/bin/sh
      |# Loads environment variables to determine the correct JDBC driver to use.
      |source /etc/environment
      |# Quoting the label (i.e. EOF) with single quotes to disable variable interpolation.
      |cat << 'EOF' > /databricks/driver/conf/00-custom-spark.conf
      |[driver] {
      |    # Hive specific configuration options.
      |    # spark.hadoop prefix is added to make sure these Hive specific options will propagate to the metastore client.
      |    # JDBC connect string for a JDBC metastore
      |    ""spark.hadoop.javax.jdo.option.ConnectionURL"" = ""<mssql-connection-string>""
      |
      |    # Username to use against metastore database
      |    ""spark.hadoop.javax.jdo.option.ConnectionUserName"" = ""<mssql-username>""
      |
      |    # Password to use against metastore database
      |    ""spark.hadoop.javax.jdo.option.ConnectionPassword"" = ""<mssql-password>""
      |
      |    # Driver class name for a JDBC metastore
      |    ""spark.hadoop.javax.jdo.option.ConnectionDriverName"" = ""com.microsoft.sqlserver.jdbc.SQLServerDriver""
      |
      |    # Spark specific configuration options
      |    ""spark.sql.hive.metastore.version"" = ""<hive-version>""
      |    # Skip this one if <hive-version> is 0.13.x.
      |    ""spark.sql.hive.metastore.jars"" = ""<hive-jar-source>""
      |}
      |EOF
      |"""""".stripMargin,
    overwrite = true
)
 Python Python Copy contents = """"""#!/bin/sh
# Loads environment variables to determine the correct JDBC driver to use.
source /etc/environment
# Quoting the label (i.e. EOF) with single quotes to disable variable interpolation.
cat << 'EOF' > /databricks/driver/conf/00-custom-spark.conf
[driver] {
    # Hive specific configuration options.
    # spark.hadoop prefix is added to make sure these Hive specific options will propagate to the metastore client.
    # JDBC connect string for a JDBC metastore
    ""spark.hadoop.javax.jdo.option.ConnectionURL"" = ""<mssql-connection-string>""

    # Username to use against metastore database
    ""spark.hadoop.javax.jdo.option.ConnectionUserName"" = ""<mssql-username>""

    # Password to use against metastore database
    ""spark.hadoop.javax.jdo.option.ConnectionPassword"" = ""<mssql-password>""

    # Driver class name for a JDBC metastore
    ""spark.hadoop.javax.jdo.option.ConnectionDriverName"" = ""com.microsoft.sqlserver.jdbc.SQLServerDriver""

    # Spark specific configuration options
    ""spark.sql.hive.metastore.version"" = ""<hive-version>""
    # Skip this one if <hive-version> is 0.13.x.
    ""spark.sql.hive.metastore.jars"" = ""<hive-jar-source>""
    }
EOF
""""""

dbutils.fs.put(
    file = ""/databricks/scripts/external-metastore.sh"",
    contents = contents,
    overwrite = True
)
 Configure your cluster with the init script. Restart the cluster. Troubleshooting Clusters do not start (due to incorrect init script settings) If an init script for setting up the external metastore causes cluster creation failure, configure the init script to log, and debug the init script using the logs. Error in SQL statement: InvocationTargetException Error message pattern in the full exception stack trace: Copy Caused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = [...]
 External metastore JDBC connection information is misconfigured. Verify the configured hostname, port, username, password, and JDBC driver class name. Also, make sure that the username has the right privilege to access the metastore database. Error message pattern in the full exception stack trace: Copy Required table missing : ""`DBS`"" in Catalog """" Schema """". DataNucleus requires this table to perform its persistence operations. [...]
 External metastore database not properly initialized. Verify that you created the metastore database and put the correct database name in the JDBC connection string. Then, start a new cluster with the following two Spark configuration options: ini Copy datanucleus.autoCreateSchema true
datanucleus.fixedDatastore false
 In this way, the Hive client library will try to create and initialize tables in the metastore database automatically when it tries to access them but finds them absent. Error in SQL statement: AnalysisException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetastoreClient Error message in the full exception stacktrace: Copy The specified datastore driver (driver name) was not found in the CLASSPATH
 The cluster is configured to use an incorrect JDBC driver. Setting datanucleus.autoCreateSchema to true doesn’t work as expected By default, Databricks also sets datanucleus.fixedDatastore to true, which prevents any accidental structural changes to the metastore databases. Therefore, the Hive client library cannot create metastore tables even if you set datanucleus.autoCreateSchema to true. This strategy is, in general, safer for production environments since it prevents the metastore database to be accidentally upgraded. If you do want to use datanucleus.autoCreateSchema to help initialize the metastore database, make sure you set datanucleus.fixedDatastore to false. Also, you may want to flip both flags after initializing the metastore database to provide better protection to your production environment.",External Apache Hive metastore
15,Hive INSERT OVERWRITE query error,"Customer Stated that 

When they use the INSERT OVERWRITE with Hive 3.0 it is throwing error class not found.

Error in SQL statement: ClassNotFoundException: org.apache.hadoop.hive.ql.plan.LoadTableDesc$LoadFileType

>>This is the first time customer is facing the issue.
>>The timestamp of the error - May 30, 2022, 12:00 AM (UTC+08:00)
>>Before getting error cx changes to  Hive 3.0 upgrade.

 cluster URL : https://adb-3493366230612343.3.azuredatabricks.net/?o=3493366230612343#setting/clusters/0823-233301-amble624/configuration

Notebook URL : https://adb-3493366230612343.3.azuredatabricks.net/?o=3493366230612343#notebook/3262557635909213/command/3476138511132159

please provide some insights on this issue.",https://docs.microsoft.com/en-us/azure/databricks/data/metastores/external-hive-metastore,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents External Apache Hive metastore Article 06/16/2022 8 minutes to read 6 contributors In this article Hive metastore setup Cluster configurations Set up an external metastore using the UI Set up an external metastore using an init script Troubleshooting This article describes how to set up Azure Databricks clusters to connect to existing external Apache Hive metastores. It provides information about recommended metastore setup and cluster configuration requirements, followed by instructions for configuring clusters to connect to an external metastore. The following table summarizes which Hive metastore versions are supported in each version of Databricks Runtime. Databricks Runtime Version 0.13 - 1.2.1 2.1 2.2 2.3 3.1.0 7.x Yes Yes Yes Yes Yes 6.x Yes Yes Yes Yes Yes 5.3 and above Yes Yes Yes Yes Yes 5.1 - 5.2 and 4.x Yes Yes Yes Yes No 3.x Yes Yes No No No Important SQL Server does not work as the underlying metastore database for Hive 2.0 and above; however, Azure SQL Database does work and is used as the example throughout this article. You can use a Hive 1.2.0 or 1.2.1 metastore of an HDInsight cluster as an external metastore. See Use external metadata stores in Azure HDInsight. If you use Azure Database for MySQL as an external metastore, you must change the value of the lower_case_table_names property from 1 (the default) to 2 in the server-side database configuration. For details, see Identifier Case Sensitivity. Hive metastore setup The metastore client running inside a cluster connects to your underlying metastore database directly using JDBC. To test network connectivity from a cluster to the metastore, you can run the following command inside a notebook: Bash Copy %sh
nc -vz <DNS name> <port>
 where <DNS name> is the server name of Azure SQL Database. <port> is the port of the database. Cluster configurations You must set two sets of configuration options to connect a cluster to an external metastore: Spark options configure Spark with the Hive metastore version and the JARs for the metastore client. Hive options configure the metastore client to connect to the external metastore. Spark configuration options Set spark.sql.hive.metastore.version to the version of your Hive metastore and spark.sql.hive.metastore.jars as follows: Hive 0.13: do not set spark.sql.hive.metastore.jars. Hive 1.2.0 or 1.2.1 (Databricks Runtime 6.6 and below): set spark.sql.hive.metastore.jars to builtin. Note Hive 1.2.0 and 1.2.1 are not the built-in metastore on Databricks Runtime 7.0 and above. If you want to use Hive 1.2.0 or 1.2.1 with Databricks Runtime 7.0 and above, follow the procedure described in Download the metastore jars and point to them. Hive 2.3.7 (Databricks Runtime 7.0 - 9.x) or Hive 2.3.9 (Databricks Runtime 10.0 and above): set spark.sql.hive.metastore.jars to builtin. For all other Hive versions, Azure Databricks recommends that you download the metastore JARs and set the configuration spark.sql.hive.metastore.jars to point to the downloaded JARs using the procedure described in Download the metastore jars and point to them. Download the metastore jars and point to them Create a cluster with spark.sql.hive.metastore.jars set to maven and spark.sql.hive.metastore.version to match the version of your metastore. When the cluster is running, search the driver log and find a line like the following: Copy 17/11/18 22:41:19 INFO IsolatedClientLoader: Downloaded metastore jars to <path>
 The directory <path> is the location of downloaded JARs in the driver node of the cluster. Alternatively you can run the following code in a Scala notebook to print the location of the JARs: Scala Copy import com.typesafe.config.ConfigFactory
val path = ConfigFactory.load().getString(""java.io.tmpdir"")

println(s""\nHive JARs are downloaded to the path: $path \n"")
 Run %sh cp -r <path> /dbfs/hive_metastore_jar (replacing <path> with your cluster’s info) to copy this directory to a directory in DBFS called hive_metastore_jar through the FUSE client in the driver node. Create an init script that copies /dbfs/hive_metastore_jar to the local filesystem of the node, making sure to make the init script sleep a few seconds before it accesses the DBFS FUSE client. This ensures that the client is ready. Set spark.sql.hive.metastore.jars to use this directory. If your init script copies /dbfs/hive_metastore_jar to /databricks/hive_metastore_jars/, set spark.sql.hive.metastore.jars to /databricks/hive_metastore_jars/*. The location must include the trailing /*. Restart the cluster. Hive configuration options This section describes options specific to Hive. To connect to an external metastore using local mode, set the following Hive configuration options: ini Copy # JDBC connect string for a JDBC metastore
javax.jdo.option.ConnectionURL <mssql-connection-string>

# Username to use against metastore database
javax.jdo.option.ConnectionUserName <mssql-username>

# Password to use against metastore database
javax.jdo.option.ConnectionPassword <mssql-password>

# Driver class name for a JDBC metastore
javax.jdo.option.ConnectionDriverName com.microsoft.sqlserver.jdbc.SQLServerDriver
 where <mssql-connection-string> is the JDBC connection string (which you can get in the Azure portal). You do not need to include username and password in the connection string, because these will be set by javax.jdo.option.ConnectionUserName and javax.jdo.option.ConnectionDriverName. <mssql-username> and <mssql-password> specify the username and password of your Azure SQL Database account that has read/write access to the database. Note For production environments, we recommend that you set hive.metastore.schema.verification to true. This prevents Hive metastore client from implicitly modifying the metastore database schema when the metastore client version does not match the metastore database version. When enabling this setting for metastore client versions lower than Hive 1.2.0, make sure that the metastore client has the write permission to the metastore database (to prevent the issue described in HIVE-9749). For Hive metastore 1.2.0 and higher, set hive.metastore.schema.verification.record.version to true to enable hive.metastore.schema.verification. For Hive metastore 2.1.1 and higher, set hive.metastore.schema.verification.record.version to true as it is set to false by default. Set up an external metastore using the UI To set up an external metastore using the Azure Databricks UI: Click the Clusters button on the sidebar. Click Create Cluster. Enter the following Spark configuration options: ini Copy # Hive-specific configuration options.
# spark.hadoop prefix is added to make sure these Hive specific options propagate to the metastore client.
# JDBC connect string for a JDBC metastore
spark.hadoop.javax.jdo.option.ConnectionURL <mssql-connection-string>

# Username to use against metastore database
spark.hadoop.javax.jdo.option.ConnectionUserName <mssql-username>

# Password to use against metastore database
spark.hadoop.javax.jdo.option.ConnectionPassword <mssql-password>

# Driver class name for a JDBC metastore
spark.hadoop.javax.jdo.option.ConnectionDriverName com.microsoft.sqlserver.jdbc.SQLServerDriver

# Spark specific configuration options
spark.sql.hive.metastore.version <hive-version>
# Skip this one if <hive-version> is 0.13.x.
spark.sql.hive.metastore.jars <hive-jar-source>
 Continue your cluster configuration, following the instructions in Configure clusters. Click Create Cluster to create the cluster. Set up an external metastore using an init script Init scripts let you connect to an existing Hive metastore without manually setting required configurations. Create the base directory you want to store the init script in if it does not exist. The following example uses dbfs:/databricks/scripts. Run the following snippet in a notebook. The snippet creates the init script /databricks/scripts/external-metastore.sh in Databricks File System (DBFS). Alternatively, you can use the DBFS REST API’s put operation to create the init script. This init script writes required configuration options to a configuration file named 00-custom-spark.conf in a JSON-like format under /databricks/driver/conf/ inside every node of the cluster, whenever a cluster with the name specified as <cluster-name> starts. Azure Databricks provides default Spark configurations in the /databricks/driver/conf/spark-branch.conf file. Configuration files in the /databricks/driver/conf directory apply in reverse alphabetical order. If you want to change the name of the 00-custom-spark.conf file, make sure that it continues to apply before the spark-branch.conf file. Scala Scala Copy dbutils.fs.put(
    ""/databricks/scripts/external-metastore.sh"",
    """"""#!/bin/sh
      |# Loads environment variables to determine the correct JDBC driver to use.
      |source /etc/environment
      |# Quoting the label (i.e. EOF) with single quotes to disable variable interpolation.
      |cat << 'EOF' > /databricks/driver/conf/00-custom-spark.conf
      |[driver] {
      |    # Hive specific configuration options.
      |    # spark.hadoop prefix is added to make sure these Hive specific options will propagate to the metastore client.
      |    # JDBC connect string for a JDBC metastore
      |    ""spark.hadoop.javax.jdo.option.ConnectionURL"" = ""<mssql-connection-string>""
      |
      |    # Username to use against metastore database
      |    ""spark.hadoop.javax.jdo.option.ConnectionUserName"" = ""<mssql-username>""
      |
      |    # Password to use against metastore database
      |    ""spark.hadoop.javax.jdo.option.ConnectionPassword"" = ""<mssql-password>""
      |
      |    # Driver class name for a JDBC metastore
      |    ""spark.hadoop.javax.jdo.option.ConnectionDriverName"" = ""com.microsoft.sqlserver.jdbc.SQLServerDriver""
      |
      |    # Spark specific configuration options
      |    ""spark.sql.hive.metastore.version"" = ""<hive-version>""
      |    # Skip this one if <hive-version> is 0.13.x.
      |    ""spark.sql.hive.metastore.jars"" = ""<hive-jar-source>""
      |}
      |EOF
      |"""""".stripMargin,
    overwrite = true
)
 Python Python Copy contents = """"""#!/bin/sh
# Loads environment variables to determine the correct JDBC driver to use.
source /etc/environment
# Quoting the label (i.e. EOF) with single quotes to disable variable interpolation.
cat << 'EOF' > /databricks/driver/conf/00-custom-spark.conf
[driver] {
    # Hive specific configuration options.
    # spark.hadoop prefix is added to make sure these Hive specific options will propagate to the metastore client.
    # JDBC connect string for a JDBC metastore
    ""spark.hadoop.javax.jdo.option.ConnectionURL"" = ""<mssql-connection-string>""

    # Username to use against metastore database
    ""spark.hadoop.javax.jdo.option.ConnectionUserName"" = ""<mssql-username>""

    # Password to use against metastore database
    ""spark.hadoop.javax.jdo.option.ConnectionPassword"" = ""<mssql-password>""

    # Driver class name for a JDBC metastore
    ""spark.hadoop.javax.jdo.option.ConnectionDriverName"" = ""com.microsoft.sqlserver.jdbc.SQLServerDriver""

    # Spark specific configuration options
    ""spark.sql.hive.metastore.version"" = ""<hive-version>""
    # Skip this one if <hive-version> is 0.13.x.
    ""spark.sql.hive.metastore.jars"" = ""<hive-jar-source>""
    }
EOF
""""""

dbutils.fs.put(
    file = ""/databricks/scripts/external-metastore.sh"",
    contents = contents,
    overwrite = True
)
 Configure your cluster with the init script. Restart the cluster. Troubleshooting Clusters do not start (due to incorrect init script settings) If an init script for setting up the external metastore causes cluster creation failure, configure the init script to log, and debug the init script using the logs. Error in SQL statement: InvocationTargetException Error message pattern in the full exception stack trace: Copy Caused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = [...]
 External metastore JDBC connection information is misconfigured. Verify the configured hostname, port, username, password, and JDBC driver class name. Also, make sure that the username has the right privilege to access the metastore database. Error message pattern in the full exception stack trace: Copy Required table missing : ""`DBS`"" in Catalog """" Schema """". DataNucleus requires this table to perform its persistence operations. [...]
 External metastore database not properly initialized. Verify that you created the metastore database and put the correct database name in the JDBC connection string. Then, start a new cluster with the following two Spark configuration options: ini Copy datanucleus.autoCreateSchema true
datanucleus.fixedDatastore false
 In this way, the Hive client library will try to create and initialize tables in the metastore database automatically when it tries to access them but finds them absent. Error in SQL statement: AnalysisException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetastoreClient Error message in the full exception stacktrace: Copy The specified datastore driver (driver name) was not found in the CLASSPATH
 The cluster is configured to use an incorrect JDBC driver. Setting datanucleus.autoCreateSchema to true doesn’t work as expected By default, Databricks also sets datanucleus.fixedDatastore to true, which prevents any accidental structural changes to the metastore databases. Therefore, the Hive client library cannot create metastore tables even if you set datanucleus.autoCreateSchema to true. This strategy is, in general, safer for production environments since it prevents the metastore database to be accidentally upgraded. If you do want to use datanucleus.autoCreateSchema to help initialize the metastore database, make sure you set datanucleus.fixedDatastore to false. Also, you may want to flip both flags after initializing the metastore database to provide better protection to your production environment.",External Apache Hive metastore
16,ARR | 2205040040003792 | Metastore operations with ADLS Gen2 conf,"Follow up to 00145648. Databricks RSA ankur.nayyar@databricks.com requested to create an SF ticket for databricks support team to take it further to the engineering team.

Issue:

Customer is running jobs from ADF using job submit API. Job is looping through a list of SQL statements. The access to underlying storage is also given in this API program. The code is packages as a wheel, installed on the cluster and then invoked via notebook.

module = importlib.import_module(module)
func = getattr(module, attr)
result = func(*args, **kwargs)
 
Where module and attr are the module and callable for the entrypoint to the program.
https://adb-5340021565253101.1.azuredatabricks.net/?o=5340021565253101#job/125344265090910/run/3541650
https://adb-5340021565253101.1.azuredatabricks.net/?o=5340021565253101#setting/clusters/0502-174006-gl0anede/configuration

The notebook is failing with below error when running MSCK repair.

[INFO] 2022-05-02 17:45:03,738 - sql-job: Running SQL:
create database if not exists ext_br_bees_services_mongo_item
[INFO] 2022-05-02 17:45:08,622 - sql-job: Running SQL:
create table if not exists ext_br_bees_services_mongo_item.item_dm using parquet location 'abfss://gold-abi@[REDACTED]dfs.core.windows.net/br/mongodb-batch/item/br-gz-item-mongodb-batch.parquet'
[INFO] 2022-05-02 17:45:23,583 - sql-job: Running SQL:
refresh table ext_br_bees_services_mongo_item.item_dm
[INFO] 2022-05-02 17:45:24,161 - sql-job: Running SQL:
msck repair table ext_br_bees_services_mongo_item.item_dm sync partitions
 
AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Got exception:
shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.contracts.exceptions.KeyProviderException Failure to initialize configuration)

Ask to Databricks:

Storage configuration is done using  spark.conf.set from API itself. From the last ticket, it is mentioned that this behavior is expected as Databricks as metastore service is initialized during cluster startup. But this doesn't satisfy customer requirement. Please reach out to ankur.nayyar@databricks.com for further details.",https://docs.microsoft.com/en-us/azure/databricks/data/metastores/external-hive-metastore,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents External Apache Hive metastore Article 06/16/2022 8 minutes to read 6 contributors In this article Hive metastore setup Cluster configurations Set up an external metastore using the UI Set up an external metastore using an init script Troubleshooting This article describes how to set up Azure Databricks clusters to connect to existing external Apache Hive metastores. It provides information about recommended metastore setup and cluster configuration requirements, followed by instructions for configuring clusters to connect to an external metastore. The following table summarizes which Hive metastore versions are supported in each version of Databricks Runtime. Databricks Runtime Version 0.13 - 1.2.1 2.1 2.2 2.3 3.1.0 7.x Yes Yes Yes Yes Yes 6.x Yes Yes Yes Yes Yes 5.3 and above Yes Yes Yes Yes Yes 5.1 - 5.2 and 4.x Yes Yes Yes Yes No 3.x Yes Yes No No No Important SQL Server does not work as the underlying metastore database for Hive 2.0 and above; however, Azure SQL Database does work and is used as the example throughout this article. You can use a Hive 1.2.0 or 1.2.1 metastore of an HDInsight cluster as an external metastore. See Use external metadata stores in Azure HDInsight. If you use Azure Database for MySQL as an external metastore, you must change the value of the lower_case_table_names property from 1 (the default) to 2 in the server-side database configuration. For details, see Identifier Case Sensitivity. Hive metastore setup The metastore client running inside a cluster connects to your underlying metastore database directly using JDBC. To test network connectivity from a cluster to the metastore, you can run the following command inside a notebook: Bash Copy %sh
nc -vz <DNS name> <port>
 where <DNS name> is the server name of Azure SQL Database. <port> is the port of the database. Cluster configurations You must set two sets of configuration options to connect a cluster to an external metastore: Spark options configure Spark with the Hive metastore version and the JARs for the metastore client. Hive options configure the metastore client to connect to the external metastore. Spark configuration options Set spark.sql.hive.metastore.version to the version of your Hive metastore and spark.sql.hive.metastore.jars as follows: Hive 0.13: do not set spark.sql.hive.metastore.jars. Hive 1.2.0 or 1.2.1 (Databricks Runtime 6.6 and below): set spark.sql.hive.metastore.jars to builtin. Note Hive 1.2.0 and 1.2.1 are not the built-in metastore on Databricks Runtime 7.0 and above. If you want to use Hive 1.2.0 or 1.2.1 with Databricks Runtime 7.0 and above, follow the procedure described in Download the metastore jars and point to them. Hive 2.3.7 (Databricks Runtime 7.0 - 9.x) or Hive 2.3.9 (Databricks Runtime 10.0 and above): set spark.sql.hive.metastore.jars to builtin. For all other Hive versions, Azure Databricks recommends that you download the metastore JARs and set the configuration spark.sql.hive.metastore.jars to point to the downloaded JARs using the procedure described in Download the metastore jars and point to them. Download the metastore jars and point to them Create a cluster with spark.sql.hive.metastore.jars set to maven and spark.sql.hive.metastore.version to match the version of your metastore. When the cluster is running, search the driver log and find a line like the following: Copy 17/11/18 22:41:19 INFO IsolatedClientLoader: Downloaded metastore jars to <path>
 The directory <path> is the location of downloaded JARs in the driver node of the cluster. Alternatively you can run the following code in a Scala notebook to print the location of the JARs: Scala Copy import com.typesafe.config.ConfigFactory
val path = ConfigFactory.load().getString(""java.io.tmpdir"")

println(s""\nHive JARs are downloaded to the path: $path \n"")
 Run %sh cp -r <path> /dbfs/hive_metastore_jar (replacing <path> with your cluster’s info) to copy this directory to a directory in DBFS called hive_metastore_jar through the FUSE client in the driver node. Create an init script that copies /dbfs/hive_metastore_jar to the local filesystem of the node, making sure to make the init script sleep a few seconds before it accesses the DBFS FUSE client. This ensures that the client is ready. Set spark.sql.hive.metastore.jars to use this directory. If your init script copies /dbfs/hive_metastore_jar to /databricks/hive_metastore_jars/, set spark.sql.hive.metastore.jars to /databricks/hive_metastore_jars/*. The location must include the trailing /*. Restart the cluster. Hive configuration options This section describes options specific to Hive. To connect to an external metastore using local mode, set the following Hive configuration options: ini Copy # JDBC connect string for a JDBC metastore
javax.jdo.option.ConnectionURL <mssql-connection-string>

# Username to use against metastore database
javax.jdo.option.ConnectionUserName <mssql-username>

# Password to use against metastore database
javax.jdo.option.ConnectionPassword <mssql-password>

# Driver class name for a JDBC metastore
javax.jdo.option.ConnectionDriverName com.microsoft.sqlserver.jdbc.SQLServerDriver
 where <mssql-connection-string> is the JDBC connection string (which you can get in the Azure portal). You do not need to include username and password in the connection string, because these will be set by javax.jdo.option.ConnectionUserName and javax.jdo.option.ConnectionDriverName. <mssql-username> and <mssql-password> specify the username and password of your Azure SQL Database account that has read/write access to the database. Note For production environments, we recommend that you set hive.metastore.schema.verification to true. This prevents Hive metastore client from implicitly modifying the metastore database schema when the metastore client version does not match the metastore database version. When enabling this setting for metastore client versions lower than Hive 1.2.0, make sure that the metastore client has the write permission to the metastore database (to prevent the issue described in HIVE-9749). For Hive metastore 1.2.0 and higher, set hive.metastore.schema.verification.record.version to true to enable hive.metastore.schema.verification. For Hive metastore 2.1.1 and higher, set hive.metastore.schema.verification.record.version to true as it is set to false by default. Set up an external metastore using the UI To set up an external metastore using the Azure Databricks UI: Click the Clusters button on the sidebar. Click Create Cluster. Enter the following Spark configuration options: ini Copy # Hive-specific configuration options.
# spark.hadoop prefix is added to make sure these Hive specific options propagate to the metastore client.
# JDBC connect string for a JDBC metastore
spark.hadoop.javax.jdo.option.ConnectionURL <mssql-connection-string>

# Username to use against metastore database
spark.hadoop.javax.jdo.option.ConnectionUserName <mssql-username>

# Password to use against metastore database
spark.hadoop.javax.jdo.option.ConnectionPassword <mssql-password>

# Driver class name for a JDBC metastore
spark.hadoop.javax.jdo.option.ConnectionDriverName com.microsoft.sqlserver.jdbc.SQLServerDriver

# Spark specific configuration options
spark.sql.hive.metastore.version <hive-version>
# Skip this one if <hive-version> is 0.13.x.
spark.sql.hive.metastore.jars <hive-jar-source>
 Continue your cluster configuration, following the instructions in Configure clusters. Click Create Cluster to create the cluster. Set up an external metastore using an init script Init scripts let you connect to an existing Hive metastore without manually setting required configurations. Create the base directory you want to store the init script in if it does not exist. The following example uses dbfs:/databricks/scripts. Run the following snippet in a notebook. The snippet creates the init script /databricks/scripts/external-metastore.sh in Databricks File System (DBFS). Alternatively, you can use the DBFS REST API’s put operation to create the init script. This init script writes required configuration options to a configuration file named 00-custom-spark.conf in a JSON-like format under /databricks/driver/conf/ inside every node of the cluster, whenever a cluster with the name specified as <cluster-name> starts. Azure Databricks provides default Spark configurations in the /databricks/driver/conf/spark-branch.conf file. Configuration files in the /databricks/driver/conf directory apply in reverse alphabetical order. If you want to change the name of the 00-custom-spark.conf file, make sure that it continues to apply before the spark-branch.conf file. Scala Scala Copy dbutils.fs.put(
    ""/databricks/scripts/external-metastore.sh"",
    """"""#!/bin/sh
      |# Loads environment variables to determine the correct JDBC driver to use.
      |source /etc/environment
      |# Quoting the label (i.e. EOF) with single quotes to disable variable interpolation.
      |cat << 'EOF' > /databricks/driver/conf/00-custom-spark.conf
      |[driver] {
      |    # Hive specific configuration options.
      |    # spark.hadoop prefix is added to make sure these Hive specific options will propagate to the metastore client.
      |    # JDBC connect string for a JDBC metastore
      |    ""spark.hadoop.javax.jdo.option.ConnectionURL"" = ""<mssql-connection-string>""
      |
      |    # Username to use against metastore database
      |    ""spark.hadoop.javax.jdo.option.ConnectionUserName"" = ""<mssql-username>""
      |
      |    # Password to use against metastore database
      |    ""spark.hadoop.javax.jdo.option.ConnectionPassword"" = ""<mssql-password>""
      |
      |    # Driver class name for a JDBC metastore
      |    ""spark.hadoop.javax.jdo.option.ConnectionDriverName"" = ""com.microsoft.sqlserver.jdbc.SQLServerDriver""
      |
      |    # Spark specific configuration options
      |    ""spark.sql.hive.metastore.version"" = ""<hive-version>""
      |    # Skip this one if <hive-version> is 0.13.x.
      |    ""spark.sql.hive.metastore.jars"" = ""<hive-jar-source>""
      |}
      |EOF
      |"""""".stripMargin,
    overwrite = true
)
 Python Python Copy contents = """"""#!/bin/sh
# Loads environment variables to determine the correct JDBC driver to use.
source /etc/environment
# Quoting the label (i.e. EOF) with single quotes to disable variable interpolation.
cat << 'EOF' > /databricks/driver/conf/00-custom-spark.conf
[driver] {
    # Hive specific configuration options.
    # spark.hadoop prefix is added to make sure these Hive specific options will propagate to the metastore client.
    # JDBC connect string for a JDBC metastore
    ""spark.hadoop.javax.jdo.option.ConnectionURL"" = ""<mssql-connection-string>""

    # Username to use against metastore database
    ""spark.hadoop.javax.jdo.option.ConnectionUserName"" = ""<mssql-username>""

    # Password to use against metastore database
    ""spark.hadoop.javax.jdo.option.ConnectionPassword"" = ""<mssql-password>""

    # Driver class name for a JDBC metastore
    ""spark.hadoop.javax.jdo.option.ConnectionDriverName"" = ""com.microsoft.sqlserver.jdbc.SQLServerDriver""

    # Spark specific configuration options
    ""spark.sql.hive.metastore.version"" = ""<hive-version>""
    # Skip this one if <hive-version> is 0.13.x.
    ""spark.sql.hive.metastore.jars"" = ""<hive-jar-source>""
    }
EOF
""""""

dbutils.fs.put(
    file = ""/databricks/scripts/external-metastore.sh"",
    contents = contents,
    overwrite = True
)
 Configure your cluster with the init script. Restart the cluster. Troubleshooting Clusters do not start (due to incorrect init script settings) If an init script for setting up the external metastore causes cluster creation failure, configure the init script to log, and debug the init script using the logs. Error in SQL statement: InvocationTargetException Error message pattern in the full exception stack trace: Copy Caused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = [...]
 External metastore JDBC connection information is misconfigured. Verify the configured hostname, port, username, password, and JDBC driver class name. Also, make sure that the username has the right privilege to access the metastore database. Error message pattern in the full exception stack trace: Copy Required table missing : ""`DBS`"" in Catalog """" Schema """". DataNucleus requires this table to perform its persistence operations. [...]
 External metastore database not properly initialized. Verify that you created the metastore database and put the correct database name in the JDBC connection string. Then, start a new cluster with the following two Spark configuration options: ini Copy datanucleus.autoCreateSchema true
datanucleus.fixedDatastore false
 In this way, the Hive client library will try to create and initialize tables in the metastore database automatically when it tries to access them but finds them absent. Error in SQL statement: AnalysisException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetastoreClient Error message in the full exception stacktrace: Copy The specified datastore driver (driver name) was not found in the CLASSPATH
 The cluster is configured to use an incorrect JDBC driver. Setting datanucleus.autoCreateSchema to true doesn’t work as expected By default, Databricks also sets datanucleus.fixedDatastore to true, which prevents any accidental structural changes to the metastore databases. Therefore, the Hive client library cannot create metastore tables even if you set datanucleus.autoCreateSchema to true. This strategy is, in general, safer for production environments since it prevents the metastore database to be accidentally upgraded. If you do want to use datanucleus.autoCreateSchema to help initialize the metastore database, make sure you set datanucleus.fixedDatastore to false. Also, you may want to flip both flags after initializing the metastore database to provide better protection to your production environment.",External Apache Hive metastore
17,[ARR] [Sev C] SR-2205310030001397-Understand Utilization of Databricks,"1. Issue:  
The customer would like to understand the current utilization of our Databricks Clusters. What are the metrices that can be considered and how this data could be pulled. Could you please share how best this could be identified. 


2. Ask
Q1) We checked the Ganglia metrics, it can be seen as per 15 mins, however we cannot see the week and month base. It is empty.  Is there anyway to see it by week or month? If any other Tool?

Q2)How is the cost calculated for Azure Databricks? Is it based on DBU and the VM utilization?

Q3)Is this the formula used for the calculation? The customer checked the following doc.
Reference: https://cprosenjit.medium.com/azure-databricks-cost-optimizations-5e1e17b39125

Formula used:
1. VM Cost = [Total Hours] X [No. of Instances] X [Linux VM Price]
2. DBU Cost = [Total Hours] X [No. of Instances] X [DBU] X [DBU Price/hour - Standard / Premium Tier]

Any sort of Resource Calculator available for Databricks?

 
※If it is possible could you kindly assgin the APAC or IST time engineer?


3. What has done :
We shared the following to customer. However during the call yesterday. The customer want to know detail about above questions.

Suggestion #1
Ganglia metrics:
It help you to monitor cluster performance and health. You can access Ganglia metrics from the cluster details page. You can check this metrics report for overview of cluster in every 15 minutes and you can monitor historical metrics of cluster usage.

<Reference>
https://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage#ganglia-metrics

Suggestion #2
Grafana Dashboard:
You can also set up a 3rd party Grafana dashboard to visualize Databricks metrics for monitoring performance. From Grafana you can check the ""Cluster throughput"" graph shows the number of jobs, stages, and tasks completed per minute. You can also check the visualization  shows the sum of task execution latency per host running on a cluster by using ""sum Task Execution Per Host"" graph. The data will be gathered from azure monitor logs.

need to use a library ( https://github.com/mspnp/spark-monitoring) to enable logging of metrics. Please see and follow the steps in the document below.

<Reference>
-https://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/dashboards
-https://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/performance-troubleshooting",https://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Manage clusters Article 07/12/2022 15 minutes to read 6 contributors In this article Display clusters Pin a cluster View a cluster configuration as a JSON file Edit a cluster Clone a cluster Control access to clusters Start a cluster Terminate a cluster Delete a cluster Restart a cluster to update it with the latest images View cluster information in the Apache Spark UI View cluster logs Monitor performance Decommission spot instances This article describes how to manage Azure Databricks clusters, including displaying, editing, starting, terminating, deleting, controlling access, and monitoring performance and logs. Display clusters To display the clusters in your workspace, click Compute in the sidebar. The Compute page displays clusters in two tabs: All-purpose clusters and Job clusters. At the left side are two columns indicating if the cluster has been pinned and the status of the cluster: Pinned Starting , Terminating Standard cluster Running Terminated High concurrency cluster Running Terminated Access Denied Running Terminated Table ACLs enabled Running Terminated At the far right of the right side of the All-purpose clusters tab is an icon you can use to terminate the cluster. You can use the three-button menu to restart, clone, delete, or edit permissions for the cluster. Menu options that are not available are grayed out. The All-purpose clusters tab shows the numbers of notebooks attached to the cluster. Filter cluster list You can filter the cluster lists using the buttons and search box at the top right: Pin a cluster 30 days after a cluster is terminated, it is permanently deleted. To keep an all-purpose cluster configuration even after a cluster has been terminated for more than 30 days, an administrator can pin the cluster. Up to 100 clusters can be pinned. You can pin a cluster from the cluster list or the cluster detail page: Pin cluster from cluster list To pin or unpin a cluster, click the pin icon to the left of the cluster name. Pin cluster from cluster detail page To pin or unpin a cluster, click the pin icon to the right of the cluster name. You can also invoke the Pin API endpoint to programmatically pin a cluster. View a cluster configuration as a JSON file Sometimes it can be helpful to view your cluster configuration as JSON. This is especially useful when you want to create similar clusters using the Clusters API 2.0. When you view an existing cluster, simply go to the Configuration tab, click JSON in the top right of the tab, copy the JSON, and paste it into your API call. JSON view is ready-only. Edit a cluster You edit a cluster configuration from the cluster detail page. To display the cluster detail page, click the cluster name on the Compute page. You can also invoke the Edit API endpoint to programmatically edit the cluster. Note Notebooks and jobs that were attached to the cluster remain attached after editing. Libraries installed on the cluster remain installed after editing. If you edit any attribute of a running cluster (except for the cluster size and permissions), you must restart it. This can disrupt users who are currently using the cluster. You can edit only running or terminated clusters. You can, however, update permissions for clusters that are not in those states on the cluster details page. For detailed information about cluster configuration properties you can edit, see Configure clusters. Clone a cluster You can create a new cluster by cloning an existing cluster. From the cluster list, click the three-button menu and select Clone from the drop down. From the cluster detail page, click and select Clone from the drop down. The cluster creation form is opened prepopulated with the cluster configuration. The following attributes from the existing cluster are not included in the clone: Cluster permissions Installed libraries Attached notebooks Control access to clusters Cluster access control within the Admin Console allows admins and delegated users to give fine-grained cluster access to other users. There are two types of cluster access control: Cluster creation permission: Admins can choose which users are allowed to create clusters. Cluster-level permissions: A user who has the Can manage permission for a cluster can configure whether other users can attach to, restart, resize, and manage that cluster from the cluster list or the cluster details page. From the cluster list, click the three-button menu and select Edit Permissions. From the cluster detail page, click and select Permissions. To learn how to configure cluster access control and cluster-level permissions, see Cluster access control. Start a cluster Apart from creating a new cluster, you can also start a previously terminated cluster. This lets you re-create a previously terminated cluster with its original configuration. You can start a cluster from the cluster list, the cluster detail page, or a notebook. To start a cluster from the cluster list, click the arrow: To start a cluster from the cluster detail page, click Start: Notebook cluster attach drop-down You can also invoke the Start API endpoint to programmatically start a cluster. Azure Databricks identifies a cluster with a unique cluster ID. When you start a terminated cluster, Databricks re-creates the cluster with the same ID, automatically installs all the libraries, and re-attaches the notebooks. Note If you are using a Trial workspace and the trial has expired, you will not be able to start a cluster. Cluster autostart for jobs When a job assigned to an existing terminated cluster is scheduled to run or you connect to a terminated cluster from a JDBC/ODBC interface, the cluster is automatically restarted. See Create a job and JDBC connect. Cluster autostart allows you to configure clusters to autoterminate without requiring manual intervention to restart the clusters for scheduled jobs. Furthermore, you can schedule cluster initialization by scheduling a job to run on a terminated cluster. Before a cluster is restarted automatically, cluster and job access control permissions are checked. Note If your cluster was created in Azure Databricks platform version 2.70 or earlier, there is no autostart: jobs scheduled to run on terminated clusters will fail. Terminate a cluster To save cluster resources, you can terminate a cluster. A terminated cluster cannot run notebooks or jobs, but its configuration is stored so that it can be reused (or—in the case of some types of jobs—autostarted) at a later time. You can manually terminate a cluster or configure the cluster to automatically terminate after a specified period of inactivity. Azure Databricks records information whenever a cluster is terminated. When the number of terminated clusters exceeds 150, the oldest clusters are deleted. Unless a cluster is pinned, 30 days after the cluster is terminated, it is automatically and permanently deleted. Terminated clusters appear in the cluster list with a gray circle at the left of the cluster name. Note When you run a job on a New Job Cluster (which is usually recommended), the cluster terminates and is unavailable for restarting when the job is complete. On the other hand, if you schedule a job to run on an Existing All-Purpose Cluster that has been terminated, that cluster will autostart. Important If you are using a Trial Premium workspace, all running clusters are terminated: When you upgrade a workspace to full Premium. If the workspace is not upgraded and the trial expires. Manual termination You can manually terminate a cluster from the cluster list or the cluster detail page. To terminate a cluster from the cluster list, click the square: To terminate a cluster from the cluster detail page, click Terminate: Automatic termination You can also set auto termination for a cluster. During cluster creation, you can specify an inactivity period in minutes after which you want the cluster to terminate. If the difference between the current time and the last command run on the cluster is more than the inactivity period specified, Azure Databricks automatically terminates that cluster. A cluster is considered inactive when all commands on the cluster, including Spark jobs, Structured Streaming, and JDBC calls, have finished executing. Warning Clusters do not report activity resulting from the use of DStreams. This means that an autoterminating cluster may be terminated while it is running DStreams. Turn off auto termination for clusters running DStreams or consider using Structured Streaming. The auto termination feature monitors only Spark jobs, not user-defined local processes. Therefore, if all Spark jobs have completed, a cluster may be terminated even if local processes are running. Idle clusters continue to accumulate DBU and cloud instance charges during the inactivity period before termination. Configure automatic termination You configure automatic termination in the Auto Termination field in the Autopilot Options box on the cluster creation page: Important The default value of the auto terminate setting depends on whether you choose to create a standard or high concurrency cluster: Standard clusters are configured to terminate automatically after 120 minutes. High concurrency clusters are configured to not terminate automatically. You can opt out of auto termination by clearing the Auto Termination checkbox or by specifying an inactivity period of 0. Note Auto termination is best supported in the latest Spark versions. Older Spark versions have known limitations which can result in inaccurate reporting of cluster activity. For example, clusters running JDBC, R, or streaming commands can report a stale activity time that leads to premature cluster termination. Please upgrade to the most recent Spark version to benefit from bug fixes and improvements to auto termination. Unexpected termination Sometimes a cluster is terminated unexpectedly, not as a result of a manual termination or a configured automatic termination. For a list of termination reasons and remediation steps, see the Knowledge Base. Delete a cluster Deleting a cluster terminates the cluster and removes its configuration. Warning You cannot undo this action. You cannot delete a pinned cluster. In order to delete a pinned cluster, it must first be unpinned by an administrator. From the cluster list, click the three-button menu and select Delete from the drop down. From the cluster detail page, click and select Delete from the drop down. You can also invoke the Permanent delete API endpoint to programmatically delete a cluster. Restart a cluster to update it with the latest images When you restart a cluster, it gets the latest images for the compute resource containers and the VM hosts. It is particularly important to schedule regular restarts for long-running clusters, which are often used for some applications such as processing streaming data. It is your responsibility to restart all compute resources regularly to keep the image up-to-date with the latest image version. Important If you enable the compliance security profile for your account or your workspace, long-running clusters are automatically restarted after 25 days. Databricks recommends that admins restart clusters before they run for 25 days and do so during a scheduled maintenance window. This reduces the risk of an auto-restart disrupting a scheduled job. You can restart a cluster in multiple ways: Use the UI to restart a cluster from the cluster detail page. To display the cluster detail page, click the cluster name on the Compute page. Click Restart. Use the Clusters API to restart a cluster. Use the script that Azure Databricks provides that determines how long your clusters have run, and optionally restarts them if they exceed a specified number of days since they were started Run a script that determines how many days your clusters have been running, and optionally restart them If you are a workspace admin, you can run a script that determines how long each of your clusters has been running, and optionally restart them if they are older than a specified number of days. Azure Databricks provides this script as a notebook. The first lines of the script define configuration parameters: min_age_output: The maximum number of days that a cluster can run. Default is 1. perform_restart: If True, the script restarts clusters with age greater than the number of days specified by min_age_output. The default is False, which identifies the long running clusters but does not restart them. secret_configuration: Replace REPLACE_WITH_SCOPE and REPLACE_WITH_KEY with a secret scope and key name. For more details of setting up the secrets, see the notebook. Warning If you set perform_restart to True, the script automatically restarts eligible clusters, which can cause active jobs to fail and reset open notebooks. To reduce the risk of disrupting your workspace’s business critical jobs, plan a scheduled maintenance window and be sure to notify workspace users. Identify and optionally restart long-running clusters notebook Get notebook View cluster information in the Apache Spark UI You can view detailed information about Spark jobs in the Spark UI, which you can access from the Spark UI tab on the cluster details page. You can get details about active and terminated clusters. If you restart a terminated cluster, the Spark UI displays information for the restarted cluster, not the historical information for the terminated cluster. View cluster logs Azure Databricks provides three kinds of logging of cluster-related activity: Cluster event logs, which capture cluster lifecycle events, like creation, termination, configuration edits, and so on. Apache Spark driver and worker logs, which you can use for debugging. Cluster init-script logs, valuable for debugging init scripts. This section discusses cluster event logs and driver and worker logs. For details about init-script logs, see Init script logs. Cluster event logs The cluster event log displays important cluster lifecycle events that are triggered manually by user actions or automatically by Azure Databricks. Such events affect the operation of a cluster as a whole and the jobs running in the cluster. For supported event types, see the REST API ClusterEventType data structure. Events are stored for 60 days, which is comparable to other data retention times in Azure Databricks. View a cluster event log Click Compute in the sidebar. Click a cluster name. Click the Event Log tab. To filter the events, click the in the Filter by Event Type… field and select one or more event type checkboxes. Use Select all to make it easier to filter by excluding particular event types. View event details For more information about an event, click its row in the log and then click the JSON tab for details. Cluster driver and worker logs The direct print and log statements from your notebooks, jobs, and libraries go to the Spark driver logs. These logs have three outputs: Standard output Standard error Log4j logs You can access these files from the Driver logs tab on the cluster details page. Click the name of a log file to download it. To view Spark worker logs, you can use the Spark UI. You can also configure a log delivery location for the cluster. Both worker and cluster logs are delivered to the location you specify. Monitor performance To help you monitor the performance of Azure Databricks clusters, Azure Databricks provides access to Ganglia metrics from the cluster details page. In addition, you can configure an Azure Databricks cluster to send metrics to a Log Analytics workspace in Azure Monitor, the monitoring platform for Azure. You can install Datadog agents on cluster nodes to send Datadog metrics to your Datadog account. Ganglia metrics To access the Ganglia UI, navigate to the Metrics tab on the cluster details page. CPU metrics are available in the Ganglia UI for all Databricks runtimes. GPU metrics are available for GPU-enabled clusters. To view live metrics, click the Ganglia UI link. To view historical metrics, click a snapshot file. The snapshot contains aggregated metrics for the hour preceding the selected time. Configure metrics collection By default, Azure Databricks collects Ganglia metrics every 15 minutes. To configure the collection period, set the DATABRICKS_GANGLIA_SNAPSHOT_PERIOD_MINUTES environment variable using an init script or in the spark_env_vars field in the Cluster Create API. Azure Monitor You can configure an Azure Databricks cluster to send metrics to a Log Analytics workspace in Azure Monitor, the monitoring platform for Azure. For complete instructions, see Monitoring Azure Databricks. Note If you have deployed the Azure Databricks workspace in your own virtual network and you have configured network security groups (NSG) to deny all outbound traffic that is not required by Azure Databricks, then you must configure an additional outbound rule for the “AzureMonitor” service tag. Datadog metrics You can install Datadog agents on cluster nodes to send Datadog metrics to your Datadog account. The following notebook demonstrates how to install a Datadog agent on a cluster using a cluster-scoped init script. To install the Datadog agent on all clusters, use a global init script after testing the cluster-scoped init script. Install Datadog agent init script notebook Get notebook Decommission spot instances Note This feature is available on Databricks Runtime 8.0 and above. Because spot instances can reduce costs, creating clusters using spot instances rather than on-demand instances is a common way to run jobs. However, spot instances can be preempted by cloud provider scheduling mechanisms. Preemption of spot instances can cause issues with jobs that are running, including: Shuffle fetch failures Shuffle data loss RDD data loss Job failures You can enable decommissioning to help address these issues. Decommissioning takes advantage of the notification that the cloud provider usually sends before a spot instance is decommissioned. When a spot instance containing an executor receives a preemption notification, the decommissioning process will attempt to migrate shuffle and RDD data to healthy executors. The duration before the final preemption is typically 30 seconds to 2 minutes, depending on the cloud provider. Databricks recommends enabling data migration when decommissioning is also enabled. Generally, the possibility of errors decreases as more data is migrated, including shuffle fetching failures, shuffle data loss, and RDD data loss. Data migration can also lead to less re-computation and save cost. Decommissioning is best effort and does not guarantee that all data can be migrated before final preemption. Decommissioning cannot guarantee against shuffle fetch failures when running tasks are fetching shuffle data from the executor. With decommissioning enabled, task failures caused by spot instance preemption are not added to the total number of failed attempts. Task failures caused by preemption are not counted as failed attempts because the cause of the failure is external to the task and will not result in job failure. To enable decommissioning, you set Spark configuration settings and environment variables when you create a cluster: To enable decommissioning for applications: Copy spark.decommission.enabled true
 To enable shuffle data migration during decommissioning: Copy spark.storage.decommission.enabled true
spark.storage.decommission.shuffleBlocks.enabled true
 To enable RDD cache data migration during decommissioning: Note When RDD StorageLevel replication is set to more than 1, Databricks does not recommend enabling RDD data migration since the replicas ensure RDDs will not lose data. Copy spark.storage.decommission.enabled true
spark.storage.decommission.rddBlocks.enabled true
 To enable decommissioning for workers: Copy SPARK_WORKER_OPTS=""-Dspark.decommission.enabled=true""
 To set these custom Spark configuration properties: On the New Cluster page, click the Advanced Options toggle. Click the Spark tab. To access a worker’s decommission status from the UI, navigate to the Spark Cluster UI - Master tab: When the decommissioning finishes, the executor that decommissioned shows the loss reason in the Spark UI > Executors tab on the cluster’s details page:",Manage clusters
18,ARR|| MGM Resorts International || Databricks clusters were removed without notice || 2206210010003378,"There were two Production clusters that were removed and couldnt find trace of how and who removed them. 
We have now recreated them and these are available as the attached image shows. 
We suspect that the clusters were removed between May 10th and June 21st.
Cluster names:
edh-dmdata-prod-10.4
edh-dmdata-uat-10.4

We did run the LA query as recommended by MSFT Document: 

https://docs.microsoft.com/en-us/azure/databricks/kb/administration/who-deleted-cluster?WT.mc_id=Portal-Microsoft_Azure_Support

DatabricksClusters
| where ActionName == 'permanentDelete'
   and Response contains '\\'statusCode\\':200'
  //   and RequestParams contains '\\'cluster_id\\':\\'0210-024915-bore731\\''  // Add cluster_id filter if cluster id is known
     and TimeGenerated between(datetime('2022-05-10 00:00:00') .. datetime('2022-06-22 00:00:00'))  // Add timestamp (in UTC) filter to narrow down the result.
| extend id = parse_json(Identity)
| extend requestParams = parse_json(RequestParams)
| project UserEmail=id.email,clusterId = requestParams.cluster_id, SourceIPAddress, EventTime=TimeGenerated

No results returned. 

We could not find anything related on cluster events log.

Ask: Need to know when did the clusters got removed and what caused the clusters to be removed ?

Problem start date and time
Tue, May 10, 2022, 12:00 AM (UTC-08:00) Pacific Time (US & Canada)",https://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Manage clusters Article 07/12/2022 15 minutes to read 6 contributors In this article Display clusters Pin a cluster View a cluster configuration as a JSON file Edit a cluster Clone a cluster Control access to clusters Start a cluster Terminate a cluster Delete a cluster Restart a cluster to update it with the latest images View cluster information in the Apache Spark UI View cluster logs Monitor performance Decommission spot instances This article describes how to manage Azure Databricks clusters, including displaying, editing, starting, terminating, deleting, controlling access, and monitoring performance and logs. Display clusters To display the clusters in your workspace, click Compute in the sidebar. The Compute page displays clusters in two tabs: All-purpose clusters and Job clusters. At the left side are two columns indicating if the cluster has been pinned and the status of the cluster: Pinned Starting , Terminating Standard cluster Running Terminated High concurrency cluster Running Terminated Access Denied Running Terminated Table ACLs enabled Running Terminated At the far right of the right side of the All-purpose clusters tab is an icon you can use to terminate the cluster. You can use the three-button menu to restart, clone, delete, or edit permissions for the cluster. Menu options that are not available are grayed out. The All-purpose clusters tab shows the numbers of notebooks attached to the cluster. Filter cluster list You can filter the cluster lists using the buttons and search box at the top right: Pin a cluster 30 days after a cluster is terminated, it is permanently deleted. To keep an all-purpose cluster configuration even after a cluster has been terminated for more than 30 days, an administrator can pin the cluster. Up to 100 clusters can be pinned. You can pin a cluster from the cluster list or the cluster detail page: Pin cluster from cluster list To pin or unpin a cluster, click the pin icon to the left of the cluster name. Pin cluster from cluster detail page To pin or unpin a cluster, click the pin icon to the right of the cluster name. You can also invoke the Pin API endpoint to programmatically pin a cluster. View a cluster configuration as a JSON file Sometimes it can be helpful to view your cluster configuration as JSON. This is especially useful when you want to create similar clusters using the Clusters API 2.0. When you view an existing cluster, simply go to the Configuration tab, click JSON in the top right of the tab, copy the JSON, and paste it into your API call. JSON view is ready-only. Edit a cluster You edit a cluster configuration from the cluster detail page. To display the cluster detail page, click the cluster name on the Compute page. You can also invoke the Edit API endpoint to programmatically edit the cluster. Note Notebooks and jobs that were attached to the cluster remain attached after editing. Libraries installed on the cluster remain installed after editing. If you edit any attribute of a running cluster (except for the cluster size and permissions), you must restart it. This can disrupt users who are currently using the cluster. You can edit only running or terminated clusters. You can, however, update permissions for clusters that are not in those states on the cluster details page. For detailed information about cluster configuration properties you can edit, see Configure clusters. Clone a cluster You can create a new cluster by cloning an existing cluster. From the cluster list, click the three-button menu and select Clone from the drop down. From the cluster detail page, click and select Clone from the drop down. The cluster creation form is opened prepopulated with the cluster configuration. The following attributes from the existing cluster are not included in the clone: Cluster permissions Installed libraries Attached notebooks Control access to clusters Cluster access control within the Admin Console allows admins and delegated users to give fine-grained cluster access to other users. There are two types of cluster access control: Cluster creation permission: Admins can choose which users are allowed to create clusters. Cluster-level permissions: A user who has the Can manage permission for a cluster can configure whether other users can attach to, restart, resize, and manage that cluster from the cluster list or the cluster details page. From the cluster list, click the three-button menu and select Edit Permissions. From the cluster detail page, click and select Permissions. To learn how to configure cluster access control and cluster-level permissions, see Cluster access control. Start a cluster Apart from creating a new cluster, you can also start a previously terminated cluster. This lets you re-create a previously terminated cluster with its original configuration. You can start a cluster from the cluster list, the cluster detail page, or a notebook. To start a cluster from the cluster list, click the arrow: To start a cluster from the cluster detail page, click Start: Notebook cluster attach drop-down You can also invoke the Start API endpoint to programmatically start a cluster. Azure Databricks identifies a cluster with a unique cluster ID. When you start a terminated cluster, Databricks re-creates the cluster with the same ID, automatically installs all the libraries, and re-attaches the notebooks. Note If you are using a Trial workspace and the trial has expired, you will not be able to start a cluster. Cluster autostart for jobs When a job assigned to an existing terminated cluster is scheduled to run or you connect to a terminated cluster from a JDBC/ODBC interface, the cluster is automatically restarted. See Create a job and JDBC connect. Cluster autostart allows you to configure clusters to autoterminate without requiring manual intervention to restart the clusters for scheduled jobs. Furthermore, you can schedule cluster initialization by scheduling a job to run on a terminated cluster. Before a cluster is restarted automatically, cluster and job access control permissions are checked. Note If your cluster was created in Azure Databricks platform version 2.70 or earlier, there is no autostart: jobs scheduled to run on terminated clusters will fail. Terminate a cluster To save cluster resources, you can terminate a cluster. A terminated cluster cannot run notebooks or jobs, but its configuration is stored so that it can be reused (or—in the case of some types of jobs—autostarted) at a later time. You can manually terminate a cluster or configure the cluster to automatically terminate after a specified period of inactivity. Azure Databricks records information whenever a cluster is terminated. When the number of terminated clusters exceeds 150, the oldest clusters are deleted. Unless a cluster is pinned, 30 days after the cluster is terminated, it is automatically and permanently deleted. Terminated clusters appear in the cluster list with a gray circle at the left of the cluster name. Note When you run a job on a New Job Cluster (which is usually recommended), the cluster terminates and is unavailable for restarting when the job is complete. On the other hand, if you schedule a job to run on an Existing All-Purpose Cluster that has been terminated, that cluster will autostart. Important If you are using a Trial Premium workspace, all running clusters are terminated: When you upgrade a workspace to full Premium. If the workspace is not upgraded and the trial expires. Manual termination You can manually terminate a cluster from the cluster list or the cluster detail page. To terminate a cluster from the cluster list, click the square: To terminate a cluster from the cluster detail page, click Terminate: Automatic termination You can also set auto termination for a cluster. During cluster creation, you can specify an inactivity period in minutes after which you want the cluster to terminate. If the difference between the current time and the last command run on the cluster is more than the inactivity period specified, Azure Databricks automatically terminates that cluster. A cluster is considered inactive when all commands on the cluster, including Spark jobs, Structured Streaming, and JDBC calls, have finished executing. Warning Clusters do not report activity resulting from the use of DStreams. This means that an autoterminating cluster may be terminated while it is running DStreams. Turn off auto termination for clusters running DStreams or consider using Structured Streaming. The auto termination feature monitors only Spark jobs, not user-defined local processes. Therefore, if all Spark jobs have completed, a cluster may be terminated even if local processes are running. Idle clusters continue to accumulate DBU and cloud instance charges during the inactivity period before termination. Configure automatic termination You configure automatic termination in the Auto Termination field in the Autopilot Options box on the cluster creation page: Important The default value of the auto terminate setting depends on whether you choose to create a standard or high concurrency cluster: Standard clusters are configured to terminate automatically after 120 minutes. High concurrency clusters are configured to not terminate automatically. You can opt out of auto termination by clearing the Auto Termination checkbox or by specifying an inactivity period of 0. Note Auto termination is best supported in the latest Spark versions. Older Spark versions have known limitations which can result in inaccurate reporting of cluster activity. For example, clusters running JDBC, R, or streaming commands can report a stale activity time that leads to premature cluster termination. Please upgrade to the most recent Spark version to benefit from bug fixes and improvements to auto termination. Unexpected termination Sometimes a cluster is terminated unexpectedly, not as a result of a manual termination or a configured automatic termination. For a list of termination reasons and remediation steps, see the Knowledge Base. Delete a cluster Deleting a cluster terminates the cluster and removes its configuration. Warning You cannot undo this action. You cannot delete a pinned cluster. In order to delete a pinned cluster, it must first be unpinned by an administrator. From the cluster list, click the three-button menu and select Delete from the drop down. From the cluster detail page, click and select Delete from the drop down. You can also invoke the Permanent delete API endpoint to programmatically delete a cluster. Restart a cluster to update it with the latest images When you restart a cluster, it gets the latest images for the compute resource containers and the VM hosts. It is particularly important to schedule regular restarts for long-running clusters, which are often used for some applications such as processing streaming data. It is your responsibility to restart all compute resources regularly to keep the image up-to-date with the latest image version. Important If you enable the compliance security profile for your account or your workspace, long-running clusters are automatically restarted after 25 days. Databricks recommends that admins restart clusters before they run for 25 days and do so during a scheduled maintenance window. This reduces the risk of an auto-restart disrupting a scheduled job. You can restart a cluster in multiple ways: Use the UI to restart a cluster from the cluster detail page. To display the cluster detail page, click the cluster name on the Compute page. Click Restart. Use the Clusters API to restart a cluster. Use the script that Azure Databricks provides that determines how long your clusters have run, and optionally restarts them if they exceed a specified number of days since they were started Run a script that determines how many days your clusters have been running, and optionally restart them If you are a workspace admin, you can run a script that determines how long each of your clusters has been running, and optionally restart them if they are older than a specified number of days. Azure Databricks provides this script as a notebook. The first lines of the script define configuration parameters: min_age_output: The maximum number of days that a cluster can run. Default is 1. perform_restart: If True, the script restarts clusters with age greater than the number of days specified by min_age_output. The default is False, which identifies the long running clusters but does not restart them. secret_configuration: Replace REPLACE_WITH_SCOPE and REPLACE_WITH_KEY with a secret scope and key name. For more details of setting up the secrets, see the notebook. Warning If you set perform_restart to True, the script automatically restarts eligible clusters, which can cause active jobs to fail and reset open notebooks. To reduce the risk of disrupting your workspace’s business critical jobs, plan a scheduled maintenance window and be sure to notify workspace users. Identify and optionally restart long-running clusters notebook Get notebook View cluster information in the Apache Spark UI You can view detailed information about Spark jobs in the Spark UI, which you can access from the Spark UI tab on the cluster details page. You can get details about active and terminated clusters. If you restart a terminated cluster, the Spark UI displays information for the restarted cluster, not the historical information for the terminated cluster. View cluster logs Azure Databricks provides three kinds of logging of cluster-related activity: Cluster event logs, which capture cluster lifecycle events, like creation, termination, configuration edits, and so on. Apache Spark driver and worker logs, which you can use for debugging. Cluster init-script logs, valuable for debugging init scripts. This section discusses cluster event logs and driver and worker logs. For details about init-script logs, see Init script logs. Cluster event logs The cluster event log displays important cluster lifecycle events that are triggered manually by user actions or automatically by Azure Databricks. Such events affect the operation of a cluster as a whole and the jobs running in the cluster. For supported event types, see the REST API ClusterEventType data structure. Events are stored for 60 days, which is comparable to other data retention times in Azure Databricks. View a cluster event log Click Compute in the sidebar. Click a cluster name. Click the Event Log tab. To filter the events, click the in the Filter by Event Type… field and select one or more event type checkboxes. Use Select all to make it easier to filter by excluding particular event types. View event details For more information about an event, click its row in the log and then click the JSON tab for details. Cluster driver and worker logs The direct print and log statements from your notebooks, jobs, and libraries go to the Spark driver logs. These logs have three outputs: Standard output Standard error Log4j logs You can access these files from the Driver logs tab on the cluster details page. Click the name of a log file to download it. To view Spark worker logs, you can use the Spark UI. You can also configure a log delivery location for the cluster. Both worker and cluster logs are delivered to the location you specify. Monitor performance To help you monitor the performance of Azure Databricks clusters, Azure Databricks provides access to Ganglia metrics from the cluster details page. In addition, you can configure an Azure Databricks cluster to send metrics to a Log Analytics workspace in Azure Monitor, the monitoring platform for Azure. You can install Datadog agents on cluster nodes to send Datadog metrics to your Datadog account. Ganglia metrics To access the Ganglia UI, navigate to the Metrics tab on the cluster details page. CPU metrics are available in the Ganglia UI for all Databricks runtimes. GPU metrics are available for GPU-enabled clusters. To view live metrics, click the Ganglia UI link. To view historical metrics, click a snapshot file. The snapshot contains aggregated metrics for the hour preceding the selected time. Configure metrics collection By default, Azure Databricks collects Ganglia metrics every 15 minutes. To configure the collection period, set the DATABRICKS_GANGLIA_SNAPSHOT_PERIOD_MINUTES environment variable using an init script or in the spark_env_vars field in the Cluster Create API. Azure Monitor You can configure an Azure Databricks cluster to send metrics to a Log Analytics workspace in Azure Monitor, the monitoring platform for Azure. For complete instructions, see Monitoring Azure Databricks. Note If you have deployed the Azure Databricks workspace in your own virtual network and you have configured network security groups (NSG) to deny all outbound traffic that is not required by Azure Databricks, then you must configure an additional outbound rule for the “AzureMonitor” service tag. Datadog metrics You can install Datadog agents on cluster nodes to send Datadog metrics to your Datadog account. The following notebook demonstrates how to install a Datadog agent on a cluster using a cluster-scoped init script. To install the Datadog agent on all clusters, use a global init script after testing the cluster-scoped init script. Install Datadog agent init script notebook Get notebook Decommission spot instances Note This feature is available on Databricks Runtime 8.0 and above. Because spot instances can reduce costs, creating clusters using spot instances rather than on-demand instances is a common way to run jobs. However, spot instances can be preempted by cloud provider scheduling mechanisms. Preemption of spot instances can cause issues with jobs that are running, including: Shuffle fetch failures Shuffle data loss RDD data loss Job failures You can enable decommissioning to help address these issues. Decommissioning takes advantage of the notification that the cloud provider usually sends before a spot instance is decommissioned. When a spot instance containing an executor receives a preemption notification, the decommissioning process will attempt to migrate shuffle and RDD data to healthy executors. The duration before the final preemption is typically 30 seconds to 2 minutes, depending on the cloud provider. Databricks recommends enabling data migration when decommissioning is also enabled. Generally, the possibility of errors decreases as more data is migrated, including shuffle fetching failures, shuffle data loss, and RDD data loss. Data migration can also lead to less re-computation and save cost. Decommissioning is best effort and does not guarantee that all data can be migrated before final preemption. Decommissioning cannot guarantee against shuffle fetch failures when running tasks are fetching shuffle data from the executor. With decommissioning enabled, task failures caused by spot instance preemption are not added to the total number of failed attempts. Task failures caused by preemption are not counted as failed attempts because the cause of the failure is external to the task and will not result in job failure. To enable decommissioning, you set Spark configuration settings and environment variables when you create a cluster: To enable decommissioning for applications: Copy spark.decommission.enabled true
 To enable shuffle data migration during decommissioning: Copy spark.storage.decommission.enabled true
spark.storage.decommission.shuffleBlocks.enabled true
 To enable RDD cache data migration during decommissioning: Note When RDD StorageLevel replication is set to more than 1, Databricks does not recommend enabling RDD data migration since the replicas ensure RDDs will not lose data. Copy spark.storage.decommission.enabled true
spark.storage.decommission.rddBlocks.enabled true
 To enable decommissioning for workers: Copy SPARK_WORKER_OPTS=""-Dspark.decommission.enabled=true""
 To set these custom Spark configuration properties: On the New Cluster page, click the Advanced Options toggle. Click the Spark tab. To access a worker’s decommission status from the UI, navigate to the Spark Cluster UI - Master tab: When the decommissioning finishes, the executor that decommissioned shows the loss reason in the Spark UI > Executors tab on the cluster’s details page:",Manage clusters
19,ARR | Operation failed: 'The key vault key is not found to unwrap the encryption key.',"Issue Summary
##############################
* Cx is frequently getting the following error in their streaming job: “Failed during autoloader stream execution Caused by: StreamingQueryException: Query [id = 5000d9da-49c1-4ccb-ba37-5632a4dedff6, runId = 70e86435-f04c-40d4-8440-b9edee02282f] terminated with exception: Transformation error Caused by: StlException: Transformation error Caused by: Job aborted. Caused by: Job aborted due to stage failure. Caused by: Task failed while writing rows. Caused by: AbfsRestOperationException: Operation failed: 'The key vault key is not found to unwrap the encryption key.', 403, PUT, https://datalakeeastus2prd.dfs.core.windows.net/cricket-edr/cricket_flow_edr/data_dt%3D20220607/part-01415-57c15ba3-de40-42a3-b357-848e001850a2.c000.snappy.parquet?resource=file&timeout=90, KeyVaultEncryptionKeyNotFound, 'The key vault key is not found to unwrap the encryption key. RequestId:2612092a-801f-0035-1cab-7a36c3000000 Time:2022-06-07T20:19:51.4603359Z'”.

* Sample of failed job run: https://adb-2576137007542488.8.azuredatabricks.net/?o=2576137007542488#job/43367/run/41607548.

* Cx accesses ADLS 2 storage “datalakeeastus2prd” directly with client’s secret saved in Azure Key Vault.
* Azure Key Vault details:
> Scope Name: dl-eastus2-prd-sec-kv-scope
> Azure Key-Vault DNS Name: https://dl-eastus2-prd-sec-kv.vault.azure.net/
> Azure Key-Vault Resource URI: /subscriptions/4f267f79-c226-4fc8-af38-7baa2a326c54/resourceGroups/datalake-eastus2-prd-rg/providers/Microsoft.KeyVault/vaults/dl-eastus2-prd-sec-kv

Troubleshooting
##############################
* The issue is intermittent but frequent. 
* Cx confirmed that there is no change in authentication details such as client’s secret.
* I tried to look up similar incidents having the error but couldn’t find any.
* I’m suspecting that the issue is either in pulling the scope details from Databricks or retrieving the secret scope value from Azure Key Value itself.
* I’m contacting Azure Key Vault team for the same.

Ask
##############################
Help in identify if the issue is in pulling the scope details from Databricks.",https://docs.microsoft.com/en-us/azure/databricks/security/keys/customer-managed-keys-dbfs/cmk-dbfs-azure-cli,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure customer-managed keys for DBFS using the Azure CLI Article 05/25/2022 2 minutes to read 3 contributors In this article Install the Azure Databricks CLI extension Prepare a new or existing Azure Databricks workspace for encryption Create a new key vault Configure the key vault access policy Create a new key Configure DBFS encryption with customer-managed keys Disable customer-managed keys Note This feature is available only in the Premium Plan. You can use the Azure CLI to configure your own encryption key to encrypt the DBFS root storage account. You must use Azure Key Vault to store the key. For more information about customer-managed keys for DBFS, see Configure customer-managed keys for DBFS root. Install the Azure Databricks CLI extension Install the Azure CLI. Install the Azure Databricks CLI extension. Bash Copy az extension add --name databricks
 Prepare a new or existing Azure Databricks workspace for encryption Replace the placeholder values in brackets with your own values. The <workspace-name> is the resource name as displayed in the Azure portal. Bash Copy az login
az account set --subscription <subscription-id>
 Prepare for encryption during workspace creation: Bash Copy az databricks workspace create --name <workspace-name> --location <workspace-location> --resource-group <resource-group> --sku premium --prepare-encryption
 Prepare an existing workspace for encryption: Bash Copy az databricks workspace update --name <workspace-name> --resource-group <resource-group> --prepare-encryption
 Note the principalId field in the storageAccountIdentity section of the command output. You will provide it as the managed identity value when you configure your key vault. For more information about Azure CLI commands for Azure Databricks workspaces, see the az databricks workspace command reference. Create a new key vault The key vault that you use to store customer-managed keys for root DBFS must have two key protection settings enabled, Soft Delete and Purge Protection. To create a new key vault with these settings enabled, run the following commands. Replace the placeholder values in brackets with your own values. Bash Copy az keyvault create \
        --name <key-vault> \
        --resource-group <resource-group> \
        --location <region> \
        --enable-soft-delete \
        --enable-purge-protection
 For more information about enabling Soft Delete and Purge Protection using the Azure CLI, see How to use Key Vault soft-delete with CLI. Configure the key vault access policy Set the access policy for the key vault so that the Azure Databricks workspace has permission to access it, using the az keyvault set-policy command. Replace the placeholder values in brackets with your own values. Bash Copy az keyvault set-policy \
        --name <key-vault> \
        --resource-group <resource-group> \
        --object-id <managed-identity>  \
        --key-permissions get unwrapKey wrapKey
 Replace <managed-identity> with the principalId value that you noted when you prepared your workspace for encryption. Create a new key Create a key in the key vault using the az keyvault key create command. Replace the placeholder values in brackets with your own values. Bash Copy az keyvault key create \
       --name <key> \
       --vault-name <key-vault>
 DBFS root storage supports RSA and RSA-HSM keys of sizes 2048, 3072 and 4096. For more information about keys, see About Key Vault keys. Configure DBFS encryption with customer-managed keys Configure your Azure Databricks workspace to use the key you created in your Azure Key Vault. Replace the placeholder values in brackets with your own values. Bash Copy key_vault_uri=$(az keyvault show \
 --name <key-vault> \
 --resource-group <resource-group> \
 --query properties.vaultUri \
--output tsv)
 Bash Copy key_version=$(az keyvault key list-versions \
 --name <key> \ --vault-name <key-vault> \
 --query [-1].kid \
--output tsv | cut -d '/' -f 6)
 Bash Copy az databricks workspace update --name <workspace-name> --resource-group <resource-group> --key-source Microsoft.KeyVault --key-name <key> --key-vault $key_vault_uri --key-version $key_version
 Disable customer-managed keys When you disable customer-managed keys, your storage account is once again encrypted with Microsoft-managed keys. Replace the placeholder values in brackets with your own values and use the variables defined in the previous steps. Bash Copy az databricks workspace update --name <workspace-name> --resource-group <resource-group> --key-source Default",Configure customer-managed keys for DBFS using the Azure CLI
20,2203250050002287 - pip install using Pypi artifacts instead of customer artifacts hosted on Devops,"cx is having intermittent issues with their framework_v3 init script which references other init scripts: 
 
Error:

Cluster scoped init script dbfs:/databricks/init/scripts/framework_v3.sh failed: Script exit status is non-zero
 
/dbfs/databricks/init/scripts/framework_v3.sh:
#!/bin/sh INIT_ROOT=/dbfs/databricks/init/scripts 
source ""${INIT_ROOT}/master_init.sh"" 
source ""${INIT_ROOT}/04_odbc.sh""
FILE_PATH=""/dbfs/FileStore/jars/spark_xml_2.12.jar"" 
[[ -f ""$FILE_PATH"" ]] || { wget --quiet -O ""$FILE_PATH"" https://repo1.maven.org/maven2/com/databricks/spark-xml_2.12/0.10.0/spark-xml_2.12-0.10.0.jar } 
cp $FILE_PATH /databricks/jars/. 
pip install dtautils==3.*
 
/dbfs/databricks/init/scripts/master_init.sh
#!/bin/sh set -e INIT_ROOT=/dbfs/databricks/init/scripts 
source ""${INIT_ROOT}/00_adls_auth.sh"" 
source ""${INIT_ROOT}/01_external_metastore.sh"" 
source ""${INIT_ROOT}/02_metastore_fix.sh"" 
source ""${INIT_ROOT}/03_env_setup.sh""
 
/dbfs/databricks/init/scripts/04_odbc.sh
#!/bin/sh pip install pyodbc 
curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - 
curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list > /etc/apt/sources.list.d/mssql-release.list 
sudo apt-get update 
sudo ACCEPT_EULA=Y apt-get install msodbcsql17
 
 
/dbfs/databricks/init/scripts/03_env_setup.sh
#!/bin/sh set -e INIT_ROOT=/dbfs/databricks/init/scripts 
source ""${INIT_ROOT}/00_adls_auth.sh"" 
source ""${INIT_ROOT}/01_external_metastore.sh"" 
source ""${INIT_ROOT}/02_metastore_fix.sh"" 
source ""${INIT_ROOT}/03_env_setup.sh""
 
Customer believes that the failure is on the pip install dtautils line as it is using the pypi dtautils instead of their homebuilt one hosted on Artifacts. However, we cannot confirm that it is so since there are many parts/references to other scripts in this script which could be the cause of the issue.
 
cx is stating that random workarounds seem to mitigate the issue. like keeping the cluster running for 2-3 hours and then retrying the script. or logging in as an admin user to the DevOps artifact location and then retrying the install.
 
cx tried adding the DevOps Artifacts keyring as a parameter to pip install but the issue is still ongoing.

Recent failed job URLs:
https://northeurope.azuredatabricks.net/?o=1633172783118298#job/510173289706999/run/117176179
https://northeurope.azuredatabricks.net/?o=1633172783118298#job/211278261913885/run/117165543",https://docs.microsoft.com/en-us/azure/databricks/clusters/configure,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure clusters Article 07/14/2022 17 minutes to read 7 contributors In this article Cluster policy Cluster mode Pools Databricks Runtime Cluster node type Cluster size and autoscaling Autoscaling local storage Local disk encryption Security mode Spark configuration Retrieve a Spark configuration property from a secret Environment variables Cluster tags SSH access to clusters Cluster log delivery Init scripts This article explains the configuration options available when you create and edit Azure Databricks clusters. It focuses on creating and editing clusters using the UI. For other methods, see Clusters CLI, Clusters API 2.0, and Databricks Terraform provider. For help deciding what combination of configuration options suits your needs best, see cluster configuration best practices. Cluster policy A cluster policy limits the ability to configure clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. Cluster policies have ACLs that limit their use to specific users and groups and thus limit which policies you can select when you create a cluster. To configure a cluster policy, select the cluster policy in the Policy drop-down. Note If no policies have been created in the workspace, the Policy drop-down does not display. If you have: Cluster create permission, you can select the Unrestricted policy and create fully-configurable clusters. The Unrestricted policy does not limit any cluster attributes or attribute values. Both cluster create permission and access to cluster policies, you can select the Unrestricted policy and the policies you have access to. Access to cluster policies only, you can select the policies you have access to. Cluster mode Azure Databricks supports three cluster modes: Standard, High Concurrency, and Single Node. The default cluster mode is Standard. Important If your workspace is assigned to a Unity Catalog metastore, High Concurrency clusters are not available. Instead, you use security mode to ensure the integrity of access controls and enforce strong isolation guarantees. See also Create a Data Science & Engineering cluster. You cannot change the cluster mode after a cluster is created. If you want a different cluster mode, you must create a new cluster. Note The cluster configuration includes an auto terminate setting whose default value depends on cluster mode: Standard and Single Node clusters terminate automatically after 120 minutes by default. High Concurrency clusters do not terminate automatically by default. Standard clusters A Standard cluster is recommended for a single user. Standard clusters can run workloads developed in any language: Python, SQL, R, and Scala. High Concurrency clusters A High Concurrency cluster is a managed cloud resource. The key benefits of High Concurrency clusters are that they provide fine-grained sharing for maximum resource utilization and minimum query latencies. High Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. In addition, only High Concurrency clusters support table access control. To create a High Concurrency cluster, set Cluster Mode to High Concurrency. For an example of how to create a High Concurrency cluster using the Clusters API, see High Concurrency cluster example. Single Node clusters A Single Node cluster has no workers and runs Spark jobs on the driver node. In contrast, a Standard cluster requires at least one Spark worker node in addition to the driver node to execute Spark jobs. To create a Single Node cluster, set Cluster Mode to Single Node. To learn more about working with Single Node clusters, see Single Node clusters. Pools To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances, for the driver and worker nodes. The cluster is created using instances in the pools. If a pool does not have sufficient idle resources to create the requested driver or worker nodes, the pool expands by allocating new instances from the instance provider. When an attached cluster is terminated, the instances it used are returned to the pools and can be reused by a different cluster. If you select a pool for worker nodes but not for the driver node, the driver node inherit the pool from the worker node configuration. Important If you attempt to select a pool for the driver node but not for worker nodes, an error occurs and your cluster isn’t created. This requirement prevents a situation where the driver node has to wait for worker nodes to be created, or vice versa. See Pools to learn more about working with pools in Azure Databricks. Databricks Runtime Databricks runtimes are the set of core components that run on your clusters. All Databricks runtimes include Apache Spark and add components and updates that improve usability, performance, and security. For details, see Databricks runtimes. Azure Databricks offers several types of runtimes and several versions of those runtime types in the Databricks Runtime Version drop-down when you create or edit a cluster. Photon acceleration Important This feature is in Public Preview. Note Available in Databricks Runtime 8.3 and above. To enable Photon acceleration, select the Use Photon Acceleration checkbox. If desired, you can specify the instance type in the Worker Type and Driver Type drop-down. Databricks recommends the following instance types for optimal price and performance: Standard_E4ds_v4 Standard_E8ds_v4 Standard_E16ds_v4 You can view Photon activity in the Spark UI. The following screenshot shows the query details DAG. There are two indications of Photon in the DAG. First, Photon operators start with “Photon”, for example, PhotonGroupingAgg. Second, in the DAG, Photon operators and stages are colored peach, while the non-Photon ones are blue. Docker images For some Databricks Runtime versions, you can specify a Docker image when you create a cluster. Example use cases include library customization, a golden container environment that doesn’t change, and Docker CI/CD integration. You can also use Docker images to create custom deep learning environments on clusters with GPU devices. For instructions, see Customize containers with Databricks Container Services and Databricks Container Services on GPU clusters. Cluster node type A cluster consists of one driver node and zero or more worker nodes. You can pick separate cloud provider instance types for the driver and worker nodes, although by default the driver node uses the same instance type as the worker node. Different families of instance types fit different use cases, such as memory-intensive or compute-intensive workloads. Note If your security requirements include compute isolation, select a Standard_F72s_V2 instance as your worker type. These instance types represent isolated virtual machines that consume the entire physical host and provide the necessary level of isolation required to support, for example, US Department of Defense Impact Level 5 (IL5) workloads. Driver node Worker node GPU instance types Spot instances Driver node The driver node maintains state information of all notebooks attached to the cluster. The driver node also maintains the SparkContext and interprets all the commands you run from a notebook or a library on the cluster, and runs the Apache Spark master that coordinates with the Spark executors. The default value of the driver node type is the same as the worker node type. You can choose a larger driver node type with more memory if you are planning to collect() a lot of data from Spark workers and analyze them in the notebook. Tip Since the driver node maintains all of the state information of the notebooks attached, make sure to detach unused notebooks from the driver node. Worker node Azure Databricks worker nodes run the Spark executors and other services required for the proper functioning of the clusters. When you distribute your workload with Spark, all of the distributed processing happens on worker nodes. Azure Databricks runs one executor per worker node; therefore the terms executor and worker are used interchangeably in the context of the Azure Databricks architecture. Tip To run a Spark job, you need at least one worker node. If a cluster has zero workers, you can run non-Spark commands on the driver node, but Spark commands will fail. GPU instance types For computationally challenging tasks that demand high performance, like those associated with deep learning, Azure Databricks supports clusters accelerated with graphics processing units (GPUs). For more information, see GPU-enabled clusters. Spot instances To save cost, you can choose to use spot instances, also known as Azure Spot VMs by checking the Spot instances checkbox. The first instance will always be on-demand (the driver node is always on-demand) and subsequent instances will be spot instances. If spot instances are evicted due to unavailability, on-demand instances are deployed to replace evicted instances. Cluster size and autoscaling When you create a Azure Databricks cluster, you can either provide a fixed number of workers for the cluster or provide a minimum and maximum number of workers for the cluster. When you provide a fixed size cluster, Azure Databricks ensures that your cluster has the specified number of workers. When you provide a range for the number of workers, Databricks chooses the appropriate number of workers required to run your job. This is referred to as autoscaling. With autoscaling, Azure Databricks dynamically reallocates workers to account for the characteristics of your job. Certain parts of your pipeline may be more computationally demanding than others, and Databricks automatically adds additional workers during these phases of your job (and removes them when they’re no longer needed). Autoscaling makes it easier to achieve high cluster utilization, because you don’t need to provision the cluster to match a workload. This applies especially to workloads whose requirements change over time (like exploring a dataset during the course of a day), but it can also apply to a one-time shorter workload whose provisioning requirements are unknown. Autoscaling thus offers two advantages: Workloads can run faster compared to a constant-sized under-provisioned cluster. Autoscaling clusters can reduce overall costs compared to a statically-sized cluster. Depending on the constant size of the cluster and the workload, autoscaling gives you one or both of these benefits at the same time. The cluster size can go below the minimum number of workers selected when the cloud provider terminates instances. In this case, Azure Databricks continuously retries to re-provision instances in order to maintain the minimum number of workers. Note Autoscaling is not available for spark-submit jobs. How autoscaling behaves Scales up from min to max in 2 steps. Can scale down even if the cluster is not idle by looking at shuffle file state. Scales down based on a percentage of current nodes. On job clusters, scales down if the cluster is underutilized over the last 40 seconds. On all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds. The spark.databricks.aggressiveWindowDownS Spark configuration property specifies in seconds how often a cluster makes down-scaling decisions. Increasing the value causes a cluster to scale down more slowly. The maximum value is 600. Enable and configure autoscaling To allow Azure Databricks to resize your cluster automatically, you enable autoscaling for the cluster and provide the min and max range of workers. Enable autoscaling. All-Purpose cluster - On the Create Cluster page, select the Enable autoscaling checkbox in the Autopilot Options box: Job cluster - On the Configure Cluster page, select the Enable autoscaling checkbox in the Autopilot Options box: Configure the min and max workers. When the cluster is running, the cluster detail page displays the number of allocated workers. You can compare number of allocated workers with the worker configuration and make adjustments as needed. Important If you are using an instance pool: Make sure the cluster size requested is less than or equal to the minimum number of idle instances in the pool. If it is larger, cluster startup time will be equivalent to a cluster that doesn’t use a pool. Make sure the maximum cluster size is less than or equal to the maximum capacity of the pool. If it is larger, the cluster creation will fail. Autoscaling example If you reconfigure a static cluster to be an autoscaling cluster, Azure Databricks immediately resizes the cluster within the minimum and maximum bounds and then starts autoscaling. As an example, the following table demonstrates what happens to clusters with a certain initial size if you reconfigure a cluster to autoscale between 5 and 10 nodes. Initial size Size after reconfiguration 6 6 12 10 3 5 Autoscaling local storage It can often be difficult to estimate how much disk space a particular job will take. To save you from having to estimate how many gigabytes of managed disk to attach to your cluster at creation time, Azure Databricks automatically enables autoscaling local storage on all Azure Databricks clusters. With autoscaling local storage, Azure Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new managed disk to the worker before it runs out of disk space. Disks are attached up to a limit of 5 TB of total disk space per virtual machine (including the virtual machine’s initial local storage). The managed disks attached to a virtual machine are detached only when the virtual machine is returned to Azure. That is, managed disks are never detached from a virtual machine as long as it is part of a running cluster. To scale down managed disk usage, Azure Databricks recommends using this feature in a cluster configured with Spot instances or Automatic termination. Local disk encryption Important This feature is in Public Preview. Some instance types you use to run clusters may have locally attached disks. Azure Databricks may store shuffle data or ephemeral data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data that is stored temporarily on your cluster’s local disks, you can enable local disk encryption. Important Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. When local disk encryption is enabled, Azure Databricks generates an encryption key locally that is unique to each cluster node and is used to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. To enable local disk encryption, you must use the Clusters API 2.0. During cluster creation or edit, set: JSON Copy {
  ""enable_local_disk_encryption"": true
}
 See Create and Edit in the Clusters API reference for examples of how to invoke these APIs. Here is an example of a cluster create call that enables local disk encryption: JSON Copy {
  ""cluster_name"": ""my-cluster"",
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2"",
  ""enable_local_disk_encryption"": true,
  ""spark_conf"": {
    ""spark.speculation"": true
  },
  ""num_workers"": 25
}
 Security mode If your workspace is assigned to a Unity Catalog metastore, you use security mode instead of High Concurrency cluster mode to ensure the integrity of access controls and enforce strong isolation guarantees. High Concurrency cluster mode is not available with Unity Catalog. Under Advanced options, select from the following cluster security modes: None: No isolation. Does not enforce workspace-local table access control or credential passthrough. Cannot access Unity Catalog data. Single User: Can be used only by a single user (by default, the user who created the cluster). Other users cannot attach to the cluster. When accessing a view from a cluster with Single User security mode, the view is executed with the user’s permissions. Single-user clusters support workloads using Python, Scala, and R. Init scripts, library installation, and DBFS FUSE mounts are supported on single-user clusters. Automated jobs should use single-user clusters. User Isolation: Can be shared by multiple users. Only SQL workloads are supported. Library installation, init scripts, and DBFS FUSE mounts are disabled to enforce strict isolation among the cluster users. Table ACL only (Legacy): Enforces workspace-local table access control, but cannot access Unity Catalog data. Passthrough only (Legacy): Enforces workspace-local credential passthrough, but cannot access Unity Catalog data. The only security modes supported for Unity Catalog workloads are Single User and User Isolation. For more information, see Cluster security mode. Spark configuration To fine tune Spark jobs, you can provide custom Spark configuration properties in a cluster configuration. On the cluster configuration page, click the Advanced Options toggle. Click the Spark tab. In Spark config, enter the configuration properties as one key-value pair per line. When you configure a cluster using the Clusters API 2.0, set Spark properties in the spark_conf field in the Create cluster request or Edit cluster request. To set Spark properties for all clusters, create a global init script: Scala Copy dbutils.fs.put(""dbfs:/databricks/init/set_spark_params.sh"",""""""
  |#!/bin/bash
  |
  |cat << 'EOF' > /databricks/driver/conf/00-custom-spark-driver-defaults.conf
  |[driver] {
  |  ""spark.sql.sources.partitionOverwriteMode"" = ""DYNAMIC""
  |}
  |EOF
  """""".stripMargin, true)
 Retrieve a Spark configuration property from a secret Databricks recommends storing sensitive information, such as passwords, in a secret instead of plaintext. To reference a secret in the Spark configuration, use the following syntax: ini Copy spark.<property-name> {{secrets/<scope-name>/<secret-name>}}
 For example, to set a Spark configuration property called password to the value of the secret stored in secrets/acme_app/password: ini Copy spark.password {{secrets/acme-app/password}}
 For more information, see Syntax for referencing secrets in a Spark configuration property or environment variable. Environment variables You can configure custom environment variables that you can access from init scripts running on a cluster. Databricks also provides predefined environment variables that you can use in init scripts. You cannot override these predefined environment variables. On the cluster configuration page, click the Advanced Options toggle. Click the Spark tab. Set the environment variables in the Environment Variables field. You can also set environment variables using the spark_env_vars field in the Create cluster request or Edit cluster request Clusters API endpoints. Cluster tags Cluster tags allow you to easily monitor the cost of cloud resources used by various groups in your organization. You can specify tags as key-value pairs when you create a cluster, and Azure Databricks applies these tags to cloud resources like VMs and disk volumes, as well as DBU usage reports. For clusters launched from pools, the custom cluster tags are only applied to DBU usage reports and do not propagate to cloud resources. For detailed information about how pool and cluster tag types work together, see Monitor usage using cluster, pool, and workspace tags. For convenience, Azure Databricks applies four default tags to each cluster: Vendor, Creator, ClusterName, and ClusterId. In addition, on job clusters, Azure Databricks applies two default tags: RunName and JobId. On resources used by Databricks SQL, Azure Databricks also applies the default tag SqlWarehouseId. Warning Do not assign a custom tag with the key Name to a cluster. Every cluster has a tag Name whose value is set by Azure Databricks. If you change the value associated with the key Name, the cluster can no longer be tracked by Azure Databricks. As a consequence, the cluster might not be terminated after becoming idle and will continue to incur usage costs. You can add custom tags when you create a cluster. To configure cluster tags: On the cluster configuration page, click the Advanced Options toggle. At the bottom of the page, click the Tags tab. Add a key-value pair for each custom tag. You can add up to 43 custom tags. For more details, see Monitor usage using cluster, pool, and workspace tags. SSH access to clusters For security reasons, in Azure Databricks the SSH port is closed by default. If you want to enable SSH access to your Spark clusters, contact Azure Databricks support. Note SSH can be enabled only if your workspace is deployed in your own Azure virtual network. Cluster log delivery When you create a cluster, you can specify a location to deliver the logs for the Spark driver node, worker nodes, and events. Logs are delivered every five minutes to your chosen destination. When a cluster is terminated, Azure Databricks guarantees to deliver all logs generated up until the cluster was terminated. The destination of the logs depends on the cluster ID. If the specified destination is dbfs:/cluster-log-delivery, cluster logs for 0630-191345-leap375 are delivered to dbfs:/cluster-log-delivery/0630-191345-leap375. To configure the log delivery location: On the cluster configuration page, click the Advanced Options toggle. Click the Logging tab. Select a destination type. Enter the cluster log path. Note This feature is also available in the REST API. See Clusters API 2.0 and Cluster log delivery examples. Init scripts A cluster node initialization—or init—script is a shell script that runs during startup for each cluster node before the Spark driver or worker JVM starts. You can use init scripts to install packages and libraries not included in the Databricks runtime, modify the JVM system classpath, set system properties and environment variables used by the JVM, or modify Spark configuration parameters, among other configuration tasks. You can attach init scripts to a cluster by expanding the Advanced Options section and clicking the Init Scripts tab. For detailed instructions, see Cluster node initialization scripts.",Configure clusters
21,Spark Job Failing Intermittently,"We have been noticing failures in our Spark Jobs running on Databricks.
we wanted to check if the failure has anything to do with Datatabricks. We are getting this error :
Caused by: java.lang.IllegalArgumentException: message does not exist. 
We checked our Database records, nowhere this message field is missing. Tried to check everything, but somehow things on our side seem to be fine. 
So needed your help in identifying the root cause.",https://docs.microsoft.com/en-us/azure/databricks/clusters/configure,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure clusters Article 07/14/2022 17 minutes to read 7 contributors In this article Cluster policy Cluster mode Pools Databricks Runtime Cluster node type Cluster size and autoscaling Autoscaling local storage Local disk encryption Security mode Spark configuration Retrieve a Spark configuration property from a secret Environment variables Cluster tags SSH access to clusters Cluster log delivery Init scripts This article explains the configuration options available when you create and edit Azure Databricks clusters. It focuses on creating and editing clusters using the UI. For other methods, see Clusters CLI, Clusters API 2.0, and Databricks Terraform provider. For help deciding what combination of configuration options suits your needs best, see cluster configuration best practices. Cluster policy A cluster policy limits the ability to configure clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. Cluster policies have ACLs that limit their use to specific users and groups and thus limit which policies you can select when you create a cluster. To configure a cluster policy, select the cluster policy in the Policy drop-down. Note If no policies have been created in the workspace, the Policy drop-down does not display. If you have: Cluster create permission, you can select the Unrestricted policy and create fully-configurable clusters. The Unrestricted policy does not limit any cluster attributes or attribute values. Both cluster create permission and access to cluster policies, you can select the Unrestricted policy and the policies you have access to. Access to cluster policies only, you can select the policies you have access to. Cluster mode Azure Databricks supports three cluster modes: Standard, High Concurrency, and Single Node. The default cluster mode is Standard. Important If your workspace is assigned to a Unity Catalog metastore, High Concurrency clusters are not available. Instead, you use security mode to ensure the integrity of access controls and enforce strong isolation guarantees. See also Create a Data Science & Engineering cluster. You cannot change the cluster mode after a cluster is created. If you want a different cluster mode, you must create a new cluster. Note The cluster configuration includes an auto terminate setting whose default value depends on cluster mode: Standard and Single Node clusters terminate automatically after 120 minutes by default. High Concurrency clusters do not terminate automatically by default. Standard clusters A Standard cluster is recommended for a single user. Standard clusters can run workloads developed in any language: Python, SQL, R, and Scala. High Concurrency clusters A High Concurrency cluster is a managed cloud resource. The key benefits of High Concurrency clusters are that they provide fine-grained sharing for maximum resource utilization and minimum query latencies. High Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. In addition, only High Concurrency clusters support table access control. To create a High Concurrency cluster, set Cluster Mode to High Concurrency. For an example of how to create a High Concurrency cluster using the Clusters API, see High Concurrency cluster example. Single Node clusters A Single Node cluster has no workers and runs Spark jobs on the driver node. In contrast, a Standard cluster requires at least one Spark worker node in addition to the driver node to execute Spark jobs. To create a Single Node cluster, set Cluster Mode to Single Node. To learn more about working with Single Node clusters, see Single Node clusters. Pools To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances, for the driver and worker nodes. The cluster is created using instances in the pools. If a pool does not have sufficient idle resources to create the requested driver or worker nodes, the pool expands by allocating new instances from the instance provider. When an attached cluster is terminated, the instances it used are returned to the pools and can be reused by a different cluster. If you select a pool for worker nodes but not for the driver node, the driver node inherit the pool from the worker node configuration. Important If you attempt to select a pool for the driver node but not for worker nodes, an error occurs and your cluster isn’t created. This requirement prevents a situation where the driver node has to wait for worker nodes to be created, or vice versa. See Pools to learn more about working with pools in Azure Databricks. Databricks Runtime Databricks runtimes are the set of core components that run on your clusters. All Databricks runtimes include Apache Spark and add components and updates that improve usability, performance, and security. For details, see Databricks runtimes. Azure Databricks offers several types of runtimes and several versions of those runtime types in the Databricks Runtime Version drop-down when you create or edit a cluster. Photon acceleration Important This feature is in Public Preview. Note Available in Databricks Runtime 8.3 and above. To enable Photon acceleration, select the Use Photon Acceleration checkbox. If desired, you can specify the instance type in the Worker Type and Driver Type drop-down. Databricks recommends the following instance types for optimal price and performance: Standard_E4ds_v4 Standard_E8ds_v4 Standard_E16ds_v4 You can view Photon activity in the Spark UI. The following screenshot shows the query details DAG. There are two indications of Photon in the DAG. First, Photon operators start with “Photon”, for example, PhotonGroupingAgg. Second, in the DAG, Photon operators and stages are colored peach, while the non-Photon ones are blue. Docker images For some Databricks Runtime versions, you can specify a Docker image when you create a cluster. Example use cases include library customization, a golden container environment that doesn’t change, and Docker CI/CD integration. You can also use Docker images to create custom deep learning environments on clusters with GPU devices. For instructions, see Customize containers with Databricks Container Services and Databricks Container Services on GPU clusters. Cluster node type A cluster consists of one driver node and zero or more worker nodes. You can pick separate cloud provider instance types for the driver and worker nodes, although by default the driver node uses the same instance type as the worker node. Different families of instance types fit different use cases, such as memory-intensive or compute-intensive workloads. Note If your security requirements include compute isolation, select a Standard_F72s_V2 instance as your worker type. These instance types represent isolated virtual machines that consume the entire physical host and provide the necessary level of isolation required to support, for example, US Department of Defense Impact Level 5 (IL5) workloads. Driver node Worker node GPU instance types Spot instances Driver node The driver node maintains state information of all notebooks attached to the cluster. The driver node also maintains the SparkContext and interprets all the commands you run from a notebook or a library on the cluster, and runs the Apache Spark master that coordinates with the Spark executors. The default value of the driver node type is the same as the worker node type. You can choose a larger driver node type with more memory if you are planning to collect() a lot of data from Spark workers and analyze them in the notebook. Tip Since the driver node maintains all of the state information of the notebooks attached, make sure to detach unused notebooks from the driver node. Worker node Azure Databricks worker nodes run the Spark executors and other services required for the proper functioning of the clusters. When you distribute your workload with Spark, all of the distributed processing happens on worker nodes. Azure Databricks runs one executor per worker node; therefore the terms executor and worker are used interchangeably in the context of the Azure Databricks architecture. Tip To run a Spark job, you need at least one worker node. If a cluster has zero workers, you can run non-Spark commands on the driver node, but Spark commands will fail. GPU instance types For computationally challenging tasks that demand high performance, like those associated with deep learning, Azure Databricks supports clusters accelerated with graphics processing units (GPUs). For more information, see GPU-enabled clusters. Spot instances To save cost, you can choose to use spot instances, also known as Azure Spot VMs by checking the Spot instances checkbox. The first instance will always be on-demand (the driver node is always on-demand) and subsequent instances will be spot instances. If spot instances are evicted due to unavailability, on-demand instances are deployed to replace evicted instances. Cluster size and autoscaling When you create a Azure Databricks cluster, you can either provide a fixed number of workers for the cluster or provide a minimum and maximum number of workers for the cluster. When you provide a fixed size cluster, Azure Databricks ensures that your cluster has the specified number of workers. When you provide a range for the number of workers, Databricks chooses the appropriate number of workers required to run your job. This is referred to as autoscaling. With autoscaling, Azure Databricks dynamically reallocates workers to account for the characteristics of your job. Certain parts of your pipeline may be more computationally demanding than others, and Databricks automatically adds additional workers during these phases of your job (and removes them when they’re no longer needed). Autoscaling makes it easier to achieve high cluster utilization, because you don’t need to provision the cluster to match a workload. This applies especially to workloads whose requirements change over time (like exploring a dataset during the course of a day), but it can also apply to a one-time shorter workload whose provisioning requirements are unknown. Autoscaling thus offers two advantages: Workloads can run faster compared to a constant-sized under-provisioned cluster. Autoscaling clusters can reduce overall costs compared to a statically-sized cluster. Depending on the constant size of the cluster and the workload, autoscaling gives you one or both of these benefits at the same time. The cluster size can go below the minimum number of workers selected when the cloud provider terminates instances. In this case, Azure Databricks continuously retries to re-provision instances in order to maintain the minimum number of workers. Note Autoscaling is not available for spark-submit jobs. How autoscaling behaves Scales up from min to max in 2 steps. Can scale down even if the cluster is not idle by looking at shuffle file state. Scales down based on a percentage of current nodes. On job clusters, scales down if the cluster is underutilized over the last 40 seconds. On all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds. The spark.databricks.aggressiveWindowDownS Spark configuration property specifies in seconds how often a cluster makes down-scaling decisions. Increasing the value causes a cluster to scale down more slowly. The maximum value is 600. Enable and configure autoscaling To allow Azure Databricks to resize your cluster automatically, you enable autoscaling for the cluster and provide the min and max range of workers. Enable autoscaling. All-Purpose cluster - On the Create Cluster page, select the Enable autoscaling checkbox in the Autopilot Options box: Job cluster - On the Configure Cluster page, select the Enable autoscaling checkbox in the Autopilot Options box: Configure the min and max workers. When the cluster is running, the cluster detail page displays the number of allocated workers. You can compare number of allocated workers with the worker configuration and make adjustments as needed. Important If you are using an instance pool: Make sure the cluster size requested is less than or equal to the minimum number of idle instances in the pool. If it is larger, cluster startup time will be equivalent to a cluster that doesn’t use a pool. Make sure the maximum cluster size is less than or equal to the maximum capacity of the pool. If it is larger, the cluster creation will fail. Autoscaling example If you reconfigure a static cluster to be an autoscaling cluster, Azure Databricks immediately resizes the cluster within the minimum and maximum bounds and then starts autoscaling. As an example, the following table demonstrates what happens to clusters with a certain initial size if you reconfigure a cluster to autoscale between 5 and 10 nodes. Initial size Size after reconfiguration 6 6 12 10 3 5 Autoscaling local storage It can often be difficult to estimate how much disk space a particular job will take. To save you from having to estimate how many gigabytes of managed disk to attach to your cluster at creation time, Azure Databricks automatically enables autoscaling local storage on all Azure Databricks clusters. With autoscaling local storage, Azure Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new managed disk to the worker before it runs out of disk space. Disks are attached up to a limit of 5 TB of total disk space per virtual machine (including the virtual machine’s initial local storage). The managed disks attached to a virtual machine are detached only when the virtual machine is returned to Azure. That is, managed disks are never detached from a virtual machine as long as it is part of a running cluster. To scale down managed disk usage, Azure Databricks recommends using this feature in a cluster configured with Spot instances or Automatic termination. Local disk encryption Important This feature is in Public Preview. Some instance types you use to run clusters may have locally attached disks. Azure Databricks may store shuffle data or ephemeral data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data that is stored temporarily on your cluster’s local disks, you can enable local disk encryption. Important Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. When local disk encryption is enabled, Azure Databricks generates an encryption key locally that is unique to each cluster node and is used to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. To enable local disk encryption, you must use the Clusters API 2.0. During cluster creation or edit, set: JSON Copy {
  ""enable_local_disk_encryption"": true
}
 See Create and Edit in the Clusters API reference for examples of how to invoke these APIs. Here is an example of a cluster create call that enables local disk encryption: JSON Copy {
  ""cluster_name"": ""my-cluster"",
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2"",
  ""enable_local_disk_encryption"": true,
  ""spark_conf"": {
    ""spark.speculation"": true
  },
  ""num_workers"": 25
}
 Security mode If your workspace is assigned to a Unity Catalog metastore, you use security mode instead of High Concurrency cluster mode to ensure the integrity of access controls and enforce strong isolation guarantees. High Concurrency cluster mode is not available with Unity Catalog. Under Advanced options, select from the following cluster security modes: None: No isolation. Does not enforce workspace-local table access control or credential passthrough. Cannot access Unity Catalog data. Single User: Can be used only by a single user (by default, the user who created the cluster). Other users cannot attach to the cluster. When accessing a view from a cluster with Single User security mode, the view is executed with the user’s permissions. Single-user clusters support workloads using Python, Scala, and R. Init scripts, library installation, and DBFS FUSE mounts are supported on single-user clusters. Automated jobs should use single-user clusters. User Isolation: Can be shared by multiple users. Only SQL workloads are supported. Library installation, init scripts, and DBFS FUSE mounts are disabled to enforce strict isolation among the cluster users. Table ACL only (Legacy): Enforces workspace-local table access control, but cannot access Unity Catalog data. Passthrough only (Legacy): Enforces workspace-local credential passthrough, but cannot access Unity Catalog data. The only security modes supported for Unity Catalog workloads are Single User and User Isolation. For more information, see Cluster security mode. Spark configuration To fine tune Spark jobs, you can provide custom Spark configuration properties in a cluster configuration. On the cluster configuration page, click the Advanced Options toggle. Click the Spark tab. In Spark config, enter the configuration properties as one key-value pair per line. When you configure a cluster using the Clusters API 2.0, set Spark properties in the spark_conf field in the Create cluster request or Edit cluster request. To set Spark properties for all clusters, create a global init script: Scala Copy dbutils.fs.put(""dbfs:/databricks/init/set_spark_params.sh"",""""""
  |#!/bin/bash
  |
  |cat << 'EOF' > /databricks/driver/conf/00-custom-spark-driver-defaults.conf
  |[driver] {
  |  ""spark.sql.sources.partitionOverwriteMode"" = ""DYNAMIC""
  |}
  |EOF
  """""".stripMargin, true)
 Retrieve a Spark configuration property from a secret Databricks recommends storing sensitive information, such as passwords, in a secret instead of plaintext. To reference a secret in the Spark configuration, use the following syntax: ini Copy spark.<property-name> {{secrets/<scope-name>/<secret-name>}}
 For example, to set a Spark configuration property called password to the value of the secret stored in secrets/acme_app/password: ini Copy spark.password {{secrets/acme-app/password}}
 For more information, see Syntax for referencing secrets in a Spark configuration property or environment variable. Environment variables You can configure custom environment variables that you can access from init scripts running on a cluster. Databricks also provides predefined environment variables that you can use in init scripts. You cannot override these predefined environment variables. On the cluster configuration page, click the Advanced Options toggle. Click the Spark tab. Set the environment variables in the Environment Variables field. You can also set environment variables using the spark_env_vars field in the Create cluster request or Edit cluster request Clusters API endpoints. Cluster tags Cluster tags allow you to easily monitor the cost of cloud resources used by various groups in your organization. You can specify tags as key-value pairs when you create a cluster, and Azure Databricks applies these tags to cloud resources like VMs and disk volumes, as well as DBU usage reports. For clusters launched from pools, the custom cluster tags are only applied to DBU usage reports and do not propagate to cloud resources. For detailed information about how pool and cluster tag types work together, see Monitor usage using cluster, pool, and workspace tags. For convenience, Azure Databricks applies four default tags to each cluster: Vendor, Creator, ClusterName, and ClusterId. In addition, on job clusters, Azure Databricks applies two default tags: RunName and JobId. On resources used by Databricks SQL, Azure Databricks also applies the default tag SqlWarehouseId. Warning Do not assign a custom tag with the key Name to a cluster. Every cluster has a tag Name whose value is set by Azure Databricks. If you change the value associated with the key Name, the cluster can no longer be tracked by Azure Databricks. As a consequence, the cluster might not be terminated after becoming idle and will continue to incur usage costs. You can add custom tags when you create a cluster. To configure cluster tags: On the cluster configuration page, click the Advanced Options toggle. At the bottom of the page, click the Tags tab. Add a key-value pair for each custom tag. You can add up to 43 custom tags. For more details, see Monitor usage using cluster, pool, and workspace tags. SSH access to clusters For security reasons, in Azure Databricks the SSH port is closed by default. If you want to enable SSH access to your Spark clusters, contact Azure Databricks support. Note SSH can be enabled only if your workspace is deployed in your own Azure virtual network. Cluster log delivery When you create a cluster, you can specify a location to deliver the logs for the Spark driver node, worker nodes, and events. Logs are delivered every five minutes to your chosen destination. When a cluster is terminated, Azure Databricks guarantees to deliver all logs generated up until the cluster was terminated. The destination of the logs depends on the cluster ID. If the specified destination is dbfs:/cluster-log-delivery, cluster logs for 0630-191345-leap375 are delivered to dbfs:/cluster-log-delivery/0630-191345-leap375. To configure the log delivery location: On the cluster configuration page, click the Advanced Options toggle. Click the Logging tab. Select a destination type. Enter the cluster log path. Note This feature is also available in the REST API. See Clusters API 2.0 and Cluster log delivery examples. Init scripts A cluster node initialization—or init—script is a shell script that runs during startup for each cluster node before the Spark driver or worker JVM starts. You can use init scripts to install packages and libraries not included in the Databricks runtime, modify the JVM system classpath, set system properties and environment variables used by the JVM, or modify Spark configuration parameters, among other configuration tasks. You can attach init scripts to a cluster by expanding the Advanced Options section and clicking the Init Scripts tab. For detailed instructions, see Cluster node initialization scripts.",Configure clusters
22,Databricks Job Fails with NoClassDefFoundError when scaling up,Databricks Job Fails with NoClassDefFoundError when scaling up,https://docs.microsoft.com/en-us/azure/databricks/clusters/configure,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure clusters Article 07/14/2022 17 minutes to read 7 contributors In this article Cluster policy Cluster mode Pools Databricks Runtime Cluster node type Cluster size and autoscaling Autoscaling local storage Local disk encryption Security mode Spark configuration Retrieve a Spark configuration property from a secret Environment variables Cluster tags SSH access to clusters Cluster log delivery Init scripts This article explains the configuration options available when you create and edit Azure Databricks clusters. It focuses on creating and editing clusters using the UI. For other methods, see Clusters CLI, Clusters API 2.0, and Databricks Terraform provider. For help deciding what combination of configuration options suits your needs best, see cluster configuration best practices. Cluster policy A cluster policy limits the ability to configure clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. Cluster policies have ACLs that limit their use to specific users and groups and thus limit which policies you can select when you create a cluster. To configure a cluster policy, select the cluster policy in the Policy drop-down. Note If no policies have been created in the workspace, the Policy drop-down does not display. If you have: Cluster create permission, you can select the Unrestricted policy and create fully-configurable clusters. The Unrestricted policy does not limit any cluster attributes or attribute values. Both cluster create permission and access to cluster policies, you can select the Unrestricted policy and the policies you have access to. Access to cluster policies only, you can select the policies you have access to. Cluster mode Azure Databricks supports three cluster modes: Standard, High Concurrency, and Single Node. The default cluster mode is Standard. Important If your workspace is assigned to a Unity Catalog metastore, High Concurrency clusters are not available. Instead, you use security mode to ensure the integrity of access controls and enforce strong isolation guarantees. See also Create a Data Science & Engineering cluster. You cannot change the cluster mode after a cluster is created. If you want a different cluster mode, you must create a new cluster. Note The cluster configuration includes an auto terminate setting whose default value depends on cluster mode: Standard and Single Node clusters terminate automatically after 120 minutes by default. High Concurrency clusters do not terminate automatically by default. Standard clusters A Standard cluster is recommended for a single user. Standard clusters can run workloads developed in any language: Python, SQL, R, and Scala. High Concurrency clusters A High Concurrency cluster is a managed cloud resource. The key benefits of High Concurrency clusters are that they provide fine-grained sharing for maximum resource utilization and minimum query latencies. High Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. In addition, only High Concurrency clusters support table access control. To create a High Concurrency cluster, set Cluster Mode to High Concurrency. For an example of how to create a High Concurrency cluster using the Clusters API, see High Concurrency cluster example. Single Node clusters A Single Node cluster has no workers and runs Spark jobs on the driver node. In contrast, a Standard cluster requires at least one Spark worker node in addition to the driver node to execute Spark jobs. To create a Single Node cluster, set Cluster Mode to Single Node. To learn more about working with Single Node clusters, see Single Node clusters. Pools To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances, for the driver and worker nodes. The cluster is created using instances in the pools. If a pool does not have sufficient idle resources to create the requested driver or worker nodes, the pool expands by allocating new instances from the instance provider. When an attached cluster is terminated, the instances it used are returned to the pools and can be reused by a different cluster. If you select a pool for worker nodes but not for the driver node, the driver node inherit the pool from the worker node configuration. Important If you attempt to select a pool for the driver node but not for worker nodes, an error occurs and your cluster isn’t created. This requirement prevents a situation where the driver node has to wait for worker nodes to be created, or vice versa. See Pools to learn more about working with pools in Azure Databricks. Databricks Runtime Databricks runtimes are the set of core components that run on your clusters. All Databricks runtimes include Apache Spark and add components and updates that improve usability, performance, and security. For details, see Databricks runtimes. Azure Databricks offers several types of runtimes and several versions of those runtime types in the Databricks Runtime Version drop-down when you create or edit a cluster. Photon acceleration Important This feature is in Public Preview. Note Available in Databricks Runtime 8.3 and above. To enable Photon acceleration, select the Use Photon Acceleration checkbox. If desired, you can specify the instance type in the Worker Type and Driver Type drop-down. Databricks recommends the following instance types for optimal price and performance: Standard_E4ds_v4 Standard_E8ds_v4 Standard_E16ds_v4 You can view Photon activity in the Spark UI. The following screenshot shows the query details DAG. There are two indications of Photon in the DAG. First, Photon operators start with “Photon”, for example, PhotonGroupingAgg. Second, in the DAG, Photon operators and stages are colored peach, while the non-Photon ones are blue. Docker images For some Databricks Runtime versions, you can specify a Docker image when you create a cluster. Example use cases include library customization, a golden container environment that doesn’t change, and Docker CI/CD integration. You can also use Docker images to create custom deep learning environments on clusters with GPU devices. For instructions, see Customize containers with Databricks Container Services and Databricks Container Services on GPU clusters. Cluster node type A cluster consists of one driver node and zero or more worker nodes. You can pick separate cloud provider instance types for the driver and worker nodes, although by default the driver node uses the same instance type as the worker node. Different families of instance types fit different use cases, such as memory-intensive or compute-intensive workloads. Note If your security requirements include compute isolation, select a Standard_F72s_V2 instance as your worker type. These instance types represent isolated virtual machines that consume the entire physical host and provide the necessary level of isolation required to support, for example, US Department of Defense Impact Level 5 (IL5) workloads. Driver node Worker node GPU instance types Spot instances Driver node The driver node maintains state information of all notebooks attached to the cluster. The driver node also maintains the SparkContext and interprets all the commands you run from a notebook or a library on the cluster, and runs the Apache Spark master that coordinates with the Spark executors. The default value of the driver node type is the same as the worker node type. You can choose a larger driver node type with more memory if you are planning to collect() a lot of data from Spark workers and analyze them in the notebook. Tip Since the driver node maintains all of the state information of the notebooks attached, make sure to detach unused notebooks from the driver node. Worker node Azure Databricks worker nodes run the Spark executors and other services required for the proper functioning of the clusters. When you distribute your workload with Spark, all of the distributed processing happens on worker nodes. Azure Databricks runs one executor per worker node; therefore the terms executor and worker are used interchangeably in the context of the Azure Databricks architecture. Tip To run a Spark job, you need at least one worker node. If a cluster has zero workers, you can run non-Spark commands on the driver node, but Spark commands will fail. GPU instance types For computationally challenging tasks that demand high performance, like those associated with deep learning, Azure Databricks supports clusters accelerated with graphics processing units (GPUs). For more information, see GPU-enabled clusters. Spot instances To save cost, you can choose to use spot instances, also known as Azure Spot VMs by checking the Spot instances checkbox. The first instance will always be on-demand (the driver node is always on-demand) and subsequent instances will be spot instances. If spot instances are evicted due to unavailability, on-demand instances are deployed to replace evicted instances. Cluster size and autoscaling When you create a Azure Databricks cluster, you can either provide a fixed number of workers for the cluster or provide a minimum and maximum number of workers for the cluster. When you provide a fixed size cluster, Azure Databricks ensures that your cluster has the specified number of workers. When you provide a range for the number of workers, Databricks chooses the appropriate number of workers required to run your job. This is referred to as autoscaling. With autoscaling, Azure Databricks dynamically reallocates workers to account for the characteristics of your job. Certain parts of your pipeline may be more computationally demanding than others, and Databricks automatically adds additional workers during these phases of your job (and removes them when they’re no longer needed). Autoscaling makes it easier to achieve high cluster utilization, because you don’t need to provision the cluster to match a workload. This applies especially to workloads whose requirements change over time (like exploring a dataset during the course of a day), but it can also apply to a one-time shorter workload whose provisioning requirements are unknown. Autoscaling thus offers two advantages: Workloads can run faster compared to a constant-sized under-provisioned cluster. Autoscaling clusters can reduce overall costs compared to a statically-sized cluster. Depending on the constant size of the cluster and the workload, autoscaling gives you one or both of these benefits at the same time. The cluster size can go below the minimum number of workers selected when the cloud provider terminates instances. In this case, Azure Databricks continuously retries to re-provision instances in order to maintain the minimum number of workers. Note Autoscaling is not available for spark-submit jobs. How autoscaling behaves Scales up from min to max in 2 steps. Can scale down even if the cluster is not idle by looking at shuffle file state. Scales down based on a percentage of current nodes. On job clusters, scales down if the cluster is underutilized over the last 40 seconds. On all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds. The spark.databricks.aggressiveWindowDownS Spark configuration property specifies in seconds how often a cluster makes down-scaling decisions. Increasing the value causes a cluster to scale down more slowly. The maximum value is 600. Enable and configure autoscaling To allow Azure Databricks to resize your cluster automatically, you enable autoscaling for the cluster and provide the min and max range of workers. Enable autoscaling. All-Purpose cluster - On the Create Cluster page, select the Enable autoscaling checkbox in the Autopilot Options box: Job cluster - On the Configure Cluster page, select the Enable autoscaling checkbox in the Autopilot Options box: Configure the min and max workers. When the cluster is running, the cluster detail page displays the number of allocated workers. You can compare number of allocated workers with the worker configuration and make adjustments as needed. Important If you are using an instance pool: Make sure the cluster size requested is less than or equal to the minimum number of idle instances in the pool. If it is larger, cluster startup time will be equivalent to a cluster that doesn’t use a pool. Make sure the maximum cluster size is less than or equal to the maximum capacity of the pool. If it is larger, the cluster creation will fail. Autoscaling example If you reconfigure a static cluster to be an autoscaling cluster, Azure Databricks immediately resizes the cluster within the minimum and maximum bounds and then starts autoscaling. As an example, the following table demonstrates what happens to clusters with a certain initial size if you reconfigure a cluster to autoscale between 5 and 10 nodes. Initial size Size after reconfiguration 6 6 12 10 3 5 Autoscaling local storage It can often be difficult to estimate how much disk space a particular job will take. To save you from having to estimate how many gigabytes of managed disk to attach to your cluster at creation time, Azure Databricks automatically enables autoscaling local storage on all Azure Databricks clusters. With autoscaling local storage, Azure Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new managed disk to the worker before it runs out of disk space. Disks are attached up to a limit of 5 TB of total disk space per virtual machine (including the virtual machine’s initial local storage). The managed disks attached to a virtual machine are detached only when the virtual machine is returned to Azure. That is, managed disks are never detached from a virtual machine as long as it is part of a running cluster. To scale down managed disk usage, Azure Databricks recommends using this feature in a cluster configured with Spot instances or Automatic termination. Local disk encryption Important This feature is in Public Preview. Some instance types you use to run clusters may have locally attached disks. Azure Databricks may store shuffle data or ephemeral data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data that is stored temporarily on your cluster’s local disks, you can enable local disk encryption. Important Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. When local disk encryption is enabled, Azure Databricks generates an encryption key locally that is unique to each cluster node and is used to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. To enable local disk encryption, you must use the Clusters API 2.0. During cluster creation or edit, set: JSON Copy {
  ""enable_local_disk_encryption"": true
}
 See Create and Edit in the Clusters API reference for examples of how to invoke these APIs. Here is an example of a cluster create call that enables local disk encryption: JSON Copy {
  ""cluster_name"": ""my-cluster"",
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2"",
  ""enable_local_disk_encryption"": true,
  ""spark_conf"": {
    ""spark.speculation"": true
  },
  ""num_workers"": 25
}
 Security mode If your workspace is assigned to a Unity Catalog metastore, you use security mode instead of High Concurrency cluster mode to ensure the integrity of access controls and enforce strong isolation guarantees. High Concurrency cluster mode is not available with Unity Catalog. Under Advanced options, select from the following cluster security modes: None: No isolation. Does not enforce workspace-local table access control or credential passthrough. Cannot access Unity Catalog data. Single User: Can be used only by a single user (by default, the user who created the cluster). Other users cannot attach to the cluster. When accessing a view from a cluster with Single User security mode, the view is executed with the user’s permissions. Single-user clusters support workloads using Python, Scala, and R. Init scripts, library installation, and DBFS FUSE mounts are supported on single-user clusters. Automated jobs should use single-user clusters. User Isolation: Can be shared by multiple users. Only SQL workloads are supported. Library installation, init scripts, and DBFS FUSE mounts are disabled to enforce strict isolation among the cluster users. Table ACL only (Legacy): Enforces workspace-local table access control, but cannot access Unity Catalog data. Passthrough only (Legacy): Enforces workspace-local credential passthrough, but cannot access Unity Catalog data. The only security modes supported for Unity Catalog workloads are Single User and User Isolation. For more information, see Cluster security mode. Spark configuration To fine tune Spark jobs, you can provide custom Spark configuration properties in a cluster configuration. On the cluster configuration page, click the Advanced Options toggle. Click the Spark tab. In Spark config, enter the configuration properties as one key-value pair per line. When you configure a cluster using the Clusters API 2.0, set Spark properties in the spark_conf field in the Create cluster request or Edit cluster request. To set Spark properties for all clusters, create a global init script: Scala Copy dbutils.fs.put(""dbfs:/databricks/init/set_spark_params.sh"",""""""
  |#!/bin/bash
  |
  |cat << 'EOF' > /databricks/driver/conf/00-custom-spark-driver-defaults.conf
  |[driver] {
  |  ""spark.sql.sources.partitionOverwriteMode"" = ""DYNAMIC""
  |}
  |EOF
  """""".stripMargin, true)
 Retrieve a Spark configuration property from a secret Databricks recommends storing sensitive information, such as passwords, in a secret instead of plaintext. To reference a secret in the Spark configuration, use the following syntax: ini Copy spark.<property-name> {{secrets/<scope-name>/<secret-name>}}
 For example, to set a Spark configuration property called password to the value of the secret stored in secrets/acme_app/password: ini Copy spark.password {{secrets/acme-app/password}}
 For more information, see Syntax for referencing secrets in a Spark configuration property or environment variable. Environment variables You can configure custom environment variables that you can access from init scripts running on a cluster. Databricks also provides predefined environment variables that you can use in init scripts. You cannot override these predefined environment variables. On the cluster configuration page, click the Advanced Options toggle. Click the Spark tab. Set the environment variables in the Environment Variables field. You can also set environment variables using the spark_env_vars field in the Create cluster request or Edit cluster request Clusters API endpoints. Cluster tags Cluster tags allow you to easily monitor the cost of cloud resources used by various groups in your organization. You can specify tags as key-value pairs when you create a cluster, and Azure Databricks applies these tags to cloud resources like VMs and disk volumes, as well as DBU usage reports. For clusters launched from pools, the custom cluster tags are only applied to DBU usage reports and do not propagate to cloud resources. For detailed information about how pool and cluster tag types work together, see Monitor usage using cluster, pool, and workspace tags. For convenience, Azure Databricks applies four default tags to each cluster: Vendor, Creator, ClusterName, and ClusterId. In addition, on job clusters, Azure Databricks applies two default tags: RunName and JobId. On resources used by Databricks SQL, Azure Databricks also applies the default tag SqlWarehouseId. Warning Do not assign a custom tag with the key Name to a cluster. Every cluster has a tag Name whose value is set by Azure Databricks. If you change the value associated with the key Name, the cluster can no longer be tracked by Azure Databricks. As a consequence, the cluster might not be terminated after becoming idle and will continue to incur usage costs. You can add custom tags when you create a cluster. To configure cluster tags: On the cluster configuration page, click the Advanced Options toggle. At the bottom of the page, click the Tags tab. Add a key-value pair for each custom tag. You can add up to 43 custom tags. For more details, see Monitor usage using cluster, pool, and workspace tags. SSH access to clusters For security reasons, in Azure Databricks the SSH port is closed by default. If you want to enable SSH access to your Spark clusters, contact Azure Databricks support. Note SSH can be enabled only if your workspace is deployed in your own Azure virtual network. Cluster log delivery When you create a cluster, you can specify a location to deliver the logs for the Spark driver node, worker nodes, and events. Logs are delivered every five minutes to your chosen destination. When a cluster is terminated, Azure Databricks guarantees to deliver all logs generated up until the cluster was terminated. The destination of the logs depends on the cluster ID. If the specified destination is dbfs:/cluster-log-delivery, cluster logs for 0630-191345-leap375 are delivered to dbfs:/cluster-log-delivery/0630-191345-leap375. To configure the log delivery location: On the cluster configuration page, click the Advanced Options toggle. Click the Logging tab. Select a destination type. Enter the cluster log path. Note This feature is also available in the REST API. See Clusters API 2.0 and Cluster log delivery examples. Init scripts A cluster node initialization—or init—script is a shell script that runs during startup for each cluster node before the Spark driver or worker JVM starts. You can use init scripts to install packages and libraries not included in the Databricks runtime, modify the JVM system classpath, set system properties and environment variables used by the JVM, or modify Spark configuration parameters, among other configuration tasks. You can attach init scripts to a cluster by expanding the Advanced Options section and clicking the Init Scripts tab. For detailed instructions, see Cluster node initialization scripts.",Configure clusters
23,"ARR || Safeway || DID_NOT_EXPAND_DISK,FAILED_TO_EXPAND_DISK errors on Databricks cluster ||  2206230010003162","Our Databricks cluster used for Power BI refreshes is giving FAILED_TO_EXPAND_DISK errors. Also, some report refreshes are taking too long.

Problem start date and time
Thu, Jun 23, 2022, 8:00 AM (UTC-08:00) Pacific Time (US & Canada)


<Start:Agent_Additional_Properties_Do_Not_Edit>
- ProblemStartTime: 06/23/2022 15:00:00
- Cloud: Azure
- AzureProductSubscriptionID: 59cccdde-e87a-4f0b-ae4f-84e0177d5105
- AzureProductSubscriptionName: az-entaks-prod-01
- Tenant Id: b7f604a0-00a9-4188-9248-42f3a5aac2e9
- Object Id: 854b4222-3892-4663-a669-b1c9cfc4d944
- SubscriptionType: Premier
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Premier with Azure Rapid response
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: True

- Location: westus
- ResourceUri: /subscriptions/59cccdde-e87a-4f0b-ae4f-84e0177d5105/resourceGroups/az-entaks-prod-01-dasc-prod-westus-rg-01/providers/Microsoft.Databricks/workspaces/dasc-prod-databricks-01",https://docs.microsoft.com/en-us/azure/databricks/clusters/configure,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure clusters Article 07/14/2022 17 minutes to read 7 contributors In this article Cluster policy Cluster mode Pools Databricks Runtime Cluster node type Cluster size and autoscaling Autoscaling local storage Local disk encryption Security mode Spark configuration Retrieve a Spark configuration property from a secret Environment variables Cluster tags SSH access to clusters Cluster log delivery Init scripts This article explains the configuration options available when you create and edit Azure Databricks clusters. It focuses on creating and editing clusters using the UI. For other methods, see Clusters CLI, Clusters API 2.0, and Databricks Terraform provider. For help deciding what combination of configuration options suits your needs best, see cluster configuration best practices. Cluster policy A cluster policy limits the ability to configure clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. Cluster policies have ACLs that limit their use to specific users and groups and thus limit which policies you can select when you create a cluster. To configure a cluster policy, select the cluster policy in the Policy drop-down. Note If no policies have been created in the workspace, the Policy drop-down does not display. If you have: Cluster create permission, you can select the Unrestricted policy and create fully-configurable clusters. The Unrestricted policy does not limit any cluster attributes or attribute values. Both cluster create permission and access to cluster policies, you can select the Unrestricted policy and the policies you have access to. Access to cluster policies only, you can select the policies you have access to. Cluster mode Azure Databricks supports three cluster modes: Standard, High Concurrency, and Single Node. The default cluster mode is Standard. Important If your workspace is assigned to a Unity Catalog metastore, High Concurrency clusters are not available. Instead, you use security mode to ensure the integrity of access controls and enforce strong isolation guarantees. See also Create a Data Science & Engineering cluster. You cannot change the cluster mode after a cluster is created. If you want a different cluster mode, you must create a new cluster. Note The cluster configuration includes an auto terminate setting whose default value depends on cluster mode: Standard and Single Node clusters terminate automatically after 120 minutes by default. High Concurrency clusters do not terminate automatically by default. Standard clusters A Standard cluster is recommended for a single user. Standard clusters can run workloads developed in any language: Python, SQL, R, and Scala. High Concurrency clusters A High Concurrency cluster is a managed cloud resource. The key benefits of High Concurrency clusters are that they provide fine-grained sharing for maximum resource utilization and minimum query latencies. High Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. In addition, only High Concurrency clusters support table access control. To create a High Concurrency cluster, set Cluster Mode to High Concurrency. For an example of how to create a High Concurrency cluster using the Clusters API, see High Concurrency cluster example. Single Node clusters A Single Node cluster has no workers and runs Spark jobs on the driver node. In contrast, a Standard cluster requires at least one Spark worker node in addition to the driver node to execute Spark jobs. To create a Single Node cluster, set Cluster Mode to Single Node. To learn more about working with Single Node clusters, see Single Node clusters. Pools To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances, for the driver and worker nodes. The cluster is created using instances in the pools. If a pool does not have sufficient idle resources to create the requested driver or worker nodes, the pool expands by allocating new instances from the instance provider. When an attached cluster is terminated, the instances it used are returned to the pools and can be reused by a different cluster. If you select a pool for worker nodes but not for the driver node, the driver node inherit the pool from the worker node configuration. Important If you attempt to select a pool for the driver node but not for worker nodes, an error occurs and your cluster isn’t created. This requirement prevents a situation where the driver node has to wait for worker nodes to be created, or vice versa. See Pools to learn more about working with pools in Azure Databricks. Databricks Runtime Databricks runtimes are the set of core components that run on your clusters. All Databricks runtimes include Apache Spark and add components and updates that improve usability, performance, and security. For details, see Databricks runtimes. Azure Databricks offers several types of runtimes and several versions of those runtime types in the Databricks Runtime Version drop-down when you create or edit a cluster. Photon acceleration Important This feature is in Public Preview. Note Available in Databricks Runtime 8.3 and above. To enable Photon acceleration, select the Use Photon Acceleration checkbox. If desired, you can specify the instance type in the Worker Type and Driver Type drop-down. Databricks recommends the following instance types for optimal price and performance: Standard_E4ds_v4 Standard_E8ds_v4 Standard_E16ds_v4 You can view Photon activity in the Spark UI. The following screenshot shows the query details DAG. There are two indications of Photon in the DAG. First, Photon operators start with “Photon”, for example, PhotonGroupingAgg. Second, in the DAG, Photon operators and stages are colored peach, while the non-Photon ones are blue. Docker images For some Databricks Runtime versions, you can specify a Docker image when you create a cluster. Example use cases include library customization, a golden container environment that doesn’t change, and Docker CI/CD integration. You can also use Docker images to create custom deep learning environments on clusters with GPU devices. For instructions, see Customize containers with Databricks Container Services and Databricks Container Services on GPU clusters. Cluster node type A cluster consists of one driver node and zero or more worker nodes. You can pick separate cloud provider instance types for the driver and worker nodes, although by default the driver node uses the same instance type as the worker node. Different families of instance types fit different use cases, such as memory-intensive or compute-intensive workloads. Note If your security requirements include compute isolation, select a Standard_F72s_V2 instance as your worker type. These instance types represent isolated virtual machines that consume the entire physical host and provide the necessary level of isolation required to support, for example, US Department of Defense Impact Level 5 (IL5) workloads. Driver node Worker node GPU instance types Spot instances Driver node The driver node maintains state information of all notebooks attached to the cluster. The driver node also maintains the SparkContext and interprets all the commands you run from a notebook or a library on the cluster, and runs the Apache Spark master that coordinates with the Spark executors. The default value of the driver node type is the same as the worker node type. You can choose a larger driver node type with more memory if you are planning to collect() a lot of data from Spark workers and analyze them in the notebook. Tip Since the driver node maintains all of the state information of the notebooks attached, make sure to detach unused notebooks from the driver node. Worker node Azure Databricks worker nodes run the Spark executors and other services required for the proper functioning of the clusters. When you distribute your workload with Spark, all of the distributed processing happens on worker nodes. Azure Databricks runs one executor per worker node; therefore the terms executor and worker are used interchangeably in the context of the Azure Databricks architecture. Tip To run a Spark job, you need at least one worker node. If a cluster has zero workers, you can run non-Spark commands on the driver node, but Spark commands will fail. GPU instance types For computationally challenging tasks that demand high performance, like those associated with deep learning, Azure Databricks supports clusters accelerated with graphics processing units (GPUs). For more information, see GPU-enabled clusters. Spot instances To save cost, you can choose to use spot instances, also known as Azure Spot VMs by checking the Spot instances checkbox. The first instance will always be on-demand (the driver node is always on-demand) and subsequent instances will be spot instances. If spot instances are evicted due to unavailability, on-demand instances are deployed to replace evicted instances. Cluster size and autoscaling When you create a Azure Databricks cluster, you can either provide a fixed number of workers for the cluster or provide a minimum and maximum number of workers for the cluster. When you provide a fixed size cluster, Azure Databricks ensures that your cluster has the specified number of workers. When you provide a range for the number of workers, Databricks chooses the appropriate number of workers required to run your job. This is referred to as autoscaling. With autoscaling, Azure Databricks dynamically reallocates workers to account for the characteristics of your job. Certain parts of your pipeline may be more computationally demanding than others, and Databricks automatically adds additional workers during these phases of your job (and removes them when they’re no longer needed). Autoscaling makes it easier to achieve high cluster utilization, because you don’t need to provision the cluster to match a workload. This applies especially to workloads whose requirements change over time (like exploring a dataset during the course of a day), but it can also apply to a one-time shorter workload whose provisioning requirements are unknown. Autoscaling thus offers two advantages: Workloads can run faster compared to a constant-sized under-provisioned cluster. Autoscaling clusters can reduce overall costs compared to a statically-sized cluster. Depending on the constant size of the cluster and the workload, autoscaling gives you one or both of these benefits at the same time. The cluster size can go below the minimum number of workers selected when the cloud provider terminates instances. In this case, Azure Databricks continuously retries to re-provision instances in order to maintain the minimum number of workers. Note Autoscaling is not available for spark-submit jobs. How autoscaling behaves Scales up from min to max in 2 steps. Can scale down even if the cluster is not idle by looking at shuffle file state. Scales down based on a percentage of current nodes. On job clusters, scales down if the cluster is underutilized over the last 40 seconds. On all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds. The spark.databricks.aggressiveWindowDownS Spark configuration property specifies in seconds how often a cluster makes down-scaling decisions. Increasing the value causes a cluster to scale down more slowly. The maximum value is 600. Enable and configure autoscaling To allow Azure Databricks to resize your cluster automatically, you enable autoscaling for the cluster and provide the min and max range of workers. Enable autoscaling. All-Purpose cluster - On the Create Cluster page, select the Enable autoscaling checkbox in the Autopilot Options box: Job cluster - On the Configure Cluster page, select the Enable autoscaling checkbox in the Autopilot Options box: Configure the min and max workers. When the cluster is running, the cluster detail page displays the number of allocated workers. You can compare number of allocated workers with the worker configuration and make adjustments as needed. Important If you are using an instance pool: Make sure the cluster size requested is less than or equal to the minimum number of idle instances in the pool. If it is larger, cluster startup time will be equivalent to a cluster that doesn’t use a pool. Make sure the maximum cluster size is less than or equal to the maximum capacity of the pool. If it is larger, the cluster creation will fail. Autoscaling example If you reconfigure a static cluster to be an autoscaling cluster, Azure Databricks immediately resizes the cluster within the minimum and maximum bounds and then starts autoscaling. As an example, the following table demonstrates what happens to clusters with a certain initial size if you reconfigure a cluster to autoscale between 5 and 10 nodes. Initial size Size after reconfiguration 6 6 12 10 3 5 Autoscaling local storage It can often be difficult to estimate how much disk space a particular job will take. To save you from having to estimate how many gigabytes of managed disk to attach to your cluster at creation time, Azure Databricks automatically enables autoscaling local storage on all Azure Databricks clusters. With autoscaling local storage, Azure Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new managed disk to the worker before it runs out of disk space. Disks are attached up to a limit of 5 TB of total disk space per virtual machine (including the virtual machine’s initial local storage). The managed disks attached to a virtual machine are detached only when the virtual machine is returned to Azure. That is, managed disks are never detached from a virtual machine as long as it is part of a running cluster. To scale down managed disk usage, Azure Databricks recommends using this feature in a cluster configured with Spot instances or Automatic termination. Local disk encryption Important This feature is in Public Preview. Some instance types you use to run clusters may have locally attached disks. Azure Databricks may store shuffle data or ephemeral data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data that is stored temporarily on your cluster’s local disks, you can enable local disk encryption. Important Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. When local disk encryption is enabled, Azure Databricks generates an encryption key locally that is unique to each cluster node and is used to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. To enable local disk encryption, you must use the Clusters API 2.0. During cluster creation or edit, set: JSON Copy {
  ""enable_local_disk_encryption"": true
}
 See Create and Edit in the Clusters API reference for examples of how to invoke these APIs. Here is an example of a cluster create call that enables local disk encryption: JSON Copy {
  ""cluster_name"": ""my-cluster"",
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2"",
  ""enable_local_disk_encryption"": true,
  ""spark_conf"": {
    ""spark.speculation"": true
  },
  ""num_workers"": 25
}
 Security mode If your workspace is assigned to a Unity Catalog metastore, you use security mode instead of High Concurrency cluster mode to ensure the integrity of access controls and enforce strong isolation guarantees. High Concurrency cluster mode is not available with Unity Catalog. Under Advanced options, select from the following cluster security modes: None: No isolation. Does not enforce workspace-local table access control or credential passthrough. Cannot access Unity Catalog data. Single User: Can be used only by a single user (by default, the user who created the cluster). Other users cannot attach to the cluster. When accessing a view from a cluster with Single User security mode, the view is executed with the user’s permissions. Single-user clusters support workloads using Python, Scala, and R. Init scripts, library installation, and DBFS FUSE mounts are supported on single-user clusters. Automated jobs should use single-user clusters. User Isolation: Can be shared by multiple users. Only SQL workloads are supported. Library installation, init scripts, and DBFS FUSE mounts are disabled to enforce strict isolation among the cluster users. Table ACL only (Legacy): Enforces workspace-local table access control, but cannot access Unity Catalog data. Passthrough only (Legacy): Enforces workspace-local credential passthrough, but cannot access Unity Catalog data. The only security modes supported for Unity Catalog workloads are Single User and User Isolation. For more information, see Cluster security mode. Spark configuration To fine tune Spark jobs, you can provide custom Spark configuration properties in a cluster configuration. On the cluster configuration page, click the Advanced Options toggle. Click the Spark tab. In Spark config, enter the configuration properties as one key-value pair per line. When you configure a cluster using the Clusters API 2.0, set Spark properties in the spark_conf field in the Create cluster request or Edit cluster request. To set Spark properties for all clusters, create a global init script: Scala Copy dbutils.fs.put(""dbfs:/databricks/init/set_spark_params.sh"",""""""
  |#!/bin/bash
  |
  |cat << 'EOF' > /databricks/driver/conf/00-custom-spark-driver-defaults.conf
  |[driver] {
  |  ""spark.sql.sources.partitionOverwriteMode"" = ""DYNAMIC""
  |}
  |EOF
  """""".stripMargin, true)
 Retrieve a Spark configuration property from a secret Databricks recommends storing sensitive information, such as passwords, in a secret instead of plaintext. To reference a secret in the Spark configuration, use the following syntax: ini Copy spark.<property-name> {{secrets/<scope-name>/<secret-name>}}
 For example, to set a Spark configuration property called password to the value of the secret stored in secrets/acme_app/password: ini Copy spark.password {{secrets/acme-app/password}}
 For more information, see Syntax for referencing secrets in a Spark configuration property or environment variable. Environment variables You can configure custom environment variables that you can access from init scripts running on a cluster. Databricks also provides predefined environment variables that you can use in init scripts. You cannot override these predefined environment variables. On the cluster configuration page, click the Advanced Options toggle. Click the Spark tab. Set the environment variables in the Environment Variables field. You can also set environment variables using the spark_env_vars field in the Create cluster request or Edit cluster request Clusters API endpoints. Cluster tags Cluster tags allow you to easily monitor the cost of cloud resources used by various groups in your organization. You can specify tags as key-value pairs when you create a cluster, and Azure Databricks applies these tags to cloud resources like VMs and disk volumes, as well as DBU usage reports. For clusters launched from pools, the custom cluster tags are only applied to DBU usage reports and do not propagate to cloud resources. For detailed information about how pool and cluster tag types work together, see Monitor usage using cluster, pool, and workspace tags. For convenience, Azure Databricks applies four default tags to each cluster: Vendor, Creator, ClusterName, and ClusterId. In addition, on job clusters, Azure Databricks applies two default tags: RunName and JobId. On resources used by Databricks SQL, Azure Databricks also applies the default tag SqlWarehouseId. Warning Do not assign a custom tag with the key Name to a cluster. Every cluster has a tag Name whose value is set by Azure Databricks. If you change the value associated with the key Name, the cluster can no longer be tracked by Azure Databricks. As a consequence, the cluster might not be terminated after becoming idle and will continue to incur usage costs. You can add custom tags when you create a cluster. To configure cluster tags: On the cluster configuration page, click the Advanced Options toggle. At the bottom of the page, click the Tags tab. Add a key-value pair for each custom tag. You can add up to 43 custom tags. For more details, see Monitor usage using cluster, pool, and workspace tags. SSH access to clusters For security reasons, in Azure Databricks the SSH port is closed by default. If you want to enable SSH access to your Spark clusters, contact Azure Databricks support. Note SSH can be enabled only if your workspace is deployed in your own Azure virtual network. Cluster log delivery When you create a cluster, you can specify a location to deliver the logs for the Spark driver node, worker nodes, and events. Logs are delivered every five minutes to your chosen destination. When a cluster is terminated, Azure Databricks guarantees to deliver all logs generated up until the cluster was terminated. The destination of the logs depends on the cluster ID. If the specified destination is dbfs:/cluster-log-delivery, cluster logs for 0630-191345-leap375 are delivered to dbfs:/cluster-log-delivery/0630-191345-leap375. To configure the log delivery location: On the cluster configuration page, click the Advanced Options toggle. Click the Logging tab. Select a destination type. Enter the cluster log path. Note This feature is also available in the REST API. See Clusters API 2.0 and Cluster log delivery examples. Init scripts A cluster node initialization—or init—script is a shell script that runs during startup for each cluster node before the Spark driver or worker JVM starts. You can use init scripts to install packages and libraries not included in the Databricks runtime, modify the JVM system classpath, set system properties and environment variables used by the JVM, or modify Spark configuration parameters, among other configuration tasks. You can attach init scripts to a cluster by expanding the Advanced Options section and clicking the Init Scripts tab. For detailed instructions, see Cluster node initialization scripts.",Configure clusters
24,Updated Databrick connector on Power BI breaks the connectiom,"Connector from Databricks on Power BI Desktop was updated, and no users cannot connect , attached are the logs from the odbc connection",https://docs.microsoft.com/en-us/azure/databricks/kb/bi/powerbi-proxy-ssl-configuration,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Power BI proxy and SSL configuration Article 03/11/2022 3 minutes to read 2 contributors In this article Driver configurations Troubleshooting Driver configurations You can set driver configurations using the microsoft.sparkodbc.ini file which can be found in the ODBC Drivers\Simba Spark ODBC Driver directory. The absolute path of the microsoft.sparkodbc.ini directory depends on whether you are using Power BI Desktop or on-premises Power BI Gateway: Power BI Desktop: C:\Program Files\Microsoft Power BI Desktop\bin\ODBC Drivers\Simba Spark ODBC Driver\microsoft.sparkodbc.ini Power BI Gateway: m\ODBC Drivers\Simba Spark ODBC Driver\microsoft.sparkodbc.ini, where m is placed inside the gateway installation directory. Set driver configurations Check if the microsoft.sparkodbc.ini file was already created. If it is then jump to step 3. Open Notepad or File Explorer as Run As Administrator and create a file at ODBC DriversSimba Spark ODBC Drivermicrosoft.sparkodbc.ini. Add the new driver configurations to the file below the header [Driver] by using the syntax =. Configuration keys can be found in the manual provided with the installation of the Databricks ODBC Driver. The manual is located at C:\Program Files\Simba Spark ODBC Driver\Simba Apache Spark ODBC Connector Install and Configuration Guide.html. Configuring a proxy To configure a proxy, add the following configurations to the driver configuration in the microsoft.sparkodbc.ini file: Console Copy [Driver]
UseProxy=1
ProxyHost=<proxy.example.com>
ProxyPort=<port>
ProxyUID=<username>
ProxyPWD=<password>
 Depending on the firewall configuration it might also be necessary to add: Console Copy [Driver]
CheckCertRevocation=0
 Troubleshooting Error: SSL_connect: certificate verify failed When SSL issues occur, the ODBC driver returns a generic error SSL_connect: certificate verify failed. You can get more detailed SSL debugging logs by setting in the ODBC DriversSimba Spark ODBC Drivermicrosoft.sparkodbc.inimicrosoft.sparkodbc.ini file the following two configurations: Console Copy [Driver]
AllowDetailedSSLErrorMessages=1
EnableCurlDebugLogging=1
 Diagnose issues by analyzing CryptoAPI logs Most issues can be diagnosed by using Windows CryptoAPI logs, which can be found in the Event Viewer. The following steps describe how to capture these logs. Open Event Viewer and go to Applications and Services Logs > Microsoft > Windows > CAPI2 > Operational. In Filter Current Log, check the boxes Critical, Error and Warning and click OK. In the Event Viewer, go to Actions > Enable Log to start collecting logs. Connect Power BI to Azure Databricks to reproduce the issue. In the Event Viewer, go to Actions > Disable Log to stop collecting logs. Click Refresh to retrieve the list of collected events. Export logs by clicking Actions > Save Filtered Log File As. Diagnose Build Chain or Verify Chain Policy event errors If the collected logs contain an error on the Build Chain or Verify Chain Policy events, this likely points to the issue. More details can be found by selecting the event and reading the Details section. Two fields of interest are Result, and RevocationResult. The revocation status of the certificate or one of the certificates in the certificate chain is unknown. CAPI2 error: RevocationResult: [80092013] The revocation function was unable to check revocation because the revocation server was offline. Cause: The revocation check failed due to an unavailable certificate revocation server. Resolution: Disable certificate revocation checking. The certificate chain is not complete. CAPI2 error: Result: [800B010A] A certificate chain could not be built to a trusted root authority. Cause: The certificate advertised by the VPN or proxy server is incomplete and does not contain a full chain to the trusted root authority. Resolution: The preferred solution is to configure the VPN or proxy server to advertise the full chain. If this is not possible, a workaround is to obtain the intermediate certificates for the Databricks workspace, and install these in the Intermediate Certification Authorities store, to enable Windows to find the unadvertised certificates. See Install intermediate certificates. If possible, it is recommended to install these certificates for all Power BI users using a group policy in Windows. This has to be set up by the system administrator. Certificate configurations Disable certificate revocation checking If the ODBC driver is unable to reach the certificate revocation list server, for example because of a firewall configuration, it will fail to validate the certificate. This can be resolved by disabling this check. To disable certificate revocation checking, set the configuration CheckCertRevocation=0 to the microsoft.sparkodbc.ini file. Install intermediate certificates Open your Azure Databricks workspace URL in Chrome and go to View site information by clicking the padlock icon in the address bar. Click Certificate > Certificate Path and repeat steps 3 to 6 for every intermediate certificate in the chain. Choose an intermediate certificate and go to Details > Copy to File > Next to export the certificate. Select the location of the certificate and click Finish. Open the exported certificate and click Install Certificate > Next. From the Certificate Import Wizard click Place all certificates in the following store > Browse and choose Intermediate Certification Authorities.",Power BI proxy and SSL configuration
25,Updated Databrick connector on Power BI breaks the connectiom,"Connector from Databricks on Power BI Desktop was updated, and no users cannot connect , attached are the logs from the odbc connection",https://docs.microsoft.com/en-us/azure/databricks/kb/bi/powerbi-proxy-ssl-configuration,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Power BI proxy and SSL configuration Article 03/11/2022 3 minutes to read 2 contributors In this article Driver configurations Troubleshooting Driver configurations You can set driver configurations using the microsoft.sparkodbc.ini file which can be found in the ODBC Drivers\Simba Spark ODBC Driver directory. The absolute path of the microsoft.sparkodbc.ini directory depends on whether you are using Power BI Desktop or on-premises Power BI Gateway: Power BI Desktop: C:\Program Files\Microsoft Power BI Desktop\bin\ODBC Drivers\Simba Spark ODBC Driver\microsoft.sparkodbc.ini Power BI Gateway: m\ODBC Drivers\Simba Spark ODBC Driver\microsoft.sparkodbc.ini, where m is placed inside the gateway installation directory. Set driver configurations Check if the microsoft.sparkodbc.ini file was already created. If it is then jump to step 3. Open Notepad or File Explorer as Run As Administrator and create a file at ODBC DriversSimba Spark ODBC Drivermicrosoft.sparkodbc.ini. Add the new driver configurations to the file below the header [Driver] by using the syntax =. Configuration keys can be found in the manual provided with the installation of the Databricks ODBC Driver. The manual is located at C:\Program Files\Simba Spark ODBC Driver\Simba Apache Spark ODBC Connector Install and Configuration Guide.html. Configuring a proxy To configure a proxy, add the following configurations to the driver configuration in the microsoft.sparkodbc.ini file: Console Copy [Driver]
UseProxy=1
ProxyHost=<proxy.example.com>
ProxyPort=<port>
ProxyUID=<username>
ProxyPWD=<password>
 Depending on the firewall configuration it might also be necessary to add: Console Copy [Driver]
CheckCertRevocation=0
 Troubleshooting Error: SSL_connect: certificate verify failed When SSL issues occur, the ODBC driver returns a generic error SSL_connect: certificate verify failed. You can get more detailed SSL debugging logs by setting in the ODBC DriversSimba Spark ODBC Drivermicrosoft.sparkodbc.inimicrosoft.sparkodbc.ini file the following two configurations: Console Copy [Driver]
AllowDetailedSSLErrorMessages=1
EnableCurlDebugLogging=1
 Diagnose issues by analyzing CryptoAPI logs Most issues can be diagnosed by using Windows CryptoAPI logs, which can be found in the Event Viewer. The following steps describe how to capture these logs. Open Event Viewer and go to Applications and Services Logs > Microsoft > Windows > CAPI2 > Operational. In Filter Current Log, check the boxes Critical, Error and Warning and click OK. In the Event Viewer, go to Actions > Enable Log to start collecting logs. Connect Power BI to Azure Databricks to reproduce the issue. In the Event Viewer, go to Actions > Disable Log to stop collecting logs. Click Refresh to retrieve the list of collected events. Export logs by clicking Actions > Save Filtered Log File As. Diagnose Build Chain or Verify Chain Policy event errors If the collected logs contain an error on the Build Chain or Verify Chain Policy events, this likely points to the issue. More details can be found by selecting the event and reading the Details section. Two fields of interest are Result, and RevocationResult. The revocation status of the certificate or one of the certificates in the certificate chain is unknown. CAPI2 error: RevocationResult: [80092013] The revocation function was unable to check revocation because the revocation server was offline. Cause: The revocation check failed due to an unavailable certificate revocation server. Resolution: Disable certificate revocation checking. The certificate chain is not complete. CAPI2 error: Result: [800B010A] A certificate chain could not be built to a trusted root authority. Cause: The certificate advertised by the VPN or proxy server is incomplete and does not contain a full chain to the trusted root authority. Resolution: The preferred solution is to configure the VPN or proxy server to advertise the full chain. If this is not possible, a workaround is to obtain the intermediate certificates for the Databricks workspace, and install these in the Intermediate Certification Authorities store, to enable Windows to find the unadvertised certificates. See Install intermediate certificates. If possible, it is recommended to install these certificates for all Power BI users using a group policy in Windows. This has to be set up by the system administrator. Certificate configurations Disable certificate revocation checking If the ODBC driver is unable to reach the certificate revocation list server, for example because of a firewall configuration, it will fail to validate the certificate. This can be resolved by disabling this check. To disable certificate revocation checking, set the configuration CheckCertRevocation=0 to the microsoft.sparkodbc.ini file. Install intermediate certificates Open your Azure Databricks workspace URL in Chrome and go to View site information by clicking the padlock icon in the address bar. Click Certificate > Certificate Path and repeat steps 3 to 6 for every intermediate certificate in the chain. Choose an intermediate certificate and go to Details > Copy to File > Next to export the certificate. Select the location of the certificate and click Finish. Open the exported certificate and click Install Certificate > Next. From the Certificate Import Wizard click Place all certificates in the following store > Browse and choose Intermediate Certification Authorities.",Power BI proxy and SSL configuration
26,PowerBI COnnectivity to Databricks SQL,"Customer's PowerBI Desktop is not able to connect to Databricks SQL

Databricks SQL endpoint: (screenshot1 shared)
Error they are getting in powerBI:(Screenshot2 shared)

Could you please provide some insights on this.",https://docs.microsoft.com/en-us/azure/databricks/kb/bi/powerbi-proxy-ssl-configuration,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Power BI proxy and SSL configuration Article 03/11/2022 3 minutes to read 2 contributors In this article Driver configurations Troubleshooting Driver configurations You can set driver configurations using the microsoft.sparkodbc.ini file which can be found in the ODBC Drivers\Simba Spark ODBC Driver directory. The absolute path of the microsoft.sparkodbc.ini directory depends on whether you are using Power BI Desktop or on-premises Power BI Gateway: Power BI Desktop: C:\Program Files\Microsoft Power BI Desktop\bin\ODBC Drivers\Simba Spark ODBC Driver\microsoft.sparkodbc.ini Power BI Gateway: m\ODBC Drivers\Simba Spark ODBC Driver\microsoft.sparkodbc.ini, where m is placed inside the gateway installation directory. Set driver configurations Check if the microsoft.sparkodbc.ini file was already created. If it is then jump to step 3. Open Notepad or File Explorer as Run As Administrator and create a file at ODBC DriversSimba Spark ODBC Drivermicrosoft.sparkodbc.ini. Add the new driver configurations to the file below the header [Driver] by using the syntax =. Configuration keys can be found in the manual provided with the installation of the Databricks ODBC Driver. The manual is located at C:\Program Files\Simba Spark ODBC Driver\Simba Apache Spark ODBC Connector Install and Configuration Guide.html. Configuring a proxy To configure a proxy, add the following configurations to the driver configuration in the microsoft.sparkodbc.ini file: Console Copy [Driver]
UseProxy=1
ProxyHost=<proxy.example.com>
ProxyPort=<port>
ProxyUID=<username>
ProxyPWD=<password>
 Depending on the firewall configuration it might also be necessary to add: Console Copy [Driver]
CheckCertRevocation=0
 Troubleshooting Error: SSL_connect: certificate verify failed When SSL issues occur, the ODBC driver returns a generic error SSL_connect: certificate verify failed. You can get more detailed SSL debugging logs by setting in the ODBC DriversSimba Spark ODBC Drivermicrosoft.sparkodbc.inimicrosoft.sparkodbc.ini file the following two configurations: Console Copy [Driver]
AllowDetailedSSLErrorMessages=1
EnableCurlDebugLogging=1
 Diagnose issues by analyzing CryptoAPI logs Most issues can be diagnosed by using Windows CryptoAPI logs, which can be found in the Event Viewer. The following steps describe how to capture these logs. Open Event Viewer and go to Applications and Services Logs > Microsoft > Windows > CAPI2 > Operational. In Filter Current Log, check the boxes Critical, Error and Warning and click OK. In the Event Viewer, go to Actions > Enable Log to start collecting logs. Connect Power BI to Azure Databricks to reproduce the issue. In the Event Viewer, go to Actions > Disable Log to stop collecting logs. Click Refresh to retrieve the list of collected events. Export logs by clicking Actions > Save Filtered Log File As. Diagnose Build Chain or Verify Chain Policy event errors If the collected logs contain an error on the Build Chain or Verify Chain Policy events, this likely points to the issue. More details can be found by selecting the event and reading the Details section. Two fields of interest are Result, and RevocationResult. The revocation status of the certificate or one of the certificates in the certificate chain is unknown. CAPI2 error: RevocationResult: [80092013] The revocation function was unable to check revocation because the revocation server was offline. Cause: The revocation check failed due to an unavailable certificate revocation server. Resolution: Disable certificate revocation checking. The certificate chain is not complete. CAPI2 error: Result: [800B010A] A certificate chain could not be built to a trusted root authority. Cause: The certificate advertised by the VPN or proxy server is incomplete and does not contain a full chain to the trusted root authority. Resolution: The preferred solution is to configure the VPN or proxy server to advertise the full chain. If this is not possible, a workaround is to obtain the intermediate certificates for the Databricks workspace, and install these in the Intermediate Certification Authorities store, to enable Windows to find the unadvertised certificates. See Install intermediate certificates. If possible, it is recommended to install these certificates for all Power BI users using a group policy in Windows. This has to be set up by the system administrator. Certificate configurations Disable certificate revocation checking If the ODBC driver is unable to reach the certificate revocation list server, for example because of a firewall configuration, it will fail to validate the certificate. This can be resolved by disabling this check. To disable certificate revocation checking, set the configuration CheckCertRevocation=0 to the microsoft.sparkodbc.ini file. Install intermediate certificates Open your Azure Databricks workspace URL in Chrome and go to View site information by clicking the padlock icon in the address bar. Click Certificate > Certificate Path and repeat steps 3 to 6 for every intermediate certificate in the chain. Choose an intermediate certificate and go to Details > Copy to File > Next to export the certificate. Select the location of the certificate and click Finish. Open the exported certificate and click Install Certificate > Next. From the Certificate Import Wizard click Place all certificates in the following store > Browse and choose Intermediate Certification Authorities.",Power BI proxy and SSL configuration
27,PowerBI to DBX connectivity issues,PowerBI to DBX connectivity issues,https://docs.microsoft.com/en-us/azure/databricks/kb/bi/powerbi-proxy-ssl-configuration,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Power BI proxy and SSL configuration Article 03/11/2022 3 minutes to read 2 contributors In this article Driver configurations Troubleshooting Driver configurations You can set driver configurations using the microsoft.sparkodbc.ini file which can be found in the ODBC Drivers\Simba Spark ODBC Driver directory. The absolute path of the microsoft.sparkodbc.ini directory depends on whether you are using Power BI Desktop or on-premises Power BI Gateway: Power BI Desktop: C:\Program Files\Microsoft Power BI Desktop\bin\ODBC Drivers\Simba Spark ODBC Driver\microsoft.sparkodbc.ini Power BI Gateway: m\ODBC Drivers\Simba Spark ODBC Driver\microsoft.sparkodbc.ini, where m is placed inside the gateway installation directory. Set driver configurations Check if the microsoft.sparkodbc.ini file was already created. If it is then jump to step 3. Open Notepad or File Explorer as Run As Administrator and create a file at ODBC DriversSimba Spark ODBC Drivermicrosoft.sparkodbc.ini. Add the new driver configurations to the file below the header [Driver] by using the syntax =. Configuration keys can be found in the manual provided with the installation of the Databricks ODBC Driver. The manual is located at C:\Program Files\Simba Spark ODBC Driver\Simba Apache Spark ODBC Connector Install and Configuration Guide.html. Configuring a proxy To configure a proxy, add the following configurations to the driver configuration in the microsoft.sparkodbc.ini file: Console Copy [Driver]
UseProxy=1
ProxyHost=<proxy.example.com>
ProxyPort=<port>
ProxyUID=<username>
ProxyPWD=<password>
 Depending on the firewall configuration it might also be necessary to add: Console Copy [Driver]
CheckCertRevocation=0
 Troubleshooting Error: SSL_connect: certificate verify failed When SSL issues occur, the ODBC driver returns a generic error SSL_connect: certificate verify failed. You can get more detailed SSL debugging logs by setting in the ODBC DriversSimba Spark ODBC Drivermicrosoft.sparkodbc.inimicrosoft.sparkodbc.ini file the following two configurations: Console Copy [Driver]
AllowDetailedSSLErrorMessages=1
EnableCurlDebugLogging=1
 Diagnose issues by analyzing CryptoAPI logs Most issues can be diagnosed by using Windows CryptoAPI logs, which can be found in the Event Viewer. The following steps describe how to capture these logs. Open Event Viewer and go to Applications and Services Logs > Microsoft > Windows > CAPI2 > Operational. In Filter Current Log, check the boxes Critical, Error and Warning and click OK. In the Event Viewer, go to Actions > Enable Log to start collecting logs. Connect Power BI to Azure Databricks to reproduce the issue. In the Event Viewer, go to Actions > Disable Log to stop collecting logs. Click Refresh to retrieve the list of collected events. Export logs by clicking Actions > Save Filtered Log File As. Diagnose Build Chain or Verify Chain Policy event errors If the collected logs contain an error on the Build Chain or Verify Chain Policy events, this likely points to the issue. More details can be found by selecting the event and reading the Details section. Two fields of interest are Result, and RevocationResult. The revocation status of the certificate or one of the certificates in the certificate chain is unknown. CAPI2 error: RevocationResult: [80092013] The revocation function was unable to check revocation because the revocation server was offline. Cause: The revocation check failed due to an unavailable certificate revocation server. Resolution: Disable certificate revocation checking. The certificate chain is not complete. CAPI2 error: Result: [800B010A] A certificate chain could not be built to a trusted root authority. Cause: The certificate advertised by the VPN or proxy server is incomplete and does not contain a full chain to the trusted root authority. Resolution: The preferred solution is to configure the VPN or proxy server to advertise the full chain. If this is not possible, a workaround is to obtain the intermediate certificates for the Databricks workspace, and install these in the Intermediate Certification Authorities store, to enable Windows to find the unadvertised certificates. See Install intermediate certificates. If possible, it is recommended to install these certificates for all Power BI users using a group policy in Windows. This has to be set up by the system administrator. Certificate configurations Disable certificate revocation checking If the ODBC driver is unable to reach the certificate revocation list server, for example because of a firewall configuration, it will fail to validate the certificate. This can be resolved by disabling this check. To disable certificate revocation checking, set the configuration CheckCertRevocation=0 to the microsoft.sparkodbc.ini file. Install intermediate certificates Open your Azure Databricks workspace URL in Chrome and go to View site information by clicking the padlock icon in the address bar. Click Certificate > Certificate Path and repeat steps 3 to 6 for every intermediate certificate in the chain. Choose an intermediate certificate and go to Details > Copy to File > Next to export the certificate. Select the location of the certificate and click Finish. Open the exported certificate and click Install Certificate > Next. From the Certificate Import Wizard click Place all certificates in the following store > Browse and choose Intermediate Certification Authorities.",Power BI proxy and SSL configuration
28,Import com.sap.db.jdbc.Driver fails,"Question: What time did the problem begin?
Answer: Tue, Mar 1, 2022, 12:00 AM (UTC-08:00) Pacific Time (US & Canada)

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer: 

Question: Was it working in previous DBR?
Answer: Other, don't know or not applicable

Question: Notebook URL if available
Answer: https://adb-7369653848088828.8.azuredatabricks.net/?o=7369653848088828#job/309493103359550/run/98544

Question: Cluster URL if notebook URL is not available
Answer: https://adb-7369653848088828.8.azuredatabricks.net/?o=7369653848088828#setting/clusters/0209-104622-carpi835/configuration

Question: Additional details about the issue
Answer: Importing the driver class  com.sap.db.jdbc.Driver is failing intermittently with the error message 'ClassNotFoundException: com.sap.db.jdbc.Driver'.



<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Databricks:
What time did the problem begin? - 2022-03-01T08:00:00.000Z;
Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank - ;
Was it working in previous DBR? - Other, don't know or not applicable;
Notebook URL if available - https://adb-7369653848088828.8.azuredatabricks.net/?o=7369653848088828#job/309493103359550/run/98544;
Cluster URL if notebook URL is not available - https://adb-7369653848088828.8.azuredatabricks.net/?o=7369653848088828#setting/clusters/0209-104622-carpi835/configuration;
Additional details about the issue - Importing the driver class  com.sap.db.jdbc.Driver is failing intermittently with the error message 'ClassNotFoundException: com.sap.db.jdbc.Driver'.;

- ProblemStartTime: 03/01/2022 08:00:00
- Cloud: Azure
- AzureProductSubscriptionID: a0776237-28a9-41f7-88f3-a1e65953748f
- AzureProductSubscriptionName: IT Corp - SubProd Vendor Appliances (SV)
- PUID: 100320010EE05FFD
- Tenant Id: 8bcff170-9979-491e-8683-d8ced0850bad
- Object Id: 182bc918-aab0-4c68-bca9-d69f28ad7298
- SubscriptionType: Premier
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Premier with Azure Rapid response
- GrantPermission: False
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: False

- Location: westus2
- ResourceUri: /subscriptions/a0776237-28a9-41f7-88f3-a1e65953748f/resourceGroups/sv-wus2-subprod-databricks-rg/providers/Microsoft.Databricks/workspaces/sv-wus2-subprod-databricks-ws

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs Article 07/14/2022 29 minutes to read 4 contributors In this article Create a job Run a job View jobs View runs for a job View job run details View recent job runs Export job run results Edit a job Edit a task Clone a job Clone a task Delete a job Delete a task Copy a task path Run jobs using notebooks in a remote Git repository Best practices A job is a way to run non-interactive code in an Azure Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. You can also run jobs interactively in the notebook UI. You can create and run a job using the UI, the CLI, or by invoking the Jobs API. You can repair and re-run a failed or canceled job using the UI or API. You can monitor job run results using the UI, CLI, API, and email notifications. This article focuses on performing job tasks using the UI. For the other methods, see Jobs CLI and Jobs API 2.1. Your job can consist of a single task or can be a large, multi-task workflow with complex dependencies. Azure Databricks manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs. You can run your jobs immediately or periodically through an easy-to-use scheduling system. You can implement a task in a JAR, an Azure Databricks notebook, a Delta Live Tables pipeline, or an application written in Scala, Java, or Python. Legacy Spark Submit applications are also supported. You control the execution order of tasks by specifying dependencies between the tasks. You can configure tasks to run in sequence or parallel. The following diagram illustrates a workflow that: Ingests raw clickstream data and performs processing to sessionize the records. Ingests order data and joins it with the sessionized clickstream data to create a prepared data set for analysis. Extracts features from the prepared data. Performs tasks in parallel to persist the features and train a machine learning model. To create your first workflow with an Azure Databricks job, see the quickstart. Important You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. A workspace is limited to 1000 concurrent job runs. A 429 Too Many Requests response is returned when you request a run that cannot start immediately. The number of jobs a workspace can create in an hour is limited to 5000 (includes “run now” and “runs submit”). This limit also affects jobs created by the REST API and notebook workflows. Create a job Do one of the following: Click Workflows in the sidebar and click . In the sidebar, click Create and select Job from the menu. The Tasks tab appears with the create task dialog. Replace Add a name for your job… with your job name. Enter a name for the task in the Task name field. Specify the type of task to run. In the Type drop-down, select Notebook, JAR, Spark Submit, Python, or Pipeline. Notebook: In the Source drop-down, select a location for the notebook; either Workspace for a notebook located in a Azure Databricks workspace folder or Git provider for a notebook located in a remote Git repository. Workspace: Use the file browser to find the notebook, click the notebook name, and click Confirm. Git provider: Click Edit and enter the Git repository information. See Run jobs using notebooks in a remote Git repository. JAR: Specify the Main class. Use the fully qualified name of the class containing the main method, for example, org.apache.spark.examples.SparkPi. Then click Add under Dependent Libraries to add libraries required to run the task. One of these libraries must contain the main class. To learn more about JAR tasks, see JAR jobs. Spark Submit: In the Parameters text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the DFSReadWriteTest from the Apache Spark examples: JSON Copy [""--class"",""org.apache.spark.examples.DFSReadWriteTest"",""dbfs:/FileStore/libraries/spark_examples_2_12_3_1_1.jar"",""/dbfs/databricks-datasets/README.md"",""/FileStore/examples/output/""]
 Important There are several limitations for spark-submit tasks: You can run spark-submit tasks only on new clusters. Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see Cluster autoscaling. Spark-submit does not support Databricks Utilities. To use Databricks Utilities, use JAR tasks instead. Python: In the Path textbox, enter the URI of a Python script on DBFS or cloud storage; for example, dbfs:/FileStore/myscript.py. Pipeline: In the Pipeline drop-down, select an existing Delta Live Tables pipeline. Python Wheel: In the Package name text box, enter the package to import, for example, myWheel-1.0-py2.py3-none-any.whl. In the Entry Point text box, enter the function to call when starting the wheel. Click Add under Dependent Libraries to add libraries required to run the task. Configure the cluster where the task runs. In the Cluster drop-down, select either New Job Cluster or Existing All-Purpose Clusters. New Job Cluster: Click Edit in the Cluster drop-down and complete the cluster configuration. Existing All-Purpose Cluster: Select an existing cluster in the Cluster drop-down. To open the cluster in a new page, click the icon to the right of the cluster name and description. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters. Notebook: Click Add and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the Run a job with different parameters option. Parameters set the value of the notebook widget specified by the key of the parameter. Use task parameter variables to pass a limited set of dynamic values as part of a parameter value. JAR: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See Configure JAR job parameters. Spark Submit task: Parameters are specified as a JSON-formatted array of strings. Conforming to the Apache Spark spark-submit convention, parameters after the JAR path are passed to the main method of the main class. Python: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the argparse module in Python. Python Wheel: In the Parameters drop-down, select Positional arguments to enter parameters as a JSON-formatted array of strings, or select Keyword arguments > Add to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. To access additional options, including Dependent Libraries, Retry Policy, and Timeouts, click Advanced Options. See Edit a task. Click Create. To optionally set the job’s schedule, click Edit schedule in the Job details panel. See Schedule a job. To optionally allow multiple concurrent runs of the same job, click Edit concurrent runs in the Job details panel. See Maximum concurrent runs. To optionally specify email addresses to receive notifications on job events, click Edit notifications in the Job details panel. See Notifications. To optionally control permission levels on the job, click Edit permissions in the Job details panel. See Control access to jobs. To add another task, click below the task you just created. A shared cluster option is provided if you have configured a New Job Cluster for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Run a job Click Workflows in the sidebar. Select a job and click the Runs tab. You can run a job immediately or schedule the job to run later. If one or more tasks in a job with multiple tasks are not successful, you can re-run the subset of unsuccessful tasks. See Repair an unsuccessful job run. Run a job immediately To run the job immediately, click . Tip You can perform a test run of a job with a notebook task by clicking Run Now. If you need to make changes to the notebook, clicking Run Now again after editing the notebook will automatically run the new version of the notebook. Run a job with different parameters You can use Run Now with Different Parameters to re-run a job with different parameters or different values for existing parameters. Click next to Run Now and select Run Now with Different Parameters or, in the Active Runs table, click Run Now with Different Parameters. Enter the new parameters depending on the type of task. Notebook: You can enter parameters as key-value pairs or a JSON object. The provided parameters are merged with the default parameters for the triggered run. You can use this dialog to set the values of widgets. JAR and spark-submit: You can enter a list of parameters or a JSON document. If you delete keys, the default parameters are used. You can also add task parameter variables for the run. Click Run. Repair an unsuccessful job run You can repair failed or canceled multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks and any tasks that depend on them are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs. You can change job or task settings before repairing the job run. Unsuccessful tasks are re-run with the current job and task settings. For example, if you change the path to a notebook or a cluster setting, the task is re-run with the updated notebook or cluster settings. You can view the history of all task runs on the Task run details page. Note If one or more tasks share a job cluster, a repair run creates a new job cluster; for example, if the original run used the job cluster my_job_cluster, the first repair run uses the new job cluster my_job_cluster_v1, allowing you to easily see the cluster and cluster settings used by the initial run and any repair runs. The settings for my_job_cluster_v1 are the same as the current settings for my_job_cluster. Repair is supported only with jobs that orchestrate two or more tasks. The Duration value displayed in the Runs tab includes the time the first run started until the time when the latest repair run finished. For example, if a run failed twice and succeeded on the third run, the duration includes the time for all three runs. To repair an unsuccessful job run: Click Jobs in the sidebar. In the Name column, click a job name. The Runs tab shows active runs and completed runs, including any unsuccessful runs. Click the link for the unsuccessful run in the Start time column of the Completed Runs (past 60 days) table. The Job run details page appears. Click Repair run. The Repair job run dialog appears, listing all unsuccessful tasks and any dependent tasks that will be re-run. To add or edit parameters for the tasks to repair, enter the parameters in the Repair job run dialog. Parameters you enter in the Repair job run dialog override existing values. On subsequent repair runs, you can return a parameter to its original value by clearing the key and value in the Repair job run dialog. Click Repair run in the Repair job run dialog. View task run history To view the run history of a task, including successful and unsuccessful runs: Click on a task on the Job run details page. The Task run details page appears. Select the task run in the run history drop-down. Schedule a job To define a schedule for the job: Click Edit schedule in the Job details panel and set the Schedule Type to Scheduled. Specify the period, starting time, and time zone. Optionally select the Show Cron Syntax checkbox to display and edit the schedule in Quartz Cron Syntax. Note Azure Databricks enforces a minimum interval of 10 seconds between subsequent runs triggered by the schedule of a job regardless of the seconds configuration in the cron expression. You can choose a time zone that observes daylight saving time or UTC. If you select a zone that observes daylight saving time, an hourly job will be skipped or may appear to not fire for an hour or two when daylight saving time begins or ends. To run at every hour (absolute time), choose UTC. The job scheduler is not intended for low latency jobs. Due to network or cloud issues, job runs may occasionally be delayed up to several minutes. In these situations, scheduled jobs will run immediately upon service availability. Click Save. Pause and resume a job schedule To pause a job, you can either: Click Pause in the Job details panel. Click Edit schedule in the Job details panel and set the Schedule Type to Manual (Paused) To resume a paused job schedule, set the Schedule Type to Scheduled. View jobs Click Workflows in the sidebar. The Jobs list appears. The Jobs page lists all defined jobs, the cluster definition, the schedule, if any, and the result of the last run. You can filter jobs in the Jobs list: Using keywords. Selecting only the jobs you own. Selecting all jobs you have permissions to access. Access to this filter requires that Jobs access control is enabled. Using tags. To search for a tag created with only a key, type the key into the search box. To search for a tag created with a key and value, you can search by the key, the value, or both the key and value. For example, for a tag with the key department and the value finance, you can search for department or finance to find matching jobs. To search by both the key and value, enter the key and value separated by a colon; for example, department:finance. You can also click any column header to sort the list of jobs (either descending or ascending) by that column. The default sorting is by job name in ascending order. View runs for a job Click Workflows in the sidebar. In the Name column, click a job name. The Runs tab appears with a table of active runs and completed runs. To switch to a matrix view, click Matrix. The matrix view shows a history of runs for the job, including each job task. The Job Runs row of the matrix displays the total duration of the run and the state of the run. To view details of the run, including the start time, duration, and status, hover over the bar in the Job Runs row. Each cell in the Tasks row represents a task and the corresponding status of the task. To view details of each task, including the start time, duration, cluster, and status, hover over the cell for that task. The job run and task run bars are color-coded to indicate the status of the run. Successful runs are green, unsuccessful runs are red, and skipped runs are pink. The height of the individual job run and task run bars provides a visual indication of the run duration. Azure Databricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends that you export results before they expire. For more information, see Export job run results. View job run details The job run details page contains job output and links to logs, including information about the success or failure of each task in the job run. You can access job run details from the Runs tab for the job. To view job run details from the Runs tab, click the link for the run in the Run column of the Completed Runs (past 60 days) table. To return to the Runs tab for the job, click on the Job ID value. Click on a task to view task run details, including: the cluster that ran the task the Spark UI for the task logs for the task metrics for the task Click the Job ID value to return to the Runs tab for the job. Click the Job run ID value to return to the job run details. View recent job runs You can view a list of currently running and recently completed runs for all jobs in a workspace you have access to, including runs started by external orchestration tools such as Apache Airflow or Azure Data Factory. To view the list of recent job runs: Click Workflows in the sidebar. The Jobs list appears. Click the Job runs tab. The Job runs list appears. The Job runs list displays: The start time for the run. The name of the job associated with the run. The user name that the job runs as. Whether the run was triggered by a job schedule or an API request, or was manually started. The time elapsed for a currently running job, or the total running time for a completed run. The status of the run, either Pending, Running, Skipped, Succeeded, Failed, Terminating, Terminated, Internal Error, Timed Out, Canceled, Canceling, or Waiting for Retry. To view job run details, click the link in the Start time column for the run. To view job details, click the job name in the Job column. Export job run results You can export notebook run results and job run logs for all job types. Export notebook run results You can persist job runs by exporting their results. For notebook job runs, you can export a rendered notebook that can later be imported into your Azure Databricks workspace. To export notebook run results for a job with a single task: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click Export to HTML. To export notebook run results for a job with multiple tasks: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click the notebook task to export. Click Export to HTML. Export job run logs You can also export the logs for your job run. You can set up your job to automatically deliver logs to DBFS through the Job API. See the new_cluster.cluster_log_conf object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. Edit a job Some configuration options are available on the job, and other options are available on individual tasks. For example, the maximum concurrent runs can be set on the job only, while parameters must be defined for each task. To change the configuration for a job: Click Workflows in the sidebar. In the Name column, click the job name. The side panel displays the Job details. You can change the schedule, cluster configuration, notifications, maximum number of concurrent runs, and add or change tags. If job access control is enabled, you can also edit job permissions. Tags To add labels or key:value attributes to your job, you can add tags when you edit the job. You can use tags to filter jobs in the Jobs list; for example, you can use a department tag to filter all jobs that belong to a specific department. Note Because job tags are not designed to store sensitive information such as personally identifiable information or passwords, Databricks recommends using tags for non-sensitive values only. Tags also propagate to job clusters created when a job is run, allowing you to use tags with your existing cluster monitoring. To add or edit tags, click + Tag in the Job details side panel. You can add the tag as a key and value, or a label. To add a label, enter the label in the Key field and leave the Value field empty. Clusters To see tasks associated with a cluster, hover over the cluster in the side panel. To change the cluster configuration for all associated tasks, click Configure under the cluster. To configure a new cluster for all associated tasks, click Swap under the cluster. Maximum concurrent runs The maximum number of parallel runs for this job. Azure Databricks skips the run if the job has already reached its maximum number of active runs when attempting to start a new run. Set this value higher than the default of 1 to perform multiple runs of the same job concurrently. This is useful, for example, if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or you want to trigger multiple runs that differ by their input parameters. Notifications You can add one or more email addresses to notify when runs of this job begin, complete, or fail: Click Edit notifications. Click Add. Enter an email address and click the check box for each notification type to send to that address. To enter another email address for notification, click Add. If you do not want to receive notifications for skipped job runs, click the check box. Click Confirm. Integrate these email notifications with your favorite notification tools, including: PagerDuty Slack Control access to jobs Job access control enables job owners and administrators to grant fine-grained permissions on their jobs. Job owners can choose which other users or groups can view the results of the job. Owners can also choose who can manage their job runs (Run now and Cancel run permissions). See Jobs access control for details. Edit a task To set task configuration options: Click Workflows in the sidebar. In the Name column, click the job name. Click the Tasks tab. Task dependencies You can define the order of execution of tasks in a job using the Depends on drop-down. You can set this field to one or more tasks in the job. Note Depends on is not visible if the job consists of only a single task. Configuring task dependencies creates a Directed Acyclic Graph (DAG) of task execution, a common way of representing execution order in job schedulers. For example, consider the following job consisting of four tasks: Task 1 is the root task and does not depend on any other task. Task 2 and Task 3 depend on Task 1 completing first. Finally, Task 4 depends on Task 2 and Task 3 completing successfully. Azure Databricks runs upstream tasks before running downstream tasks, running as many of them in parallel as possible. The following diagram illustrates the order of processing for these tasks: Individual task configuration options Individual tasks have the following configuration options: In this section: Cluster Dependent libraries Task parameter variables Timeout Retries Cluster To configure the cluster where a task runs, click the Cluster drop-down. You can edit a shared job cluster, but you cannot delete a shared cluster if it is still used by other tasks. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Dependent libraries Dependent libraries will be installed on the cluster before the task runs. You must set all task dependencies to ensure they are installed before the run starts. To add a dependent library, click Advanced options and select Add Dependent Libraries to open the Add Dependent Library chooser. Follow the recommendations in Library dependencies for specifying dependencies. Important If you have configured a library to install on all clusters automatically, or you select an existing terminated cluster that has libraries installed, the job execution does not wait for library installation to complete. If a job requires a specific library, you should attach the library to the job in the Dependent Libraries field. Task parameter variables You can pass templated variables into a job task as part of the task’s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job’s start time. When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named MyJobId with a value of my-job-6 for any run of job ID 6, add the following task parameter: JSON Copy {
  ""MyJobID"": ""my-job-{{job_id}}""
}
 The contents of the double curly braces are not evaluated as expressions, so you cannot do operations or functions within double-curly braces. Whitespace is not stripped inside the curly braces, so {{ job_id }} will not be evaluated. The following task parameter variables are supported: Variable Description {{job_id}} The unique identifier assigned to a job {{run_id}} The unique identifier assigned to a job run {{start_date}} The date a task run started. The format is yyyy-MM-dd in UTC timezone. {{start_time}} The timestamp of the run’s start of execution after the cluster is created and ready. The format is milliseconds since UNIX epoch in UTC timezone, as returned by System.currentTimeMillis(). {{task_retry_count}} The number of retries that have been attempted to run a task if the first attempt fails. The value is 0 for the first attempt and increments with each retry. {{parent_run_id}} The unique identifier assigned to the run of a job with multiple tasks. {{task_key}} The unique name assigned to a task that’s part of a job with multiple tasks. You can set these variables with any task when you Create a job, Edit a job, or Run a job with different parameters. Timeout The maximum completion time for a job. If the job does not complete in this time, Azure Databricks sets its status to “Timed Out”. Retries A policy that determines when and how many times failed runs are retried. To set the retries for the task, click Advanced options and select Edit Retry Policy. The retry interval is calculated in milliseconds between the start of the failed run and the subsequent retry run. Note If you configure both Timeout and Retries, the timeout applies to each retry. Clone a job You can quickly create a new job by cloning an existing job. Cloning a job creates an identical copy of the job, except for the job ID. On the job’s page, click More … next to the job’s name and select Clone from the drop-down menu. Clone a task You can quickly create a new task by cloning an existing task: On the job’s page, click the Tasks tab. Select the task to clone. Click and select Clone task. Delete a job To delete a job, on the job’s page, click More … next to the job’s name and select Delete from the drop-down menu. Delete a task To delete a task: Click the Tasks tab. Select the task to be deleted. Click and select Remove task. Copy a task path To copy the path to a task, for example, a notebook path: Click the Tasks tab. Select the task containing the path to copy. Click next to the task path to copy the path to the clipboard. Run jobs using notebooks in a remote Git repository Important This feature is in Public Preview. You can run jobs with notebooks located in a remote Git repository. This feature simplifies creation and management of production jobs and automates continuous deployment: You don’t need to create a separate production repo in Azure Databricks, manage permissions for it, and keep it updated. You can prevent unintentional changes to a production job, such as local edits in the production repo or changes from switching a branch. The job definition process has a single source of truth in the remote repository. To use notebooks in a remote Git repository, you must Set up Git integration with Databricks Repos. To create a task with a notebook located in a remote Git repository: In the Type drop-down, select Notebook. In the Source drop-down, select Git provider. The Git information dialog appears. In the Git Information dialog, enter details for the repository. For Path, enter a relative path to the notebook location, such as etl/notebooks/. When you enter the relative path, don’t begin it with / or ./ and don’t include the notebook file extension, such as .py. Additional notebook tasks in a multitask job reference the same commit in the remote repository in one of the following ways: sha of $branch/head when git_branch is set sha of $tag when git_tag is set the value of git_commit In a multitask job, there cannot be a task that uses a local notebook and another task that uses a remote repository. This restriction doesn’t apply to non-notebook tasks. Best practices In this section: Cluster configuration tips Notebook job tips Streaming tasks JAR jobs Library dependencies Cluster configuration tips Cluster configuration is important when you operationalize a job. The following provides general guidance on choosing and configuring job clusters, followed by recommendations for specific job types. Use shared job clusters To optimize resource usage with jobs that orchestrate multiple tasks, use shared job clusters. A shared job cluster allows multiple tasks in the same job run to reuse the cluster. You can use a single job cluster to run all tasks that are part of the job, or multiple job clusters optimized for specific workloads. To use a shared job cluster: Select New Job Clusters when you create a task and complete the cluster configuration. Select the new cluster when adding a task to the job, or create a new job cluster. Any cluster you configure when you select New Job Clusters is available to any task in the job. A shared job cluster is scoped to a single job run, and cannot be used by other jobs or runs of the same job. Libraries cannot be declared in a shared job cluster configuration. You must add dependent libraries in task settings. Choose the correct cluster type for your job New Job Clusters are dedicated clusters for a job or task run. A shared job cluster is created and started when the first task using the cluster starts and terminates after the last task using the cluster completes. The cluster is not terminated when idle but terminates only after all tasks using it have completed. If a shared job cluster fails or is terminated before all tasks have finished, a new cluster is created. A cluster scoped to a single task is created and started when the task starts and terminates when the task completes. In production, Databricks recommends using new shared or task scoped clusters so that each job or task runs in a fully isolated environment. When you run a task on a new cluster, the task is treated as a data engineering (task) workload, subject to the task workload pricing. When you run a task on an existing all-purpose cluster, the task is treated as a data analytics (all-purpose) workload, subject to all-purpose workload pricing. If you select a terminated existing cluster and the job owner has Can Restart permission, Azure Databricks starts the cluster when the job is scheduled to run. Existing all-purpose clusters work best for tasks such as updating dashboards at regular intervals. Use a pool to reduce cluster start times To decrease new job cluster start time, create a pool and configure the job’s cluster to use the pool. Notebook job tips Total notebook cell output (the combined output of all notebook cells) is subject to a 20MB size limit. Additionally, individual cell output is subject to an 8MB size limit. If total cell output exceeds 20MB in size, or if the output of an individual cell is larger than 8MB, the run is canceled and marked as failed. If you need help finding cells near or beyond the limit, run the notebook against an all-purpose cluster and use this notebook autosave technique. Streaming tasks Spark Streaming jobs should never have maximum concurrent runs set to greater than 1. Streaming jobs should be set to run using the cron expression ""* * * * * ?"" (every minute). Since a streaming task runs continuously, it should always be the final task in a job. JAR jobs When running a JAR job, keep in mind the following: Output size limits Note Available in Databricks Runtime 6.3 and above. Job output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run is canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to Azure Databricks by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default, the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster’s log files. Setting this flag is recommended only for job clusters for JAR jobs because it will disable notebook results. Use the shared SparkContext Because Azure Databricks is a managed service, some code changes may be necessary to ensure that your Apache Spark jobs run correctly. JAR job programs must use the shared SparkContext API to get the SparkContext. Because Azure Databricks initializes the SparkContext, programs that invoke new SparkContext() will fail. To get the SparkContext, use only the shared SparkContext created by Azure Databricks: Scala Copy val goodSparkContext = SparkContext.getOrCreate()
val goodSparkSession = SparkSession.builder().getOrCreate()
 There are also several methods you should avoid when using the shared SparkContext. Do not call SparkContext.stop(). Do not call System.exit(0) or sc.stop() at the end of your Main program. This can cause undefined behavior. Use try-finally blocks for job clean up Consider a JAR that consists of two parts: jobBody() which contains the main part of the job. jobCleanup() which has to be executed after jobBody() whether that function succeeded or returned an exception. As an example, jobBody() may create tables, and you can use jobCleanup() to drop these tables. The safe way to ensure that the clean up method is called is to put a try-finally block in the code: Scala Copy try {
  jobBody()
} finally {
  jobCleanup()
}
 You should not try to clean up using sys.addShutdownHook(jobCleanup) or the following code: Scala Copy val cleanupThread = new Thread { override def run = jobCleanup() }
Runtime.getRuntime.addShutdownHook(cleanupThread)
 Due to the way the lifetime of Spark containers is managed in Azure Databricks, the shutdown hooks are not run reliably. Configure JAR job parameters You pass parameters to JAR jobs with a JSON string array. See the spark_jar_task object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. To access these parameters, inspect the String array passed into your main function. Library dependencies The Spark driver has certain library dependencies that cannot be overridden. These libraries take priority over any of your libraries that conflict with them. To get the full list of the driver library dependencies, run the following command inside a notebook attached to a cluster of the same Spark version (or the cluster with the driver you want to examine). Bash Copy %sh
ls /databricks/jars
 Manage library dependencies A good rule of thumb when dealing with library dependencies while creating JARs for jobs is to list Spark and Hadoop as provided dependencies. On Maven, add Spark and Hadoop as provided dependencies, as shown in the following example: XML Copy <dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.3.0</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-core</artifactId>
  <version>1.2.1</version>
  <scope>provided</scope>
</dependency>
 In sbt, add Spark and Hadoop as provided dependencies, as shown in the following example: Scala Copy libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.3.0"" % ""provided""
libraryDependencies += ""org.apache.hadoop"" %% ""hadoop-core"" % ""1.2.1"" % ""provided""
 Tip Specify the correct Scala version for your dependencies based on the version you are running.",Jobs
29,Import com.sap.db.jdbc.Driver fails,"Question: What time did the problem begin?
Answer: Tue, Mar 1, 2022, 12:00 AM (UTC-08:00) Pacific Time (US & Canada)

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer: 

Question: Was it working in previous DBR?
Answer: Other, don't know or not applicable

Question: Notebook URL if available
Answer: https://adb-7369653848088828.8.azuredatabricks.net/?o=7369653848088828#job/309493103359550/run/98544

Question: Cluster URL if notebook URL is not available
Answer: https://adb-7369653848088828.8.azuredatabricks.net/?o=7369653848088828#setting/clusters/0209-104622-carpi835/configuration

Question: Additional details about the issue
Answer: Importing the driver class  com.sap.db.jdbc.Driver is failing intermittently with the error message 'ClassNotFoundException: com.sap.db.jdbc.Driver'.



<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Databricks:
What time did the problem begin? - 2022-03-01T08:00:00.000Z;
Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank - ;
Was it working in previous DBR? - Other, don't know or not applicable;
Notebook URL if available - https://adb-7369653848088828.8.azuredatabricks.net/?o=7369653848088828#job/309493103359550/run/98544;
Cluster URL if notebook URL is not available - https://adb-7369653848088828.8.azuredatabricks.net/?o=7369653848088828#setting/clusters/0209-104622-carpi835/configuration;
Additional details about the issue - Importing the driver class  com.sap.db.jdbc.Driver is failing intermittently with the error message 'ClassNotFoundException: com.sap.db.jdbc.Driver'.;

- ProblemStartTime: 03/01/2022 08:00:00
- Cloud: Azure
- AzureProductSubscriptionID: a0776237-28a9-41f7-88f3-a1e65953748f
- AzureProductSubscriptionName: IT Corp - SubProd Vendor Appliances (SV)
- PUID: 100320010EE05FFD
- Tenant Id: 8bcff170-9979-491e-8683-d8ced0850bad
- Object Id: 182bc918-aab0-4c68-bca9-d69f28ad7298
- SubscriptionType: Premier
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Premier with Azure Rapid response
- GrantPermission: False
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: False

- Location: westus2
- ResourceUri: /subscriptions/a0776237-28a9-41f7-88f3-a1e65953748f/resourceGroups/sv-wus2-subprod-databricks-rg/providers/Microsoft.Databricks/workspaces/sv-wus2-subprod-databricks-ws

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs Article 07/14/2022 29 minutes to read 4 contributors In this article Create a job Run a job View jobs View runs for a job View job run details View recent job runs Export job run results Edit a job Edit a task Clone a job Clone a task Delete a job Delete a task Copy a task path Run jobs using notebooks in a remote Git repository Best practices A job is a way to run non-interactive code in an Azure Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. You can also run jobs interactively in the notebook UI. You can create and run a job using the UI, the CLI, or by invoking the Jobs API. You can repair and re-run a failed or canceled job using the UI or API. You can monitor job run results using the UI, CLI, API, and email notifications. This article focuses on performing job tasks using the UI. For the other methods, see Jobs CLI and Jobs API 2.1. Your job can consist of a single task or can be a large, multi-task workflow with complex dependencies. Azure Databricks manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs. You can run your jobs immediately or periodically through an easy-to-use scheduling system. You can implement a task in a JAR, an Azure Databricks notebook, a Delta Live Tables pipeline, or an application written in Scala, Java, or Python. Legacy Spark Submit applications are also supported. You control the execution order of tasks by specifying dependencies between the tasks. You can configure tasks to run in sequence or parallel. The following diagram illustrates a workflow that: Ingests raw clickstream data and performs processing to sessionize the records. Ingests order data and joins it with the sessionized clickstream data to create a prepared data set for analysis. Extracts features from the prepared data. Performs tasks in parallel to persist the features and train a machine learning model. To create your first workflow with an Azure Databricks job, see the quickstart. Important You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. A workspace is limited to 1000 concurrent job runs. A 429 Too Many Requests response is returned when you request a run that cannot start immediately. The number of jobs a workspace can create in an hour is limited to 5000 (includes “run now” and “runs submit”). This limit also affects jobs created by the REST API and notebook workflows. Create a job Do one of the following: Click Workflows in the sidebar and click . In the sidebar, click Create and select Job from the menu. The Tasks tab appears with the create task dialog. Replace Add a name for your job… with your job name. Enter a name for the task in the Task name field. Specify the type of task to run. In the Type drop-down, select Notebook, JAR, Spark Submit, Python, or Pipeline. Notebook: In the Source drop-down, select a location for the notebook; either Workspace for a notebook located in a Azure Databricks workspace folder or Git provider for a notebook located in a remote Git repository. Workspace: Use the file browser to find the notebook, click the notebook name, and click Confirm. Git provider: Click Edit and enter the Git repository information. See Run jobs using notebooks in a remote Git repository. JAR: Specify the Main class. Use the fully qualified name of the class containing the main method, for example, org.apache.spark.examples.SparkPi. Then click Add under Dependent Libraries to add libraries required to run the task. One of these libraries must contain the main class. To learn more about JAR tasks, see JAR jobs. Spark Submit: In the Parameters text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the DFSReadWriteTest from the Apache Spark examples: JSON Copy [""--class"",""org.apache.spark.examples.DFSReadWriteTest"",""dbfs:/FileStore/libraries/spark_examples_2_12_3_1_1.jar"",""/dbfs/databricks-datasets/README.md"",""/FileStore/examples/output/""]
 Important There are several limitations for spark-submit tasks: You can run spark-submit tasks only on new clusters. Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see Cluster autoscaling. Spark-submit does not support Databricks Utilities. To use Databricks Utilities, use JAR tasks instead. Python: In the Path textbox, enter the URI of a Python script on DBFS or cloud storage; for example, dbfs:/FileStore/myscript.py. Pipeline: In the Pipeline drop-down, select an existing Delta Live Tables pipeline. Python Wheel: In the Package name text box, enter the package to import, for example, myWheel-1.0-py2.py3-none-any.whl. In the Entry Point text box, enter the function to call when starting the wheel. Click Add under Dependent Libraries to add libraries required to run the task. Configure the cluster where the task runs. In the Cluster drop-down, select either New Job Cluster or Existing All-Purpose Clusters. New Job Cluster: Click Edit in the Cluster drop-down and complete the cluster configuration. Existing All-Purpose Cluster: Select an existing cluster in the Cluster drop-down. To open the cluster in a new page, click the icon to the right of the cluster name and description. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters. Notebook: Click Add and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the Run a job with different parameters option. Parameters set the value of the notebook widget specified by the key of the parameter. Use task parameter variables to pass a limited set of dynamic values as part of a parameter value. JAR: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See Configure JAR job parameters. Spark Submit task: Parameters are specified as a JSON-formatted array of strings. Conforming to the Apache Spark spark-submit convention, parameters after the JAR path are passed to the main method of the main class. Python: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the argparse module in Python. Python Wheel: In the Parameters drop-down, select Positional arguments to enter parameters as a JSON-formatted array of strings, or select Keyword arguments > Add to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. To access additional options, including Dependent Libraries, Retry Policy, and Timeouts, click Advanced Options. See Edit a task. Click Create. To optionally set the job’s schedule, click Edit schedule in the Job details panel. See Schedule a job. To optionally allow multiple concurrent runs of the same job, click Edit concurrent runs in the Job details panel. See Maximum concurrent runs. To optionally specify email addresses to receive notifications on job events, click Edit notifications in the Job details panel. See Notifications. To optionally control permission levels on the job, click Edit permissions in the Job details panel. See Control access to jobs. To add another task, click below the task you just created. A shared cluster option is provided if you have configured a New Job Cluster for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Run a job Click Workflows in the sidebar. Select a job and click the Runs tab. You can run a job immediately or schedule the job to run later. If one or more tasks in a job with multiple tasks are not successful, you can re-run the subset of unsuccessful tasks. See Repair an unsuccessful job run. Run a job immediately To run the job immediately, click . Tip You can perform a test run of a job with a notebook task by clicking Run Now. If you need to make changes to the notebook, clicking Run Now again after editing the notebook will automatically run the new version of the notebook. Run a job with different parameters You can use Run Now with Different Parameters to re-run a job with different parameters or different values for existing parameters. Click next to Run Now and select Run Now with Different Parameters or, in the Active Runs table, click Run Now with Different Parameters. Enter the new parameters depending on the type of task. Notebook: You can enter parameters as key-value pairs or a JSON object. The provided parameters are merged with the default parameters for the triggered run. You can use this dialog to set the values of widgets. JAR and spark-submit: You can enter a list of parameters or a JSON document. If you delete keys, the default parameters are used. You can also add task parameter variables for the run. Click Run. Repair an unsuccessful job run You can repair failed or canceled multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks and any tasks that depend on them are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs. You can change job or task settings before repairing the job run. Unsuccessful tasks are re-run with the current job and task settings. For example, if you change the path to a notebook or a cluster setting, the task is re-run with the updated notebook or cluster settings. You can view the history of all task runs on the Task run details page. Note If one or more tasks share a job cluster, a repair run creates a new job cluster; for example, if the original run used the job cluster my_job_cluster, the first repair run uses the new job cluster my_job_cluster_v1, allowing you to easily see the cluster and cluster settings used by the initial run and any repair runs. The settings for my_job_cluster_v1 are the same as the current settings for my_job_cluster. Repair is supported only with jobs that orchestrate two or more tasks. The Duration value displayed in the Runs tab includes the time the first run started until the time when the latest repair run finished. For example, if a run failed twice and succeeded on the third run, the duration includes the time for all three runs. To repair an unsuccessful job run: Click Jobs in the sidebar. In the Name column, click a job name. The Runs tab shows active runs and completed runs, including any unsuccessful runs. Click the link for the unsuccessful run in the Start time column of the Completed Runs (past 60 days) table. The Job run details page appears. Click Repair run. The Repair job run dialog appears, listing all unsuccessful tasks and any dependent tasks that will be re-run. To add or edit parameters for the tasks to repair, enter the parameters in the Repair job run dialog. Parameters you enter in the Repair job run dialog override existing values. On subsequent repair runs, you can return a parameter to its original value by clearing the key and value in the Repair job run dialog. Click Repair run in the Repair job run dialog. View task run history To view the run history of a task, including successful and unsuccessful runs: Click on a task on the Job run details page. The Task run details page appears. Select the task run in the run history drop-down. Schedule a job To define a schedule for the job: Click Edit schedule in the Job details panel and set the Schedule Type to Scheduled. Specify the period, starting time, and time zone. Optionally select the Show Cron Syntax checkbox to display and edit the schedule in Quartz Cron Syntax. Note Azure Databricks enforces a minimum interval of 10 seconds between subsequent runs triggered by the schedule of a job regardless of the seconds configuration in the cron expression. You can choose a time zone that observes daylight saving time or UTC. If you select a zone that observes daylight saving time, an hourly job will be skipped or may appear to not fire for an hour or two when daylight saving time begins or ends. To run at every hour (absolute time), choose UTC. The job scheduler is not intended for low latency jobs. Due to network or cloud issues, job runs may occasionally be delayed up to several minutes. In these situations, scheduled jobs will run immediately upon service availability. Click Save. Pause and resume a job schedule To pause a job, you can either: Click Pause in the Job details panel. Click Edit schedule in the Job details panel and set the Schedule Type to Manual (Paused) To resume a paused job schedule, set the Schedule Type to Scheduled. View jobs Click Workflows in the sidebar. The Jobs list appears. The Jobs page lists all defined jobs, the cluster definition, the schedule, if any, and the result of the last run. You can filter jobs in the Jobs list: Using keywords. Selecting only the jobs you own. Selecting all jobs you have permissions to access. Access to this filter requires that Jobs access control is enabled. Using tags. To search for a tag created with only a key, type the key into the search box. To search for a tag created with a key and value, you can search by the key, the value, or both the key and value. For example, for a tag with the key department and the value finance, you can search for department or finance to find matching jobs. To search by both the key and value, enter the key and value separated by a colon; for example, department:finance. You can also click any column header to sort the list of jobs (either descending or ascending) by that column. The default sorting is by job name in ascending order. View runs for a job Click Workflows in the sidebar. In the Name column, click a job name. The Runs tab appears with a table of active runs and completed runs. To switch to a matrix view, click Matrix. The matrix view shows a history of runs for the job, including each job task. The Job Runs row of the matrix displays the total duration of the run and the state of the run. To view details of the run, including the start time, duration, and status, hover over the bar in the Job Runs row. Each cell in the Tasks row represents a task and the corresponding status of the task. To view details of each task, including the start time, duration, cluster, and status, hover over the cell for that task. The job run and task run bars are color-coded to indicate the status of the run. Successful runs are green, unsuccessful runs are red, and skipped runs are pink. The height of the individual job run and task run bars provides a visual indication of the run duration. Azure Databricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends that you export results before they expire. For more information, see Export job run results. View job run details The job run details page contains job output and links to logs, including information about the success or failure of each task in the job run. You can access job run details from the Runs tab for the job. To view job run details from the Runs tab, click the link for the run in the Run column of the Completed Runs (past 60 days) table. To return to the Runs tab for the job, click on the Job ID value. Click on a task to view task run details, including: the cluster that ran the task the Spark UI for the task logs for the task metrics for the task Click the Job ID value to return to the Runs tab for the job. Click the Job run ID value to return to the job run details. View recent job runs You can view a list of currently running and recently completed runs for all jobs in a workspace you have access to, including runs started by external orchestration tools such as Apache Airflow or Azure Data Factory. To view the list of recent job runs: Click Workflows in the sidebar. The Jobs list appears. Click the Job runs tab. The Job runs list appears. The Job runs list displays: The start time for the run. The name of the job associated with the run. The user name that the job runs as. Whether the run was triggered by a job schedule or an API request, or was manually started. The time elapsed for a currently running job, or the total running time for a completed run. The status of the run, either Pending, Running, Skipped, Succeeded, Failed, Terminating, Terminated, Internal Error, Timed Out, Canceled, Canceling, or Waiting for Retry. To view job run details, click the link in the Start time column for the run. To view job details, click the job name in the Job column. Export job run results You can export notebook run results and job run logs for all job types. Export notebook run results You can persist job runs by exporting their results. For notebook job runs, you can export a rendered notebook that can later be imported into your Azure Databricks workspace. To export notebook run results for a job with a single task: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click Export to HTML. To export notebook run results for a job with multiple tasks: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click the notebook task to export. Click Export to HTML. Export job run logs You can also export the logs for your job run. You can set up your job to automatically deliver logs to DBFS through the Job API. See the new_cluster.cluster_log_conf object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. Edit a job Some configuration options are available on the job, and other options are available on individual tasks. For example, the maximum concurrent runs can be set on the job only, while parameters must be defined for each task. To change the configuration for a job: Click Workflows in the sidebar. In the Name column, click the job name. The side panel displays the Job details. You can change the schedule, cluster configuration, notifications, maximum number of concurrent runs, and add or change tags. If job access control is enabled, you can also edit job permissions. Tags To add labels or key:value attributes to your job, you can add tags when you edit the job. You can use tags to filter jobs in the Jobs list; for example, you can use a department tag to filter all jobs that belong to a specific department. Note Because job tags are not designed to store sensitive information such as personally identifiable information or passwords, Databricks recommends using tags for non-sensitive values only. Tags also propagate to job clusters created when a job is run, allowing you to use tags with your existing cluster monitoring. To add or edit tags, click + Tag in the Job details side panel. You can add the tag as a key and value, or a label. To add a label, enter the label in the Key field and leave the Value field empty. Clusters To see tasks associated with a cluster, hover over the cluster in the side panel. To change the cluster configuration for all associated tasks, click Configure under the cluster. To configure a new cluster for all associated tasks, click Swap under the cluster. Maximum concurrent runs The maximum number of parallel runs for this job. Azure Databricks skips the run if the job has already reached its maximum number of active runs when attempting to start a new run. Set this value higher than the default of 1 to perform multiple runs of the same job concurrently. This is useful, for example, if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or you want to trigger multiple runs that differ by their input parameters. Notifications You can add one or more email addresses to notify when runs of this job begin, complete, or fail: Click Edit notifications. Click Add. Enter an email address and click the check box for each notification type to send to that address. To enter another email address for notification, click Add. If you do not want to receive notifications for skipped job runs, click the check box. Click Confirm. Integrate these email notifications with your favorite notification tools, including: PagerDuty Slack Control access to jobs Job access control enables job owners and administrators to grant fine-grained permissions on their jobs. Job owners can choose which other users or groups can view the results of the job. Owners can also choose who can manage their job runs (Run now and Cancel run permissions). See Jobs access control for details. Edit a task To set task configuration options: Click Workflows in the sidebar. In the Name column, click the job name. Click the Tasks tab. Task dependencies You can define the order of execution of tasks in a job using the Depends on drop-down. You can set this field to one or more tasks in the job. Note Depends on is not visible if the job consists of only a single task. Configuring task dependencies creates a Directed Acyclic Graph (DAG) of task execution, a common way of representing execution order in job schedulers. For example, consider the following job consisting of four tasks: Task 1 is the root task and does not depend on any other task. Task 2 and Task 3 depend on Task 1 completing first. Finally, Task 4 depends on Task 2 and Task 3 completing successfully. Azure Databricks runs upstream tasks before running downstream tasks, running as many of them in parallel as possible. The following diagram illustrates the order of processing for these tasks: Individual task configuration options Individual tasks have the following configuration options: In this section: Cluster Dependent libraries Task parameter variables Timeout Retries Cluster To configure the cluster where a task runs, click the Cluster drop-down. You can edit a shared job cluster, but you cannot delete a shared cluster if it is still used by other tasks. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Dependent libraries Dependent libraries will be installed on the cluster before the task runs. You must set all task dependencies to ensure they are installed before the run starts. To add a dependent library, click Advanced options and select Add Dependent Libraries to open the Add Dependent Library chooser. Follow the recommendations in Library dependencies for specifying dependencies. Important If you have configured a library to install on all clusters automatically, or you select an existing terminated cluster that has libraries installed, the job execution does not wait for library installation to complete. If a job requires a specific library, you should attach the library to the job in the Dependent Libraries field. Task parameter variables You can pass templated variables into a job task as part of the task’s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job’s start time. When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named MyJobId with a value of my-job-6 for any run of job ID 6, add the following task parameter: JSON Copy {
  ""MyJobID"": ""my-job-{{job_id}}""
}
 The contents of the double curly braces are not evaluated as expressions, so you cannot do operations or functions within double-curly braces. Whitespace is not stripped inside the curly braces, so {{ job_id }} will not be evaluated. The following task parameter variables are supported: Variable Description {{job_id}} The unique identifier assigned to a job {{run_id}} The unique identifier assigned to a job run {{start_date}} The date a task run started. The format is yyyy-MM-dd in UTC timezone. {{start_time}} The timestamp of the run’s start of execution after the cluster is created and ready. The format is milliseconds since UNIX epoch in UTC timezone, as returned by System.currentTimeMillis(). {{task_retry_count}} The number of retries that have been attempted to run a task if the first attempt fails. The value is 0 for the first attempt and increments with each retry. {{parent_run_id}} The unique identifier assigned to the run of a job with multiple tasks. {{task_key}} The unique name assigned to a task that’s part of a job with multiple tasks. You can set these variables with any task when you Create a job, Edit a job, or Run a job with different parameters. Timeout The maximum completion time for a job. If the job does not complete in this time, Azure Databricks sets its status to “Timed Out”. Retries A policy that determines when and how many times failed runs are retried. To set the retries for the task, click Advanced options and select Edit Retry Policy. The retry interval is calculated in milliseconds between the start of the failed run and the subsequent retry run. Note If you configure both Timeout and Retries, the timeout applies to each retry. Clone a job You can quickly create a new job by cloning an existing job. Cloning a job creates an identical copy of the job, except for the job ID. On the job’s page, click More … next to the job’s name and select Clone from the drop-down menu. Clone a task You can quickly create a new task by cloning an existing task: On the job’s page, click the Tasks tab. Select the task to clone. Click and select Clone task. Delete a job To delete a job, on the job’s page, click More … next to the job’s name and select Delete from the drop-down menu. Delete a task To delete a task: Click the Tasks tab. Select the task to be deleted. Click and select Remove task. Copy a task path To copy the path to a task, for example, a notebook path: Click the Tasks tab. Select the task containing the path to copy. Click next to the task path to copy the path to the clipboard. Run jobs using notebooks in a remote Git repository Important This feature is in Public Preview. You can run jobs with notebooks located in a remote Git repository. This feature simplifies creation and management of production jobs and automates continuous deployment: You don’t need to create a separate production repo in Azure Databricks, manage permissions for it, and keep it updated. You can prevent unintentional changes to a production job, such as local edits in the production repo or changes from switching a branch. The job definition process has a single source of truth in the remote repository. To use notebooks in a remote Git repository, you must Set up Git integration with Databricks Repos. To create a task with a notebook located in a remote Git repository: In the Type drop-down, select Notebook. In the Source drop-down, select Git provider. The Git information dialog appears. In the Git Information dialog, enter details for the repository. For Path, enter a relative path to the notebook location, such as etl/notebooks/. When you enter the relative path, don’t begin it with / or ./ and don’t include the notebook file extension, such as .py. Additional notebook tasks in a multitask job reference the same commit in the remote repository in one of the following ways: sha of $branch/head when git_branch is set sha of $tag when git_tag is set the value of git_commit In a multitask job, there cannot be a task that uses a local notebook and another task that uses a remote repository. This restriction doesn’t apply to non-notebook tasks. Best practices In this section: Cluster configuration tips Notebook job tips Streaming tasks JAR jobs Library dependencies Cluster configuration tips Cluster configuration is important when you operationalize a job. The following provides general guidance on choosing and configuring job clusters, followed by recommendations for specific job types. Use shared job clusters To optimize resource usage with jobs that orchestrate multiple tasks, use shared job clusters. A shared job cluster allows multiple tasks in the same job run to reuse the cluster. You can use a single job cluster to run all tasks that are part of the job, or multiple job clusters optimized for specific workloads. To use a shared job cluster: Select New Job Clusters when you create a task and complete the cluster configuration. Select the new cluster when adding a task to the job, or create a new job cluster. Any cluster you configure when you select New Job Clusters is available to any task in the job. A shared job cluster is scoped to a single job run, and cannot be used by other jobs or runs of the same job. Libraries cannot be declared in a shared job cluster configuration. You must add dependent libraries in task settings. Choose the correct cluster type for your job New Job Clusters are dedicated clusters for a job or task run. A shared job cluster is created and started when the first task using the cluster starts and terminates after the last task using the cluster completes. The cluster is not terminated when idle but terminates only after all tasks using it have completed. If a shared job cluster fails or is terminated before all tasks have finished, a new cluster is created. A cluster scoped to a single task is created and started when the task starts and terminates when the task completes. In production, Databricks recommends using new shared or task scoped clusters so that each job or task runs in a fully isolated environment. When you run a task on a new cluster, the task is treated as a data engineering (task) workload, subject to the task workload pricing. When you run a task on an existing all-purpose cluster, the task is treated as a data analytics (all-purpose) workload, subject to all-purpose workload pricing. If you select a terminated existing cluster and the job owner has Can Restart permission, Azure Databricks starts the cluster when the job is scheduled to run. Existing all-purpose clusters work best for tasks such as updating dashboards at regular intervals. Use a pool to reduce cluster start times To decrease new job cluster start time, create a pool and configure the job’s cluster to use the pool. Notebook job tips Total notebook cell output (the combined output of all notebook cells) is subject to a 20MB size limit. Additionally, individual cell output is subject to an 8MB size limit. If total cell output exceeds 20MB in size, or if the output of an individual cell is larger than 8MB, the run is canceled and marked as failed. If you need help finding cells near or beyond the limit, run the notebook against an all-purpose cluster and use this notebook autosave technique. Streaming tasks Spark Streaming jobs should never have maximum concurrent runs set to greater than 1. Streaming jobs should be set to run using the cron expression ""* * * * * ?"" (every minute). Since a streaming task runs continuously, it should always be the final task in a job. JAR jobs When running a JAR job, keep in mind the following: Output size limits Note Available in Databricks Runtime 6.3 and above. Job output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run is canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to Azure Databricks by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default, the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster’s log files. Setting this flag is recommended only for job clusters for JAR jobs because it will disable notebook results. Use the shared SparkContext Because Azure Databricks is a managed service, some code changes may be necessary to ensure that your Apache Spark jobs run correctly. JAR job programs must use the shared SparkContext API to get the SparkContext. Because Azure Databricks initializes the SparkContext, programs that invoke new SparkContext() will fail. To get the SparkContext, use only the shared SparkContext created by Azure Databricks: Scala Copy val goodSparkContext = SparkContext.getOrCreate()
val goodSparkSession = SparkSession.builder().getOrCreate()
 There are also several methods you should avoid when using the shared SparkContext. Do not call SparkContext.stop(). Do not call System.exit(0) or sc.stop() at the end of your Main program. This can cause undefined behavior. Use try-finally blocks for job clean up Consider a JAR that consists of two parts: jobBody() which contains the main part of the job. jobCleanup() which has to be executed after jobBody() whether that function succeeded or returned an exception. As an example, jobBody() may create tables, and you can use jobCleanup() to drop these tables. The safe way to ensure that the clean up method is called is to put a try-finally block in the code: Scala Copy try {
  jobBody()
} finally {
  jobCleanup()
}
 You should not try to clean up using sys.addShutdownHook(jobCleanup) or the following code: Scala Copy val cleanupThread = new Thread { override def run = jobCleanup() }
Runtime.getRuntime.addShutdownHook(cleanupThread)
 Due to the way the lifetime of Spark containers is managed in Azure Databricks, the shutdown hooks are not run reliably. Configure JAR job parameters You pass parameters to JAR jobs with a JSON string array. See the spark_jar_task object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. To access these parameters, inspect the String array passed into your main function. Library dependencies The Spark driver has certain library dependencies that cannot be overridden. These libraries take priority over any of your libraries that conflict with them. To get the full list of the driver library dependencies, run the following command inside a notebook attached to a cluster of the same Spark version (or the cluster with the driver you want to examine). Bash Copy %sh
ls /databricks/jars
 Manage library dependencies A good rule of thumb when dealing with library dependencies while creating JARs for jobs is to list Spark and Hadoop as provided dependencies. On Maven, add Spark and Hadoop as provided dependencies, as shown in the following example: XML Copy <dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.3.0</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-core</artifactId>
  <version>1.2.1</version>
  <scope>provided</scope>
</dependency>
 In sbt, add Spark and Hadoop as provided dependencies, as shown in the following example: Scala Copy libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.3.0"" % ""provided""
libraryDependencies += ""org.apache.hadoop"" %% ""hadoop-core"" % ""1.2.1"" % ""provided""
 Tip Specify the correct Scala version for your dependencies based on the version you are running.",Jobs
30,Jobs are timing out on job-cluster after running approximately 17 mins,"Hi,

We are observing this strange issue with the job cluster, our jobs are timing out approximately 17 mins after the job-cluster is started

This is using the recently created policy - MSCAT-LTS9.1-Job-70577(this is newly created, we have 0 successful runs with this)

 

https://adb-5929987106376936.16.azuredatabricks.net/?o=5929987106376936#job/872884905370457/run/221690

https://adb-5929987106376936.16.azuredatabricks.net/?o=5929987106376936#job/150659825046105/run/220864

https://adb-5929987106376936.16.azuredatabricks.net/?o=5929987106376936#job/632779428770523/run/224598

https://adb-5929987106376936.16.azuredatabricks.net/?o=5929987106376936#job/642781312230219/run/225451

https://adb-5929987106376936.16.azuredatabricks.net/?o=5929987106376936#job/969812056099066/run/275388

https://adb-5929987106376936.16.azuredatabricks.net/?o=5929987106376936#job/1037341076937862/run/358118

However, the same code is running on all-purpose cluster

https://adb-5929987106376936.16.azuredatabricks.net/?o=5929987106376936#job/437344657870808/run/262099

 

There is no useful information in the log as well indicating what is the problem

 

22/06/07 18:21:56 INFO DriverCorral$: Cleaning the wrapper ReplId-16bba-dc012-a95c7-3 (currently in status Running(ReplId-16bba-dc012-a95c7-3,ExecutionId(job-150659825046105-run-220864-action-5197158232281529),RunnableCommandId(6119462978228379392)))

22/06/07 18:21:56 INFO DAGScheduler: Asked to cancel job group 1638093929937722483_6119462978228379392_job-150659825046105-run-220864-action-5197158232281529

22/06/07 18:21:56 INFO ScalaDriverLocal: cancelled jobGroup:1638093929937722483_6119462978228379392_job-150659825046105-run-220864-action-5197158232281529

22/06/07 18:21:56 INFO ScalaDriverWrapper: Stopping streams for commandId pattern: CommandIdPattern(1638093929937722483,None,Some(job-150659825046105-run-220864-action-5197158232281529)).

22/06/07 18:21:58 ERROR TaskSchedulerImpl: Lost executor 4 on 100.101.4.27: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.

22/06/07 18:21:58 INFO DAGScheduler: Executor lost: 4 (epoch 0)

22/06/07 18:21:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220607181120-0000/4 is now LOST (worker lost)

22/06/07 18:21:58 INFO BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.

22/06/07 18:21:58 INFO StandaloneSchedulerBackend: Executor app-20220607181120-0000/4 removed: worker lost

22/06/07 18:21:58 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(4, 100.101.4.27, 45027, None)

22/06/07 18:21:58 INFO BlockManagerMasterEndpoint: Trying to remove executor 4 from BlockManagerMaster.

22/06/07 18:21:58 INFO BlockManagerMaster: Removal of executor 4 requested

22/06/07 18:21:58 INFO BlockManagerMaster: Removed 4 successfully in removeExecutor

22/06/07 18:21:58 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asked to remove non-existent executor 4

22/06/07 18:21:58 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20220607181124-100.101.4.27-33125: 100.101.4.27:33125 got disassociated

22/06/07 18:21:58 INFO StandaloneSchedulerBackend: Worker worker-20220607181124-100.101.4.27-33125 removed: 100.101.4.27:33125 got disassociated

22/06/07 18:21:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20220607181120-0000/10 is now LOST (worker lost)

22/06/07 18:21:58 INFO StandaloneSchedulerBackend: Executor app-20220607181120-0000/10 removed: worker lost

22/06/07 18:21:58 INFO StandaloneAppClient$ClientEndpoint: Master removed worker worker-20220607181125-100.101.4.20-34213: 100.101.4.20:34213 got disassociated

22/06/07 18:21:58 INFO StandaloneSchedulerBackend: Worker worker-20220607181125-100.101.4.20-34213 removed: 100.101.4.20:34213 got disassociated

22/06/07 18:21:58 INFO TaskSchedulerImpl: Handle removed worker worker-20220607181124-100.101.4.27-33125: 100.101.4.27:33125 got disassociated

22/06/07 18:21:58 ERROR TaskSchedulerImpl: Lost executor 10 on 100.101.4.20: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.

We tried different jar(s) as well but it is timing out for all in approx 17 mins.

Thanks

Soumyajit

Problem start date and time
Mon, Jun 6, 2022, 10:15 AM (UTC-05:00) Eastern Time (US & Canada)


<Start:Agent_Additional_Properties_Do_Not_Edit>
- ProblemStartTime: 06/06/2022 14:15:00
- Cloud: Azure
- AzureProductSubscriptionID: c21fe596-f75a-4473-954d-0de4583a983c
- AzureProductSubscriptionName: isg-dev-genpop-g001
- PUID: 10037FFEAB561FBB
- Tenant Id: e29b8111-49f8-418d-ac2a-935335a52614
- Object Id: de72070d-3d64-47e8-9cd5-9202421cdb43
- SubscriptionType: Premier
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Premier with Azure Rapid response
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: False

- ResourceUri: 

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs Article 07/14/2022 29 minutes to read 4 contributors In this article Create a job Run a job View jobs View runs for a job View job run details View recent job runs Export job run results Edit a job Edit a task Clone a job Clone a task Delete a job Delete a task Copy a task path Run jobs using notebooks in a remote Git repository Best practices A job is a way to run non-interactive code in an Azure Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. You can also run jobs interactively in the notebook UI. You can create and run a job using the UI, the CLI, or by invoking the Jobs API. You can repair and re-run a failed or canceled job using the UI or API. You can monitor job run results using the UI, CLI, API, and email notifications. This article focuses on performing job tasks using the UI. For the other methods, see Jobs CLI and Jobs API 2.1. Your job can consist of a single task or can be a large, multi-task workflow with complex dependencies. Azure Databricks manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs. You can run your jobs immediately or periodically through an easy-to-use scheduling system. You can implement a task in a JAR, an Azure Databricks notebook, a Delta Live Tables pipeline, or an application written in Scala, Java, or Python. Legacy Spark Submit applications are also supported. You control the execution order of tasks by specifying dependencies between the tasks. You can configure tasks to run in sequence or parallel. The following diagram illustrates a workflow that: Ingests raw clickstream data and performs processing to sessionize the records. Ingests order data and joins it with the sessionized clickstream data to create a prepared data set for analysis. Extracts features from the prepared data. Performs tasks in parallel to persist the features and train a machine learning model. To create your first workflow with an Azure Databricks job, see the quickstart. Important You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. A workspace is limited to 1000 concurrent job runs. A 429 Too Many Requests response is returned when you request a run that cannot start immediately. The number of jobs a workspace can create in an hour is limited to 5000 (includes “run now” and “runs submit”). This limit also affects jobs created by the REST API and notebook workflows. Create a job Do one of the following: Click Workflows in the sidebar and click . In the sidebar, click Create and select Job from the menu. The Tasks tab appears with the create task dialog. Replace Add a name for your job… with your job name. Enter a name for the task in the Task name field. Specify the type of task to run. In the Type drop-down, select Notebook, JAR, Spark Submit, Python, or Pipeline. Notebook: In the Source drop-down, select a location for the notebook; either Workspace for a notebook located in a Azure Databricks workspace folder or Git provider for a notebook located in a remote Git repository. Workspace: Use the file browser to find the notebook, click the notebook name, and click Confirm. Git provider: Click Edit and enter the Git repository information. See Run jobs using notebooks in a remote Git repository. JAR: Specify the Main class. Use the fully qualified name of the class containing the main method, for example, org.apache.spark.examples.SparkPi. Then click Add under Dependent Libraries to add libraries required to run the task. One of these libraries must contain the main class. To learn more about JAR tasks, see JAR jobs. Spark Submit: In the Parameters text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the DFSReadWriteTest from the Apache Spark examples: JSON Copy [""--class"",""org.apache.spark.examples.DFSReadWriteTest"",""dbfs:/FileStore/libraries/spark_examples_2_12_3_1_1.jar"",""/dbfs/databricks-datasets/README.md"",""/FileStore/examples/output/""]
 Important There are several limitations for spark-submit tasks: You can run spark-submit tasks only on new clusters. Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see Cluster autoscaling. Spark-submit does not support Databricks Utilities. To use Databricks Utilities, use JAR tasks instead. Python: In the Path textbox, enter the URI of a Python script on DBFS or cloud storage; for example, dbfs:/FileStore/myscript.py. Pipeline: In the Pipeline drop-down, select an existing Delta Live Tables pipeline. Python Wheel: In the Package name text box, enter the package to import, for example, myWheel-1.0-py2.py3-none-any.whl. In the Entry Point text box, enter the function to call when starting the wheel. Click Add under Dependent Libraries to add libraries required to run the task. Configure the cluster where the task runs. In the Cluster drop-down, select either New Job Cluster or Existing All-Purpose Clusters. New Job Cluster: Click Edit in the Cluster drop-down and complete the cluster configuration. Existing All-Purpose Cluster: Select an existing cluster in the Cluster drop-down. To open the cluster in a new page, click the icon to the right of the cluster name and description. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters. Notebook: Click Add and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the Run a job with different parameters option. Parameters set the value of the notebook widget specified by the key of the parameter. Use task parameter variables to pass a limited set of dynamic values as part of a parameter value. JAR: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See Configure JAR job parameters. Spark Submit task: Parameters are specified as a JSON-formatted array of strings. Conforming to the Apache Spark spark-submit convention, parameters after the JAR path are passed to the main method of the main class. Python: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the argparse module in Python. Python Wheel: In the Parameters drop-down, select Positional arguments to enter parameters as a JSON-formatted array of strings, or select Keyword arguments > Add to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. To access additional options, including Dependent Libraries, Retry Policy, and Timeouts, click Advanced Options. See Edit a task. Click Create. To optionally set the job’s schedule, click Edit schedule in the Job details panel. See Schedule a job. To optionally allow multiple concurrent runs of the same job, click Edit concurrent runs in the Job details panel. See Maximum concurrent runs. To optionally specify email addresses to receive notifications on job events, click Edit notifications in the Job details panel. See Notifications. To optionally control permission levels on the job, click Edit permissions in the Job details panel. See Control access to jobs. To add another task, click below the task you just created. A shared cluster option is provided if you have configured a New Job Cluster for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Run a job Click Workflows in the sidebar. Select a job and click the Runs tab. You can run a job immediately or schedule the job to run later. If one or more tasks in a job with multiple tasks are not successful, you can re-run the subset of unsuccessful tasks. See Repair an unsuccessful job run. Run a job immediately To run the job immediately, click . Tip You can perform a test run of a job with a notebook task by clicking Run Now. If you need to make changes to the notebook, clicking Run Now again after editing the notebook will automatically run the new version of the notebook. Run a job with different parameters You can use Run Now with Different Parameters to re-run a job with different parameters or different values for existing parameters. Click next to Run Now and select Run Now with Different Parameters or, in the Active Runs table, click Run Now with Different Parameters. Enter the new parameters depending on the type of task. Notebook: You can enter parameters as key-value pairs or a JSON object. The provided parameters are merged with the default parameters for the triggered run. You can use this dialog to set the values of widgets. JAR and spark-submit: You can enter a list of parameters or a JSON document. If you delete keys, the default parameters are used. You can also add task parameter variables for the run. Click Run. Repair an unsuccessful job run You can repair failed or canceled multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks and any tasks that depend on them are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs. You can change job or task settings before repairing the job run. Unsuccessful tasks are re-run with the current job and task settings. For example, if you change the path to a notebook or a cluster setting, the task is re-run with the updated notebook or cluster settings. You can view the history of all task runs on the Task run details page. Note If one or more tasks share a job cluster, a repair run creates a new job cluster; for example, if the original run used the job cluster my_job_cluster, the first repair run uses the new job cluster my_job_cluster_v1, allowing you to easily see the cluster and cluster settings used by the initial run and any repair runs. The settings for my_job_cluster_v1 are the same as the current settings for my_job_cluster. Repair is supported only with jobs that orchestrate two or more tasks. The Duration value displayed in the Runs tab includes the time the first run started until the time when the latest repair run finished. For example, if a run failed twice and succeeded on the third run, the duration includes the time for all three runs. To repair an unsuccessful job run: Click Jobs in the sidebar. In the Name column, click a job name. The Runs tab shows active runs and completed runs, including any unsuccessful runs. Click the link for the unsuccessful run in the Start time column of the Completed Runs (past 60 days) table. The Job run details page appears. Click Repair run. The Repair job run dialog appears, listing all unsuccessful tasks and any dependent tasks that will be re-run. To add or edit parameters for the tasks to repair, enter the parameters in the Repair job run dialog. Parameters you enter in the Repair job run dialog override existing values. On subsequent repair runs, you can return a parameter to its original value by clearing the key and value in the Repair job run dialog. Click Repair run in the Repair job run dialog. View task run history To view the run history of a task, including successful and unsuccessful runs: Click on a task on the Job run details page. The Task run details page appears. Select the task run in the run history drop-down. Schedule a job To define a schedule for the job: Click Edit schedule in the Job details panel and set the Schedule Type to Scheduled. Specify the period, starting time, and time zone. Optionally select the Show Cron Syntax checkbox to display and edit the schedule in Quartz Cron Syntax. Note Azure Databricks enforces a minimum interval of 10 seconds between subsequent runs triggered by the schedule of a job regardless of the seconds configuration in the cron expression. You can choose a time zone that observes daylight saving time or UTC. If you select a zone that observes daylight saving time, an hourly job will be skipped or may appear to not fire for an hour or two when daylight saving time begins or ends. To run at every hour (absolute time), choose UTC. The job scheduler is not intended for low latency jobs. Due to network or cloud issues, job runs may occasionally be delayed up to several minutes. In these situations, scheduled jobs will run immediately upon service availability. Click Save. Pause and resume a job schedule To pause a job, you can either: Click Pause in the Job details panel. Click Edit schedule in the Job details panel and set the Schedule Type to Manual (Paused) To resume a paused job schedule, set the Schedule Type to Scheduled. View jobs Click Workflows in the sidebar. The Jobs list appears. The Jobs page lists all defined jobs, the cluster definition, the schedule, if any, and the result of the last run. You can filter jobs in the Jobs list: Using keywords. Selecting only the jobs you own. Selecting all jobs you have permissions to access. Access to this filter requires that Jobs access control is enabled. Using tags. To search for a tag created with only a key, type the key into the search box. To search for a tag created with a key and value, you can search by the key, the value, or both the key and value. For example, for a tag with the key department and the value finance, you can search for department or finance to find matching jobs. To search by both the key and value, enter the key and value separated by a colon; for example, department:finance. You can also click any column header to sort the list of jobs (either descending or ascending) by that column. The default sorting is by job name in ascending order. View runs for a job Click Workflows in the sidebar. In the Name column, click a job name. The Runs tab appears with a table of active runs and completed runs. To switch to a matrix view, click Matrix. The matrix view shows a history of runs for the job, including each job task. The Job Runs row of the matrix displays the total duration of the run and the state of the run. To view details of the run, including the start time, duration, and status, hover over the bar in the Job Runs row. Each cell in the Tasks row represents a task and the corresponding status of the task. To view details of each task, including the start time, duration, cluster, and status, hover over the cell for that task. The job run and task run bars are color-coded to indicate the status of the run. Successful runs are green, unsuccessful runs are red, and skipped runs are pink. The height of the individual job run and task run bars provides a visual indication of the run duration. Azure Databricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends that you export results before they expire. For more information, see Export job run results. View job run details The job run details page contains job output and links to logs, including information about the success or failure of each task in the job run. You can access job run details from the Runs tab for the job. To view job run details from the Runs tab, click the link for the run in the Run column of the Completed Runs (past 60 days) table. To return to the Runs tab for the job, click on the Job ID value. Click on a task to view task run details, including: the cluster that ran the task the Spark UI for the task logs for the task metrics for the task Click the Job ID value to return to the Runs tab for the job. Click the Job run ID value to return to the job run details. View recent job runs You can view a list of currently running and recently completed runs for all jobs in a workspace you have access to, including runs started by external orchestration tools such as Apache Airflow or Azure Data Factory. To view the list of recent job runs: Click Workflows in the sidebar. The Jobs list appears. Click the Job runs tab. The Job runs list appears. The Job runs list displays: The start time for the run. The name of the job associated with the run. The user name that the job runs as. Whether the run was triggered by a job schedule or an API request, or was manually started. The time elapsed for a currently running job, or the total running time for a completed run. The status of the run, either Pending, Running, Skipped, Succeeded, Failed, Terminating, Terminated, Internal Error, Timed Out, Canceled, Canceling, or Waiting for Retry. To view job run details, click the link in the Start time column for the run. To view job details, click the job name in the Job column. Export job run results You can export notebook run results and job run logs for all job types. Export notebook run results You can persist job runs by exporting their results. For notebook job runs, you can export a rendered notebook that can later be imported into your Azure Databricks workspace. To export notebook run results for a job with a single task: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click Export to HTML. To export notebook run results for a job with multiple tasks: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click the notebook task to export. Click Export to HTML. Export job run logs You can also export the logs for your job run. You can set up your job to automatically deliver logs to DBFS through the Job API. See the new_cluster.cluster_log_conf object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. Edit a job Some configuration options are available on the job, and other options are available on individual tasks. For example, the maximum concurrent runs can be set on the job only, while parameters must be defined for each task. To change the configuration for a job: Click Workflows in the sidebar. In the Name column, click the job name. The side panel displays the Job details. You can change the schedule, cluster configuration, notifications, maximum number of concurrent runs, and add or change tags. If job access control is enabled, you can also edit job permissions. Tags To add labels or key:value attributes to your job, you can add tags when you edit the job. You can use tags to filter jobs in the Jobs list; for example, you can use a department tag to filter all jobs that belong to a specific department. Note Because job tags are not designed to store sensitive information such as personally identifiable information or passwords, Databricks recommends using tags for non-sensitive values only. Tags also propagate to job clusters created when a job is run, allowing you to use tags with your existing cluster monitoring. To add or edit tags, click + Tag in the Job details side panel. You can add the tag as a key and value, or a label. To add a label, enter the label in the Key field and leave the Value field empty. Clusters To see tasks associated with a cluster, hover over the cluster in the side panel. To change the cluster configuration for all associated tasks, click Configure under the cluster. To configure a new cluster for all associated tasks, click Swap under the cluster. Maximum concurrent runs The maximum number of parallel runs for this job. Azure Databricks skips the run if the job has already reached its maximum number of active runs when attempting to start a new run. Set this value higher than the default of 1 to perform multiple runs of the same job concurrently. This is useful, for example, if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or you want to trigger multiple runs that differ by their input parameters. Notifications You can add one or more email addresses to notify when runs of this job begin, complete, or fail: Click Edit notifications. Click Add. Enter an email address and click the check box for each notification type to send to that address. To enter another email address for notification, click Add. If you do not want to receive notifications for skipped job runs, click the check box. Click Confirm. Integrate these email notifications with your favorite notification tools, including: PagerDuty Slack Control access to jobs Job access control enables job owners and administrators to grant fine-grained permissions on their jobs. Job owners can choose which other users or groups can view the results of the job. Owners can also choose who can manage their job runs (Run now and Cancel run permissions). See Jobs access control for details. Edit a task To set task configuration options: Click Workflows in the sidebar. In the Name column, click the job name. Click the Tasks tab. Task dependencies You can define the order of execution of tasks in a job using the Depends on drop-down. You can set this field to one or more tasks in the job. Note Depends on is not visible if the job consists of only a single task. Configuring task dependencies creates a Directed Acyclic Graph (DAG) of task execution, a common way of representing execution order in job schedulers. For example, consider the following job consisting of four tasks: Task 1 is the root task and does not depend on any other task. Task 2 and Task 3 depend on Task 1 completing first. Finally, Task 4 depends on Task 2 and Task 3 completing successfully. Azure Databricks runs upstream tasks before running downstream tasks, running as many of them in parallel as possible. The following diagram illustrates the order of processing for these tasks: Individual task configuration options Individual tasks have the following configuration options: In this section: Cluster Dependent libraries Task parameter variables Timeout Retries Cluster To configure the cluster where a task runs, click the Cluster drop-down. You can edit a shared job cluster, but you cannot delete a shared cluster if it is still used by other tasks. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Dependent libraries Dependent libraries will be installed on the cluster before the task runs. You must set all task dependencies to ensure they are installed before the run starts. To add a dependent library, click Advanced options and select Add Dependent Libraries to open the Add Dependent Library chooser. Follow the recommendations in Library dependencies for specifying dependencies. Important If you have configured a library to install on all clusters automatically, or you select an existing terminated cluster that has libraries installed, the job execution does not wait for library installation to complete. If a job requires a specific library, you should attach the library to the job in the Dependent Libraries field. Task parameter variables You can pass templated variables into a job task as part of the task’s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job’s start time. When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named MyJobId with a value of my-job-6 for any run of job ID 6, add the following task parameter: JSON Copy {
  ""MyJobID"": ""my-job-{{job_id}}""
}
 The contents of the double curly braces are not evaluated as expressions, so you cannot do operations or functions within double-curly braces. Whitespace is not stripped inside the curly braces, so {{ job_id }} will not be evaluated. The following task parameter variables are supported: Variable Description {{job_id}} The unique identifier assigned to a job {{run_id}} The unique identifier assigned to a job run {{start_date}} The date a task run started. The format is yyyy-MM-dd in UTC timezone. {{start_time}} The timestamp of the run’s start of execution after the cluster is created and ready. The format is milliseconds since UNIX epoch in UTC timezone, as returned by System.currentTimeMillis(). {{task_retry_count}} The number of retries that have been attempted to run a task if the first attempt fails. The value is 0 for the first attempt and increments with each retry. {{parent_run_id}} The unique identifier assigned to the run of a job with multiple tasks. {{task_key}} The unique name assigned to a task that’s part of a job with multiple tasks. You can set these variables with any task when you Create a job, Edit a job, or Run a job with different parameters. Timeout The maximum completion time for a job. If the job does not complete in this time, Azure Databricks sets its status to “Timed Out”. Retries A policy that determines when and how many times failed runs are retried. To set the retries for the task, click Advanced options and select Edit Retry Policy. The retry interval is calculated in milliseconds between the start of the failed run and the subsequent retry run. Note If you configure both Timeout and Retries, the timeout applies to each retry. Clone a job You can quickly create a new job by cloning an existing job. Cloning a job creates an identical copy of the job, except for the job ID. On the job’s page, click More … next to the job’s name and select Clone from the drop-down menu. Clone a task You can quickly create a new task by cloning an existing task: On the job’s page, click the Tasks tab. Select the task to clone. Click and select Clone task. Delete a job To delete a job, on the job’s page, click More … next to the job’s name and select Delete from the drop-down menu. Delete a task To delete a task: Click the Tasks tab. Select the task to be deleted. Click and select Remove task. Copy a task path To copy the path to a task, for example, a notebook path: Click the Tasks tab. Select the task containing the path to copy. Click next to the task path to copy the path to the clipboard. Run jobs using notebooks in a remote Git repository Important This feature is in Public Preview. You can run jobs with notebooks located in a remote Git repository. This feature simplifies creation and management of production jobs and automates continuous deployment: You don’t need to create a separate production repo in Azure Databricks, manage permissions for it, and keep it updated. You can prevent unintentional changes to a production job, such as local edits in the production repo or changes from switching a branch. The job definition process has a single source of truth in the remote repository. To use notebooks in a remote Git repository, you must Set up Git integration with Databricks Repos. To create a task with a notebook located in a remote Git repository: In the Type drop-down, select Notebook. In the Source drop-down, select Git provider. The Git information dialog appears. In the Git Information dialog, enter details for the repository. For Path, enter a relative path to the notebook location, such as etl/notebooks/. When you enter the relative path, don’t begin it with / or ./ and don’t include the notebook file extension, such as .py. Additional notebook tasks in a multitask job reference the same commit in the remote repository in one of the following ways: sha of $branch/head when git_branch is set sha of $tag when git_tag is set the value of git_commit In a multitask job, there cannot be a task that uses a local notebook and another task that uses a remote repository. This restriction doesn’t apply to non-notebook tasks. Best practices In this section: Cluster configuration tips Notebook job tips Streaming tasks JAR jobs Library dependencies Cluster configuration tips Cluster configuration is important when you operationalize a job. The following provides general guidance on choosing and configuring job clusters, followed by recommendations for specific job types. Use shared job clusters To optimize resource usage with jobs that orchestrate multiple tasks, use shared job clusters. A shared job cluster allows multiple tasks in the same job run to reuse the cluster. You can use a single job cluster to run all tasks that are part of the job, or multiple job clusters optimized for specific workloads. To use a shared job cluster: Select New Job Clusters when you create a task and complete the cluster configuration. Select the new cluster when adding a task to the job, or create a new job cluster. Any cluster you configure when you select New Job Clusters is available to any task in the job. A shared job cluster is scoped to a single job run, and cannot be used by other jobs or runs of the same job. Libraries cannot be declared in a shared job cluster configuration. You must add dependent libraries in task settings. Choose the correct cluster type for your job New Job Clusters are dedicated clusters for a job or task run. A shared job cluster is created and started when the first task using the cluster starts and terminates after the last task using the cluster completes. The cluster is not terminated when idle but terminates only after all tasks using it have completed. If a shared job cluster fails or is terminated before all tasks have finished, a new cluster is created. A cluster scoped to a single task is created and started when the task starts and terminates when the task completes. In production, Databricks recommends using new shared or task scoped clusters so that each job or task runs in a fully isolated environment. When you run a task on a new cluster, the task is treated as a data engineering (task) workload, subject to the task workload pricing. When you run a task on an existing all-purpose cluster, the task is treated as a data analytics (all-purpose) workload, subject to all-purpose workload pricing. If you select a terminated existing cluster and the job owner has Can Restart permission, Azure Databricks starts the cluster when the job is scheduled to run. Existing all-purpose clusters work best for tasks such as updating dashboards at regular intervals. Use a pool to reduce cluster start times To decrease new job cluster start time, create a pool and configure the job’s cluster to use the pool. Notebook job tips Total notebook cell output (the combined output of all notebook cells) is subject to a 20MB size limit. Additionally, individual cell output is subject to an 8MB size limit. If total cell output exceeds 20MB in size, or if the output of an individual cell is larger than 8MB, the run is canceled and marked as failed. If you need help finding cells near or beyond the limit, run the notebook against an all-purpose cluster and use this notebook autosave technique. Streaming tasks Spark Streaming jobs should never have maximum concurrent runs set to greater than 1. Streaming jobs should be set to run using the cron expression ""* * * * * ?"" (every minute). Since a streaming task runs continuously, it should always be the final task in a job. JAR jobs When running a JAR job, keep in mind the following: Output size limits Note Available in Databricks Runtime 6.3 and above. Job output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run is canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to Azure Databricks by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default, the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster’s log files. Setting this flag is recommended only for job clusters for JAR jobs because it will disable notebook results. Use the shared SparkContext Because Azure Databricks is a managed service, some code changes may be necessary to ensure that your Apache Spark jobs run correctly. JAR job programs must use the shared SparkContext API to get the SparkContext. Because Azure Databricks initializes the SparkContext, programs that invoke new SparkContext() will fail. To get the SparkContext, use only the shared SparkContext created by Azure Databricks: Scala Copy val goodSparkContext = SparkContext.getOrCreate()
val goodSparkSession = SparkSession.builder().getOrCreate()
 There are also several methods you should avoid when using the shared SparkContext. Do not call SparkContext.stop(). Do not call System.exit(0) or sc.stop() at the end of your Main program. This can cause undefined behavior. Use try-finally blocks for job clean up Consider a JAR that consists of two parts: jobBody() which contains the main part of the job. jobCleanup() which has to be executed after jobBody() whether that function succeeded or returned an exception. As an example, jobBody() may create tables, and you can use jobCleanup() to drop these tables. The safe way to ensure that the clean up method is called is to put a try-finally block in the code: Scala Copy try {
  jobBody()
} finally {
  jobCleanup()
}
 You should not try to clean up using sys.addShutdownHook(jobCleanup) or the following code: Scala Copy val cleanupThread = new Thread { override def run = jobCleanup() }
Runtime.getRuntime.addShutdownHook(cleanupThread)
 Due to the way the lifetime of Spark containers is managed in Azure Databricks, the shutdown hooks are not run reliably. Configure JAR job parameters You pass parameters to JAR jobs with a JSON string array. See the spark_jar_task object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. To access these parameters, inspect the String array passed into your main function. Library dependencies The Spark driver has certain library dependencies that cannot be overridden. These libraries take priority over any of your libraries that conflict with them. To get the full list of the driver library dependencies, run the following command inside a notebook attached to a cluster of the same Spark version (or the cluster with the driver you want to examine). Bash Copy %sh
ls /databricks/jars
 Manage library dependencies A good rule of thumb when dealing with library dependencies while creating JARs for jobs is to list Spark and Hadoop as provided dependencies. On Maven, add Spark and Hadoop as provided dependencies, as shown in the following example: XML Copy <dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.3.0</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-core</artifactId>
  <version>1.2.1</version>
  <scope>provided</scope>
</dependency>
 In sbt, add Spark and Hadoop as provided dependencies, as shown in the following example: Scala Copy libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.3.0"" % ""provided""
libraryDependencies += ""org.apache.hadoop"" %% ""hadoop-core"" % ""1.2.1"" % ""provided""
 Tip Specify the correct Scala version for your dependencies based on the version you are running.",Jobs
31,ARR| Caused by: com.microsoft.azure.eventhubs.ReceiverDisconnectedException| 2206100030001353,"Caused by: com.microsoft.azure.eventhubs.ReceiverDisconnectedException

Question: What time did the problem begin?
Answer: Thu, Jun 2, 2022, 12:00 AM (UTC+05:30) Chennai, Kolkata, Mumbai, New Delhi

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer:

Question: Is this a new problem, or it has happened before?
Answer: Never worked

Question: Job URL for the job with issue
Answer: https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729/runs

Question: Job URL from last known good run if available
Answer:

Question: Azure storage account resource ID if job involved reading from or writing to Azure storage account
Answer:

Question: Cluster URL in case job URL is not available
Answer: https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729

Question: Additional details about the issue
Answer: JobName: JOBCOMPUTE_PRD_NGCA_PRIMARY-STREAMING_AA run
Workspace: https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729/run/19350431

Issue 1: Data bricks job failing and retry on it own but it is not creating new RUN. This issue come with 10.4 LTS only.

Issue 2: Data bricks job fail while in-progress. Here is the error message,
Caused by: com.microsoft.azure.eventhubs.ReceiverDisconnectedException: New receiver 'spark-0-1222448' with higher epoch of '0' is created hence current receiver 'spark-0-1222445' with epoch '0' is getting disconnected. If you are recreating the receiver, make sure a higher epoch is used. TrackingId:29cd206a00043323005eaa2162a1646a_G16_B11, SystemTracker:eaasedlprdehngca:eventhub:specto~19661|gen2primary, Timestamp:2022-06-09T03:09:40, errorContext[NS: eaasedlprdehngca.servicebus.windows.net, PATH: specto/ConsumerGroups/gen2primary/Partitions/5, REFERENCE_ID: LN_93364f_1654744170242_05d_G16, PREFETCH_COUNT: 500, LINK_CREDIT: 500, PREFETCH_Q_LEN: 0]



<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Databricks:
What time did the problem begin? - 2022-06-01T18:30:00.000Z;
Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank - ;
Is this a new problem, or it has happened before? - Never worked;
Job URL for the job with issue - https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729/runs;
Job URL from last known good run if available - ;
Azure storage account resource ID if job involved reading from or writing to Azure storage account - ;
Cluster URL in case job URL is not available - https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729;
Additional details about the issue - JobName: JOBCOMPUTE_PRD_NGCA_PRIMARY-STREAMING_AA run
Workspace: https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729/run/19350431

Issue 1: Data bricks job failing and retry on it own but it is not creating new RUN. This issue come with 10.4 LTS only.

Issue 2: Data bricks job fail while in-progress. Here is the error message,
Caused by: com.microsoft.azure.eventhubs.ReceiverDisconnectedException",https://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs Article 07/14/2022 29 minutes to read 4 contributors In this article Create a job Run a job View jobs View runs for a job View job run details View recent job runs Export job run results Edit a job Edit a task Clone a job Clone a task Delete a job Delete a task Copy a task path Run jobs using notebooks in a remote Git repository Best practices A job is a way to run non-interactive code in an Azure Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. You can also run jobs interactively in the notebook UI. You can create and run a job using the UI, the CLI, or by invoking the Jobs API. You can repair and re-run a failed or canceled job using the UI or API. You can monitor job run results using the UI, CLI, API, and email notifications. This article focuses on performing job tasks using the UI. For the other methods, see Jobs CLI and Jobs API 2.1. Your job can consist of a single task or can be a large, multi-task workflow with complex dependencies. Azure Databricks manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs. You can run your jobs immediately or periodically through an easy-to-use scheduling system. You can implement a task in a JAR, an Azure Databricks notebook, a Delta Live Tables pipeline, or an application written in Scala, Java, or Python. Legacy Spark Submit applications are also supported. You control the execution order of tasks by specifying dependencies between the tasks. You can configure tasks to run in sequence or parallel. The following diagram illustrates a workflow that: Ingests raw clickstream data and performs processing to sessionize the records. Ingests order data and joins it with the sessionized clickstream data to create a prepared data set for analysis. Extracts features from the prepared data. Performs tasks in parallel to persist the features and train a machine learning model. To create your first workflow with an Azure Databricks job, see the quickstart. Important You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. A workspace is limited to 1000 concurrent job runs. A 429 Too Many Requests response is returned when you request a run that cannot start immediately. The number of jobs a workspace can create in an hour is limited to 5000 (includes “run now” and “runs submit”). This limit also affects jobs created by the REST API and notebook workflows. Create a job Do one of the following: Click Workflows in the sidebar and click . In the sidebar, click Create and select Job from the menu. The Tasks tab appears with the create task dialog. Replace Add a name for your job… with your job name. Enter a name for the task in the Task name field. Specify the type of task to run. In the Type drop-down, select Notebook, JAR, Spark Submit, Python, or Pipeline. Notebook: In the Source drop-down, select a location for the notebook; either Workspace for a notebook located in a Azure Databricks workspace folder or Git provider for a notebook located in a remote Git repository. Workspace: Use the file browser to find the notebook, click the notebook name, and click Confirm. Git provider: Click Edit and enter the Git repository information. See Run jobs using notebooks in a remote Git repository. JAR: Specify the Main class. Use the fully qualified name of the class containing the main method, for example, org.apache.spark.examples.SparkPi. Then click Add under Dependent Libraries to add libraries required to run the task. One of these libraries must contain the main class. To learn more about JAR tasks, see JAR jobs. Spark Submit: In the Parameters text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the DFSReadWriteTest from the Apache Spark examples: JSON Copy [""--class"",""org.apache.spark.examples.DFSReadWriteTest"",""dbfs:/FileStore/libraries/spark_examples_2_12_3_1_1.jar"",""/dbfs/databricks-datasets/README.md"",""/FileStore/examples/output/""]
 Important There are several limitations for spark-submit tasks: You can run spark-submit tasks only on new clusters. Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see Cluster autoscaling. Spark-submit does not support Databricks Utilities. To use Databricks Utilities, use JAR tasks instead. Python: In the Path textbox, enter the URI of a Python script on DBFS or cloud storage; for example, dbfs:/FileStore/myscript.py. Pipeline: In the Pipeline drop-down, select an existing Delta Live Tables pipeline. Python Wheel: In the Package name text box, enter the package to import, for example, myWheel-1.0-py2.py3-none-any.whl. In the Entry Point text box, enter the function to call when starting the wheel. Click Add under Dependent Libraries to add libraries required to run the task. Configure the cluster where the task runs. In the Cluster drop-down, select either New Job Cluster or Existing All-Purpose Clusters. New Job Cluster: Click Edit in the Cluster drop-down and complete the cluster configuration. Existing All-Purpose Cluster: Select an existing cluster in the Cluster drop-down. To open the cluster in a new page, click the icon to the right of the cluster name and description. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters. Notebook: Click Add and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the Run a job with different parameters option. Parameters set the value of the notebook widget specified by the key of the parameter. Use task parameter variables to pass a limited set of dynamic values as part of a parameter value. JAR: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See Configure JAR job parameters. Spark Submit task: Parameters are specified as a JSON-formatted array of strings. Conforming to the Apache Spark spark-submit convention, parameters after the JAR path are passed to the main method of the main class. Python: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the argparse module in Python. Python Wheel: In the Parameters drop-down, select Positional arguments to enter parameters as a JSON-formatted array of strings, or select Keyword arguments > Add to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. To access additional options, including Dependent Libraries, Retry Policy, and Timeouts, click Advanced Options. See Edit a task. Click Create. To optionally set the job’s schedule, click Edit schedule in the Job details panel. See Schedule a job. To optionally allow multiple concurrent runs of the same job, click Edit concurrent runs in the Job details panel. See Maximum concurrent runs. To optionally specify email addresses to receive notifications on job events, click Edit notifications in the Job details panel. See Notifications. To optionally control permission levels on the job, click Edit permissions in the Job details panel. See Control access to jobs. To add another task, click below the task you just created. A shared cluster option is provided if you have configured a New Job Cluster for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Run a job Click Workflows in the sidebar. Select a job and click the Runs tab. You can run a job immediately or schedule the job to run later. If one or more tasks in a job with multiple tasks are not successful, you can re-run the subset of unsuccessful tasks. See Repair an unsuccessful job run. Run a job immediately To run the job immediately, click . Tip You can perform a test run of a job with a notebook task by clicking Run Now. If you need to make changes to the notebook, clicking Run Now again after editing the notebook will automatically run the new version of the notebook. Run a job with different parameters You can use Run Now with Different Parameters to re-run a job with different parameters or different values for existing parameters. Click next to Run Now and select Run Now with Different Parameters or, in the Active Runs table, click Run Now with Different Parameters. Enter the new parameters depending on the type of task. Notebook: You can enter parameters as key-value pairs or a JSON object. The provided parameters are merged with the default parameters for the triggered run. You can use this dialog to set the values of widgets. JAR and spark-submit: You can enter a list of parameters or a JSON document. If you delete keys, the default parameters are used. You can also add task parameter variables for the run. Click Run. Repair an unsuccessful job run You can repair failed or canceled multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks and any tasks that depend on them are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs. You can change job or task settings before repairing the job run. Unsuccessful tasks are re-run with the current job and task settings. For example, if you change the path to a notebook or a cluster setting, the task is re-run with the updated notebook or cluster settings. You can view the history of all task runs on the Task run details page. Note If one or more tasks share a job cluster, a repair run creates a new job cluster; for example, if the original run used the job cluster my_job_cluster, the first repair run uses the new job cluster my_job_cluster_v1, allowing you to easily see the cluster and cluster settings used by the initial run and any repair runs. The settings for my_job_cluster_v1 are the same as the current settings for my_job_cluster. Repair is supported only with jobs that orchestrate two or more tasks. The Duration value displayed in the Runs tab includes the time the first run started until the time when the latest repair run finished. For example, if a run failed twice and succeeded on the third run, the duration includes the time for all three runs. To repair an unsuccessful job run: Click Jobs in the sidebar. In the Name column, click a job name. The Runs tab shows active runs and completed runs, including any unsuccessful runs. Click the link for the unsuccessful run in the Start time column of the Completed Runs (past 60 days) table. The Job run details page appears. Click Repair run. The Repair job run dialog appears, listing all unsuccessful tasks and any dependent tasks that will be re-run. To add or edit parameters for the tasks to repair, enter the parameters in the Repair job run dialog. Parameters you enter in the Repair job run dialog override existing values. On subsequent repair runs, you can return a parameter to its original value by clearing the key and value in the Repair job run dialog. Click Repair run in the Repair job run dialog. View task run history To view the run history of a task, including successful and unsuccessful runs: Click on a task on the Job run details page. The Task run details page appears. Select the task run in the run history drop-down. Schedule a job To define a schedule for the job: Click Edit schedule in the Job details panel and set the Schedule Type to Scheduled. Specify the period, starting time, and time zone. Optionally select the Show Cron Syntax checkbox to display and edit the schedule in Quartz Cron Syntax. Note Azure Databricks enforces a minimum interval of 10 seconds between subsequent runs triggered by the schedule of a job regardless of the seconds configuration in the cron expression. You can choose a time zone that observes daylight saving time or UTC. If you select a zone that observes daylight saving time, an hourly job will be skipped or may appear to not fire for an hour or two when daylight saving time begins or ends. To run at every hour (absolute time), choose UTC. The job scheduler is not intended for low latency jobs. Due to network or cloud issues, job runs may occasionally be delayed up to several minutes. In these situations, scheduled jobs will run immediately upon service availability. Click Save. Pause and resume a job schedule To pause a job, you can either: Click Pause in the Job details panel. Click Edit schedule in the Job details panel and set the Schedule Type to Manual (Paused) To resume a paused job schedule, set the Schedule Type to Scheduled. View jobs Click Workflows in the sidebar. The Jobs list appears. The Jobs page lists all defined jobs, the cluster definition, the schedule, if any, and the result of the last run. You can filter jobs in the Jobs list: Using keywords. Selecting only the jobs you own. Selecting all jobs you have permissions to access. Access to this filter requires that Jobs access control is enabled. Using tags. To search for a tag created with only a key, type the key into the search box. To search for a tag created with a key and value, you can search by the key, the value, or both the key and value. For example, for a tag with the key department and the value finance, you can search for department or finance to find matching jobs. To search by both the key and value, enter the key and value separated by a colon; for example, department:finance. You can also click any column header to sort the list of jobs (either descending or ascending) by that column. The default sorting is by job name in ascending order. View runs for a job Click Workflows in the sidebar. In the Name column, click a job name. The Runs tab appears with a table of active runs and completed runs. To switch to a matrix view, click Matrix. The matrix view shows a history of runs for the job, including each job task. The Job Runs row of the matrix displays the total duration of the run and the state of the run. To view details of the run, including the start time, duration, and status, hover over the bar in the Job Runs row. Each cell in the Tasks row represents a task and the corresponding status of the task. To view details of each task, including the start time, duration, cluster, and status, hover over the cell for that task. The job run and task run bars are color-coded to indicate the status of the run. Successful runs are green, unsuccessful runs are red, and skipped runs are pink. The height of the individual job run and task run bars provides a visual indication of the run duration. Azure Databricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends that you export results before they expire. For more information, see Export job run results. View job run details The job run details page contains job output and links to logs, including information about the success or failure of each task in the job run. You can access job run details from the Runs tab for the job. To view job run details from the Runs tab, click the link for the run in the Run column of the Completed Runs (past 60 days) table. To return to the Runs tab for the job, click on the Job ID value. Click on a task to view task run details, including: the cluster that ran the task the Spark UI for the task logs for the task metrics for the task Click the Job ID value to return to the Runs tab for the job. Click the Job run ID value to return to the job run details. View recent job runs You can view a list of currently running and recently completed runs for all jobs in a workspace you have access to, including runs started by external orchestration tools such as Apache Airflow or Azure Data Factory. To view the list of recent job runs: Click Workflows in the sidebar. The Jobs list appears. Click the Job runs tab. The Job runs list appears. The Job runs list displays: The start time for the run. The name of the job associated with the run. The user name that the job runs as. Whether the run was triggered by a job schedule or an API request, or was manually started. The time elapsed for a currently running job, or the total running time for a completed run. The status of the run, either Pending, Running, Skipped, Succeeded, Failed, Terminating, Terminated, Internal Error, Timed Out, Canceled, Canceling, or Waiting for Retry. To view job run details, click the link in the Start time column for the run. To view job details, click the job name in the Job column. Export job run results You can export notebook run results and job run logs for all job types. Export notebook run results You can persist job runs by exporting their results. For notebook job runs, you can export a rendered notebook that can later be imported into your Azure Databricks workspace. To export notebook run results for a job with a single task: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click Export to HTML. To export notebook run results for a job with multiple tasks: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click the notebook task to export. Click Export to HTML. Export job run logs You can also export the logs for your job run. You can set up your job to automatically deliver logs to DBFS through the Job API. See the new_cluster.cluster_log_conf object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. Edit a job Some configuration options are available on the job, and other options are available on individual tasks. For example, the maximum concurrent runs can be set on the job only, while parameters must be defined for each task. To change the configuration for a job: Click Workflows in the sidebar. In the Name column, click the job name. The side panel displays the Job details. You can change the schedule, cluster configuration, notifications, maximum number of concurrent runs, and add or change tags. If job access control is enabled, you can also edit job permissions. Tags To add labels or key:value attributes to your job, you can add tags when you edit the job. You can use tags to filter jobs in the Jobs list; for example, you can use a department tag to filter all jobs that belong to a specific department. Note Because job tags are not designed to store sensitive information such as personally identifiable information or passwords, Databricks recommends using tags for non-sensitive values only. Tags also propagate to job clusters created when a job is run, allowing you to use tags with your existing cluster monitoring. To add or edit tags, click + Tag in the Job details side panel. You can add the tag as a key and value, or a label. To add a label, enter the label in the Key field and leave the Value field empty. Clusters To see tasks associated with a cluster, hover over the cluster in the side panel. To change the cluster configuration for all associated tasks, click Configure under the cluster. To configure a new cluster for all associated tasks, click Swap under the cluster. Maximum concurrent runs The maximum number of parallel runs for this job. Azure Databricks skips the run if the job has already reached its maximum number of active runs when attempting to start a new run. Set this value higher than the default of 1 to perform multiple runs of the same job concurrently. This is useful, for example, if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or you want to trigger multiple runs that differ by their input parameters. Notifications You can add one or more email addresses to notify when runs of this job begin, complete, or fail: Click Edit notifications. Click Add. Enter an email address and click the check box for each notification type to send to that address. To enter another email address for notification, click Add. If you do not want to receive notifications for skipped job runs, click the check box. Click Confirm. Integrate these email notifications with your favorite notification tools, including: PagerDuty Slack Control access to jobs Job access control enables job owners and administrators to grant fine-grained permissions on their jobs. Job owners can choose which other users or groups can view the results of the job. Owners can also choose who can manage their job runs (Run now and Cancel run permissions). See Jobs access control for details. Edit a task To set task configuration options: Click Workflows in the sidebar. In the Name column, click the job name. Click the Tasks tab. Task dependencies You can define the order of execution of tasks in a job using the Depends on drop-down. You can set this field to one or more tasks in the job. Note Depends on is not visible if the job consists of only a single task. Configuring task dependencies creates a Directed Acyclic Graph (DAG) of task execution, a common way of representing execution order in job schedulers. For example, consider the following job consisting of four tasks: Task 1 is the root task and does not depend on any other task. Task 2 and Task 3 depend on Task 1 completing first. Finally, Task 4 depends on Task 2 and Task 3 completing successfully. Azure Databricks runs upstream tasks before running downstream tasks, running as many of them in parallel as possible. The following diagram illustrates the order of processing for these tasks: Individual task configuration options Individual tasks have the following configuration options: In this section: Cluster Dependent libraries Task parameter variables Timeout Retries Cluster To configure the cluster where a task runs, click the Cluster drop-down. You can edit a shared job cluster, but you cannot delete a shared cluster if it is still used by other tasks. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Dependent libraries Dependent libraries will be installed on the cluster before the task runs. You must set all task dependencies to ensure they are installed before the run starts. To add a dependent library, click Advanced options and select Add Dependent Libraries to open the Add Dependent Library chooser. Follow the recommendations in Library dependencies for specifying dependencies. Important If you have configured a library to install on all clusters automatically, or you select an existing terminated cluster that has libraries installed, the job execution does not wait for library installation to complete. If a job requires a specific library, you should attach the library to the job in the Dependent Libraries field. Task parameter variables You can pass templated variables into a job task as part of the task’s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job’s start time. When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named MyJobId with a value of my-job-6 for any run of job ID 6, add the following task parameter: JSON Copy {
  ""MyJobID"": ""my-job-{{job_id}}""
}
 The contents of the double curly braces are not evaluated as expressions, so you cannot do operations or functions within double-curly braces. Whitespace is not stripped inside the curly braces, so {{ job_id }} will not be evaluated. The following task parameter variables are supported: Variable Description {{job_id}} The unique identifier assigned to a job {{run_id}} The unique identifier assigned to a job run {{start_date}} The date a task run started. The format is yyyy-MM-dd in UTC timezone. {{start_time}} The timestamp of the run’s start of execution after the cluster is created and ready. The format is milliseconds since UNIX epoch in UTC timezone, as returned by System.currentTimeMillis(). {{task_retry_count}} The number of retries that have been attempted to run a task if the first attempt fails. The value is 0 for the first attempt and increments with each retry. {{parent_run_id}} The unique identifier assigned to the run of a job with multiple tasks. {{task_key}} The unique name assigned to a task that’s part of a job with multiple tasks. You can set these variables with any task when you Create a job, Edit a job, or Run a job with different parameters. Timeout The maximum completion time for a job. If the job does not complete in this time, Azure Databricks sets its status to “Timed Out”. Retries A policy that determines when and how many times failed runs are retried. To set the retries for the task, click Advanced options and select Edit Retry Policy. The retry interval is calculated in milliseconds between the start of the failed run and the subsequent retry run. Note If you configure both Timeout and Retries, the timeout applies to each retry. Clone a job You can quickly create a new job by cloning an existing job. Cloning a job creates an identical copy of the job, except for the job ID. On the job’s page, click More … next to the job’s name and select Clone from the drop-down menu. Clone a task You can quickly create a new task by cloning an existing task: On the job’s page, click the Tasks tab. Select the task to clone. Click and select Clone task. Delete a job To delete a job, on the job’s page, click More … next to the job’s name and select Delete from the drop-down menu. Delete a task To delete a task: Click the Tasks tab. Select the task to be deleted. Click and select Remove task. Copy a task path To copy the path to a task, for example, a notebook path: Click the Tasks tab. Select the task containing the path to copy. Click next to the task path to copy the path to the clipboard. Run jobs using notebooks in a remote Git repository Important This feature is in Public Preview. You can run jobs with notebooks located in a remote Git repository. This feature simplifies creation and management of production jobs and automates continuous deployment: You don’t need to create a separate production repo in Azure Databricks, manage permissions for it, and keep it updated. You can prevent unintentional changes to a production job, such as local edits in the production repo or changes from switching a branch. The job definition process has a single source of truth in the remote repository. To use notebooks in a remote Git repository, you must Set up Git integration with Databricks Repos. To create a task with a notebook located in a remote Git repository: In the Type drop-down, select Notebook. In the Source drop-down, select Git provider. The Git information dialog appears. In the Git Information dialog, enter details for the repository. For Path, enter a relative path to the notebook location, such as etl/notebooks/. When you enter the relative path, don’t begin it with / or ./ and don’t include the notebook file extension, such as .py. Additional notebook tasks in a multitask job reference the same commit in the remote repository in one of the following ways: sha of $branch/head when git_branch is set sha of $tag when git_tag is set the value of git_commit In a multitask job, there cannot be a task that uses a local notebook and another task that uses a remote repository. This restriction doesn’t apply to non-notebook tasks. Best practices In this section: Cluster configuration tips Notebook job tips Streaming tasks JAR jobs Library dependencies Cluster configuration tips Cluster configuration is important when you operationalize a job. The following provides general guidance on choosing and configuring job clusters, followed by recommendations for specific job types. Use shared job clusters To optimize resource usage with jobs that orchestrate multiple tasks, use shared job clusters. A shared job cluster allows multiple tasks in the same job run to reuse the cluster. You can use a single job cluster to run all tasks that are part of the job, or multiple job clusters optimized for specific workloads. To use a shared job cluster: Select New Job Clusters when you create a task and complete the cluster configuration. Select the new cluster when adding a task to the job, or create a new job cluster. Any cluster you configure when you select New Job Clusters is available to any task in the job. A shared job cluster is scoped to a single job run, and cannot be used by other jobs or runs of the same job. Libraries cannot be declared in a shared job cluster configuration. You must add dependent libraries in task settings. Choose the correct cluster type for your job New Job Clusters are dedicated clusters for a job or task run. A shared job cluster is created and started when the first task using the cluster starts and terminates after the last task using the cluster completes. The cluster is not terminated when idle but terminates only after all tasks using it have completed. If a shared job cluster fails or is terminated before all tasks have finished, a new cluster is created. A cluster scoped to a single task is created and started when the task starts and terminates when the task completes. In production, Databricks recommends using new shared or task scoped clusters so that each job or task runs in a fully isolated environment. When you run a task on a new cluster, the task is treated as a data engineering (task) workload, subject to the task workload pricing. When you run a task on an existing all-purpose cluster, the task is treated as a data analytics (all-purpose) workload, subject to all-purpose workload pricing. If you select a terminated existing cluster and the job owner has Can Restart permission, Azure Databricks starts the cluster when the job is scheduled to run. Existing all-purpose clusters work best for tasks such as updating dashboards at regular intervals. Use a pool to reduce cluster start times To decrease new job cluster start time, create a pool and configure the job’s cluster to use the pool. Notebook job tips Total notebook cell output (the combined output of all notebook cells) is subject to a 20MB size limit. Additionally, individual cell output is subject to an 8MB size limit. If total cell output exceeds 20MB in size, or if the output of an individual cell is larger than 8MB, the run is canceled and marked as failed. If you need help finding cells near or beyond the limit, run the notebook against an all-purpose cluster and use this notebook autosave technique. Streaming tasks Spark Streaming jobs should never have maximum concurrent runs set to greater than 1. Streaming jobs should be set to run using the cron expression ""* * * * * ?"" (every minute). Since a streaming task runs continuously, it should always be the final task in a job. JAR jobs When running a JAR job, keep in mind the following: Output size limits Note Available in Databricks Runtime 6.3 and above. Job output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run is canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to Azure Databricks by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default, the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster’s log files. Setting this flag is recommended only for job clusters for JAR jobs because it will disable notebook results. Use the shared SparkContext Because Azure Databricks is a managed service, some code changes may be necessary to ensure that your Apache Spark jobs run correctly. JAR job programs must use the shared SparkContext API to get the SparkContext. Because Azure Databricks initializes the SparkContext, programs that invoke new SparkContext() will fail. To get the SparkContext, use only the shared SparkContext created by Azure Databricks: Scala Copy val goodSparkContext = SparkContext.getOrCreate()
val goodSparkSession = SparkSession.builder().getOrCreate()
 There are also several methods you should avoid when using the shared SparkContext. Do not call SparkContext.stop(). Do not call System.exit(0) or sc.stop() at the end of your Main program. This can cause undefined behavior. Use try-finally blocks for job clean up Consider a JAR that consists of two parts: jobBody() which contains the main part of the job. jobCleanup() which has to be executed after jobBody() whether that function succeeded or returned an exception. As an example, jobBody() may create tables, and you can use jobCleanup() to drop these tables. The safe way to ensure that the clean up method is called is to put a try-finally block in the code: Scala Copy try {
  jobBody()
} finally {
  jobCleanup()
}
 You should not try to clean up using sys.addShutdownHook(jobCleanup) or the following code: Scala Copy val cleanupThread = new Thread { override def run = jobCleanup() }
Runtime.getRuntime.addShutdownHook(cleanupThread)
 Due to the way the lifetime of Spark containers is managed in Azure Databricks, the shutdown hooks are not run reliably. Configure JAR job parameters You pass parameters to JAR jobs with a JSON string array. See the spark_jar_task object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. To access these parameters, inspect the String array passed into your main function. Library dependencies The Spark driver has certain library dependencies that cannot be overridden. These libraries take priority over any of your libraries that conflict with them. To get the full list of the driver library dependencies, run the following command inside a notebook attached to a cluster of the same Spark version (or the cluster with the driver you want to examine). Bash Copy %sh
ls /databricks/jars
 Manage library dependencies A good rule of thumb when dealing with library dependencies while creating JARs for jobs is to list Spark and Hadoop as provided dependencies. On Maven, add Spark and Hadoop as provided dependencies, as shown in the following example: XML Copy <dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.3.0</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-core</artifactId>
  <version>1.2.1</version>
  <scope>provided</scope>
</dependency>
 In sbt, add Spark and Hadoop as provided dependencies, as shown in the following example: Scala Copy libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.3.0"" % ""provided""
libraryDependencies += ""org.apache.hadoop"" %% ""hadoop-core"" % ""1.2.1"" % ""provided""
 Tip Specify the correct Scala version for your dependencies based on the version you are running.",Jobs
32,ARR| Caused by: com.microsoft.azure.eventhubs.ReceiverDisconnectedException| 2206100030001353,"Caused by: com.microsoft.azure.eventhubs.ReceiverDisconnectedException

Question: What time did the problem begin?
Answer: Thu, Jun 2, 2022, 12:00 AM (UTC+05:30) Chennai, Kolkata, Mumbai, New Delhi

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer:

Question: Is this a new problem, or it has happened before?
Answer: Never worked

Question: Job URL for the job with issue
Answer: https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729/runs

Question: Job URL from last known good run if available
Answer:

Question: Azure storage account resource ID if job involved reading from or writing to Azure storage account
Answer:

Question: Cluster URL in case job URL is not available
Answer: https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729

Question: Additional details about the issue
Answer: JobName: JOBCOMPUTE_PRD_NGCA_PRIMARY-STREAMING_AA run
Workspace: https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729/run/19350431

Issue 1: Data bricks job failing and retry on it own but it is not creating new RUN. This issue come with 10.4 LTS only.

Issue 2: Data bricks job fail while in-progress. Here is the error message,
Caused by: com.microsoft.azure.eventhubs.ReceiverDisconnectedException: New receiver 'spark-0-1222448' with higher epoch of '0' is created hence current receiver 'spark-0-1222445' with epoch '0' is getting disconnected. If you are recreating the receiver, make sure a higher epoch is used. TrackingId:29cd206a00043323005eaa2162a1646a_G16_B11, SystemTracker:eaasedlprdehngca:eventhub:specto~19661|gen2primary, Timestamp:2022-06-09T03:09:40, errorContext[NS: eaasedlprdehngca.servicebus.windows.net, PATH: specto/ConsumerGroups/gen2primary/Partitions/5, REFERENCE_ID: LN_93364f_1654744170242_05d_G16, PREFETCH_COUNT: 500, LINK_CREDIT: 500, PREFETCH_Q_LEN: 0]



<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Databricks:
What time did the problem begin? - 2022-06-01T18:30:00.000Z;
Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank - ;
Is this a new problem, or it has happened before? - Never worked;
Job URL for the job with issue - https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729/runs;
Job URL from last known good run if available - ;
Azure storage account resource ID if job involved reading from or writing to Azure storage account - ;
Cluster URL in case job URL is not available - https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729;
Additional details about the issue - JobName: JOBCOMPUTE_PRD_NGCA_PRIMARY-STREAMING_AA run
Workspace: https://adb-1497057797762139.19.azuredatabricks.net/?o=1497057797762139#job/78976883640729/run/19350431

Issue 1: Data bricks job failing and retry on it own but it is not creating new RUN. This issue come with 10.4 LTS only.

Issue 2: Data bricks job fail while in-progress. Here is the error message,
Caused by: com.microsoft.azure.eventhubs.ReceiverDisconnectedException",https://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs Article 07/14/2022 29 minutes to read 4 contributors In this article Create a job Run a job View jobs View runs for a job View job run details View recent job runs Export job run results Edit a job Edit a task Clone a job Clone a task Delete a job Delete a task Copy a task path Run jobs using notebooks in a remote Git repository Best practices A job is a way to run non-interactive code in an Azure Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. You can also run jobs interactively in the notebook UI. You can create and run a job using the UI, the CLI, or by invoking the Jobs API. You can repair and re-run a failed or canceled job using the UI or API. You can monitor job run results using the UI, CLI, API, and email notifications. This article focuses on performing job tasks using the UI. For the other methods, see Jobs CLI and Jobs API 2.1. Your job can consist of a single task or can be a large, multi-task workflow with complex dependencies. Azure Databricks manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs. You can run your jobs immediately or periodically through an easy-to-use scheduling system. You can implement a task in a JAR, an Azure Databricks notebook, a Delta Live Tables pipeline, or an application written in Scala, Java, or Python. Legacy Spark Submit applications are also supported. You control the execution order of tasks by specifying dependencies between the tasks. You can configure tasks to run in sequence or parallel. The following diagram illustrates a workflow that: Ingests raw clickstream data and performs processing to sessionize the records. Ingests order data and joins it with the sessionized clickstream data to create a prepared data set for analysis. Extracts features from the prepared data. Performs tasks in parallel to persist the features and train a machine learning model. To create your first workflow with an Azure Databricks job, see the quickstart. Important You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. A workspace is limited to 1000 concurrent job runs. A 429 Too Many Requests response is returned when you request a run that cannot start immediately. The number of jobs a workspace can create in an hour is limited to 5000 (includes “run now” and “runs submit”). This limit also affects jobs created by the REST API and notebook workflows. Create a job Do one of the following: Click Workflows in the sidebar and click . In the sidebar, click Create and select Job from the menu. The Tasks tab appears with the create task dialog. Replace Add a name for your job… with your job name. Enter a name for the task in the Task name field. Specify the type of task to run. In the Type drop-down, select Notebook, JAR, Spark Submit, Python, or Pipeline. Notebook: In the Source drop-down, select a location for the notebook; either Workspace for a notebook located in a Azure Databricks workspace folder or Git provider for a notebook located in a remote Git repository. Workspace: Use the file browser to find the notebook, click the notebook name, and click Confirm. Git provider: Click Edit and enter the Git repository information. See Run jobs using notebooks in a remote Git repository. JAR: Specify the Main class. Use the fully qualified name of the class containing the main method, for example, org.apache.spark.examples.SparkPi. Then click Add under Dependent Libraries to add libraries required to run the task. One of these libraries must contain the main class. To learn more about JAR tasks, see JAR jobs. Spark Submit: In the Parameters text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the DFSReadWriteTest from the Apache Spark examples: JSON Copy [""--class"",""org.apache.spark.examples.DFSReadWriteTest"",""dbfs:/FileStore/libraries/spark_examples_2_12_3_1_1.jar"",""/dbfs/databricks-datasets/README.md"",""/FileStore/examples/output/""]
 Important There are several limitations for spark-submit tasks: You can run spark-submit tasks only on new clusters. Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see Cluster autoscaling. Spark-submit does not support Databricks Utilities. To use Databricks Utilities, use JAR tasks instead. Python: In the Path textbox, enter the URI of a Python script on DBFS or cloud storage; for example, dbfs:/FileStore/myscript.py. Pipeline: In the Pipeline drop-down, select an existing Delta Live Tables pipeline. Python Wheel: In the Package name text box, enter the package to import, for example, myWheel-1.0-py2.py3-none-any.whl. In the Entry Point text box, enter the function to call when starting the wheel. Click Add under Dependent Libraries to add libraries required to run the task. Configure the cluster where the task runs. In the Cluster drop-down, select either New Job Cluster or Existing All-Purpose Clusters. New Job Cluster: Click Edit in the Cluster drop-down and complete the cluster configuration. Existing All-Purpose Cluster: Select an existing cluster in the Cluster drop-down. To open the cluster in a new page, click the icon to the right of the cluster name and description. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters. Notebook: Click Add and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the Run a job with different parameters option. Parameters set the value of the notebook widget specified by the key of the parameter. Use task parameter variables to pass a limited set of dynamic values as part of a parameter value. JAR: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See Configure JAR job parameters. Spark Submit task: Parameters are specified as a JSON-formatted array of strings. Conforming to the Apache Spark spark-submit convention, parameters after the JAR path are passed to the main method of the main class. Python: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the argparse module in Python. Python Wheel: In the Parameters drop-down, select Positional arguments to enter parameters as a JSON-formatted array of strings, or select Keyword arguments > Add to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. To access additional options, including Dependent Libraries, Retry Policy, and Timeouts, click Advanced Options. See Edit a task. Click Create. To optionally set the job’s schedule, click Edit schedule in the Job details panel. See Schedule a job. To optionally allow multiple concurrent runs of the same job, click Edit concurrent runs in the Job details panel. See Maximum concurrent runs. To optionally specify email addresses to receive notifications on job events, click Edit notifications in the Job details panel. See Notifications. To optionally control permission levels on the job, click Edit permissions in the Job details panel. See Control access to jobs. To add another task, click below the task you just created. A shared cluster option is provided if you have configured a New Job Cluster for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Run a job Click Workflows in the sidebar. Select a job and click the Runs tab. You can run a job immediately or schedule the job to run later. If one or more tasks in a job with multiple tasks are not successful, you can re-run the subset of unsuccessful tasks. See Repair an unsuccessful job run. Run a job immediately To run the job immediately, click . Tip You can perform a test run of a job with a notebook task by clicking Run Now. If you need to make changes to the notebook, clicking Run Now again after editing the notebook will automatically run the new version of the notebook. Run a job with different parameters You can use Run Now with Different Parameters to re-run a job with different parameters or different values for existing parameters. Click next to Run Now and select Run Now with Different Parameters or, in the Active Runs table, click Run Now with Different Parameters. Enter the new parameters depending on the type of task. Notebook: You can enter parameters as key-value pairs or a JSON object. The provided parameters are merged with the default parameters for the triggered run. You can use this dialog to set the values of widgets. JAR and spark-submit: You can enter a list of parameters or a JSON document. If you delete keys, the default parameters are used. You can also add task parameter variables for the run. Click Run. Repair an unsuccessful job run You can repair failed or canceled multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks and any tasks that depend on them are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs. You can change job or task settings before repairing the job run. Unsuccessful tasks are re-run with the current job and task settings. For example, if you change the path to a notebook or a cluster setting, the task is re-run with the updated notebook or cluster settings. You can view the history of all task runs on the Task run details page. Note If one or more tasks share a job cluster, a repair run creates a new job cluster; for example, if the original run used the job cluster my_job_cluster, the first repair run uses the new job cluster my_job_cluster_v1, allowing you to easily see the cluster and cluster settings used by the initial run and any repair runs. The settings for my_job_cluster_v1 are the same as the current settings for my_job_cluster. Repair is supported only with jobs that orchestrate two or more tasks. The Duration value displayed in the Runs tab includes the time the first run started until the time when the latest repair run finished. For example, if a run failed twice and succeeded on the third run, the duration includes the time for all three runs. To repair an unsuccessful job run: Click Jobs in the sidebar. In the Name column, click a job name. The Runs tab shows active runs and completed runs, including any unsuccessful runs. Click the link for the unsuccessful run in the Start time column of the Completed Runs (past 60 days) table. The Job run details page appears. Click Repair run. The Repair job run dialog appears, listing all unsuccessful tasks and any dependent tasks that will be re-run. To add or edit parameters for the tasks to repair, enter the parameters in the Repair job run dialog. Parameters you enter in the Repair job run dialog override existing values. On subsequent repair runs, you can return a parameter to its original value by clearing the key and value in the Repair job run dialog. Click Repair run in the Repair job run dialog. View task run history To view the run history of a task, including successful and unsuccessful runs: Click on a task on the Job run details page. The Task run details page appears. Select the task run in the run history drop-down. Schedule a job To define a schedule for the job: Click Edit schedule in the Job details panel and set the Schedule Type to Scheduled. Specify the period, starting time, and time zone. Optionally select the Show Cron Syntax checkbox to display and edit the schedule in Quartz Cron Syntax. Note Azure Databricks enforces a minimum interval of 10 seconds between subsequent runs triggered by the schedule of a job regardless of the seconds configuration in the cron expression. You can choose a time zone that observes daylight saving time or UTC. If you select a zone that observes daylight saving time, an hourly job will be skipped or may appear to not fire for an hour or two when daylight saving time begins or ends. To run at every hour (absolute time), choose UTC. The job scheduler is not intended for low latency jobs. Due to network or cloud issues, job runs may occasionally be delayed up to several minutes. In these situations, scheduled jobs will run immediately upon service availability. Click Save. Pause and resume a job schedule To pause a job, you can either: Click Pause in the Job details panel. Click Edit schedule in the Job details panel and set the Schedule Type to Manual (Paused) To resume a paused job schedule, set the Schedule Type to Scheduled. View jobs Click Workflows in the sidebar. The Jobs list appears. The Jobs page lists all defined jobs, the cluster definition, the schedule, if any, and the result of the last run. You can filter jobs in the Jobs list: Using keywords. Selecting only the jobs you own. Selecting all jobs you have permissions to access. Access to this filter requires that Jobs access control is enabled. Using tags. To search for a tag created with only a key, type the key into the search box. To search for a tag created with a key and value, you can search by the key, the value, or both the key and value. For example, for a tag with the key department and the value finance, you can search for department or finance to find matching jobs. To search by both the key and value, enter the key and value separated by a colon; for example, department:finance. You can also click any column header to sort the list of jobs (either descending or ascending) by that column. The default sorting is by job name in ascending order. View runs for a job Click Workflows in the sidebar. In the Name column, click a job name. The Runs tab appears with a table of active runs and completed runs. To switch to a matrix view, click Matrix. The matrix view shows a history of runs for the job, including each job task. The Job Runs row of the matrix displays the total duration of the run and the state of the run. To view details of the run, including the start time, duration, and status, hover over the bar in the Job Runs row. Each cell in the Tasks row represents a task and the corresponding status of the task. To view details of each task, including the start time, duration, cluster, and status, hover over the cell for that task. The job run and task run bars are color-coded to indicate the status of the run. Successful runs are green, unsuccessful runs are red, and skipped runs are pink. The height of the individual job run and task run bars provides a visual indication of the run duration. Azure Databricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends that you export results before they expire. For more information, see Export job run results. View job run details The job run details page contains job output and links to logs, including information about the success or failure of each task in the job run. You can access job run details from the Runs tab for the job. To view job run details from the Runs tab, click the link for the run in the Run column of the Completed Runs (past 60 days) table. To return to the Runs tab for the job, click on the Job ID value. Click on a task to view task run details, including: the cluster that ran the task the Spark UI for the task logs for the task metrics for the task Click the Job ID value to return to the Runs tab for the job. Click the Job run ID value to return to the job run details. View recent job runs You can view a list of currently running and recently completed runs for all jobs in a workspace you have access to, including runs started by external orchestration tools such as Apache Airflow or Azure Data Factory. To view the list of recent job runs: Click Workflows in the sidebar. The Jobs list appears. Click the Job runs tab. The Job runs list appears. The Job runs list displays: The start time for the run. The name of the job associated with the run. The user name that the job runs as. Whether the run was triggered by a job schedule or an API request, or was manually started. The time elapsed for a currently running job, or the total running time for a completed run. The status of the run, either Pending, Running, Skipped, Succeeded, Failed, Terminating, Terminated, Internal Error, Timed Out, Canceled, Canceling, or Waiting for Retry. To view job run details, click the link in the Start time column for the run. To view job details, click the job name in the Job column. Export job run results You can export notebook run results and job run logs for all job types. Export notebook run results You can persist job runs by exporting their results. For notebook job runs, you can export a rendered notebook that can later be imported into your Azure Databricks workspace. To export notebook run results for a job with a single task: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click Export to HTML. To export notebook run results for a job with multiple tasks: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click the notebook task to export. Click Export to HTML. Export job run logs You can also export the logs for your job run. You can set up your job to automatically deliver logs to DBFS through the Job API. See the new_cluster.cluster_log_conf object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. Edit a job Some configuration options are available on the job, and other options are available on individual tasks. For example, the maximum concurrent runs can be set on the job only, while parameters must be defined for each task. To change the configuration for a job: Click Workflows in the sidebar. In the Name column, click the job name. The side panel displays the Job details. You can change the schedule, cluster configuration, notifications, maximum number of concurrent runs, and add or change tags. If job access control is enabled, you can also edit job permissions. Tags To add labels or key:value attributes to your job, you can add tags when you edit the job. You can use tags to filter jobs in the Jobs list; for example, you can use a department tag to filter all jobs that belong to a specific department. Note Because job tags are not designed to store sensitive information such as personally identifiable information or passwords, Databricks recommends using tags for non-sensitive values only. Tags also propagate to job clusters created when a job is run, allowing you to use tags with your existing cluster monitoring. To add or edit tags, click + Tag in the Job details side panel. You can add the tag as a key and value, or a label. To add a label, enter the label in the Key field and leave the Value field empty. Clusters To see tasks associated with a cluster, hover over the cluster in the side panel. To change the cluster configuration for all associated tasks, click Configure under the cluster. To configure a new cluster for all associated tasks, click Swap under the cluster. Maximum concurrent runs The maximum number of parallel runs for this job. Azure Databricks skips the run if the job has already reached its maximum number of active runs when attempting to start a new run. Set this value higher than the default of 1 to perform multiple runs of the same job concurrently. This is useful, for example, if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or you want to trigger multiple runs that differ by their input parameters. Notifications You can add one or more email addresses to notify when runs of this job begin, complete, or fail: Click Edit notifications. Click Add. Enter an email address and click the check box for each notification type to send to that address. To enter another email address for notification, click Add. If you do not want to receive notifications for skipped job runs, click the check box. Click Confirm. Integrate these email notifications with your favorite notification tools, including: PagerDuty Slack Control access to jobs Job access control enables job owners and administrators to grant fine-grained permissions on their jobs. Job owners can choose which other users or groups can view the results of the job. Owners can also choose who can manage their job runs (Run now and Cancel run permissions). See Jobs access control for details. Edit a task To set task configuration options: Click Workflows in the sidebar. In the Name column, click the job name. Click the Tasks tab. Task dependencies You can define the order of execution of tasks in a job using the Depends on drop-down. You can set this field to one or more tasks in the job. Note Depends on is not visible if the job consists of only a single task. Configuring task dependencies creates a Directed Acyclic Graph (DAG) of task execution, a common way of representing execution order in job schedulers. For example, consider the following job consisting of four tasks: Task 1 is the root task and does not depend on any other task. Task 2 and Task 3 depend on Task 1 completing first. Finally, Task 4 depends on Task 2 and Task 3 completing successfully. Azure Databricks runs upstream tasks before running downstream tasks, running as many of them in parallel as possible. The following diagram illustrates the order of processing for these tasks: Individual task configuration options Individual tasks have the following configuration options: In this section: Cluster Dependent libraries Task parameter variables Timeout Retries Cluster To configure the cluster where a task runs, click the Cluster drop-down. You can edit a shared job cluster, but you cannot delete a shared cluster if it is still used by other tasks. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Dependent libraries Dependent libraries will be installed on the cluster before the task runs. You must set all task dependencies to ensure they are installed before the run starts. To add a dependent library, click Advanced options and select Add Dependent Libraries to open the Add Dependent Library chooser. Follow the recommendations in Library dependencies for specifying dependencies. Important If you have configured a library to install on all clusters automatically, or you select an existing terminated cluster that has libraries installed, the job execution does not wait for library installation to complete. If a job requires a specific library, you should attach the library to the job in the Dependent Libraries field. Task parameter variables You can pass templated variables into a job task as part of the task’s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job’s start time. When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named MyJobId with a value of my-job-6 for any run of job ID 6, add the following task parameter: JSON Copy {
  ""MyJobID"": ""my-job-{{job_id}}""
}
 The contents of the double curly braces are not evaluated as expressions, so you cannot do operations or functions within double-curly braces. Whitespace is not stripped inside the curly braces, so {{ job_id }} will not be evaluated. The following task parameter variables are supported: Variable Description {{job_id}} The unique identifier assigned to a job {{run_id}} The unique identifier assigned to a job run {{start_date}} The date a task run started. The format is yyyy-MM-dd in UTC timezone. {{start_time}} The timestamp of the run’s start of execution after the cluster is created and ready. The format is milliseconds since UNIX epoch in UTC timezone, as returned by System.currentTimeMillis(). {{task_retry_count}} The number of retries that have been attempted to run a task if the first attempt fails. The value is 0 for the first attempt and increments with each retry. {{parent_run_id}} The unique identifier assigned to the run of a job with multiple tasks. {{task_key}} The unique name assigned to a task that’s part of a job with multiple tasks. You can set these variables with any task when you Create a job, Edit a job, or Run a job with different parameters. Timeout The maximum completion time for a job. If the job does not complete in this time, Azure Databricks sets its status to “Timed Out”. Retries A policy that determines when and how many times failed runs are retried. To set the retries for the task, click Advanced options and select Edit Retry Policy. The retry interval is calculated in milliseconds between the start of the failed run and the subsequent retry run. Note If you configure both Timeout and Retries, the timeout applies to each retry. Clone a job You can quickly create a new job by cloning an existing job. Cloning a job creates an identical copy of the job, except for the job ID. On the job’s page, click More … next to the job’s name and select Clone from the drop-down menu. Clone a task You can quickly create a new task by cloning an existing task: On the job’s page, click the Tasks tab. Select the task to clone. Click and select Clone task. Delete a job To delete a job, on the job’s page, click More … next to the job’s name and select Delete from the drop-down menu. Delete a task To delete a task: Click the Tasks tab. Select the task to be deleted. Click and select Remove task. Copy a task path To copy the path to a task, for example, a notebook path: Click the Tasks tab. Select the task containing the path to copy. Click next to the task path to copy the path to the clipboard. Run jobs using notebooks in a remote Git repository Important This feature is in Public Preview. You can run jobs with notebooks located in a remote Git repository. This feature simplifies creation and management of production jobs and automates continuous deployment: You don’t need to create a separate production repo in Azure Databricks, manage permissions for it, and keep it updated. You can prevent unintentional changes to a production job, such as local edits in the production repo or changes from switching a branch. The job definition process has a single source of truth in the remote repository. To use notebooks in a remote Git repository, you must Set up Git integration with Databricks Repos. To create a task with a notebook located in a remote Git repository: In the Type drop-down, select Notebook. In the Source drop-down, select Git provider. The Git information dialog appears. In the Git Information dialog, enter details for the repository. For Path, enter a relative path to the notebook location, such as etl/notebooks/. When you enter the relative path, don’t begin it with / or ./ and don’t include the notebook file extension, such as .py. Additional notebook tasks in a multitask job reference the same commit in the remote repository in one of the following ways: sha of $branch/head when git_branch is set sha of $tag when git_tag is set the value of git_commit In a multitask job, there cannot be a task that uses a local notebook and another task that uses a remote repository. This restriction doesn’t apply to non-notebook tasks. Best practices In this section: Cluster configuration tips Notebook job tips Streaming tasks JAR jobs Library dependencies Cluster configuration tips Cluster configuration is important when you operationalize a job. The following provides general guidance on choosing and configuring job clusters, followed by recommendations for specific job types. Use shared job clusters To optimize resource usage with jobs that orchestrate multiple tasks, use shared job clusters. A shared job cluster allows multiple tasks in the same job run to reuse the cluster. You can use a single job cluster to run all tasks that are part of the job, or multiple job clusters optimized for specific workloads. To use a shared job cluster: Select New Job Clusters when you create a task and complete the cluster configuration. Select the new cluster when adding a task to the job, or create a new job cluster. Any cluster you configure when you select New Job Clusters is available to any task in the job. A shared job cluster is scoped to a single job run, and cannot be used by other jobs or runs of the same job. Libraries cannot be declared in a shared job cluster configuration. You must add dependent libraries in task settings. Choose the correct cluster type for your job New Job Clusters are dedicated clusters for a job or task run. A shared job cluster is created and started when the first task using the cluster starts and terminates after the last task using the cluster completes. The cluster is not terminated when idle but terminates only after all tasks using it have completed. If a shared job cluster fails or is terminated before all tasks have finished, a new cluster is created. A cluster scoped to a single task is created and started when the task starts and terminates when the task completes. In production, Databricks recommends using new shared or task scoped clusters so that each job or task runs in a fully isolated environment. When you run a task on a new cluster, the task is treated as a data engineering (task) workload, subject to the task workload pricing. When you run a task on an existing all-purpose cluster, the task is treated as a data analytics (all-purpose) workload, subject to all-purpose workload pricing. If you select a terminated existing cluster and the job owner has Can Restart permission, Azure Databricks starts the cluster when the job is scheduled to run. Existing all-purpose clusters work best for tasks such as updating dashboards at regular intervals. Use a pool to reduce cluster start times To decrease new job cluster start time, create a pool and configure the job’s cluster to use the pool. Notebook job tips Total notebook cell output (the combined output of all notebook cells) is subject to a 20MB size limit. Additionally, individual cell output is subject to an 8MB size limit. If total cell output exceeds 20MB in size, or if the output of an individual cell is larger than 8MB, the run is canceled and marked as failed. If you need help finding cells near or beyond the limit, run the notebook against an all-purpose cluster and use this notebook autosave technique. Streaming tasks Spark Streaming jobs should never have maximum concurrent runs set to greater than 1. Streaming jobs should be set to run using the cron expression ""* * * * * ?"" (every minute). Since a streaming task runs continuously, it should always be the final task in a job. JAR jobs When running a JAR job, keep in mind the following: Output size limits Note Available in Databricks Runtime 6.3 and above. Job output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run is canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to Azure Databricks by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default, the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster’s log files. Setting this flag is recommended only for job clusters for JAR jobs because it will disable notebook results. Use the shared SparkContext Because Azure Databricks is a managed service, some code changes may be necessary to ensure that your Apache Spark jobs run correctly. JAR job programs must use the shared SparkContext API to get the SparkContext. Because Azure Databricks initializes the SparkContext, programs that invoke new SparkContext() will fail. To get the SparkContext, use only the shared SparkContext created by Azure Databricks: Scala Copy val goodSparkContext = SparkContext.getOrCreate()
val goodSparkSession = SparkSession.builder().getOrCreate()
 There are also several methods you should avoid when using the shared SparkContext. Do not call SparkContext.stop(). Do not call System.exit(0) or sc.stop() at the end of your Main program. This can cause undefined behavior. Use try-finally blocks for job clean up Consider a JAR that consists of two parts: jobBody() which contains the main part of the job. jobCleanup() which has to be executed after jobBody() whether that function succeeded or returned an exception. As an example, jobBody() may create tables, and you can use jobCleanup() to drop these tables. The safe way to ensure that the clean up method is called is to put a try-finally block in the code: Scala Copy try {
  jobBody()
} finally {
  jobCleanup()
}
 You should not try to clean up using sys.addShutdownHook(jobCleanup) or the following code: Scala Copy val cleanupThread = new Thread { override def run = jobCleanup() }
Runtime.getRuntime.addShutdownHook(cleanupThread)
 Due to the way the lifetime of Spark containers is managed in Azure Databricks, the shutdown hooks are not run reliably. Configure JAR job parameters You pass parameters to JAR jobs with a JSON string array. See the spark_jar_task object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. To access these parameters, inspect the String array passed into your main function. Library dependencies The Spark driver has certain library dependencies that cannot be overridden. These libraries take priority over any of your libraries that conflict with them. To get the full list of the driver library dependencies, run the following command inside a notebook attached to a cluster of the same Spark version (or the cluster with the driver you want to examine). Bash Copy %sh
ls /databricks/jars
 Manage library dependencies A good rule of thumb when dealing with library dependencies while creating JARs for jobs is to list Spark and Hadoop as provided dependencies. On Maven, add Spark and Hadoop as provided dependencies, as shown in the following example: XML Copy <dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.3.0</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-core</artifactId>
  <version>1.2.1</version>
  <scope>provided</scope>
</dependency>
 In sbt, add Spark and Hadoop as provided dependencies, as shown in the following example: Scala Copy libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.3.0"" % ""provided""
libraryDependencies += ""org.apache.hadoop"" %% ""hadoop-core"" % ""1.2.1"" % ""provided""
 Tip Specify the correct Scala version for your dependencies based on the version you are running.",Jobs
33,Customer is facing job failure and they want to know the root cause,"Customer is facing job failure and they want to know the root cause
https://adb-5365526239844594.2.databricks.azure.cn/?o=5365526239844594#job/148933742092133/run/17931344
set access key for fs.azure.account.key.brewdatadlsbkpcnprod.dfs.core.chinacloudapi.cn set access key for spark.hadoop.fs.azure.account.key.brewdatadlsbkpcnprod.dfs.core.chinacloudapi.cn /mnt/brewdat/adf/config/job_adf_abi_cloud_coupon_hz.txt abfss://brewdat-china@brewdatadlsbkpcnprod.dfs.core.chinacloudapi.cn/root/working/abi_cloud_coupon/tmp/coupon_bees_task create database if not exists abi_cloud_coupon_tmp; drop table if exists abi_cloud_coupon_tmp.coupon_bees_task; CREATE TABLE if not exists abi_cloud_coupon_tmp.coupon_bees_task( id bigint, coupon_code bigint, poc_id bigint, poc_name string, poc_user_name string, poc_user_phone string, product_base_sku bigint, product_name string, product_spec string, coupon_quantity bigint, coupon_status int, coupon_type int, coupon_image string, biz_index int, biz_id string, biz_name string, use_lowest_count int, order_sn bigint, use_valid_days int, use_percentage int, use_start_time timestamp, use_deadline timestamp, update_by string, update_time timestamp, create_time timestamp, create_by string, is_delete int, version int, in_cart int, selected int, user_id bigint ) USING csv options(PATH = 'abfss://brewdat-china@brewdatadlsbkpcnprod.dfs.core.chinacloudapi.cn/root/working/abi_cloud_coupon/tmp/coupon_bees_task', sep = '\u0001', nullValue='\\N', nanValue='\\N', multiLine=true); Traceback (most recent call last): File ""<command-1101219604802614>"", line 13, in <module> ingest_lz_2_hz(configs) File ""<command-1101219604803367>"", line 63, in ingest_lz_2_hz spark.sql(sql) File ""/databricks/spark/python/pyspark/sql/session.py"", line 777, in sql return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped) File ""/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py"", line 1304, in __call__ return_value = get_return_value( File ""/databricks/spark/python/pyspark/sql/utils.py"", line 123, in deco raise converted from None pyspark.sql.utils.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:java.lang.NullPointerException)",https://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs Article 07/14/2022 29 minutes to read 4 contributors In this article Create a job Run a job View jobs View runs for a job View job run details View recent job runs Export job run results Edit a job Edit a task Clone a job Clone a task Delete a job Delete a task Copy a task path Run jobs using notebooks in a remote Git repository Best practices A job is a way to run non-interactive code in an Azure Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. You can also run jobs interactively in the notebook UI. You can create and run a job using the UI, the CLI, or by invoking the Jobs API. You can repair and re-run a failed or canceled job using the UI or API. You can monitor job run results using the UI, CLI, API, and email notifications. This article focuses on performing job tasks using the UI. For the other methods, see Jobs CLI and Jobs API 2.1. Your job can consist of a single task or can be a large, multi-task workflow with complex dependencies. Azure Databricks manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs. You can run your jobs immediately or periodically through an easy-to-use scheduling system. You can implement a task in a JAR, an Azure Databricks notebook, a Delta Live Tables pipeline, or an application written in Scala, Java, or Python. Legacy Spark Submit applications are also supported. You control the execution order of tasks by specifying dependencies between the tasks. You can configure tasks to run in sequence or parallel. The following diagram illustrates a workflow that: Ingests raw clickstream data and performs processing to sessionize the records. Ingests order data and joins it with the sessionized clickstream data to create a prepared data set for analysis. Extracts features from the prepared data. Performs tasks in parallel to persist the features and train a machine learning model. To create your first workflow with an Azure Databricks job, see the quickstart. Important You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. A workspace is limited to 1000 concurrent job runs. A 429 Too Many Requests response is returned when you request a run that cannot start immediately. The number of jobs a workspace can create in an hour is limited to 5000 (includes “run now” and “runs submit”). This limit also affects jobs created by the REST API and notebook workflows. Create a job Do one of the following: Click Workflows in the sidebar and click . In the sidebar, click Create and select Job from the menu. The Tasks tab appears with the create task dialog. Replace Add a name for your job… with your job name. Enter a name for the task in the Task name field. Specify the type of task to run. In the Type drop-down, select Notebook, JAR, Spark Submit, Python, or Pipeline. Notebook: In the Source drop-down, select a location for the notebook; either Workspace for a notebook located in a Azure Databricks workspace folder or Git provider for a notebook located in a remote Git repository. Workspace: Use the file browser to find the notebook, click the notebook name, and click Confirm. Git provider: Click Edit and enter the Git repository information. See Run jobs using notebooks in a remote Git repository. JAR: Specify the Main class. Use the fully qualified name of the class containing the main method, for example, org.apache.spark.examples.SparkPi. Then click Add under Dependent Libraries to add libraries required to run the task. One of these libraries must contain the main class. To learn more about JAR tasks, see JAR jobs. Spark Submit: In the Parameters text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the DFSReadWriteTest from the Apache Spark examples: JSON Copy [""--class"",""org.apache.spark.examples.DFSReadWriteTest"",""dbfs:/FileStore/libraries/spark_examples_2_12_3_1_1.jar"",""/dbfs/databricks-datasets/README.md"",""/FileStore/examples/output/""]
 Important There are several limitations for spark-submit tasks: You can run spark-submit tasks only on new clusters. Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see Cluster autoscaling. Spark-submit does not support Databricks Utilities. To use Databricks Utilities, use JAR tasks instead. Python: In the Path textbox, enter the URI of a Python script on DBFS or cloud storage; for example, dbfs:/FileStore/myscript.py. Pipeline: In the Pipeline drop-down, select an existing Delta Live Tables pipeline. Python Wheel: In the Package name text box, enter the package to import, for example, myWheel-1.0-py2.py3-none-any.whl. In the Entry Point text box, enter the function to call when starting the wheel. Click Add under Dependent Libraries to add libraries required to run the task. Configure the cluster where the task runs. In the Cluster drop-down, select either New Job Cluster or Existing All-Purpose Clusters. New Job Cluster: Click Edit in the Cluster drop-down and complete the cluster configuration. Existing All-Purpose Cluster: Select an existing cluster in the Cluster drop-down. To open the cluster in a new page, click the icon to the right of the cluster name and description. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters. Notebook: Click Add and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the Run a job with different parameters option. Parameters set the value of the notebook widget specified by the key of the parameter. Use task parameter variables to pass a limited set of dynamic values as part of a parameter value. JAR: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See Configure JAR job parameters. Spark Submit task: Parameters are specified as a JSON-formatted array of strings. Conforming to the Apache Spark spark-submit convention, parameters after the JAR path are passed to the main method of the main class. Python: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the argparse module in Python. Python Wheel: In the Parameters drop-down, select Positional arguments to enter parameters as a JSON-formatted array of strings, or select Keyword arguments > Add to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. To access additional options, including Dependent Libraries, Retry Policy, and Timeouts, click Advanced Options. See Edit a task. Click Create. To optionally set the job’s schedule, click Edit schedule in the Job details panel. See Schedule a job. To optionally allow multiple concurrent runs of the same job, click Edit concurrent runs in the Job details panel. See Maximum concurrent runs. To optionally specify email addresses to receive notifications on job events, click Edit notifications in the Job details panel. See Notifications. To optionally control permission levels on the job, click Edit permissions in the Job details panel. See Control access to jobs. To add another task, click below the task you just created. A shared cluster option is provided if you have configured a New Job Cluster for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Run a job Click Workflows in the sidebar. Select a job and click the Runs tab. You can run a job immediately or schedule the job to run later. If one or more tasks in a job with multiple tasks are not successful, you can re-run the subset of unsuccessful tasks. See Repair an unsuccessful job run. Run a job immediately To run the job immediately, click . Tip You can perform a test run of a job with a notebook task by clicking Run Now. If you need to make changes to the notebook, clicking Run Now again after editing the notebook will automatically run the new version of the notebook. Run a job with different parameters You can use Run Now with Different Parameters to re-run a job with different parameters or different values for existing parameters. Click next to Run Now and select Run Now with Different Parameters or, in the Active Runs table, click Run Now with Different Parameters. Enter the new parameters depending on the type of task. Notebook: You can enter parameters as key-value pairs or a JSON object. The provided parameters are merged with the default parameters for the triggered run. You can use this dialog to set the values of widgets. JAR and spark-submit: You can enter a list of parameters or a JSON document. If you delete keys, the default parameters are used. You can also add task parameter variables for the run. Click Run. Repair an unsuccessful job run You can repair failed or canceled multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks and any tasks that depend on them are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs. You can change job or task settings before repairing the job run. Unsuccessful tasks are re-run with the current job and task settings. For example, if you change the path to a notebook or a cluster setting, the task is re-run with the updated notebook or cluster settings. You can view the history of all task runs on the Task run details page. Note If one or more tasks share a job cluster, a repair run creates a new job cluster; for example, if the original run used the job cluster my_job_cluster, the first repair run uses the new job cluster my_job_cluster_v1, allowing you to easily see the cluster and cluster settings used by the initial run and any repair runs. The settings for my_job_cluster_v1 are the same as the current settings for my_job_cluster. Repair is supported only with jobs that orchestrate two or more tasks. The Duration value displayed in the Runs tab includes the time the first run started until the time when the latest repair run finished. For example, if a run failed twice and succeeded on the third run, the duration includes the time for all three runs. To repair an unsuccessful job run: Click Jobs in the sidebar. In the Name column, click a job name. The Runs tab shows active runs and completed runs, including any unsuccessful runs. Click the link for the unsuccessful run in the Start time column of the Completed Runs (past 60 days) table. The Job run details page appears. Click Repair run. The Repair job run dialog appears, listing all unsuccessful tasks and any dependent tasks that will be re-run. To add or edit parameters for the tasks to repair, enter the parameters in the Repair job run dialog. Parameters you enter in the Repair job run dialog override existing values. On subsequent repair runs, you can return a parameter to its original value by clearing the key and value in the Repair job run dialog. Click Repair run in the Repair job run dialog. View task run history To view the run history of a task, including successful and unsuccessful runs: Click on a task on the Job run details page. The Task run details page appears. Select the task run in the run history drop-down. Schedule a job To define a schedule for the job: Click Edit schedule in the Job details panel and set the Schedule Type to Scheduled. Specify the period, starting time, and time zone. Optionally select the Show Cron Syntax checkbox to display and edit the schedule in Quartz Cron Syntax. Note Azure Databricks enforces a minimum interval of 10 seconds between subsequent runs triggered by the schedule of a job regardless of the seconds configuration in the cron expression. You can choose a time zone that observes daylight saving time or UTC. If you select a zone that observes daylight saving time, an hourly job will be skipped or may appear to not fire for an hour or two when daylight saving time begins or ends. To run at every hour (absolute time), choose UTC. The job scheduler is not intended for low latency jobs. Due to network or cloud issues, job runs may occasionally be delayed up to several minutes. In these situations, scheduled jobs will run immediately upon service availability. Click Save. Pause and resume a job schedule To pause a job, you can either: Click Pause in the Job details panel. Click Edit schedule in the Job details panel and set the Schedule Type to Manual (Paused) To resume a paused job schedule, set the Schedule Type to Scheduled. View jobs Click Workflows in the sidebar. The Jobs list appears. The Jobs page lists all defined jobs, the cluster definition, the schedule, if any, and the result of the last run. You can filter jobs in the Jobs list: Using keywords. Selecting only the jobs you own. Selecting all jobs you have permissions to access. Access to this filter requires that Jobs access control is enabled. Using tags. To search for a tag created with only a key, type the key into the search box. To search for a tag created with a key and value, you can search by the key, the value, or both the key and value. For example, for a tag with the key department and the value finance, you can search for department or finance to find matching jobs. To search by both the key and value, enter the key and value separated by a colon; for example, department:finance. You can also click any column header to sort the list of jobs (either descending or ascending) by that column. The default sorting is by job name in ascending order. View runs for a job Click Workflows in the sidebar. In the Name column, click a job name. The Runs tab appears with a table of active runs and completed runs. To switch to a matrix view, click Matrix. The matrix view shows a history of runs for the job, including each job task. The Job Runs row of the matrix displays the total duration of the run and the state of the run. To view details of the run, including the start time, duration, and status, hover over the bar in the Job Runs row. Each cell in the Tasks row represents a task and the corresponding status of the task. To view details of each task, including the start time, duration, cluster, and status, hover over the cell for that task. The job run and task run bars are color-coded to indicate the status of the run. Successful runs are green, unsuccessful runs are red, and skipped runs are pink. The height of the individual job run and task run bars provides a visual indication of the run duration. Azure Databricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends that you export results before they expire. For more information, see Export job run results. View job run details The job run details page contains job output and links to logs, including information about the success or failure of each task in the job run. You can access job run details from the Runs tab for the job. To view job run details from the Runs tab, click the link for the run in the Run column of the Completed Runs (past 60 days) table. To return to the Runs tab for the job, click on the Job ID value. Click on a task to view task run details, including: the cluster that ran the task the Spark UI for the task logs for the task metrics for the task Click the Job ID value to return to the Runs tab for the job. Click the Job run ID value to return to the job run details. View recent job runs You can view a list of currently running and recently completed runs for all jobs in a workspace you have access to, including runs started by external orchestration tools such as Apache Airflow or Azure Data Factory. To view the list of recent job runs: Click Workflows in the sidebar. The Jobs list appears. Click the Job runs tab. The Job runs list appears. The Job runs list displays: The start time for the run. The name of the job associated with the run. The user name that the job runs as. Whether the run was triggered by a job schedule or an API request, or was manually started. The time elapsed for a currently running job, or the total running time for a completed run. The status of the run, either Pending, Running, Skipped, Succeeded, Failed, Terminating, Terminated, Internal Error, Timed Out, Canceled, Canceling, or Waiting for Retry. To view job run details, click the link in the Start time column for the run. To view job details, click the job name in the Job column. Export job run results You can export notebook run results and job run logs for all job types. Export notebook run results You can persist job runs by exporting their results. For notebook job runs, you can export a rendered notebook that can later be imported into your Azure Databricks workspace. To export notebook run results for a job with a single task: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click Export to HTML. To export notebook run results for a job with multiple tasks: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click the notebook task to export. Click Export to HTML. Export job run logs You can also export the logs for your job run. You can set up your job to automatically deliver logs to DBFS through the Job API. See the new_cluster.cluster_log_conf object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. Edit a job Some configuration options are available on the job, and other options are available on individual tasks. For example, the maximum concurrent runs can be set on the job only, while parameters must be defined for each task. To change the configuration for a job: Click Workflows in the sidebar. In the Name column, click the job name. The side panel displays the Job details. You can change the schedule, cluster configuration, notifications, maximum number of concurrent runs, and add or change tags. If job access control is enabled, you can also edit job permissions. Tags To add labels or key:value attributes to your job, you can add tags when you edit the job. You can use tags to filter jobs in the Jobs list; for example, you can use a department tag to filter all jobs that belong to a specific department. Note Because job tags are not designed to store sensitive information such as personally identifiable information or passwords, Databricks recommends using tags for non-sensitive values only. Tags also propagate to job clusters created when a job is run, allowing you to use tags with your existing cluster monitoring. To add or edit tags, click + Tag in the Job details side panel. You can add the tag as a key and value, or a label. To add a label, enter the label in the Key field and leave the Value field empty. Clusters To see tasks associated with a cluster, hover over the cluster in the side panel. To change the cluster configuration for all associated tasks, click Configure under the cluster. To configure a new cluster for all associated tasks, click Swap under the cluster. Maximum concurrent runs The maximum number of parallel runs for this job. Azure Databricks skips the run if the job has already reached its maximum number of active runs when attempting to start a new run. Set this value higher than the default of 1 to perform multiple runs of the same job concurrently. This is useful, for example, if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or you want to trigger multiple runs that differ by their input parameters. Notifications You can add one or more email addresses to notify when runs of this job begin, complete, or fail: Click Edit notifications. Click Add. Enter an email address and click the check box for each notification type to send to that address. To enter another email address for notification, click Add. If you do not want to receive notifications for skipped job runs, click the check box. Click Confirm. Integrate these email notifications with your favorite notification tools, including: PagerDuty Slack Control access to jobs Job access control enables job owners and administrators to grant fine-grained permissions on their jobs. Job owners can choose which other users or groups can view the results of the job. Owners can also choose who can manage their job runs (Run now and Cancel run permissions). See Jobs access control for details. Edit a task To set task configuration options: Click Workflows in the sidebar. In the Name column, click the job name. Click the Tasks tab. Task dependencies You can define the order of execution of tasks in a job using the Depends on drop-down. You can set this field to one or more tasks in the job. Note Depends on is not visible if the job consists of only a single task. Configuring task dependencies creates a Directed Acyclic Graph (DAG) of task execution, a common way of representing execution order in job schedulers. For example, consider the following job consisting of four tasks: Task 1 is the root task and does not depend on any other task. Task 2 and Task 3 depend on Task 1 completing first. Finally, Task 4 depends on Task 2 and Task 3 completing successfully. Azure Databricks runs upstream tasks before running downstream tasks, running as many of them in parallel as possible. The following diagram illustrates the order of processing for these tasks: Individual task configuration options Individual tasks have the following configuration options: In this section: Cluster Dependent libraries Task parameter variables Timeout Retries Cluster To configure the cluster where a task runs, click the Cluster drop-down. You can edit a shared job cluster, but you cannot delete a shared cluster if it is still used by other tasks. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Dependent libraries Dependent libraries will be installed on the cluster before the task runs. You must set all task dependencies to ensure they are installed before the run starts. To add a dependent library, click Advanced options and select Add Dependent Libraries to open the Add Dependent Library chooser. Follow the recommendations in Library dependencies for specifying dependencies. Important If you have configured a library to install on all clusters automatically, or you select an existing terminated cluster that has libraries installed, the job execution does not wait for library installation to complete. If a job requires a specific library, you should attach the library to the job in the Dependent Libraries field. Task parameter variables You can pass templated variables into a job task as part of the task’s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job’s start time. When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named MyJobId with a value of my-job-6 for any run of job ID 6, add the following task parameter: JSON Copy {
  ""MyJobID"": ""my-job-{{job_id}}""
}
 The contents of the double curly braces are not evaluated as expressions, so you cannot do operations or functions within double-curly braces. Whitespace is not stripped inside the curly braces, so {{ job_id }} will not be evaluated. The following task parameter variables are supported: Variable Description {{job_id}} The unique identifier assigned to a job {{run_id}} The unique identifier assigned to a job run {{start_date}} The date a task run started. The format is yyyy-MM-dd in UTC timezone. {{start_time}} The timestamp of the run’s start of execution after the cluster is created and ready. The format is milliseconds since UNIX epoch in UTC timezone, as returned by System.currentTimeMillis(). {{task_retry_count}} The number of retries that have been attempted to run a task if the first attempt fails. The value is 0 for the first attempt and increments with each retry. {{parent_run_id}} The unique identifier assigned to the run of a job with multiple tasks. {{task_key}} The unique name assigned to a task that’s part of a job with multiple tasks. You can set these variables with any task when you Create a job, Edit a job, or Run a job with different parameters. Timeout The maximum completion time for a job. If the job does not complete in this time, Azure Databricks sets its status to “Timed Out”. Retries A policy that determines when and how many times failed runs are retried. To set the retries for the task, click Advanced options and select Edit Retry Policy. The retry interval is calculated in milliseconds between the start of the failed run and the subsequent retry run. Note If you configure both Timeout and Retries, the timeout applies to each retry. Clone a job You can quickly create a new job by cloning an existing job. Cloning a job creates an identical copy of the job, except for the job ID. On the job’s page, click More … next to the job’s name and select Clone from the drop-down menu. Clone a task You can quickly create a new task by cloning an existing task: On the job’s page, click the Tasks tab. Select the task to clone. Click and select Clone task. Delete a job To delete a job, on the job’s page, click More … next to the job’s name and select Delete from the drop-down menu. Delete a task To delete a task: Click the Tasks tab. Select the task to be deleted. Click and select Remove task. Copy a task path To copy the path to a task, for example, a notebook path: Click the Tasks tab. Select the task containing the path to copy. Click next to the task path to copy the path to the clipboard. Run jobs using notebooks in a remote Git repository Important This feature is in Public Preview. You can run jobs with notebooks located in a remote Git repository. This feature simplifies creation and management of production jobs and automates continuous deployment: You don’t need to create a separate production repo in Azure Databricks, manage permissions for it, and keep it updated. You can prevent unintentional changes to a production job, such as local edits in the production repo or changes from switching a branch. The job definition process has a single source of truth in the remote repository. To use notebooks in a remote Git repository, you must Set up Git integration with Databricks Repos. To create a task with a notebook located in a remote Git repository: In the Type drop-down, select Notebook. In the Source drop-down, select Git provider. The Git information dialog appears. In the Git Information dialog, enter details for the repository. For Path, enter a relative path to the notebook location, such as etl/notebooks/. When you enter the relative path, don’t begin it with / or ./ and don’t include the notebook file extension, such as .py. Additional notebook tasks in a multitask job reference the same commit in the remote repository in one of the following ways: sha of $branch/head when git_branch is set sha of $tag when git_tag is set the value of git_commit In a multitask job, there cannot be a task that uses a local notebook and another task that uses a remote repository. This restriction doesn’t apply to non-notebook tasks. Best practices In this section: Cluster configuration tips Notebook job tips Streaming tasks JAR jobs Library dependencies Cluster configuration tips Cluster configuration is important when you operationalize a job. The following provides general guidance on choosing and configuring job clusters, followed by recommendations for specific job types. Use shared job clusters To optimize resource usage with jobs that orchestrate multiple tasks, use shared job clusters. A shared job cluster allows multiple tasks in the same job run to reuse the cluster. You can use a single job cluster to run all tasks that are part of the job, or multiple job clusters optimized for specific workloads. To use a shared job cluster: Select New Job Clusters when you create a task and complete the cluster configuration. Select the new cluster when adding a task to the job, or create a new job cluster. Any cluster you configure when you select New Job Clusters is available to any task in the job. A shared job cluster is scoped to a single job run, and cannot be used by other jobs or runs of the same job. Libraries cannot be declared in a shared job cluster configuration. You must add dependent libraries in task settings. Choose the correct cluster type for your job New Job Clusters are dedicated clusters for a job or task run. A shared job cluster is created and started when the first task using the cluster starts and terminates after the last task using the cluster completes. The cluster is not terminated when idle but terminates only after all tasks using it have completed. If a shared job cluster fails or is terminated before all tasks have finished, a new cluster is created. A cluster scoped to a single task is created and started when the task starts and terminates when the task completes. In production, Databricks recommends using new shared or task scoped clusters so that each job or task runs in a fully isolated environment. When you run a task on a new cluster, the task is treated as a data engineering (task) workload, subject to the task workload pricing. When you run a task on an existing all-purpose cluster, the task is treated as a data analytics (all-purpose) workload, subject to all-purpose workload pricing. If you select a terminated existing cluster and the job owner has Can Restart permission, Azure Databricks starts the cluster when the job is scheduled to run. Existing all-purpose clusters work best for tasks such as updating dashboards at regular intervals. Use a pool to reduce cluster start times To decrease new job cluster start time, create a pool and configure the job’s cluster to use the pool. Notebook job tips Total notebook cell output (the combined output of all notebook cells) is subject to a 20MB size limit. Additionally, individual cell output is subject to an 8MB size limit. If total cell output exceeds 20MB in size, or if the output of an individual cell is larger than 8MB, the run is canceled and marked as failed. If you need help finding cells near or beyond the limit, run the notebook against an all-purpose cluster and use this notebook autosave technique. Streaming tasks Spark Streaming jobs should never have maximum concurrent runs set to greater than 1. Streaming jobs should be set to run using the cron expression ""* * * * * ?"" (every minute). Since a streaming task runs continuously, it should always be the final task in a job. JAR jobs When running a JAR job, keep in mind the following: Output size limits Note Available in Databricks Runtime 6.3 and above. Job output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run is canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to Azure Databricks by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default, the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster’s log files. Setting this flag is recommended only for job clusters for JAR jobs because it will disable notebook results. Use the shared SparkContext Because Azure Databricks is a managed service, some code changes may be necessary to ensure that your Apache Spark jobs run correctly. JAR job programs must use the shared SparkContext API to get the SparkContext. Because Azure Databricks initializes the SparkContext, programs that invoke new SparkContext() will fail. To get the SparkContext, use only the shared SparkContext created by Azure Databricks: Scala Copy val goodSparkContext = SparkContext.getOrCreate()
val goodSparkSession = SparkSession.builder().getOrCreate()
 There are also several methods you should avoid when using the shared SparkContext. Do not call SparkContext.stop(). Do not call System.exit(0) or sc.stop() at the end of your Main program. This can cause undefined behavior. Use try-finally blocks for job clean up Consider a JAR that consists of two parts: jobBody() which contains the main part of the job. jobCleanup() which has to be executed after jobBody() whether that function succeeded or returned an exception. As an example, jobBody() may create tables, and you can use jobCleanup() to drop these tables. The safe way to ensure that the clean up method is called is to put a try-finally block in the code: Scala Copy try {
  jobBody()
} finally {
  jobCleanup()
}
 You should not try to clean up using sys.addShutdownHook(jobCleanup) or the following code: Scala Copy val cleanupThread = new Thread { override def run = jobCleanup() }
Runtime.getRuntime.addShutdownHook(cleanupThread)
 Due to the way the lifetime of Spark containers is managed in Azure Databricks, the shutdown hooks are not run reliably. Configure JAR job parameters You pass parameters to JAR jobs with a JSON string array. See the spark_jar_task object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. To access these parameters, inspect the String array passed into your main function. Library dependencies The Spark driver has certain library dependencies that cannot be overridden. These libraries take priority over any of your libraries that conflict with them. To get the full list of the driver library dependencies, run the following command inside a notebook attached to a cluster of the same Spark version (or the cluster with the driver you want to examine). Bash Copy %sh
ls /databricks/jars
 Manage library dependencies A good rule of thumb when dealing with library dependencies while creating JARs for jobs is to list Spark and Hadoop as provided dependencies. On Maven, add Spark and Hadoop as provided dependencies, as shown in the following example: XML Copy <dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.3.0</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-core</artifactId>
  <version>1.2.1</version>
  <scope>provided</scope>
</dependency>
 In sbt, add Spark and Hadoop as provided dependencies, as shown in the following example: Scala Copy libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.3.0"" % ""provided""
libraryDependencies += ""org.apache.hadoop"" %% ""hadoop-core"" % ""1.2.1"" % ""provided""
 Tip Specify the correct Scala version for your dependencies based on the version you are running.",Jobs
34,ARR || Geico|| Databricks schedule job doesn't retry || 2206220040008314,"Question: What time did the problem begin?
Answer: Not sure, use current time

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer: 

Question: Is this a new problem, or it has happened before?
Answer: Other, don't know or not applicable

Question: Job URL for the job with issue
Answer: https://adb-420513747001259.19.azuredatabricks.net/?o=420513747001259#job/36788296338653

Question: Job URL from last known good run if available
Answer: 

Question: Azure storage account resource ID if job involved reading from or writing to Azure storage account
Answer: gzfadbstgpd1sto001.dfs.core.windows.net

Question: Cluster URL in case job URL is not available
Answer: https://adb-420513747001259.19.azuredatabricks.net/?o=420513747001259#setting/clusters/1217-154911-2tzt2l70/configuration

Question: Additional details about the issue
Answer: 
We noticed for failed jobs in databricks, the scheduled/manual job run did not do retry for the job even when the max retry for the job was set to 1. Following are the links to some of the jobs where we observed this behavior.

https://adb-420513747001259.19.azuredatabricks.net/?o=420513747001259#job/36788296338653
https://adb-420513747001259.19.azuredatabricks.net/?o=420513747001259#job/763023231728372
https://adb-420513747001259.19.azuredatabricks.net/?o=420513747001259#job/727725317921715

Let us know if you need any additional details.





<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Databricks:
What time did the problem begin? - 2022-06-22T19:50:00.069Z;
Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank - ;
Is this a new problem, or it has happened before? - Other, don't know or not applicable;
Job URL for the job with issue - https://adb-420513747001259.19.azuredatabricks.net/?o=420513747001259#job/36788296338653;
Job URL from last known good run if available - ;
Azure storage account resource ID if job involved reading from or writing to Azure storage account - gzfadbstgpd1sto001.dfs.core.windows.net;
Cluster URL in case job URL is not available - https://adb-420513747001259.19.azuredatabricks.net/?o=420513747001259#setting/clusters/1217-154911-2tzt2l70/configuration;
Additional details about the issue - 
We noticed for failed jobs in databricks, the scheduled/manual job run did not do retry for the job even when the max retry for the job was set to 1. Following are the links to some of the jobs where we observed this behavior.

https://adb-420513747001259.19.azuredatabricks.net/?o=420513747001259#job/36788296338653
https://adb-420513747001259.19.azuredatabricks.net/?o=420513747001259#job/763023231728372
https://adb-420513747001259.19.azuredatabricks.net/?o=420513747001259#job/727725317921715

Let us know if you need any additional details.

;

- ProblemStartTime: 06/22/2022 19:50:00
- Cloud: Azure
- AzureProductSubscriptionID: 12343ffc-c458-4cd8-bad0-39de003e4bae
- AzureProductSubscriptionName: GZ-PD-SHRAPP-232-DATAINTELLIGENCE
- Tenant Id: 7389d8c0-3607-465c-a69f-7d4426502912
- Object Id: 4bdaea52-c56f-4efd-9a21-730ef77692b1
- SubscriptionType:",https://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs Article 07/14/2022 29 minutes to read 4 contributors In this article Create a job Run a job View jobs View runs for a job View job run details View recent job runs Export job run results Edit a job Edit a task Clone a job Clone a task Delete a job Delete a task Copy a task path Run jobs using notebooks in a remote Git repository Best practices A job is a way to run non-interactive code in an Azure Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. You can also run jobs interactively in the notebook UI. You can create and run a job using the UI, the CLI, or by invoking the Jobs API. You can repair and re-run a failed or canceled job using the UI or API. You can monitor job run results using the UI, CLI, API, and email notifications. This article focuses on performing job tasks using the UI. For the other methods, see Jobs CLI and Jobs API 2.1. Your job can consist of a single task or can be a large, multi-task workflow with complex dependencies. Azure Databricks manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs. You can run your jobs immediately or periodically through an easy-to-use scheduling system. You can implement a task in a JAR, an Azure Databricks notebook, a Delta Live Tables pipeline, or an application written in Scala, Java, or Python. Legacy Spark Submit applications are also supported. You control the execution order of tasks by specifying dependencies between the tasks. You can configure tasks to run in sequence or parallel. The following diagram illustrates a workflow that: Ingests raw clickstream data and performs processing to sessionize the records. Ingests order data and joins it with the sessionized clickstream data to create a prepared data set for analysis. Extracts features from the prepared data. Performs tasks in parallel to persist the features and train a machine learning model. To create your first workflow with an Azure Databricks job, see the quickstart. Important You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. A workspace is limited to 1000 concurrent job runs. A 429 Too Many Requests response is returned when you request a run that cannot start immediately. The number of jobs a workspace can create in an hour is limited to 5000 (includes “run now” and “runs submit”). This limit also affects jobs created by the REST API and notebook workflows. Create a job Do one of the following: Click Workflows in the sidebar and click . In the sidebar, click Create and select Job from the menu. The Tasks tab appears with the create task dialog. Replace Add a name for your job… with your job name. Enter a name for the task in the Task name field. Specify the type of task to run. In the Type drop-down, select Notebook, JAR, Spark Submit, Python, or Pipeline. Notebook: In the Source drop-down, select a location for the notebook; either Workspace for a notebook located in a Azure Databricks workspace folder or Git provider for a notebook located in a remote Git repository. Workspace: Use the file browser to find the notebook, click the notebook name, and click Confirm. Git provider: Click Edit and enter the Git repository information. See Run jobs using notebooks in a remote Git repository. JAR: Specify the Main class. Use the fully qualified name of the class containing the main method, for example, org.apache.spark.examples.SparkPi. Then click Add under Dependent Libraries to add libraries required to run the task. One of these libraries must contain the main class. To learn more about JAR tasks, see JAR jobs. Spark Submit: In the Parameters text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the DFSReadWriteTest from the Apache Spark examples: JSON Copy [""--class"",""org.apache.spark.examples.DFSReadWriteTest"",""dbfs:/FileStore/libraries/spark_examples_2_12_3_1_1.jar"",""/dbfs/databricks-datasets/README.md"",""/FileStore/examples/output/""]
 Important There are several limitations for spark-submit tasks: You can run spark-submit tasks only on new clusters. Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see Cluster autoscaling. Spark-submit does not support Databricks Utilities. To use Databricks Utilities, use JAR tasks instead. Python: In the Path textbox, enter the URI of a Python script on DBFS or cloud storage; for example, dbfs:/FileStore/myscript.py. Pipeline: In the Pipeline drop-down, select an existing Delta Live Tables pipeline. Python Wheel: In the Package name text box, enter the package to import, for example, myWheel-1.0-py2.py3-none-any.whl. In the Entry Point text box, enter the function to call when starting the wheel. Click Add under Dependent Libraries to add libraries required to run the task. Configure the cluster where the task runs. In the Cluster drop-down, select either New Job Cluster or Existing All-Purpose Clusters. New Job Cluster: Click Edit in the Cluster drop-down and complete the cluster configuration. Existing All-Purpose Cluster: Select an existing cluster in the Cluster drop-down. To open the cluster in a new page, click the icon to the right of the cluster name and description. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters. Notebook: Click Add and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the Run a job with different parameters option. Parameters set the value of the notebook widget specified by the key of the parameter. Use task parameter variables to pass a limited set of dynamic values as part of a parameter value. JAR: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See Configure JAR job parameters. Spark Submit task: Parameters are specified as a JSON-formatted array of strings. Conforming to the Apache Spark spark-submit convention, parameters after the JAR path are passed to the main method of the main class. Python: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the argparse module in Python. Python Wheel: In the Parameters drop-down, select Positional arguments to enter parameters as a JSON-formatted array of strings, or select Keyword arguments > Add to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. To access additional options, including Dependent Libraries, Retry Policy, and Timeouts, click Advanced Options. See Edit a task. Click Create. To optionally set the job’s schedule, click Edit schedule in the Job details panel. See Schedule a job. To optionally allow multiple concurrent runs of the same job, click Edit concurrent runs in the Job details panel. See Maximum concurrent runs. To optionally specify email addresses to receive notifications on job events, click Edit notifications in the Job details panel. See Notifications. To optionally control permission levels on the job, click Edit permissions in the Job details panel. See Control access to jobs. To add another task, click below the task you just created. A shared cluster option is provided if you have configured a New Job Cluster for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Run a job Click Workflows in the sidebar. Select a job and click the Runs tab. You can run a job immediately or schedule the job to run later. If one or more tasks in a job with multiple tasks are not successful, you can re-run the subset of unsuccessful tasks. See Repair an unsuccessful job run. Run a job immediately To run the job immediately, click . Tip You can perform a test run of a job with a notebook task by clicking Run Now. If you need to make changes to the notebook, clicking Run Now again after editing the notebook will automatically run the new version of the notebook. Run a job with different parameters You can use Run Now with Different Parameters to re-run a job with different parameters or different values for existing parameters. Click next to Run Now and select Run Now with Different Parameters or, in the Active Runs table, click Run Now with Different Parameters. Enter the new parameters depending on the type of task. Notebook: You can enter parameters as key-value pairs or a JSON object. The provided parameters are merged with the default parameters for the triggered run. You can use this dialog to set the values of widgets. JAR and spark-submit: You can enter a list of parameters or a JSON document. If you delete keys, the default parameters are used. You can also add task parameter variables for the run. Click Run. Repair an unsuccessful job run You can repair failed or canceled multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks and any tasks that depend on them are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs. You can change job or task settings before repairing the job run. Unsuccessful tasks are re-run with the current job and task settings. For example, if you change the path to a notebook or a cluster setting, the task is re-run with the updated notebook or cluster settings. You can view the history of all task runs on the Task run details page. Note If one or more tasks share a job cluster, a repair run creates a new job cluster; for example, if the original run used the job cluster my_job_cluster, the first repair run uses the new job cluster my_job_cluster_v1, allowing you to easily see the cluster and cluster settings used by the initial run and any repair runs. The settings for my_job_cluster_v1 are the same as the current settings for my_job_cluster. Repair is supported only with jobs that orchestrate two or more tasks. The Duration value displayed in the Runs tab includes the time the first run started until the time when the latest repair run finished. For example, if a run failed twice and succeeded on the third run, the duration includes the time for all three runs. To repair an unsuccessful job run: Click Jobs in the sidebar. In the Name column, click a job name. The Runs tab shows active runs and completed runs, including any unsuccessful runs. Click the link for the unsuccessful run in the Start time column of the Completed Runs (past 60 days) table. The Job run details page appears. Click Repair run. The Repair job run dialog appears, listing all unsuccessful tasks and any dependent tasks that will be re-run. To add or edit parameters for the tasks to repair, enter the parameters in the Repair job run dialog. Parameters you enter in the Repair job run dialog override existing values. On subsequent repair runs, you can return a parameter to its original value by clearing the key and value in the Repair job run dialog. Click Repair run in the Repair job run dialog. View task run history To view the run history of a task, including successful and unsuccessful runs: Click on a task on the Job run details page. The Task run details page appears. Select the task run in the run history drop-down. Schedule a job To define a schedule for the job: Click Edit schedule in the Job details panel and set the Schedule Type to Scheduled. Specify the period, starting time, and time zone. Optionally select the Show Cron Syntax checkbox to display and edit the schedule in Quartz Cron Syntax. Note Azure Databricks enforces a minimum interval of 10 seconds between subsequent runs triggered by the schedule of a job regardless of the seconds configuration in the cron expression. You can choose a time zone that observes daylight saving time or UTC. If you select a zone that observes daylight saving time, an hourly job will be skipped or may appear to not fire for an hour or two when daylight saving time begins or ends. To run at every hour (absolute time), choose UTC. The job scheduler is not intended for low latency jobs. Due to network or cloud issues, job runs may occasionally be delayed up to several minutes. In these situations, scheduled jobs will run immediately upon service availability. Click Save. Pause and resume a job schedule To pause a job, you can either: Click Pause in the Job details panel. Click Edit schedule in the Job details panel and set the Schedule Type to Manual (Paused) To resume a paused job schedule, set the Schedule Type to Scheduled. View jobs Click Workflows in the sidebar. The Jobs list appears. The Jobs page lists all defined jobs, the cluster definition, the schedule, if any, and the result of the last run. You can filter jobs in the Jobs list: Using keywords. Selecting only the jobs you own. Selecting all jobs you have permissions to access. Access to this filter requires that Jobs access control is enabled. Using tags. To search for a tag created with only a key, type the key into the search box. To search for a tag created with a key and value, you can search by the key, the value, or both the key and value. For example, for a tag with the key department and the value finance, you can search for department or finance to find matching jobs. To search by both the key and value, enter the key and value separated by a colon; for example, department:finance. You can also click any column header to sort the list of jobs (either descending or ascending) by that column. The default sorting is by job name in ascending order. View runs for a job Click Workflows in the sidebar. In the Name column, click a job name. The Runs tab appears with a table of active runs and completed runs. To switch to a matrix view, click Matrix. The matrix view shows a history of runs for the job, including each job task. The Job Runs row of the matrix displays the total duration of the run and the state of the run. To view details of the run, including the start time, duration, and status, hover over the bar in the Job Runs row. Each cell in the Tasks row represents a task and the corresponding status of the task. To view details of each task, including the start time, duration, cluster, and status, hover over the cell for that task. The job run and task run bars are color-coded to indicate the status of the run. Successful runs are green, unsuccessful runs are red, and skipped runs are pink. The height of the individual job run and task run bars provides a visual indication of the run duration. Azure Databricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends that you export results before they expire. For more information, see Export job run results. View job run details The job run details page contains job output and links to logs, including information about the success or failure of each task in the job run. You can access job run details from the Runs tab for the job. To view job run details from the Runs tab, click the link for the run in the Run column of the Completed Runs (past 60 days) table. To return to the Runs tab for the job, click on the Job ID value. Click on a task to view task run details, including: the cluster that ran the task the Spark UI for the task logs for the task metrics for the task Click the Job ID value to return to the Runs tab for the job. Click the Job run ID value to return to the job run details. View recent job runs You can view a list of currently running and recently completed runs for all jobs in a workspace you have access to, including runs started by external orchestration tools such as Apache Airflow or Azure Data Factory. To view the list of recent job runs: Click Workflows in the sidebar. The Jobs list appears. Click the Job runs tab. The Job runs list appears. The Job runs list displays: The start time for the run. The name of the job associated with the run. The user name that the job runs as. Whether the run was triggered by a job schedule or an API request, or was manually started. The time elapsed for a currently running job, or the total running time for a completed run. The status of the run, either Pending, Running, Skipped, Succeeded, Failed, Terminating, Terminated, Internal Error, Timed Out, Canceled, Canceling, or Waiting for Retry. To view job run details, click the link in the Start time column for the run. To view job details, click the job name in the Job column. Export job run results You can export notebook run results and job run logs for all job types. Export notebook run results You can persist job runs by exporting their results. For notebook job runs, you can export a rendered notebook that can later be imported into your Azure Databricks workspace. To export notebook run results for a job with a single task: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click Export to HTML. To export notebook run results for a job with multiple tasks: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click the notebook task to export. Click Export to HTML. Export job run logs You can also export the logs for your job run. You can set up your job to automatically deliver logs to DBFS through the Job API. See the new_cluster.cluster_log_conf object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. Edit a job Some configuration options are available on the job, and other options are available on individual tasks. For example, the maximum concurrent runs can be set on the job only, while parameters must be defined for each task. To change the configuration for a job: Click Workflows in the sidebar. In the Name column, click the job name. The side panel displays the Job details. You can change the schedule, cluster configuration, notifications, maximum number of concurrent runs, and add or change tags. If job access control is enabled, you can also edit job permissions. Tags To add labels or key:value attributes to your job, you can add tags when you edit the job. You can use tags to filter jobs in the Jobs list; for example, you can use a department tag to filter all jobs that belong to a specific department. Note Because job tags are not designed to store sensitive information such as personally identifiable information or passwords, Databricks recommends using tags for non-sensitive values only. Tags also propagate to job clusters created when a job is run, allowing you to use tags with your existing cluster monitoring. To add or edit tags, click + Tag in the Job details side panel. You can add the tag as a key and value, or a label. To add a label, enter the label in the Key field and leave the Value field empty. Clusters To see tasks associated with a cluster, hover over the cluster in the side panel. To change the cluster configuration for all associated tasks, click Configure under the cluster. To configure a new cluster for all associated tasks, click Swap under the cluster. Maximum concurrent runs The maximum number of parallel runs for this job. Azure Databricks skips the run if the job has already reached its maximum number of active runs when attempting to start a new run. Set this value higher than the default of 1 to perform multiple runs of the same job concurrently. This is useful, for example, if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or you want to trigger multiple runs that differ by their input parameters. Notifications You can add one or more email addresses to notify when runs of this job begin, complete, or fail: Click Edit notifications. Click Add. Enter an email address and click the check box for each notification type to send to that address. To enter another email address for notification, click Add. If you do not want to receive notifications for skipped job runs, click the check box. Click Confirm. Integrate these email notifications with your favorite notification tools, including: PagerDuty Slack Control access to jobs Job access control enables job owners and administrators to grant fine-grained permissions on their jobs. Job owners can choose which other users or groups can view the results of the job. Owners can also choose who can manage their job runs (Run now and Cancel run permissions). See Jobs access control for details. Edit a task To set task configuration options: Click Workflows in the sidebar. In the Name column, click the job name. Click the Tasks tab. Task dependencies You can define the order of execution of tasks in a job using the Depends on drop-down. You can set this field to one or more tasks in the job. Note Depends on is not visible if the job consists of only a single task. Configuring task dependencies creates a Directed Acyclic Graph (DAG) of task execution, a common way of representing execution order in job schedulers. For example, consider the following job consisting of four tasks: Task 1 is the root task and does not depend on any other task. Task 2 and Task 3 depend on Task 1 completing first. Finally, Task 4 depends on Task 2 and Task 3 completing successfully. Azure Databricks runs upstream tasks before running downstream tasks, running as many of them in parallel as possible. The following diagram illustrates the order of processing for these tasks: Individual task configuration options Individual tasks have the following configuration options: In this section: Cluster Dependent libraries Task parameter variables Timeout Retries Cluster To configure the cluster where a task runs, click the Cluster drop-down. You can edit a shared job cluster, but you cannot delete a shared cluster if it is still used by other tasks. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Dependent libraries Dependent libraries will be installed on the cluster before the task runs. You must set all task dependencies to ensure they are installed before the run starts. To add a dependent library, click Advanced options and select Add Dependent Libraries to open the Add Dependent Library chooser. Follow the recommendations in Library dependencies for specifying dependencies. Important If you have configured a library to install on all clusters automatically, or you select an existing terminated cluster that has libraries installed, the job execution does not wait for library installation to complete. If a job requires a specific library, you should attach the library to the job in the Dependent Libraries field. Task parameter variables You can pass templated variables into a job task as part of the task’s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job’s start time. When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named MyJobId with a value of my-job-6 for any run of job ID 6, add the following task parameter: JSON Copy {
  ""MyJobID"": ""my-job-{{job_id}}""
}
 The contents of the double curly braces are not evaluated as expressions, so you cannot do operations or functions within double-curly braces. Whitespace is not stripped inside the curly braces, so {{ job_id }} will not be evaluated. The following task parameter variables are supported: Variable Description {{job_id}} The unique identifier assigned to a job {{run_id}} The unique identifier assigned to a job run {{start_date}} The date a task run started. The format is yyyy-MM-dd in UTC timezone. {{start_time}} The timestamp of the run’s start of execution after the cluster is created and ready. The format is milliseconds since UNIX epoch in UTC timezone, as returned by System.currentTimeMillis(). {{task_retry_count}} The number of retries that have been attempted to run a task if the first attempt fails. The value is 0 for the first attempt and increments with each retry. {{parent_run_id}} The unique identifier assigned to the run of a job with multiple tasks. {{task_key}} The unique name assigned to a task that’s part of a job with multiple tasks. You can set these variables with any task when you Create a job, Edit a job, or Run a job with different parameters. Timeout The maximum completion time for a job. If the job does not complete in this time, Azure Databricks sets its status to “Timed Out”. Retries A policy that determines when and how many times failed runs are retried. To set the retries for the task, click Advanced options and select Edit Retry Policy. The retry interval is calculated in milliseconds between the start of the failed run and the subsequent retry run. Note If you configure both Timeout and Retries, the timeout applies to each retry. Clone a job You can quickly create a new job by cloning an existing job. Cloning a job creates an identical copy of the job, except for the job ID. On the job’s page, click More … next to the job’s name and select Clone from the drop-down menu. Clone a task You can quickly create a new task by cloning an existing task: On the job’s page, click the Tasks tab. Select the task to clone. Click and select Clone task. Delete a job To delete a job, on the job’s page, click More … next to the job’s name and select Delete from the drop-down menu. Delete a task To delete a task: Click the Tasks tab. Select the task to be deleted. Click and select Remove task. Copy a task path To copy the path to a task, for example, a notebook path: Click the Tasks tab. Select the task containing the path to copy. Click next to the task path to copy the path to the clipboard. Run jobs using notebooks in a remote Git repository Important This feature is in Public Preview. You can run jobs with notebooks located in a remote Git repository. This feature simplifies creation and management of production jobs and automates continuous deployment: You don’t need to create a separate production repo in Azure Databricks, manage permissions for it, and keep it updated. You can prevent unintentional changes to a production job, such as local edits in the production repo or changes from switching a branch. The job definition process has a single source of truth in the remote repository. To use notebooks in a remote Git repository, you must Set up Git integration with Databricks Repos. To create a task with a notebook located in a remote Git repository: In the Type drop-down, select Notebook. In the Source drop-down, select Git provider. The Git information dialog appears. In the Git Information dialog, enter details for the repository. For Path, enter a relative path to the notebook location, such as etl/notebooks/. When you enter the relative path, don’t begin it with / or ./ and don’t include the notebook file extension, such as .py. Additional notebook tasks in a multitask job reference the same commit in the remote repository in one of the following ways: sha of $branch/head when git_branch is set sha of $tag when git_tag is set the value of git_commit In a multitask job, there cannot be a task that uses a local notebook and another task that uses a remote repository. This restriction doesn’t apply to non-notebook tasks. Best practices In this section: Cluster configuration tips Notebook job tips Streaming tasks JAR jobs Library dependencies Cluster configuration tips Cluster configuration is important when you operationalize a job. The following provides general guidance on choosing and configuring job clusters, followed by recommendations for specific job types. Use shared job clusters To optimize resource usage with jobs that orchestrate multiple tasks, use shared job clusters. A shared job cluster allows multiple tasks in the same job run to reuse the cluster. You can use a single job cluster to run all tasks that are part of the job, or multiple job clusters optimized for specific workloads. To use a shared job cluster: Select New Job Clusters when you create a task and complete the cluster configuration. Select the new cluster when adding a task to the job, or create a new job cluster. Any cluster you configure when you select New Job Clusters is available to any task in the job. A shared job cluster is scoped to a single job run, and cannot be used by other jobs or runs of the same job. Libraries cannot be declared in a shared job cluster configuration. You must add dependent libraries in task settings. Choose the correct cluster type for your job New Job Clusters are dedicated clusters for a job or task run. A shared job cluster is created and started when the first task using the cluster starts and terminates after the last task using the cluster completes. The cluster is not terminated when idle but terminates only after all tasks using it have completed. If a shared job cluster fails or is terminated before all tasks have finished, a new cluster is created. A cluster scoped to a single task is created and started when the task starts and terminates when the task completes. In production, Databricks recommends using new shared or task scoped clusters so that each job or task runs in a fully isolated environment. When you run a task on a new cluster, the task is treated as a data engineering (task) workload, subject to the task workload pricing. When you run a task on an existing all-purpose cluster, the task is treated as a data analytics (all-purpose) workload, subject to all-purpose workload pricing. If you select a terminated existing cluster and the job owner has Can Restart permission, Azure Databricks starts the cluster when the job is scheduled to run. Existing all-purpose clusters work best for tasks such as updating dashboards at regular intervals. Use a pool to reduce cluster start times To decrease new job cluster start time, create a pool and configure the job’s cluster to use the pool. Notebook job tips Total notebook cell output (the combined output of all notebook cells) is subject to a 20MB size limit. Additionally, individual cell output is subject to an 8MB size limit. If total cell output exceeds 20MB in size, or if the output of an individual cell is larger than 8MB, the run is canceled and marked as failed. If you need help finding cells near or beyond the limit, run the notebook against an all-purpose cluster and use this notebook autosave technique. Streaming tasks Spark Streaming jobs should never have maximum concurrent runs set to greater than 1. Streaming jobs should be set to run using the cron expression ""* * * * * ?"" (every minute). Since a streaming task runs continuously, it should always be the final task in a job. JAR jobs When running a JAR job, keep in mind the following: Output size limits Note Available in Databricks Runtime 6.3 and above. Job output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run is canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to Azure Databricks by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default, the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster’s log files. Setting this flag is recommended only for job clusters for JAR jobs because it will disable notebook results. Use the shared SparkContext Because Azure Databricks is a managed service, some code changes may be necessary to ensure that your Apache Spark jobs run correctly. JAR job programs must use the shared SparkContext API to get the SparkContext. Because Azure Databricks initializes the SparkContext, programs that invoke new SparkContext() will fail. To get the SparkContext, use only the shared SparkContext created by Azure Databricks: Scala Copy val goodSparkContext = SparkContext.getOrCreate()
val goodSparkSession = SparkSession.builder().getOrCreate()
 There are also several methods you should avoid when using the shared SparkContext. Do not call SparkContext.stop(). Do not call System.exit(0) or sc.stop() at the end of your Main program. This can cause undefined behavior. Use try-finally blocks for job clean up Consider a JAR that consists of two parts: jobBody() which contains the main part of the job. jobCleanup() which has to be executed after jobBody() whether that function succeeded or returned an exception. As an example, jobBody() may create tables, and you can use jobCleanup() to drop these tables. The safe way to ensure that the clean up method is called is to put a try-finally block in the code: Scala Copy try {
  jobBody()
} finally {
  jobCleanup()
}
 You should not try to clean up using sys.addShutdownHook(jobCleanup) or the following code: Scala Copy val cleanupThread = new Thread { override def run = jobCleanup() }
Runtime.getRuntime.addShutdownHook(cleanupThread)
 Due to the way the lifetime of Spark containers is managed in Azure Databricks, the shutdown hooks are not run reliably. Configure JAR job parameters You pass parameters to JAR jobs with a JSON string array. See the spark_jar_task object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. To access these parameters, inspect the String array passed into your main function. Library dependencies The Spark driver has certain library dependencies that cannot be overridden. These libraries take priority over any of your libraries that conflict with them. To get the full list of the driver library dependencies, run the following command inside a notebook attached to a cluster of the same Spark version (or the cluster with the driver you want to examine). Bash Copy %sh
ls /databricks/jars
 Manage library dependencies A good rule of thumb when dealing with library dependencies while creating JARs for jobs is to list Spark and Hadoop as provided dependencies. On Maven, add Spark and Hadoop as provided dependencies, as shown in the following example: XML Copy <dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.3.0</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-core</artifactId>
  <version>1.2.1</version>
  <scope>provided</scope>
</dependency>
 In sbt, add Spark and Hadoop as provided dependencies, as shown in the following example: Scala Copy libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.3.0"" % ""provided""
libraryDependencies += ""org.apache.hadoop"" %% ""hadoop-core"" % ""1.2.1"" % ""provided""
 Tip Specify the correct Scala version for your dependencies based on the version you are running.",Jobs
35,[ARR] [Sev B] SR-2205310030001378 continue to #00182871  Request to join the call to explain the solution and issue,"[Issue]
This is the continue to the issue 00182871.
The previous owner suggested the configurations below.

﻿spark.driver.extraJavaOptions -XX:+UseConcMarkSweepGC
-XX:+ExplicitGCInvokesConcurrent
-XX:+UseCMSInitiatingOccupancyOnly
-XX:CMSInitiatingOccupancyFraction=65
-verbose:gc
spark.cleaner.periodicGC.interval 600

This works fine for previous issue and the customer is able to decrease GC issue.
On the other hands, customer face another issue.

The customer faces bad performance issue randomly.

Bad run 1h13min
https://adb-1713512114304420.0.azuredatabricks.net/?o=1713512114304420#job/628682513885275/run/53743836

Good Run 9min(the customer re-launched same interactive cluster and execute same job)
https://adb-1713512114304420.0.azuredatabricks.net/?o=1713512114304420#job/309733154533103/run/53964698

Customer is just wondering if the new performance issue is related to the configuration we suggested.
They request Databricks team to explain in detail.

[Ask]
Could you kindly join the call 12 pm IST on 06/27/2022 next Monday and work with us about the new performance issue?",https://docs.microsoft.com/en-us/azure/databricks/data-engineering/jobs/jobs,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs Article 07/14/2022 29 minutes to read 4 contributors In this article Create a job Run a job View jobs View runs for a job View job run details View recent job runs Export job run results Edit a job Edit a task Clone a job Clone a task Delete a job Delete a task Copy a task path Run jobs using notebooks in a remote Git repository Best practices A job is a way to run non-interactive code in an Azure Databricks cluster. For example, you can run an extract, transform, and load (ETL) workload interactively or on a schedule. You can also run jobs interactively in the notebook UI. You can create and run a job using the UI, the CLI, or by invoking the Jobs API. You can repair and re-run a failed or canceled job using the UI or API. You can monitor job run results using the UI, CLI, API, and email notifications. This article focuses on performing job tasks using the UI. For the other methods, see Jobs CLI and Jobs API 2.1. Your job can consist of a single task or can be a large, multi-task workflow with complex dependencies. Azure Databricks manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs. You can run your jobs immediately or periodically through an easy-to-use scheduling system. You can implement a task in a JAR, an Azure Databricks notebook, a Delta Live Tables pipeline, or an application written in Scala, Java, or Python. Legacy Spark Submit applications are also supported. You control the execution order of tasks by specifying dependencies between the tasks. You can configure tasks to run in sequence or parallel. The following diagram illustrates a workflow that: Ingests raw clickstream data and performs processing to sessionize the records. Ingests order data and joins it with the sessionized clickstream data to create a prepared data set for analysis. Extracts features from the prepared data. Performs tasks in parallel to persist the features and train a machine learning model. To create your first workflow with an Azure Databricks job, see the quickstart. Important You can create jobs only in a Data Science & Engineering workspace or a Machine Learning workspace. A workspace is limited to 1000 concurrent job runs. A 429 Too Many Requests response is returned when you request a run that cannot start immediately. The number of jobs a workspace can create in an hour is limited to 5000 (includes “run now” and “runs submit”). This limit also affects jobs created by the REST API and notebook workflows. Create a job Do one of the following: Click Workflows in the sidebar and click . In the sidebar, click Create and select Job from the menu. The Tasks tab appears with the create task dialog. Replace Add a name for your job… with your job name. Enter a name for the task in the Task name field. Specify the type of task to run. In the Type drop-down, select Notebook, JAR, Spark Submit, Python, or Pipeline. Notebook: In the Source drop-down, select a location for the notebook; either Workspace for a notebook located in a Azure Databricks workspace folder or Git provider for a notebook located in a remote Git repository. Workspace: Use the file browser to find the notebook, click the notebook name, and click Confirm. Git provider: Click Edit and enter the Git repository information. See Run jobs using notebooks in a remote Git repository. JAR: Specify the Main class. Use the fully qualified name of the class containing the main method, for example, org.apache.spark.examples.SparkPi. Then click Add under Dependent Libraries to add libraries required to run the task. One of these libraries must contain the main class. To learn more about JAR tasks, see JAR jobs. Spark Submit: In the Parameters text box, specify the main class, the path to the library JAR, and all arguments, formatted as a JSON array of strings. The following example configures a spark-submit task to run the DFSReadWriteTest from the Apache Spark examples: JSON Copy [""--class"",""org.apache.spark.examples.DFSReadWriteTest"",""dbfs:/FileStore/libraries/spark_examples_2_12_3_1_1.jar"",""/dbfs/databricks-datasets/README.md"",""/FileStore/examples/output/""]
 Important There are several limitations for spark-submit tasks: You can run spark-submit tasks only on new clusters. Spark-submit does not support cluster autoscaling. To learn more about autoscaling, see Cluster autoscaling. Spark-submit does not support Databricks Utilities. To use Databricks Utilities, use JAR tasks instead. Python: In the Path textbox, enter the URI of a Python script on DBFS or cloud storage; for example, dbfs:/FileStore/myscript.py. Pipeline: In the Pipeline drop-down, select an existing Delta Live Tables pipeline. Python Wheel: In the Package name text box, enter the package to import, for example, myWheel-1.0-py2.py3-none-any.whl. In the Entry Point text box, enter the function to call when starting the wheel. Click Add under Dependent Libraries to add libraries required to run the task. Configure the cluster where the task runs. In the Cluster drop-down, select either New Job Cluster or Existing All-Purpose Clusters. New Job Cluster: Click Edit in the Cluster drop-down and complete the cluster configuration. Existing All-Purpose Cluster: Select an existing cluster in the Cluster drop-down. To open the cluster in a new page, click the icon to the right of the cluster name and description. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. You can pass parameters for your task. Each task type has different requirements for formatting and passing the parameters. Notebook: Click Add and specify the key and value of each parameter to pass to the task. You can override or add additional parameters when you manually run a task using the Run a job with different parameters option. Parameters set the value of the notebook widget specified by the key of the parameter. Use task parameter variables to pass a limited set of dynamic values as part of a parameter value. JAR: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments to the main method of the main class. See Configure JAR job parameters. Spark Submit task: Parameters are specified as a JSON-formatted array of strings. Conforming to the Apache Spark spark-submit convention, parameters after the JAR path are passed to the main method of the main class. Python: Use a JSON-formatted array of strings to specify parameters. These strings are passed as arguments which can be parsed using the argparse module in Python. Python Wheel: In the Parameters drop-down, select Positional arguments to enter parameters as a JSON-formatted array of strings, or select Keyword arguments > Add to enter the key and value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments. To access additional options, including Dependent Libraries, Retry Policy, and Timeouts, click Advanced Options. See Edit a task. Click Create. To optionally set the job’s schedule, click Edit schedule in the Job details panel. See Schedule a job. To optionally allow multiple concurrent runs of the same job, click Edit concurrent runs in the Job details panel. See Maximum concurrent runs. To optionally specify email addresses to receive notifications on job events, click Edit notifications in the Job details panel. See Notifications. To optionally control permission levels on the job, click Edit permissions in the Job details panel. See Control access to jobs. To add another task, click below the task you just created. A shared cluster option is provided if you have configured a New Job Cluster for a previous task. You can also configure a cluster for each task when you create or edit a task. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Run a job Click Workflows in the sidebar. Select a job and click the Runs tab. You can run a job immediately or schedule the job to run later. If one or more tasks in a job with multiple tasks are not successful, you can re-run the subset of unsuccessful tasks. See Repair an unsuccessful job run. Run a job immediately To run the job immediately, click . Tip You can perform a test run of a job with a notebook task by clicking Run Now. If you need to make changes to the notebook, clicking Run Now again after editing the notebook will automatically run the new version of the notebook. Run a job with different parameters You can use Run Now with Different Parameters to re-run a job with different parameters or different values for existing parameters. Click next to Run Now and select Run Now with Different Parameters or, in the Active Runs table, click Run Now with Different Parameters. Enter the new parameters depending on the type of task. Notebook: You can enter parameters as key-value pairs or a JSON object. The provided parameters are merged with the default parameters for the triggered run. You can use this dialog to set the values of widgets. JAR and spark-submit: You can enter a list of parameters or a JSON document. If you delete keys, the default parameters are used. You can also add task parameter variables for the run. Click Run. Repair an unsuccessful job run You can repair failed or canceled multi-task jobs by running only the subset of unsuccessful tasks and any dependent tasks. Because successful tasks and any tasks that depend on them are not re-run, this feature reduces the time and resources required to recover from unsuccessful job runs. You can change job or task settings before repairing the job run. Unsuccessful tasks are re-run with the current job and task settings. For example, if you change the path to a notebook or a cluster setting, the task is re-run with the updated notebook or cluster settings. You can view the history of all task runs on the Task run details page. Note If one or more tasks share a job cluster, a repair run creates a new job cluster; for example, if the original run used the job cluster my_job_cluster, the first repair run uses the new job cluster my_job_cluster_v1, allowing you to easily see the cluster and cluster settings used by the initial run and any repair runs. The settings for my_job_cluster_v1 are the same as the current settings for my_job_cluster. Repair is supported only with jobs that orchestrate two or more tasks. The Duration value displayed in the Runs tab includes the time the first run started until the time when the latest repair run finished. For example, if a run failed twice and succeeded on the third run, the duration includes the time for all three runs. To repair an unsuccessful job run: Click Jobs in the sidebar. In the Name column, click a job name. The Runs tab shows active runs and completed runs, including any unsuccessful runs. Click the link for the unsuccessful run in the Start time column of the Completed Runs (past 60 days) table. The Job run details page appears. Click Repair run. The Repair job run dialog appears, listing all unsuccessful tasks and any dependent tasks that will be re-run. To add or edit parameters for the tasks to repair, enter the parameters in the Repair job run dialog. Parameters you enter in the Repair job run dialog override existing values. On subsequent repair runs, you can return a parameter to its original value by clearing the key and value in the Repair job run dialog. Click Repair run in the Repair job run dialog. View task run history To view the run history of a task, including successful and unsuccessful runs: Click on a task on the Job run details page. The Task run details page appears. Select the task run in the run history drop-down. Schedule a job To define a schedule for the job: Click Edit schedule in the Job details panel and set the Schedule Type to Scheduled. Specify the period, starting time, and time zone. Optionally select the Show Cron Syntax checkbox to display and edit the schedule in Quartz Cron Syntax. Note Azure Databricks enforces a minimum interval of 10 seconds between subsequent runs triggered by the schedule of a job regardless of the seconds configuration in the cron expression. You can choose a time zone that observes daylight saving time or UTC. If you select a zone that observes daylight saving time, an hourly job will be skipped or may appear to not fire for an hour or two when daylight saving time begins or ends. To run at every hour (absolute time), choose UTC. The job scheduler is not intended for low latency jobs. Due to network or cloud issues, job runs may occasionally be delayed up to several minutes. In these situations, scheduled jobs will run immediately upon service availability. Click Save. Pause and resume a job schedule To pause a job, you can either: Click Pause in the Job details panel. Click Edit schedule in the Job details panel and set the Schedule Type to Manual (Paused) To resume a paused job schedule, set the Schedule Type to Scheduled. View jobs Click Workflows in the sidebar. The Jobs list appears. The Jobs page lists all defined jobs, the cluster definition, the schedule, if any, and the result of the last run. You can filter jobs in the Jobs list: Using keywords. Selecting only the jobs you own. Selecting all jobs you have permissions to access. Access to this filter requires that Jobs access control is enabled. Using tags. To search for a tag created with only a key, type the key into the search box. To search for a tag created with a key and value, you can search by the key, the value, or both the key and value. For example, for a tag with the key department and the value finance, you can search for department or finance to find matching jobs. To search by both the key and value, enter the key and value separated by a colon; for example, department:finance. You can also click any column header to sort the list of jobs (either descending or ascending) by that column. The default sorting is by job name in ascending order. View runs for a job Click Workflows in the sidebar. In the Name column, click a job name. The Runs tab appears with a table of active runs and completed runs. To switch to a matrix view, click Matrix. The matrix view shows a history of runs for the job, including each job task. The Job Runs row of the matrix displays the total duration of the run and the state of the run. To view details of the run, including the start time, duration, and status, hover over the bar in the Job Runs row. Each cell in the Tasks row represents a task and the corresponding status of the task. To view details of each task, including the start time, duration, cluster, and status, hover over the cell for that task. The job run and task run bars are color-coded to indicate the status of the run. Successful runs are green, unsuccessful runs are red, and skipped runs are pink. The height of the individual job run and task run bars provides a visual indication of the run duration. Azure Databricks maintains a history of your job runs for up to 60 days. If you need to preserve job runs, Databricks recommends that you export results before they expire. For more information, see Export job run results. View job run details The job run details page contains job output and links to logs, including information about the success or failure of each task in the job run. You can access job run details from the Runs tab for the job. To view job run details from the Runs tab, click the link for the run in the Run column of the Completed Runs (past 60 days) table. To return to the Runs tab for the job, click on the Job ID value. Click on a task to view task run details, including: the cluster that ran the task the Spark UI for the task logs for the task metrics for the task Click the Job ID value to return to the Runs tab for the job. Click the Job run ID value to return to the job run details. View recent job runs You can view a list of currently running and recently completed runs for all jobs in a workspace you have access to, including runs started by external orchestration tools such as Apache Airflow or Azure Data Factory. To view the list of recent job runs: Click Workflows in the sidebar. The Jobs list appears. Click the Job runs tab. The Job runs list appears. The Job runs list displays: The start time for the run. The name of the job associated with the run. The user name that the job runs as. Whether the run was triggered by a job schedule or an API request, or was manually started. The time elapsed for a currently running job, or the total running time for a completed run. The status of the run, either Pending, Running, Skipped, Succeeded, Failed, Terminating, Terminated, Internal Error, Timed Out, Canceled, Canceling, or Waiting for Retry. To view job run details, click the link in the Start time column for the run. To view job details, click the job name in the Job column. Export job run results You can export notebook run results and job run logs for all job types. Export notebook run results You can persist job runs by exporting their results. For notebook job runs, you can export a rendered notebook that can later be imported into your Azure Databricks workspace. To export notebook run results for a job with a single task: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click Export to HTML. To export notebook run results for a job with multiple tasks: On the job detail page, click the View Details link for the run in the Run column of the Completed Runs (past 60 days) table. Click the notebook task to export. Click Export to HTML. Export job run logs You can also export the logs for your job run. You can set up your job to automatically deliver logs to DBFS through the Job API. See the new_cluster.cluster_log_conf object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. Edit a job Some configuration options are available on the job, and other options are available on individual tasks. For example, the maximum concurrent runs can be set on the job only, while parameters must be defined for each task. To change the configuration for a job: Click Workflows in the sidebar. In the Name column, click the job name. The side panel displays the Job details. You can change the schedule, cluster configuration, notifications, maximum number of concurrent runs, and add or change tags. If job access control is enabled, you can also edit job permissions. Tags To add labels or key:value attributes to your job, you can add tags when you edit the job. You can use tags to filter jobs in the Jobs list; for example, you can use a department tag to filter all jobs that belong to a specific department. Note Because job tags are not designed to store sensitive information such as personally identifiable information or passwords, Databricks recommends using tags for non-sensitive values only. Tags also propagate to job clusters created when a job is run, allowing you to use tags with your existing cluster monitoring. To add or edit tags, click + Tag in the Job details side panel. You can add the tag as a key and value, or a label. To add a label, enter the label in the Key field and leave the Value field empty. Clusters To see tasks associated with a cluster, hover over the cluster in the side panel. To change the cluster configuration for all associated tasks, click Configure under the cluster. To configure a new cluster for all associated tasks, click Swap under the cluster. Maximum concurrent runs The maximum number of parallel runs for this job. Azure Databricks skips the run if the job has already reached its maximum number of active runs when attempting to start a new run. Set this value higher than the default of 1 to perform multiple runs of the same job concurrently. This is useful, for example, if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or you want to trigger multiple runs that differ by their input parameters. Notifications You can add one or more email addresses to notify when runs of this job begin, complete, or fail: Click Edit notifications. Click Add. Enter an email address and click the check box for each notification type to send to that address. To enter another email address for notification, click Add. If you do not want to receive notifications for skipped job runs, click the check box. Click Confirm. Integrate these email notifications with your favorite notification tools, including: PagerDuty Slack Control access to jobs Job access control enables job owners and administrators to grant fine-grained permissions on their jobs. Job owners can choose which other users or groups can view the results of the job. Owners can also choose who can manage their job runs (Run now and Cancel run permissions). See Jobs access control for details. Edit a task To set task configuration options: Click Workflows in the sidebar. In the Name column, click the job name. Click the Tasks tab. Task dependencies You can define the order of execution of tasks in a job using the Depends on drop-down. You can set this field to one or more tasks in the job. Note Depends on is not visible if the job consists of only a single task. Configuring task dependencies creates a Directed Acyclic Graph (DAG) of task execution, a common way of representing execution order in job schedulers. For example, consider the following job consisting of four tasks: Task 1 is the root task and does not depend on any other task. Task 2 and Task 3 depend on Task 1 completing first. Finally, Task 4 depends on Task 2 and Task 3 completing successfully. Azure Databricks runs upstream tasks before running downstream tasks, running as many of them in parallel as possible. The following diagram illustrates the order of processing for these tasks: Individual task configuration options Individual tasks have the following configuration options: In this section: Cluster Dependent libraries Task parameter variables Timeout Retries Cluster To configure the cluster where a task runs, click the Cluster drop-down. You can edit a shared job cluster, but you cannot delete a shared cluster if it is still used by other tasks. To learn more about selecting and configuring clusters to run tasks, see Cluster configuration tips. Dependent libraries Dependent libraries will be installed on the cluster before the task runs. You must set all task dependencies to ensure they are installed before the run starts. To add a dependent library, click Advanced options and select Add Dependent Libraries to open the Add Dependent Library chooser. Follow the recommendations in Library dependencies for specifying dependencies. Important If you have configured a library to install on all clusters automatically, or you select an existing terminated cluster that has libraries installed, the job execution does not wait for library installation to complete. If a job requires a specific library, you should attach the library to the job in the Dependent Libraries field. Task parameter variables You can pass templated variables into a job task as part of the task’s parameters. These variables are replaced with the appropriate values when the job task runs. You can use task parameter values to pass the context about a job run, such as the run ID or the job’s start time. When a job runs, the task parameter variable surrounded by double curly braces is replaced and appended to an optional string value included as part of the value. For example, to pass a parameter named MyJobId with a value of my-job-6 for any run of job ID 6, add the following task parameter: JSON Copy {
  ""MyJobID"": ""my-job-{{job_id}}""
}
 The contents of the double curly braces are not evaluated as expressions, so you cannot do operations or functions within double-curly braces. Whitespace is not stripped inside the curly braces, so {{ job_id }} will not be evaluated. The following task parameter variables are supported: Variable Description {{job_id}} The unique identifier assigned to a job {{run_id}} The unique identifier assigned to a job run {{start_date}} The date a task run started. The format is yyyy-MM-dd in UTC timezone. {{start_time}} The timestamp of the run’s start of execution after the cluster is created and ready. The format is milliseconds since UNIX epoch in UTC timezone, as returned by System.currentTimeMillis(). {{task_retry_count}} The number of retries that have been attempted to run a task if the first attempt fails. The value is 0 for the first attempt and increments with each retry. {{parent_run_id}} The unique identifier assigned to the run of a job with multiple tasks. {{task_key}} The unique name assigned to a task that’s part of a job with multiple tasks. You can set these variables with any task when you Create a job, Edit a job, or Run a job with different parameters. Timeout The maximum completion time for a job. If the job does not complete in this time, Azure Databricks sets its status to “Timed Out”. Retries A policy that determines when and how many times failed runs are retried. To set the retries for the task, click Advanced options and select Edit Retry Policy. The retry interval is calculated in milliseconds between the start of the failed run and the subsequent retry run. Note If you configure both Timeout and Retries, the timeout applies to each retry. Clone a job You can quickly create a new job by cloning an existing job. Cloning a job creates an identical copy of the job, except for the job ID. On the job’s page, click More … next to the job’s name and select Clone from the drop-down menu. Clone a task You can quickly create a new task by cloning an existing task: On the job’s page, click the Tasks tab. Select the task to clone. Click and select Clone task. Delete a job To delete a job, on the job’s page, click More … next to the job’s name and select Delete from the drop-down menu. Delete a task To delete a task: Click the Tasks tab. Select the task to be deleted. Click and select Remove task. Copy a task path To copy the path to a task, for example, a notebook path: Click the Tasks tab. Select the task containing the path to copy. Click next to the task path to copy the path to the clipboard. Run jobs using notebooks in a remote Git repository Important This feature is in Public Preview. You can run jobs with notebooks located in a remote Git repository. This feature simplifies creation and management of production jobs and automates continuous deployment: You don’t need to create a separate production repo in Azure Databricks, manage permissions for it, and keep it updated. You can prevent unintentional changes to a production job, such as local edits in the production repo or changes from switching a branch. The job definition process has a single source of truth in the remote repository. To use notebooks in a remote Git repository, you must Set up Git integration with Databricks Repos. To create a task with a notebook located in a remote Git repository: In the Type drop-down, select Notebook. In the Source drop-down, select Git provider. The Git information dialog appears. In the Git Information dialog, enter details for the repository. For Path, enter a relative path to the notebook location, such as etl/notebooks/. When you enter the relative path, don’t begin it with / or ./ and don’t include the notebook file extension, such as .py. Additional notebook tasks in a multitask job reference the same commit in the remote repository in one of the following ways: sha of $branch/head when git_branch is set sha of $tag when git_tag is set the value of git_commit In a multitask job, there cannot be a task that uses a local notebook and another task that uses a remote repository. This restriction doesn’t apply to non-notebook tasks. Best practices In this section: Cluster configuration tips Notebook job tips Streaming tasks JAR jobs Library dependencies Cluster configuration tips Cluster configuration is important when you operationalize a job. The following provides general guidance on choosing and configuring job clusters, followed by recommendations for specific job types. Use shared job clusters To optimize resource usage with jobs that orchestrate multiple tasks, use shared job clusters. A shared job cluster allows multiple tasks in the same job run to reuse the cluster. You can use a single job cluster to run all tasks that are part of the job, or multiple job clusters optimized for specific workloads. To use a shared job cluster: Select New Job Clusters when you create a task and complete the cluster configuration. Select the new cluster when adding a task to the job, or create a new job cluster. Any cluster you configure when you select New Job Clusters is available to any task in the job. A shared job cluster is scoped to a single job run, and cannot be used by other jobs or runs of the same job. Libraries cannot be declared in a shared job cluster configuration. You must add dependent libraries in task settings. Choose the correct cluster type for your job New Job Clusters are dedicated clusters for a job or task run. A shared job cluster is created and started when the first task using the cluster starts and terminates after the last task using the cluster completes. The cluster is not terminated when idle but terminates only after all tasks using it have completed. If a shared job cluster fails or is terminated before all tasks have finished, a new cluster is created. A cluster scoped to a single task is created and started when the task starts and terminates when the task completes. In production, Databricks recommends using new shared or task scoped clusters so that each job or task runs in a fully isolated environment. When you run a task on a new cluster, the task is treated as a data engineering (task) workload, subject to the task workload pricing. When you run a task on an existing all-purpose cluster, the task is treated as a data analytics (all-purpose) workload, subject to all-purpose workload pricing. If you select a terminated existing cluster and the job owner has Can Restart permission, Azure Databricks starts the cluster when the job is scheduled to run. Existing all-purpose clusters work best for tasks such as updating dashboards at regular intervals. Use a pool to reduce cluster start times To decrease new job cluster start time, create a pool and configure the job’s cluster to use the pool. Notebook job tips Total notebook cell output (the combined output of all notebook cells) is subject to a 20MB size limit. Additionally, individual cell output is subject to an 8MB size limit. If total cell output exceeds 20MB in size, or if the output of an individual cell is larger than 8MB, the run is canceled and marked as failed. If you need help finding cells near or beyond the limit, run the notebook against an all-purpose cluster and use this notebook autosave technique. Streaming tasks Spark Streaming jobs should never have maximum concurrent runs set to greater than 1. Streaming jobs should be set to run using the cron expression ""* * * * * ?"" (every minute). Since a streaming task runs continuously, it should always be the final task in a job. JAR jobs When running a JAR job, keep in mind the following: Output size limits Note Available in Databricks Runtime 6.3 and above. Job output, such as log output emitted to stdout, is subject to a 20MB size limit. If the total output has a larger size, the run is canceled and marked as failed. To avoid encountering this limit, you can prevent stdout from being returned from the driver to Azure Databricks by setting the spark.databricks.driver.disableScalaOutput Spark configuration to true. By default, the flag value is false. The flag controls cell output for Scala JAR jobs and Scala notebooks. If the flag is enabled, Spark does not return job execution results to the client. The flag does not affect the data that is written in the cluster’s log files. Setting this flag is recommended only for job clusters for JAR jobs because it will disable notebook results. Use the shared SparkContext Because Azure Databricks is a managed service, some code changes may be necessary to ensure that your Apache Spark jobs run correctly. JAR job programs must use the shared SparkContext API to get the SparkContext. Because Azure Databricks initializes the SparkContext, programs that invoke new SparkContext() will fail. To get the SparkContext, use only the shared SparkContext created by Azure Databricks: Scala Copy val goodSparkContext = SparkContext.getOrCreate()
val goodSparkSession = SparkSession.builder().getOrCreate()
 There are also several methods you should avoid when using the shared SparkContext. Do not call SparkContext.stop(). Do not call System.exit(0) or sc.stop() at the end of your Main program. This can cause undefined behavior. Use try-finally blocks for job clean up Consider a JAR that consists of two parts: jobBody() which contains the main part of the job. jobCleanup() which has to be executed after jobBody() whether that function succeeded or returned an exception. As an example, jobBody() may create tables, and you can use jobCleanup() to drop these tables. The safe way to ensure that the clean up method is called is to put a try-finally block in the code: Scala Copy try {
  jobBody()
} finally {
  jobCleanup()
}
 You should not try to clean up using sys.addShutdownHook(jobCleanup) or the following code: Scala Copy val cleanupThread = new Thread { override def run = jobCleanup() }
Runtime.getRuntime.addShutdownHook(cleanupThread)
 Due to the way the lifetime of Spark containers is managed in Azure Databricks, the shutdown hooks are not run reliably. Configure JAR job parameters You pass parameters to JAR jobs with a JSON string array. See the spark_jar_task object in the request body passed to the Create a new job operation (POST /jobs/create) in the Jobs API. To access these parameters, inspect the String array passed into your main function. Library dependencies The Spark driver has certain library dependencies that cannot be overridden. These libraries take priority over any of your libraries that conflict with them. To get the full list of the driver library dependencies, run the following command inside a notebook attached to a cluster of the same Spark version (or the cluster with the driver you want to examine). Bash Copy %sh
ls /databricks/jars
 Manage library dependencies A good rule of thumb when dealing with library dependencies while creating JARs for jobs is to list Spark and Hadoop as provided dependencies. On Maven, add Spark and Hadoop as provided dependencies, as shown in the following example: XML Copy <dependency>
  <groupId>org.apache.spark</groupId>
  <artifactId>spark-core_2.11</artifactId>
  <version>2.3.0</version>
  <scope>provided</scope>
</dependency>
<dependency>
  <groupId>org.apache.hadoop</groupId>
  <artifactId>hadoop-core</artifactId>
  <version>1.2.1</version>
  <scope>provided</scope>
</dependency>
 In sbt, add Spark and Hadoop as provided dependencies, as shown in the following example: Scala Copy libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""2.3.0"" % ""provided""
libraryDependencies += ""org.apache.hadoop"" %% ""hadoop-core"" % ""1.2.1"" % ""provided""
 Tip Specify the correct Scala version for your dependencies based on the version you are running.",Jobs
36,[ARR] [Sev B] SR-2206030050001488 The service at /api/2.0/jobs/runs/submit is temporarily unavailable.,"[Issue]
The customer stated that while they were executing databricks notebook through azure data factory, we faced with below issue
Error: The service at /api/2.0/jobs/runs/submit is temporarily unavailable. Please try again later. [TraceId: 00-6a2a5226ea6af0561b1edfd10047b1ed-808b4739e83fe5d2-00]

[what we checked]
#1 Based on the ADF job URL you provided with us, we found error log output on 2022-06-03 13:03:27 (UTC)
**see attached img1.png

#2 We found two failed jobs within very close time. Both of them occurred at around 2022-06-03 1 pm.
** see attached img2.png

#3 We also found that in the last 5 days, most of the executions have been successful except for those two failures.
** see attached img3.png

#4 We checked the run logs at Databricks side, but found no corresponding job. The top one occurred at 1:06 pm and the bottom one at 1:02 pm, with no job run at 1:03 pm in between. Thus, we suspect that the job was not successfully committed to Databricks side.
** see attached img4.png

[ask]
#1 Why this error occurred
#2 How to ensure  this error won’t appear  for future runs",https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/common-errors-adf,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Common errors using Azure Data Factory Article 03/11/2022 3 minutes to read 3 contributors In this article Cluster could not be created Cluster ran into issues during data pipeline execution Azure Databricks service is experiencing high load Library installation timeout Azure Data Factory is a managed service that lets you author data pipelines using Azure Databricks notebooks, JARs, and Python scripts. This article describes common issues and solutions. Cluster could not be created When you create a data pipeline in Azure Data Factory that uses an Azure Databricks-related activity such as Notebook Activity, you can ask for a new cluster to be created. In Azure, cluster creation can fail for a variety of reasons: Your Azure subscription is limited in the number of virtual machines that can be provisioned. Failed to create cluster because of Azure quota indicates that the subscription you are using does not have enough quota to create the needed resources. For example, if you request 500 cores but your quota is 50 cores, the request will fail. Contact Azure Support to request a quota increase. Azure resource provider is currently under high load and requests are being throttled. This error indicates that your Azure subscription or perhaps even the region is being throttled. Simply retrying the data pipeline may not help. Learn more about this issue at Troubleshooting API throttling errors. Could not launch cluster due to cloud provider failures indicates a generic failure to provision one or more virtual machines for the cluster. Wait and try again later. Cluster ran into issues during data pipeline execution Azure Databricks includes a variety of mechanisms that increase the resilience of your Apache Spark cluster. That said, it cannot recover from every failure, leading to errors like this: Connection refused RPC timed out Exchange times out after X seconds Cluster became unreachable during run Too many execution contexts are open right now Driver was restarted during run Context ExecutionContextId is disconnected Could not reach driver of cluster for X seconds Most of the time, these errors do not indicate an issue with the underlying infrastructure of Azure. Instead, it is quite likely that the cluster has too many jobs running on it, which can overload the cluster and cause timeouts. As a general rule, you should move heavier data pipelines to run on their own Azure Databricks clusters. Integrating with Azure Monitor and observing execution metrics with Grafana can provide insight into clusters that are getting overloaded. Azure Databricks service is experiencing high load You may notice that certain data pipelines fail with errors like these: The service at {API} is temporarily unavailable Jobs is not fully initialized yet. Please retry later Failed or timeout processing HTTP request No webapps are available to handle your request These errors indicate that the Azure Databricks service is under heavy load. If this happens, try limiting the number of concurrent data pipelines that include a Azure Databricks activity. For example, if you are performing ETL with 1,000 tables from source to destination, instead of launching a data pipeline per table, either combine multiple tables in one data pipeline or stagger their execution so they don’t all trigger at once. Important Azure Databricks will not allow you to create more than 1,000 Jobs in a 3,600 second window. If you try to do so with Azure Data Factory, your data pipeline will fail. These errors can also show if you poll the Databricks Jobs API for job run status too frequently (e.g. every 5 seconds). The remedy is to reduce the frequency of polling. Library installation timeout Azure Databricks includes robust support for installing third-party libraries. Unfortunately, you may see issues like this: Failed or timed out installing libraries This happens because every time you start a cluster with a library attached, Azure Databricks downloads the library from the appropriate repository (such as PyPI). This operation can time out, causing your cluster to fail to start. There is no simple solution for this problem, other than limiting the number of libraries you attach to clusters.",Common errors using Azure Data Factory
37,[ARR] [Sev B] SR-2206030050001488 The service at /api/2.0/jobs/runs/submit is temporarily unavailable.,"[Issue]
The customer stated that while they were executing databricks notebook through azure data factory, we faced with below issue
Error: The service at /api/2.0/jobs/runs/submit is temporarily unavailable. Please try again later. [TraceId: 00-6a2a5226ea6af0561b1edfd10047b1ed-808b4739e83fe5d2-00]

[what we checked]
#1 Based on the ADF job URL you provided with us, we found error log output on 2022-06-03 13:03:27 (UTC)
**see attached img1.png

#2 We found two failed jobs within very close time. Both of them occurred at around 2022-06-03 1 pm.
** see attached img2.png

#3 We also found that in the last 5 days, most of the executions have been successful except for those two failures.
** see attached img3.png

#4 We checked the run logs at Databricks side, but found no corresponding job. The top one occurred at 1:06 pm and the bottom one at 1:02 pm, with no job run at 1:03 pm in between. Thus, we suspect that the job was not successfully committed to Databricks side.
** see attached img4.png

[ask]
#1 Why this error occurred
#2 How to ensure  this error won’t appear  for future runs",https://docs.microsoft.com/en-us/azure/databricks/kb/dev-tools/common-errors-adf,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Common errors using Azure Data Factory Article 03/11/2022 3 minutes to read 3 contributors In this article Cluster could not be created Cluster ran into issues during data pipeline execution Azure Databricks service is experiencing high load Library installation timeout Azure Data Factory is a managed service that lets you author data pipelines using Azure Databricks notebooks, JARs, and Python scripts. This article describes common issues and solutions. Cluster could not be created When you create a data pipeline in Azure Data Factory that uses an Azure Databricks-related activity such as Notebook Activity, you can ask for a new cluster to be created. In Azure, cluster creation can fail for a variety of reasons: Your Azure subscription is limited in the number of virtual machines that can be provisioned. Failed to create cluster because of Azure quota indicates that the subscription you are using does not have enough quota to create the needed resources. For example, if you request 500 cores but your quota is 50 cores, the request will fail. Contact Azure Support to request a quota increase. Azure resource provider is currently under high load and requests are being throttled. This error indicates that your Azure subscription or perhaps even the region is being throttled. Simply retrying the data pipeline may not help. Learn more about this issue at Troubleshooting API throttling errors. Could not launch cluster due to cloud provider failures indicates a generic failure to provision one or more virtual machines for the cluster. Wait and try again later. Cluster ran into issues during data pipeline execution Azure Databricks includes a variety of mechanisms that increase the resilience of your Apache Spark cluster. That said, it cannot recover from every failure, leading to errors like this: Connection refused RPC timed out Exchange times out after X seconds Cluster became unreachable during run Too many execution contexts are open right now Driver was restarted during run Context ExecutionContextId is disconnected Could not reach driver of cluster for X seconds Most of the time, these errors do not indicate an issue with the underlying infrastructure of Azure. Instead, it is quite likely that the cluster has too many jobs running on it, which can overload the cluster and cause timeouts. As a general rule, you should move heavier data pipelines to run on their own Azure Databricks clusters. Integrating with Azure Monitor and observing execution metrics with Grafana can provide insight into clusters that are getting overloaded. Azure Databricks service is experiencing high load You may notice that certain data pipelines fail with errors like these: The service at {API} is temporarily unavailable Jobs is not fully initialized yet. Please retry later Failed or timeout processing HTTP request No webapps are available to handle your request These errors indicate that the Azure Databricks service is under heavy load. If this happens, try limiting the number of concurrent data pipelines that include a Azure Databricks activity. For example, if you are performing ETL with 1,000 tables from source to destination, instead of launching a data pipeline per table, either combine multiple tables in one data pipeline or stagger their execution so they don’t all trigger at once. Important Azure Databricks will not allow you to create more than 1,000 Jobs in a 3,600 second window. If you try to do so with Azure Data Factory, your data pipeline will fail. These errors can also show if you poll the Databricks Jobs API for job run status too frequently (e.g. every 5 seconds). The remedy is to reduce the frequency of polling. Library installation timeout Azure Databricks includes robust support for installing third-party libraries. Unfortunately, you may see issues like this: Failed or timed out installing libraries This happens because every time you start a cluster with a library attached, Azure Databricks downloads the library from the appropriate repository (such as PyPI). This operation can time out, causing your cluster to fail to start. There is no simple solution for this problem, other than limiting the number of libraries you attach to clusters.",Common errors using Azure Data Factory
38,Please Enable Delta Sharing For Workspace,"Hi,

I've read https://docs.gcp.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-sharing.html that we need to enable external data sharing in order to use delta shares.
Please can this be enabled for environment  1667150717056314",https://docs.gcp.databricks.com/data/data-sources/google/gcs.html,databricks_gcp_docs,gcp_docs,"Google Cloud Storage This article describes how to read from and write to Google Cloud Storage (GCS) tables in Databricks. To read or write from a GCS bucket, you must create an attached service account and you must associate the bucket with the service account when creating a cluster. Connect to the bucket directly using the service account email address (recommended approach) or a key that you generate for the service account. For documentation for working with mounted GCS bucket, see Mounting cloud object storage on Databricks. Step 1: Set up Google Cloud service account using Google Cloud Console You must create a service account for the Databricks cluster. We recommend giving this service account the least privileges needed to perform its tasks. Important The service account must be in the Google Cloud project that you used to set up the Databricks workspace. Click IAM and Admin in the left navigation pane. Click Service Accounts. Click + CREATE SERVICE ACCOUNT. Enter the service account name and description. Click CREATE. Click CONTINUE. Click DONE. Navigate to the Google Cloud Console list of service accounts and select a service account. Copy the associated email address. You will need it when you set up Databricks clusters. Step 2: Configure your GCS bucket Create a bucket If you do not already have a bucket, create one: Click Storage in the left navigation pane. Click CREATE BUCKET. Name your bucket. Pick a globally unique and permanent name that complies with Google’s naming requirements for GCS buckets. Important To work with DBFS mounts, your bucket name must not contain an underscore. Click CREATE. Configure the bucket Configure the bucket: Configure the bucket details. Click the Permissions tab. Next to the Permissions label, click ADD. Provide the following permission to the service account on the bucket from the Cloud Storage roles: Storage Admin. Click SAVE. Step 3: Set up a Databricks cluster When you configure your cluster, expand Advanced Options and set the Google Service Account field to your service account email address. Access a GCS bucket directly In this section: Step 1: Set up Google Cloud service account using Google Cloud Console Step 2: Configure the GCS bucket Step 3: Set up Databricks cluster Step 4: Usage To read and write directly to a bucket, you can either set the service account email address or configure a key defined in your Spark config. Step 1: Set up Google Cloud service account using Google Cloud Console You must create a service account for the Databricks cluster. Databricks recommends giving this service account the least privileges needed to perform its tasks. Click IAM and Admin in the left navigation pane. Click Service Accounts. Click + CREATE SERVICE ACCOUNT. Enter the service account name and description. Click CREATE. Click CONTINUE. Click DONE. Get the service account email address or generate a key for the service account. Note Databricks recommends using the service account email address because there are no keys involved, so there is no risk of leaking the keys. One reason to use a key is if the service account needs to be in a different Google Cloud project than the project that was used when creating the workspace. Service account email address: Navigate to the Google Cloud Console list of service accounts. Select a service account. Copy the email address associated with it. You will need it in the cluster setup page. Important If you use the service account email address approach, the service account must be in the same Google Cloud project as you used to set up the Databricks workspace. Key: Create a key. See Create a key to access GCS bucket directly. Create a key to access GCS bucket directly Warning The JSON key you generate for the service account is a private key that should only be shared with authorized users as it controls access to datasets and resources in your Google Cloud account. In the Google Cloud console, in the service accounts list, click the newly created account. In the Keys section, click ADD KEY > Create new key. Accept the JSON key type. Click CREATE. The key file is downloaded to your computer. Step 2: Configure the GCS bucket Create a bucket If you do not already have a bucket, create one: Click Storage in the left navigation pane. Click CREATE BUCKET. Click CREATE. Configure the bucket Configure the bucket details. Click the Permissions tab. Next to the Permissions label, click ADD. Provide the Storage Admin permission to the service account on the bucket from the Cloud Storage roles. Click SAVE. Step 3: Set up Databricks cluster When you configure your cluster: In the Databricks Runtime Version drop-down, select 7.3 LTS or above. You can authenticate with the service account email address or a key that you generate for the service account. Service account email address: Expand Advanced Options and set Google Service Account field to your service account email address. Key: In the Spark Config tab, add the following Spark configuration. Replace <client_email>, <project_id>, <private_key>, and <private_key_id> with the values of those exact field names from your key JSON file. Important The value for <private_key_id> spans multiple lines. Paste the entire private key, including leading and trailing quotes. spark.hadoop.google.cloud.auth.service.account.enable true
spark.hadoop.fs.gs.auth.service.account.email <client_email>
spark.hadoop.fs.gs.project.id <project_id>
spark.hadoop.fs.gs.auth.service.account.private.key <private_key>
spark.hadoop.fs.gs.auth.service.account.private.key.id <private_key_id>
 Step 4: Usage To read from the GCS bucket, use a Spark read command in any supported format, for example: df = spark.read.format(""parquet"").load(""gs://<bucket-name>/<path>"")
 To write to the GCS bucket, use a Spark write command in any supported format, for example: df.write.format(""parquet"").mode(""<mode>"").save(""gs://<bucket-name>/<path>"")
 Replace <bucket-name> with the name of the bucket you created in Step 2: Configure the GCS bucket. Example notebooks Read from Google Cloud Storage notebook Open notebook in new tab Copy link for import Write to Google Cloud Storage notebook Open notebook in new tab Copy link for import",Google Cloud Storage
39,ARR | 2206080030002199 - The cluster is not able to autoscale as per requirment.,"Customer jobs are getting failed as autoscaling is not happening properly and cluster is stuck with minimum number of workers.

Cluster URL: https://adb-8512525882909319.19.azuredatabricks.net/?o=8512525882909319#setting/clusters/0929-075206-bulky509/configuration

Went on a call with customer and suggested to submit a high load job on cluster. customer ran a high load job, however, there was still no change on number of nodes and it was stuck with only 2 workers (Attaching screenshot)

When analyzed the issue from backend, got quota exhaustion error. Engaged subscription team and they confirmed that sufficient quota is available for DS_v2 instances.

I am attaching all the required details. Kindly assist.",https://docs.microsoft.com/en-us/azure/databricks/scenarios/frequently-asked-questions-databricks,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Frequently asked questions about Azure Databricks Article 06/15/2022 6 minutes to read 4 contributors In this article Can I use Azure Key Vault to store keys/secrets to be used in Azure Databricks? Can I use Azure Virtual Networks with Databricks? How do I access Azure Data Lake Storage from a notebook? Fix common problems Next steps This article lists the top questions you might have related to Azure Databricks. It also lists some common problems you might have while using Databricks. For more information, see What is Azure Databricks. Can I use Azure Key Vault to store keys/secrets to be used in Azure Databricks? Yes. You can use Azure Key Vault to store keys/secrets for use with Azure Databricks. For more information, see Azure Key Vault-backed scopes. Can I use Azure Virtual Networks with Databricks? Yes. You can use an Azure Virtual Network (VNET) with Azure Databricks. For more information, see Deploying Azure Databricks in your Azure Virtual Network. How do I access Azure Data Lake Storage from a notebook? Follow these steps: In Azure Active Directory (Azure AD), provision a service principal, and record its key. Assign the necessary permissions to the service principal in Data Lake Storage. To access a file in Data Lake Storage, use the service principal credentials in Notebook. For more information, see Use Azure Data Lake Storage with Azure Databricks. Fix common problems Here are a few problems you might encounter with Databricks. Issue: This subscription is not registered to use the namespace 'Microsoft.Databricks' Error message ""This subscription is not registered to use the namespace 'Microsoft.Databricks'. See https://aka.ms/rps-not-found for how to register subscriptions. (Code: MissingSubscriptionRegistration)"" Solution Go to the Azure portal. Select Subscriptions, the subscription you are using, and then Resource providers. In the list of resource providers, against Microsoft.Databricks, select Register. You must have the contributor or owner role on the subscription to register the resource provider. Issue: Your account {email} does not have the owner or contributor role on the Databricks workspace resource in the Azure portal Error message ""Your account {email} does not have Owner or Contributor role on the Databricks workspace resource in the Azure portal. This error can also occur if you are a guest user in the tenant. Ask your administrator to grant you access or add you as a user directly in the Databricks workspace."" (Code: AADSTS90015) Solution The following are some solutions to this issue: If you are an Azure Databricks user without the Owner or Contributor role on the Databricks workspace resource and you simply want to access the workspace: You should access it directly using the URL (for example, https://adb-5555555555555555.19.azuredatabricks.net). Do not use the Launch Workspace button in the Azure portal. If you expected to be recognized as an Owner or Contributor on the workspace resource: To initialize the tenant, you must be signed in as a regular user of the tenant, not as a guest user. You must also have the Contributor or Owner role on the Databricks workspace resource. An administrator can grant a user a role from the Access control (IAM) tab within the Azure Databricks workspace in the Azure portal. This error might also occur if your email domain name is assigned to multiple directories in Azure AD. To work around this issue, create a new user in the directory that contains the subscription with your Databricks workspace. a. In the Azure portal, go to Azure AD. Select Users and Groups > Add a user. b. Add a user with an @<tenant_name>.onmicrosoft.com email instead of @<your_domain> email. You can find this option in Custom Domains, under Azure AD in the Azure portal. c. Grant this new user the Contributor role on the Databricks workspace resource. d. Sign in to the Azure portal with the new user, and find the Databricks workspace. e. Launch the Databricks workspace as this user. Issue: Your account {email} has not been registered in Databricks Solution If you did not create the workspace, and you are added as a user, contact the person who created the workspace. Have that person add you by using the Azure Databricks Admin Console. For instructions, see Adding and managing users. If you created the workspace and still you get this error, try selecting Initialize Workspace again from the Azure portal. Issue: Cloud provider launch failure while setting up the cluster (PublicIPCountLimitReached) Error message ""Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster. For more information, see the Databricks guide. Azure error code: PublicIPCountLimitReached. Azure error message: Cannot create more than 10 public IP addresses for this subscription in this region."" Background Databricks clusters use one public IP address per node (including the driver node). Azure subscriptions have public IP address limits per region. Thus, cluster creation and scale-up operations may fail if they would cause the number of public IP addresses allocated to that subscription in that region to exceed the limit. This limit also includes public IP addresses allocated for non-Databricks usage, such as custom user-defined VMs. In general, clusters only consume public IP addresses while they are active. However, PublicIPCountLimitReached errors may continue to occur for a short period of time even after other clusters are terminated. This is because Databricks temporarily caches Azure resources when a cluster is terminated. Resource caching is by design, since it significantly reduces the latency of cluster startup and autoscaling in many common scenarios. Solution If your subscription has already reached its public IP address limit for a given region, then you should do one or the other of the following. Create new clusters in a different Databricks workspace. The other workspace must be located in a region in which you have not reached your subscription's public IP address limit. Request to increase your public IP address limit. Choose Quota as the Issue Type, and Networking: ARM as the Quota Type. In Details, request a Public IP Address quota increase. For example, if your limit is currently 60, and you want to create a 100-node cluster, request a limit increase to 160. Issue: A second type of cloud provider launch failure while setting up the cluster (MissingSubscriptionRegistration) Error message ""Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster. For more information, see the Databricks guide. Azure error code: MissingSubscriptionRegistration Azure error message: The subscription is not registered to use namespace 'Microsoft.Compute'. See https://aka.ms/rps-not-found for how to register subscriptions."" Solution Go to the Azure portal. Select Subscriptions, the subscription you are using, and then Resource providers. In the list of resource providers, against Microsoft.Compute, select Register. You must have the contributor or owner role on the subscription to register the resource provider. For more detailed instructions, see Resource providers and types. Issue: Azure Databricks needs permissions to access resources in your organization that only an admin can grant. Background Azure Databricks is integrated with Azure Active Directory. You can set permissions within Azure Databricks (for example, on notebooks or clusters) by specifying users from Azure AD. For Azure Databricks to be able to list the names of the users from your Azure AD, it requires read permission to that information and consent to be given. If the consent is not already available, you see the error. Solution Log in as a global administrator to the Azure portal. For Azure Active Directory, go to the User Settings tab and make sure Users can consent to apps accessing company data on their behalf is set to Yes. Issue: Azure Databricks does not support creation of workspace under Azure Resource Groups, which are created with Chinese characters. Solution Support for validation for this scenario as part of workspace create will be added in later release. Next steps Quickstart: Get started with Azure Databricks What is Azure Databricks?",Frequently asked questions about Azure Databricks
40,[AIA][Managed Accounts UI advisory],"https://docs.databricks.com/getting-started/account-setup.html (AWS)

=======================================
Does this web page UI exists in Azure Databricks? 

CX insists it exists.",https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Unity Catalog (Preview) Article 07/12/2022 2 minutes to read 4 contributors In this article Important Unity Catalog is in Public Preview. During the preview, some functionality is limited. See Unity Catalog public preview limitations. To participate in the preview, contact your Azure Databricks representative. Unity Catalog is a fine-grained governance solution for data and AI on the Lakehouse. Unity Catalog helps simplify security and governance of your data with the following key features: Define once, secure everywhere: Unity Catalog offers a single place to administer data access policies that apply across all workspaces and personas. Standards-compliant security model: Unity Catalog’s security model is based on standard ANSI SQL, and allows administrators to grant permissions at the level of catalogs, databases (also called schemas), tables, and views in their existing data lake using familiar syntax. Built-in auditing: Unity Catalog automatically captures user-level audit logs that record access to your data. Unity Catalog requires an Azure Databricks account on the Premium plan. In this guide: Get started using Unity Catalog Key concepts Data permissions Create compute resources Use Azure managed identities in Unity Catalog to access storage Create a metastore Create and manage catalogs Create and manage schemas (databases) Manage identities in Unity Catalog Create tables Create views Manage access to data Manage external locations and storage credentials Query data Train a machine-learning model with Python from data in Unity Catalog Connect to BI tools Audit access and activity for Unity Catalog resources Upgrade tables and views to Unity Catalog Automate Unity Catalog setup using Terraform Unity Catalog public preview limitations",Unity Catalog (Preview)
41,How to Ensure Data disks are encrypted,"Hello team,

The customer wants to know if the Data disks are encrypted or not.

Cx statement:

Netscope security scan reported as the disks are not encrypted.

 Customer would like to check if the disks are already encrypted or not.

 I have attached the Netscape scan results,  Customer is unable to find the resource mentioned in the excel report.

He wants to understand why the resources/disks are not found mentioned in the excel report.

 If we need to encrypt this databricks instance disks how we can do that.

 He also wants to know:

1. clarification on how to verify if the disks are encrypted or not.

2. If you want to create a customer-managed key or Platform managed key what are the steps, how safe it is etc.

Kindly need your help to check and assist cx more on this.",https://docs.microsoft.com/en-us/azure/databricks/security/keys/customer-managed-keys,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Customer-managed keys for encryption Article 05/25/2022 2 minutes to read 3 contributors In this article Important This feature is in Public Preview. Note This feature requires the Premium Plan. For some types of data, Azure Databricks supports adding a customer-managed key to help protect and control access to encrypted data. Azure Databricks has two customer-managed key features for different types of data: Enable customer-managed keys for managed services Configure customer-managed keys for DBFS root The following table lists which customer-managed key features are used for which types of data. Type of data Location Customer-managed key feature Notebook source and metadata Control plane Managed services Secrets stored by the secret manager APIs Control plane Managed services Databricks SQL queries and query history Control plane Managed services Customer-accessible DBFS root data Your workspace’s DBFS root in your workspace root Blob storage in your Azure subscription. This also includes workspace libraries and the FileStore area. DBFS root Job results Workspace root Blob storage instance in your Azure subscription DBFS root Databricks SQL results Workspace root Blob storage instance in your Azure subscription DBFS root Interactive notebook results By default, when you run a notebook interactively (rather than as a job) results are stored in the control plane for performance with some large results stored in your workspace root Blob storage in your Azure subscription. You can choose to configure Azure Databricks to store all interactive notebook results in your Azure subscription. For partial results in the control plane, use a customer-managed key for managed services. For results in the root Blob storage, which you can configure for all result storage, use a customer-managed key for DBFS root. Other workspace system data in the root Blob storage that is inaccessible through DBFS, such as notebook revisions. Workspace root Blob storage in your Azure subscription DBFS root For additional security for your workspace’s root Blob storage instance in your Azure subscription, you can enable double encryption for the DBFS root.",Customer-managed keys for encryption
42,Status of a job that failed to create a cluster not reported correctly,"We use jobs API to manage Databricks jobs. In particular, we use this code to check job status, as we are orchestrating our jobs in our code:

 run_get_resp = requests.get(f'{addr}/api/2.0/jobs/runs/get',
                                headers={'Authorization': f'Bearer {my_token}'},
                                json={""run_id"": run_id})
      done = run_get_resp.json()['state']['life_cycle_state'] == 'TERMINATED'

Unfortunately, if a job failed very fast due to problems creating a job cluster, the above code never gets True for 'done'.  This causes our calling job to run until timeout. We would like to see this issue fixed or maybe we should modify our code to check for another state?",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/clusters,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Clusters API 2.0 Article 07/14/2022 46 minutes to read 6 contributors In this article Create Edit Start Restart Resize Delete (terminate) Permanent delete Get Pin Unpin List List node types Runtime versions Events Data structures The Clusters API allows you to create, start, edit, list, terminate, and delete clusters. The maximum allowed size of a request to the Clusters API is 10MB. Cluster lifecycle methods require a cluster ID, which is returned from Create. To obtain a list of clusters, invoke List. Azure Databricks maps cluster node instance types to compute units known as DBUs. See the instance type pricing page for a list of the supported instance types and their corresponding DBUs. For instance provider information, see Azure instance type specifications and pricing. Azure Databricks always provides one year’s deprecation notice before ceasing support for an instance type. Important To access Databricks REST APIs, you must authenticate. Create Endpoint HTTP Method 2.0/clusters/create POST Create a new Apache Spark cluster. This method acquires new instances from the cloud provider if necessary. This method is asynchronous; the returned cluster_id can be used to poll the cluster state. When this method returns, the cluster is in a PENDING state. The cluster is usable once it enters a RUNNING state. See ClusterState. Note Azure Databricks may not be able to acquire some of the requested nodes, due to cloud provider limitations or transient network issues. If it is unable to acquire a sufficient number of the requested nodes, cluster creation will terminate with an informative error message. Examples Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy {
  ""cluster_name"": ""my-cluster"",
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2"",
  ""spark_conf"": {
    ""spark.speculation"": true
  },
  ""num_workers"": 25
}
 JSON Copy { ""cluster_id"": ""1234-567890-undid123"" }
 Here is an example for an autoscaling cluster. This cluster will start with two nodes, the minimum. Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy {
  ""cluster_name"": ""autoscaling-cluster"",
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2"",
  ""autoscale"" : {
    ""min_workers"": 2,
    ""max_workers"": 50
  }
}
 JSON Copy { ""cluster_id"": ""1234-567890-hared123"" }
 This example creates a Single Node cluster. To create a Single Node cluster: Set spark_conf and custom_tags to the exact values in the example. Set num_workers to 0. Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy {
  ""cluster_name"": ""single-node-cluster"",
  ""spark_version"": ""7.6.x-scala2.12"",
  ""node_type_id"": ""Standard_DS3_v2"",
  ""num_workers"": 0,
  ""spark_conf"": {
    ""spark.databricks.cluster.profile"": ""singleNode"",
    ""spark.master"": ""local[*]""
  },
  ""custom_tags"": {
    ""ResourceClass"": ""SingleNode""
  }
}
 JSON Copy { ""cluster_id"": ""1234-567890-pouch123"" }
 To create a job or submit a run with a new cluster using a policy, set policy_id to the policy ID: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy 
{
    ""num_workers"": null,
    ""autoscale"": {
        ""min_workers"": 2,
        ""max_workers"": 8
    },
    ""cluster_name"": ""my-cluster"",
    ""spark_version"": ""7.3.x-scala2.12"",
    ""spark_conf"": {},
    ""node_type_id"": ""Standard_D3_v2"",
    ""custom_tags"": {},
    ""spark_env_vars"": {
        ""PYSPARK_PYTHON"": ""/databricks/python3/bin/python3""
    },
    ""autotermination_minutes"": 120,
    ""init_scripts"": [],
    ""policy_id"": ""C65B864F02000008""
}
 To create a new cluster, define the cluster’s properties in new_cluster: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/job/create \
--data @create-job.json
 create-job.json: JSON Copy {
  ""run_name"": ""my spark task"",
  ""new_cluster"": {
    ""spark_version"": ""7.3.x-scala2.12"",
    ""node_type_id"": ""Standard_D3_v2"",
    ""num_workers"": 10,
    ""policy_id"": ""ABCD000000000000""
  },
  ""libraries"": [
    {
      ""jar"": ""dbfs:/my-jar.jar""
    },
    {
      ""maven"": {
        ""coordinates"": ""org.jsoup:jsoup:1.7.2""
      }
    }
  ],
  ""spark_jar_task"": {
    ""main_class_name"": ""com.databricks.ComputeModels""
  }
}
 Request structure of the cluster definition Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. This field is required. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. This field is required. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. custom_tags ClusterTag An object containing a set of tags for cluster resources. Databricks tags all cluster resources (such as VMs) with these tags in addition to default_tags. Note: * Azure Databricks allows at most 43 custom tags. * If the cluster is created on an instance pool, the cluster tags are not copied to the cluster resources. To tag resources for an instance pool, see the custom_tags field in the Instance Pools API 2.0. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of scripts can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. driver_instance_pool_id STRING The ID of the instance pool to use for drivers. You must also specify instance_pool_id. Refer to Instance Pools API 2.0 for details. instance_pool_id STRING The optional ID of the instance pool to use for cluster nodes. If driver_instance_pool_id is present, instance_pool_id is used for worker nodes only. Otherwise, it is used for both the driver and the worker nodes. Refer to Instance Pools API 2.0 for details. idempotency_token STRING An optional token that can be used to guarantee the idempotency of cluster creation requests. If the idempotency token is assigned to a cluster that is not in the TERMINATED state, the request does not create a new cluster but instead returns the ID of the existing cluster. Otherwise, a new cluster is created. The idempotency token is cleared when the cluster is terminated If you specify the idempotency token, upon failure you can retry until the request succeeds. Azure Databricks will guarantee that exactly one cluster will be launched with that idempotency token. This token should have at most 64 characters. apply_policy_default_values BOOL Whether to use policy default values for missing cluster attributes. enable_local_disk_encryption BOOL Whether encryption of disks locally attached to the cluster is enabled. azure_attributes AzureAttributes Attributes related to clusters running on Azure. If not specified at cluster creation, a set of default values is used. runtime_engine STRING The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: * PHOTON: Use the Photon runtime engine type. * STANDARD: Use the standard runtime engine type. This field is optional. Response structure Field Name Type Description cluster_id STRING Canonical identifier for the cluster. Edit Endpoint HTTP Method 2.0/clusters/edit POST Edit the configuration of a cluster to match the provided attributes and size. You can edit a cluster if it is in a RUNNING or TERMINATED state. If you edit a cluster while it is in a RUNNING state, it will be restarted so that the new attributes can take effect. If you edit a cluster while it is in a TERMINATED state, it will remain TERMINATED. The next time it is started using the clusters/start API, the new attributes will take effect. An attempt to edit a cluster in any other state will be rejected with an INVALID_STATE error code. Clusters created by the Databricks Jobs service cannot be edited. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/edit \
--data @edit-cluster.json
 edit-cluster.json: JSON Copy {
  ""cluster_id"": ""1202-211320-brick1"",
  ""num_workers"": 10,
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2""
}
 JSON Copy {}
 Request structure Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING Canonical identifier for the cluster. This field is required. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. This field is required. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. This field is required. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default Databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. apply_policy_default_values BOOL Whether to use policy default values for missing cluster attributes. enable_local_disk_encryption BOOL Whether encryption of disks locally attached to the cluster is enabled. azure_attributes AzureAttributes Attributes related to clusters running on Azure. If not specified at cluster creation, a set of default values is used. runtime_engine STRING The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: * PHOTON: Use the Photon runtime engine type. * STANDARD: Use the standard runtime engine type. This field is optional. Start Endpoint HTTP Method 2.0/clusters/start POST Start a terminated cluster given its ID. This is similar to createCluster, except: The terminated cluster ID and attributes are preserved. The cluster starts with the last specified cluster size. If the terminated cluster is an autoscaling cluster, the cluster starts with the minimum number of nodes. If the cluster is in the RESTARTING state, a 400 error is returned. You cannot start a cluster launched to run a job. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/start \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be started. This field is required. Restart Endpoint HTTP Method 2.0/clusters/restart POST Restart a cluster given its ID. The cluster must be in the RUNNING state. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/restart \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be started. This field is required. Resize Endpoint HTTP Method 2.0/clusters/resize POST Resize a cluster to have a desired number of workers. The cluster must be in the RUNNING state. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/resize \
--data '{ ""cluster_id"": ""1234-567890-reef123"", ""num_workers"": 30 }'
 JSON Copy {}
 Request structure Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING The cluster to be resized. This field is required. Delete (terminate) Endpoint HTTP Method 2.0/clusters/delete POST Terminate a cluster given its ID. The cluster is removed asynchronously. Once the termination has completed, the cluster will be in the TERMINATED state. If the cluster is already in a TERMINATING or TERMINATED state, nothing will happen. Unless a cluster is pinned, 30 days after the cluster is terminated, it is permanently deleted. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/delete \
--data '{ ""cluster_id"": ""1234-567890-frays123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be terminated. This field is required. Permanent delete Endpoint HTTP Method 2.0/clusters/permanent-delete POST Permanently delete a cluster. If the cluster is running, it is terminated and its resources are asynchronously removed. If the cluster is terminated, then it is immediately removed. You cannot perform any action, including retrieve the cluster’s permissions, on a permanently deleted cluster. A permanently deleted cluster is also no longer returned in the cluster list. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/permanent-delete \
--data '{ ""cluster_id"": ""1234-567890-frays123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be permanently deleted. This field is required. Get Endpoint HTTP Method 2.0/clusters/get GET Retrieve the information for a cluster given its identifier. Clusters can be described while they are running or up to 30 days after they are terminated. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/get \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }' \
| jq .
 JSON Copy {
  ""cluster_id"": ""1234-567890-reef123"",
  ""driver"": {
    ""node_id"": ""dced0ce388954c38abef081f54c18afd"",
    ""instance_id"": ""c69c0b119a2a499d8a2843c4d256136a"",
    ""start_timestamp"": 1619718438896,
    ""host_private_ip"": ""10.0.0.1"",
    ""private_ip"": ""10.0.0.2""
  },
  ""spark_context_id"": 5631707659504820000,
  ""jdbc_port"": 10000,
  ""cluster_name"": ""my-cluster"",
  ""spark_version"": ""8.2.x-scala2.12"",
  ""node_type_id"": ""Standard_L4s"",
  ""driver_node_type_id"": ""Standard_L4s"",
  ""custom_tags"": {
    ""ResourceClass"": ""SingleNode""
  },
  ""autotermination_minutes"": 0,
  ""enable_elastic_disk"": true,
  ""disk_spec"": {},
  ""cluster_source"": ""UI"",
  ""enable_local_disk_encryption"": false,
  ""azure_attributes"": {
    ""first_on_demand"": 1,
    ""availability"": ""ON_DEMAND_AZURE"",
    ""spot_bid_max_price"": -1
  },
  ""instance_source"": {
    ""node_type_id"": ""Standard_L4s""
  },
  ""driver_instance_source"": {
    ""node_type_id"": ""Standard_L4s""
  },
  ""state"": ""RUNNING"",
  ""state_message"": """",
  ""start_time"": 1610745129764,
  ""last_state_loss_time"": 1619718513513,
  ""num_workers"": 0,
  ""cluster_memory_mb"": 32768,
  ""cluster_cores"": 4,
  ""default_tags"": {
    ""Vendor"": ""Databricks"",
    ""Creator"": ""someone@example.com"",
    ""ClusterName"": ""my-cluster"",
    ""ClusterId"": ""1234-567890-reef123""
  },
  ""creator_user_name"": ""someone@example.com"",
  ""pinned_by_user_name"": ""3401478490056118"",
  ""init_scripts_safe_mode"": false
}
 Request structure Field Name Type Description cluster_id STRING The cluster about which to retrieve information. This field is required. Response structure Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING Canonical identifier for the cluster. This ID is retained during cluster restarts and resizes, while each new cluster has a globally unique ID. creator_user_name STRING Creator user name. The field won’t be included in the response if the user has already been deleted. driver SparkNode Node on which the Spark driver resides. The driver node contains the Spark master and the Databricks application that manages the per-notebook Spark REPLs. executors An array of SparkNode Nodes on which the Spark executors reside. spark_context_id INT64 A canonical SparkContext identifier. This value does change when the Spark driver restarts. The pair (cluster_id, spark_context_id) is a globally unique identifier over all Spark contexts. jdbc_port INT32 Port on which Spark JDBC server is listening in the driver node. No service will be listening on on this port in executor nodes. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. This field is required. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. custom_tags ClusterTag An object containing a set of tags for cluster resources. Databricks tags all cluster resources with these tags in addition to default_tags. Note: * Tags are not supported on legacy node types such as compute-optimized and memory-optimized. * Databricks allows at most 45 custom tags. * If the cluster is created on an instance pool, the cluster tags are not copied to the cluster resources. To tag resources for an instance pool, see the custom_tags field in the Instance Pools API 2.0. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default Databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. enable_elastic_disk BOOL Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space. See Autoscaling local storage for details. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. state ClusterState State of the cluster. state_message STRING A message associated with the most recent state transition (for example, the reason why the cluster entered the TERMINATED state). start_time INT64 Time (in epoch milliseconds) when the cluster creation request was received (when the cluster entered the PENDING state). terminated_time INT64 Time (in epoch milliseconds) when the cluster was terminated, if applicable. last_state_loss_time INT64 Time when the cluster driver last lost its state (due to a restart or driver failure). last_activity_time INT64 Time (in epoch milliseconds) when the cluster was last active. A cluster is active if there is at least one command that has not finished on the cluster. This field is available after the cluster has reached the RUNNING state. Updates to this field are made as best-effort attempts. Certain versions of Spark do not support reporting of cluster activity. Refer to Automatic termination for details. cluster_memory_mb INT64 Total amount of cluster memory, in megabytes. cluster_cores FLOAT Number of CPU cores available for this cluster. This can be fractional since certain node types are configured to share cores between Spark nodes on the same instance. default_tags ClusterTag An object containing a set of tags that are added by Azure Databricks regardless of any custom_tags, including: * Vendor: Databricks * Creator: * ClusterName: * ClusterId: * Name: On job clusters: * RunName: * JobId: On resources used by Databricks SQL: * SqlWarehouseId: cluster_log_status LogSyncStatus Cluster log delivery status. termination_reason TerminationReason Information about why the cluster was terminated. This field appears only when the cluster is in the TERMINATING or TERMINATED state. Pin Note You must be an Azure Databricks administrator to invoke this API. Endpoint HTTP Method 2.0/clusters/pin POST Ensure that an all-purpose cluster configuration is retained even after a cluster has been terminated for more than 30 days. Pinning ensures that the cluster is always returned by the List API. Pinning a cluster that is already pinned has no effect. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/pin \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to pin. This field is required. Unpin Note You must be an Azure Databricks administrator to invoke this API. Endpoint HTTP Method 2.0/clusters/unpin POST Allows the cluster to eventually be removed from the list returned by the List API. Unpinning a cluster that is not pinned has no effect. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/unpin \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to unpin. This field is required. List Endpoint HTTP Method 2.0/clusters/list GET Return information about all pinned clusters, active clusters, up to 200 of the most recently terminated all-purpose clusters in the past 30 days, and up to 30 of the most recently terminated job clusters in the past 30 days. For example, if there is 1 pinned cluster, 4 active clusters, 45 terminated all-purpose clusters in the past 30 days, and 50 terminated job clusters in the past 30 days, then this API returns the 1 pinned cluster, 4 active clusters, all 45 terminated all-purpose clusters, and the 30 most recently terminated job clusters. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list \
| jq .
 JSON Copy {
  ""clusters"": [
    {
      ""cluster_id"": ""1234-567890-reef123"",
      ""driver"": {
        ""node_id"": ""dced0ce388954c38abef081f54c18afd"",
        ""instance_id"": ""c69c0b119a2a499d8a2843c4d256136a"",
        ""start_timestamp"": 1619718438896,
        ""host_private_ip"": ""10.0.0.1"",
        ""private_ip"": ""10.0.0.2""
      },
      ""spark_context_id"": 5631707659504820000,
      ""jdbc_port"": 10000,
      ""cluster_name"": ""my-cluster"",
      ""spark_version"": ""8.2.x-scala2.12"",
      ""node_type_id"": ""Standard_L4s"",
      ""driver_node_type_id"": ""Standard_L4s"",
      ""custom_tags"": {
        ""ResourceClass"": ""SingleNode""
      },
      ""autotermination_minutes"": 0,
      ""enable_elastic_disk"": true,
      ""disk_spec"": {},
      ""cluster_source"": ""UI"",
      ""enable_local_disk_encryption"": false,
      ""azure_attributes"": {
        ""first_on_demand"": 1,
        ""availability"": ""ON_DEMAND_AZURE"",
        ""spot_bid_max_price"": -1
      },
      ""instance_source"": {
        ""node_type_id"": ""Standard_L4s""
      },
      ""driver_instance_source"": {
        ""node_type_id"": ""Standard_L4s""
      },
      ""state"": ""RUNNING"",
      ""state_message"": """",
      ""start_time"": 1610745129764,
      ""last_state_loss_time"": 1619718513513,
      ""num_workers"": 0,
      ""cluster_memory_mb"": 32768,
      ""cluster_cores"": 4,
      ""default_tags"": {
        ""Vendor"": ""Databricks"",
        ""Creator"": ""someone@example.com"",
        ""ClusterName"": ""my-cluster"",
        ""ClusterId"": ""1234-567890-reef123""
      },
      ""creator_user_name"": ""someone@example.com"",
      ""pinned_by_user_name"": ""3401478490056118"",
      ""init_scripts_safe_mode"": false
    },
    {
      ""...""
    }
  ]
}
 Response structure Field Name Type Description clusters An array of ClusterInfo A list of clusters. List node types Endpoint HTTP Method 2.0/clusters/list-node-types GET Return a list of supported Spark node types. These node types can be used to launch a cluster. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list-node-types \
| jq .
 JSON Copy {
  ""node_types"": [
    {
      ""node_type_id"": ""Standard_L80s_v2"",
      ""memory_mb"": 655360,
      ""num_cores"": 80,
      ""description"": ""Standard_L80s_v2"",
      ""instance_type_id"": ""Standard_L80s_v2"",
      ""is_deprecated"": false,
      ""category"": ""Storage Optimized"",
      ""support_ebs_volumes"": true,
      ""support_cluster_tags"": true,
      ""num_gpus"": 0,
      ""node_instance_type"": {
        ""instance_type_id"": ""Standard_L80s_v2"",
        ""local_disks"": 1,
        ""local_disk_size_gb"": 800,
        ""instance_family"": ""Standard LSv2 Family vCPUs"",
        ""local_nvme_disk_size_gb"": 1788,
        ""local_nvme_disks"": 10,
        ""swap_size"": ""10g""
      },
      ""is_hidden"": false,
      ""support_port_forwarding"": true,
      ""display_order"": 0,
      ""is_io_cache_enabled"": true,
      ""node_info"": {
        ""available_core_quota"": 350,
        ""total_core_quota"": 350
      }
    },
    {
      ""...""
    }
  ]
}
 Response structure Field Name Type Description node_types An array of NodeType The list of available Spark node types. Runtime versions Endpoint HTTP Method 2.0/clusters/spark-versions GET Return the list of available runtime versions. These versions can be used to launch a cluster. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/spark-versions \
| jq .
 JSON Copy {
  ""versions"": [
    {
      ""key"": ""8.2.x-scala2.12"",
      ""name"": ""8.2 (includes Apache Spark 3.1.1, Scala 2.12)""
    },
    {
      ""...""
    }
  ]
}
 Response structure Field Name Type Description versions An array of SparkVersion All the available runtime versions. Events Endpoint HTTP Method 2.0/clusters/events POST Retrieve a list of events about the activity of a cluster. You can retrieve events from active clusters (running, pending, or reconfiguring) and terminated clusters within 30 days of their last termination. This API is paginated. If there are more events to read, the response includes all the parameters necessary to request the next page of events. Example: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/events \
--data @list-events.json \
| jq .
 list-events.json: JSON Copy {
  ""cluster_id"": ""1234-567890-reef123"",
  ""start_time"": 1617238800000,
  ""end_time"": 1619485200000,
  ""order"": ""DESC"",
  ""offset"": 5,
  ""limit"": 5,
  ""event_type"": ""RUNNING""
}
 JSON Copy {
  ""events"": [
    {
      ""cluster_id"": ""1234-567890-reef123"",
      ""timestamp"": 1619471498409,
      ""type"": ""RUNNING"",
      ""details"": {
        ""current_num_workers"": 2,
        ""target_num_workers"": 2
      }
    },
    {
      ""...""
    }
  ],
  ""next_page"": {
    ""cluster_id"": ""1234-567890-reef123"",
    ""start_time"": 1617238800000,
    ""end_time"": 1619485200000,
    ""order"": ""DESC"",
    ""offset"": 10,
    ""limit"": 5
  },
  ""total_count"": 25
}
 Example request to retrieve the next page of events: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/events \
--data @list-events.json \
| jq .
 list-events.json: JSON Copy {
  ""cluster_id"": ""1234-567890-reef123"",
  ""start_time"": 1617238800000,
  ""end_time"": 1619485200000,
  ""order"": ""DESC"",
  ""offset"": 10,
  ""limit"": 5,
  ""event_type"": ""RUNNING""
}
 JSON Copy {
  ""events"": [
    {
      ""cluster_id"": ""1234-567890-reef123"",
      ""timestamp"": 1618330776302,
      ""type"": ""RUNNING"",
      ""details"": {
        ""current_num_workers"": 2,
        ""target_num_workers"": 2
      }
    },
    {
      ""...""
    }
  ],
  ""next_page"": {
    ""cluster_id"": ""1234-567890-reef123"",
    ""start_time"": 1617238800000,
    ""end_time"": 1619485200000,
    ""order"": ""DESC"",
    ""offset"": 15,
    ""limit"": 5
  },
  ""total_count"": 25
}
 Request structure Retrieve events pertaining to a specific cluster. Field Name Type Description cluster_id STRING The ID of the cluster to retrieve events about. This field is required. start_time INT64 The start time in epoch milliseconds. If empty, returns events starting from the beginning of time. end_time INT64 The end time in epoch milliseconds. If empty, returns events up to the current time. order ListOrder The order to list events in; either ASC or DESC. Defaults to DESC. event_types An array of ClusterEventType An optional set of event types to filter on. If empty, all event types are returned. offset INT64 The offset in the result set. Defaults to 0 (no offset). When an offset is specified and the results are requested in descending order, the end_time field is required. limit INT64 The maximum number of events to include in a page of events. Defaults to 50, and maximum allowed value is 500. Response structure Field Name Type Description events An array of ClusterEvent This list of matching events. next_page Request structure The parameters required to retrieve the next page of events. Omitted if there are no more events to read. total_count INT64 The total number of events filtered by the start_time, end_time, and event_types. Data structures In this section: AutoScale ClusterInfo ClusterEvent ClusterEventType EventDetails ClusterAttributes ClusterSize ListOrder ResizeCause ClusterLogConf InitScriptInfo ClusterTag DbfsStorageInfo FileStorageInfo DockerImage DockerBasicAuth LogSyncStatus NodeType ClusterCloudProviderNodeInfo ClusterCloudProviderNodeStatus ParameterPair SparkConfPair SparkEnvPair SparkNode SparkVersion TerminationReason PoolClusterTerminationCode ClusterSource ClusterState TerminationCode TerminationType TerminationParameter AzureAttributes AzureAvailability AutoScale Range defining the min and max number of cluster workers. Field Name Type Description min_workers INT32 The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation. max_workers INT32 The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers. ClusterInfo Metadata about a cluster. Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING Canonical identifier for the cluster. This ID is retained during cluster restarts and resizes, while each new cluster has a globally unique ID. creator_user_name STRING Creator user name. The field won’t be included in the response if the user has already been deleted. driver SparkNode Node on which the Spark driver resides. The driver node contains the Spark master and the Databricks application that manages the per-notebook Spark REPLs. executors An array of SparkNode Nodes on which the Spark executors reside. spark_context_id INT64 A canonical SparkContext identifier. This value does change when the Spark driver restarts. The pair (cluster_id, spark_context_id) is a globally unique identifier over all Spark contexts. jdbc_port INT32 Port on which Spark JDBC server is listening in the driver node. No service will be listening on on this port in executor nodes. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the List node types API call. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. To specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. enable_elastic_disk BOOL Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space. See Autoscaling local storage for details. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. state ClusterState State of the cluster. state_message STRING A message associated with the most recent state transition (for example, the reason why the cluster entered a TERMINATED state). start_time INT64 Time (in epoch milliseconds) when the cluster creation request was received (when the cluster entered a PENDING state). terminated_time INT64 Time (in epoch milliseconds) when the cluster was terminated, if applicable. last_state_loss_time INT64 Time when the cluster driver last lost its state (due to a restart or driver failure). last_activity_time INT64 Time (in epoch milliseconds) when the cluster was last active. A cluster is active if there is at least one command that has not finished on the cluster. This field is available after the cluster has reached a RUNNING state. Updates to this field are made as best-effort attempts. Certain versions of Spark do not support reporting of cluster activity. Refer to Automatic termination for details. cluster_memory_mb INT64 Total amount of cluster memory, in megabytes. cluster_cores FLOAT Number of CPU cores available for this cluster. This can be fractional since certain node types are configured to share cores between Spark nodes on the same instance. default_tags ClusterTag An object containing a set of tags that are added by Azure Databricks regardless of any custom_tags, including: * Vendor: Databricks * Creator: * ClusterName: * ClusterId: * Name: On job clusters: * RunName: * JobId: On resources used by Databricks SQL: * SqlWarehouseId: cluster_log_status LogSyncStatus Cluster log delivery status. termination_reason TerminationReason Information about why the cluster was terminated. This field only appears when the cluster is in a TERMINATING or TERMINATED state. ClusterEvent Cluster event information. Field Name Type Description cluster_id STRING Canonical identifier for the cluster. This field is required. timestamp INT64 The timestamp when the event occurred, stored as the number of milliseconds since the unix epoch. Assigned by the Timeline service. type ClusterEventType The event type. This field is required. details EventDetails The event details. This field is required. ClusterEventType Type of a cluster event. Event Type Description CREATING Indicates that the cluster is being created. DID_NOT_EXPAND_DISK Indicates that a disk is low on space, but adding disks would put it over the max capacity. EXPANDED_DISK Indicates that a disk was low on space and the disks were expanded. FAILED_TO_EXPAND_DISK Indicates that a disk was low on space and disk space could not be expanded. INIT_SCRIPTS_STARTING Indicates that the cluster scoped init script has started. INIT_SCRIPTS_FINISHED Indicates that the cluster scoped init script has finished. STARTING Indicates that the cluster is being started. RESTARTING Indicates that the cluster is being started. TERMINATING Indicates that the cluster is being terminated. EDITED Indicates that the cluster has been edited. RUNNING Indicates the cluster has finished being created. Includes the number of nodes in the cluster and a failure reason if some nodes could not be acquired. RESIZING Indicates a change in the target size of the cluster (upsize or downsize). UPSIZE_COMPLETED Indicates that nodes finished being added to the cluster. Includes the number of nodes in the cluster and a failure reason if some nodes could not be acquired. NODES_LOST Indicates that some nodes were lost from the cluster. DRIVER_HEALTHY Indicates that the driver is healthy and the cluster is ready for use. DRIVER_UNAVAILABLE Indicates that the driver is unavailable. SPARK_EXCEPTION Indicates that a Spark exception was thrown from the driver. DRIVER_NOT_RESPONDING Indicates that the driver is up but is not responsive, likely due to GC. DBFS_DOWN Indicates that the driver is up but DBFS is down. METASTORE_DOWN Indicates that the driver is up but the metastore is down. NODE_BLACKLISTED Indicates that a node is not allowed by Spark. PINNED Indicates that the cluster was pinned. UNPINNED Indicates that the cluster was unpinned. EventDetails Details about a cluster event. Field Name Type Description current_num_workers INT32 The number of nodes in the cluster. target_num_workers INT32 The targeted number of nodes in the cluster. previous_attributes ClusterAttributes The cluster attributes before a cluster was edited. attributes ClusterAttributes * For created clusters, the attributes of the cluster. * For edited clusters, the new attributes of the cluster. previous_cluster_size ClusterSize The size of the cluster before an edit or resize. cluster_size ClusterSize The cluster size that was set in the cluster creation or edit. cause ResizeCause The cause of a change in target size. reason TerminationReason A termination reason: * On a TERMINATED event, the reason for the termination. * On a RESIZE_COMPLETE event, indicates the reason that we failed to acquire some nodes. user STRING The user that caused the event to occur. (Empty if it was done by Azure Databricks.) ClusterAttributes Common set of attributes set during cluster creation. These attributes cannot be changed over the lifetime of a cluster. Field Name Type Description cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster, for example “5.0.x-scala2.11”. You can retrieve a list of available runtime versions by using the Runtime versions API call. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. ssh_public_keys An array of STRING SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. Up to 10 keys can be specified. custom_tags ClusterTag An object containing a set of tags for cluster resources. Databricks tags all cluster resources with these tags in addition to default_tags. Note: * Tags are not supported on legacy node types such as compute-optimized and memory-optimized. * Databricks allows at most 45 custom tags. * If the cluster is created on an instance pool, the cluster tags are not copied to the cluster resources. To tag resources for an instance pool, see the custom_tags field in the Instance Pools API 2.0. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. enable_elastic_disk BOOL Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space. See Autoscaling local storage for details. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. cluster_source ClusterSource Determines whether the cluster was created by a user through the UI, created by the Databricks Jobs scheduler, or through an API request. policy_id STRING A cluster policy ID. azure_attributes AzureAttributes Defines attributes such as the instance availability type, node placement, and max bid price. If not specified during cluster creation, a set of default values is used. ClusterSize Cluster size specification. Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field is updated to reflect the target size of 10 workers, whereas the workers listed in executors gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. ListOrder Generic ordering enum for list-based queries. Order Description DESC Descending order. ASC Ascending order. ResizeCause Reason why a cluster was resized. Cause Description AUTOSCALE Automatically resized based on load. USER_REQUEST User requested a new size. AUTORECOVERY Autorecovery monitor resized the cluster after it lost a node. ClusterLogConf Path to cluster log. Field Name Type Description dbfs DbfsStorageInfo DBFS location of cluster log. Destination must be provided. For example, { ""dbfs"" : { ""destination"" : ""dbfs:/home/cluster_log"" } } InitScriptInfo Path to an init script. For instructions on using init scripts with Databricks Container Services, see Use an init script. Note The file storage type is only available for clusters set up using Databricks Container Services. Field Name Type Description dbfs OR file DbfsStorageInfo FileStorageInfo DBFS location of init script. Destination must be provided. For example, { ""dbfs"" : { ""destination"" : ""dbfs:/home/init_script"" } } File location of init script. Destination must be provided. For example, { ""file"" : { ""destination"" : ""file:/my/local/file.sh"" } } ClusterTag Cluster tag definition. Type Description STRING The key of the tag. The key must: * Be between 1 and 512 characters long * Not contain any of the characters <>%*&+?\\/ * Not begin with azure, microsoft, or windows STRING The value of the tag. The value length must be less than or equal to 256 UTF-8 characters. DbfsStorageInfo DBFS storage information. Field Name Type Description destination STRING DBFS destination. Example: dbfs:/my/path FileStorageInfo File storage information. Note This location type is only available for clusters set up using Databricks Container Services. Field Name Type Description destination STRING File destination. Example: file:/my/file.sh DockerImage Docker image connection information. Field Type Description url string URL for the Docker image. basic_auth DockerBasicAuth Basic authentication information for Docker repository. DockerBasicAuth Docker repository basic authentication information. Field Description username User name for the Docker repository. password Password for the Docker repository. LogSyncStatus Log delivery status. Field Name Type Description last_attempted INT64 The timestamp of last attempt. If the last attempt fails, last_exception contains the exception in the last attempt. last_exception STRING The exception thrown in the last attempt, it would be null (omitted in the response) if there is no exception in last attempted. NodeType Description of a Spark node type including both the dimensions of the node and the instance type on which it will be hosted. Field Name Type Description node_type_id STRING Unique identifier for this node type. This field is required. memory_mb INT32 Memory (in MB) available for this node type. This field is required. num_cores FLOAT Number of CPU cores available for this node type. This can be fractional if the number of cores on a machine instance is not divisible by the number of Spark nodes on that machine. This field is required. description STRING A string description associated with this node type. This field is required. instance_type_id STRING An identifier for the type of hardware that this node runs on. This field is required. is_deprecated BOOL Whether the node type is deprecated. Non-deprecated node types offer greater performance. node_info ClusterCloudProviderNodeInfo Node type info reported by the cloud provider. ClusterCloudProviderNodeInfo Information about an instance supplied by a cloud provider. Field Name Type Description status ClusterCloudProviderNodeStatus Status as reported by the cloud provider. available_core_quota INT32 Available CPU core quota. total_core_quota INT32 Total CPU core quota. ClusterCloudProviderNodeStatus Status of an instance supplied by a cloud provider. Status Description NotEnabledOnSubscription Node type not available for subscription. NotAvailableInRegion Node type not available in region. ParameterPair Parameter that provides additional information about why a cluster was terminated. Type Description TerminationParameter Type of termination information. STRING The termination information. SparkConfPair Spark configuration key-value pairs. Type Description STRING A configuration property name. STRING The configuration property value. SparkEnvPair Spark environment variable key-value pairs. Important When specifying environment variables in a job cluster, the fields in this data structure accept only Latin characters (ASCII character set). Using non-ASCII characters will return an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis. Type Description STRING An environment variable name. STRING The environment variable value. SparkNode Spark driver or executor configuration. Field Name Type Description private_ip STRING Private IP address (typically a 10.x.x.x address) of the Spark node. This is different from the private IP address of the host instance. public_dns STRING Public DNS address of this node. This address can be used to access the Spark JDBC server on the driver node. node_id STRING Globally unique identifier for this node. instance_id STRING Globally unique identifier for the host instance from the cloud provider. start_timestamp INT64 The timestamp (in millisecond) when the Spark node is launched. host_private_ip STRING The private IP address of the host instance. SparkVersion Databricks Runtime version of the cluster. Field Name Type Description key STRING Databricks Runtime version key, for example 7.3.x-scala2.12. The value that should be provided as the spark_version when creating a new cluster. The exact runtime version may change over time for a “wildcard” version (that is, 7.3.x-scala2.12 is a “wildcard” version) with minor bug fixes. name STRING A descriptive name for the runtime version, for example “Databricks Runtime 7.3 LTS”. TerminationReason Reason why a cluster was terminated. Field Name Type Description code TerminationCode Status code indicating why a cluster was terminated. type TerminationType Reason indicating why a cluster was terminated. parameters ParameterPair Object containing a set of parameters that provide information about why a cluster was terminated. PoolClusterTerminationCode Status code indicating why the cluster was terminated due to a pool failure. Code Description INSTANCE_POOL_MAX_CAPACITY_FAILURE The pool max capacity has been reached. INSTANCE_POOL_NOT_FOUND_FAILURE The pool specified by the cluster is no longer active or doesn’t exist. ClusterSource Service that created the cluster. Service Description UI Cluster created through the UI. JOB Cluster created by the Databricks job scheduler. API Cluster created through an API call. ClusterState State of a cluster. The allowable state transitions are as follows: PENDING -> RUNNING PENDING -> TERMINATING RUNNING -> RESIZING RUNNING -> RESTARTING RUNNING -> TERMINATING RESTARTING -> RUNNING RESTARTING -> TERMINATING RESIZING -> RUNNING RESIZING -> TERMINATING TERMINATING -> TERMINATED State Description PENDING Indicates that a cluster is in the process of being created. RUNNING Indicates that a cluster has been started and is ready for use. RESTARTING Indicates that a cluster is in the process of restarting. RESIZING Indicates that a cluster is in the process of adding or removing nodes. TERMINATING Indicates that a cluster is in the process of being destroyed. TERMINATED Indicates that a cluster has been successfully destroyed. ERROR This state is no longer used. It was used to indicate a cluster that failed to be created. TERMINATING and TERMINATED are used instead. UNKNOWN Indicates that a cluster is in an unknown state. A cluster should never be in this state. TerminationCode Status code indicating why the cluster was terminated. Code Description USER_REQUEST A user terminated the cluster directly. Parameters should include a username field that indicates the specific user who terminated the cluster. JOB_FINISHED The cluster was launched by a job, and terminated when the job completed. INACTIVITY The cluster was terminated since it was idle. CLOUD_PROVIDER_SHUTDOWN The instance that hosted the Spark driver was terminated by the cloud provider. COMMUNICATION_LOST Azure Databricks lost connection to services on the driver instance. For example, this can happen when problems arise in cloud networking infrastructure, or when the instance itself becomes unhealthy. CLOUD_PROVIDER_LAUNCH_FAILURE Azure Databricks experienced a cloud provider failure when requesting instances to launch clusters. SPARK_STARTUP_FAILURE The cluster failed to initialize. Possible reasons may include failure to create the environment for Spark or issues launching the Spark master and worker processes. INVALID_ARGUMENT Cannot launch the cluster because the user specified an invalid argument. For example, the user might specify an invalid runtime version for the cluster. UNEXPECTED_LAUNCH_FAILURE While launching this cluster, Azure Databricks failed to complete critical setup steps, terminating the cluster. INTERNAL_ERROR Azure Databricks encountered an unexpected error that forced the running cluster to be terminated. Contact Azure Databricks support for additional details. SPARK_ERROR The Spark driver failed to start. Possible reasons may include incompatible libraries and initialization scripts that corrupted the Spark container. METASTORE_COMPONENT_UNHEALTHY The cluster failed to start because the external metastore could not be reached. Refer to Troubleshooting. DBFS_COMPONENT_UNHEALTHY The cluster failed to start because Databricks File System (DBFS) could not be reached. AZURE_RESOURCE_PROVIDER_THROTTLING Azure Databricks reached the Azure Resource Provider request limit. Specifically, the API request rate to the specific resource type (compute, network, etc.) can’t exceed the limit. Retry might help to resolve the issue. For further information, seehttps://docs.microsoft.com/azure/virtual-machines/troubleshooting/troubleshooting-throttling-errors. AZURE_RESOURCE_MANAGER_THROTTLING Azure Databricks reached the Azure Resource Manager request limit which will prevent the Azure SDK from issuing any read or write request to the Azure Resource Manager. The request limit is applied to each subscription every hour. Retry after an hour or changing to a smaller cluster size might help to resolve the issue. For further information, seehttps://docs.microsoft.com/azure/azure-resource-manager/resource-manager-request-limits. NETWORK_CONFIGURATION_FAILURE The cluster was terminated due to an error in the network configuration. For example, a workspace with VNet injection had incorrect DNS settings that blocked access to worker artifacts. DRIVER_UNREACHABLE Azure Databricks was not able to access the Spark driver, because it was not reachable. DRIVER_UNRESPONSIVE Azure Databricks was not able to access the Spark driver, because it was unresponsive. INSTANCE_UNREACHABLE Azure Databricks was not able to access instances in order to start the cluster. This can be a transient networking issue. If the problem persists, this usually indicates a networking environment misconfiguration. CONTAINER_LAUNCH_FAILURE Azure Databricks was unable to launch containers on worker nodes for the cluster. Have your admin check your network configuration. INSTANCE_POOL_CLUSTER_FAILURE Pool backed cluster specific failure. See Pools for details. REQUEST_REJECTED Azure Databricks cannot handle the request at this moment. Try again later and contact Azure Databricks if the problem persists. INIT_SCRIPT_FAILURE Azure Databricks cannot load and run a cluster-scoped init script on one of the cluster’s nodes, or the init script terminates with a non-zero exit code. See Init script logs. TRIAL_EXPIRED The Azure Databricks trial subscription expired. BOOTSTRAP_TIMEOUT The cluster failed to start because of user network configuration issues. Possible reasons include misconfiguration of firewall settings, UDR entries, DNS, or route tables. TerminationType Reason why the cluster was terminated. Type Description SUCCESS Termination succeeded. CLIENT_ERROR Non-retriable. Client must fix parameters before reattempting the cluster creation. SERVICE_FAULT Azure Databricks service issue. Client can retry. CLOUD_FAILURE Cloud provider infrastructure issue. Client can retry after the underlying issue is resolved. TerminationParameter Key that provides additional information about why a cluster was terminated. Key Description username The username of the user who terminated the cluster. databricks_error_message Additional context that may explain the reason for cluster termination. inactivity_duration_min An idle cluster was shut down after being inactive for this duration. instance_id The ID of the instance that was hosting the Spark driver. azure_error_code The Azure provided error code describing why cluster nodes could not be provisioned. For reference, see: https://docs.microsoft.com/azure/virtual-machines/windows/error-messages. azure_error_message Human-readable context of various failures from Azure. This field is unstructured, and its exact format is subject to change. instance_pool_id The ID of the instance pool the cluster is using. instance_pool_error_code The error code for cluster failures specific to a pool. AzureAttributes Attributes set during cluster creation related to Azure. Field Name Type Description first_on_demand INT32 The first first_on_demand nodes of the cluster will be placed on on-demand instances. This value must be greater than 0, or else cluster creation validation fails. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. availability AzureAvailability Availability type used for all subsequent nodes past the first_on_demand ones. spot_bid_max_price DOUBLE The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to -1 (the default), which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance. You can view historical pricing and eviction rates in the Azure portal. AzureAvailability The Azure instance availability type behavior. Type Description SPOT_AZURE Use spot instances. ON_DEMAND_AZURE Use on-demand instances. SPOT_WITH_FALLBACK_AZURE Preferably use spot instances, but fall back to on-demand instances if spot instances cannot be acquired (for example, if Azure spot prices are too high or out of quota). Does not apply to pool availability.",Clusters API 2.0
43,ARR - Jobs timing out,"Run result unavailable: job failed with error message Unexpected failure while waiting for the cluster (0620-160006-xiafh9uu) to be ready.Cause Unexpected state for cluster (0620-160006-xiafh9uu): CONTAINER_LAUNCH_FAILURE(SERVICE_FAULT): instance_id:4476bf78f35a438bbf1ea3dece4fee53,databricks_error_message:Failed to launch spark container on instance 4476bf78f35a438bbf1ea3dece4fee53. Exception: Unexpected internal error, please contact Databricks support",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/clusters,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Clusters API 2.0 Article 07/14/2022 46 minutes to read 6 contributors In this article Create Edit Start Restart Resize Delete (terminate) Permanent delete Get Pin Unpin List List node types Runtime versions Events Data structures The Clusters API allows you to create, start, edit, list, terminate, and delete clusters. The maximum allowed size of a request to the Clusters API is 10MB. Cluster lifecycle methods require a cluster ID, which is returned from Create. To obtain a list of clusters, invoke List. Azure Databricks maps cluster node instance types to compute units known as DBUs. See the instance type pricing page for a list of the supported instance types and their corresponding DBUs. For instance provider information, see Azure instance type specifications and pricing. Azure Databricks always provides one year’s deprecation notice before ceasing support for an instance type. Important To access Databricks REST APIs, you must authenticate. Create Endpoint HTTP Method 2.0/clusters/create POST Create a new Apache Spark cluster. This method acquires new instances from the cloud provider if necessary. This method is asynchronous; the returned cluster_id can be used to poll the cluster state. When this method returns, the cluster is in a PENDING state. The cluster is usable once it enters a RUNNING state. See ClusterState. Note Azure Databricks may not be able to acquire some of the requested nodes, due to cloud provider limitations or transient network issues. If it is unable to acquire a sufficient number of the requested nodes, cluster creation will terminate with an informative error message. Examples Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy {
  ""cluster_name"": ""my-cluster"",
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2"",
  ""spark_conf"": {
    ""spark.speculation"": true
  },
  ""num_workers"": 25
}
 JSON Copy { ""cluster_id"": ""1234-567890-undid123"" }
 Here is an example for an autoscaling cluster. This cluster will start with two nodes, the minimum. Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy {
  ""cluster_name"": ""autoscaling-cluster"",
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2"",
  ""autoscale"" : {
    ""min_workers"": 2,
    ""max_workers"": 50
  }
}
 JSON Copy { ""cluster_id"": ""1234-567890-hared123"" }
 This example creates a Single Node cluster. To create a Single Node cluster: Set spark_conf and custom_tags to the exact values in the example. Set num_workers to 0. Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy {
  ""cluster_name"": ""single-node-cluster"",
  ""spark_version"": ""7.6.x-scala2.12"",
  ""node_type_id"": ""Standard_DS3_v2"",
  ""num_workers"": 0,
  ""spark_conf"": {
    ""spark.databricks.cluster.profile"": ""singleNode"",
    ""spark.master"": ""local[*]""
  },
  ""custom_tags"": {
    ""ResourceClass"": ""SingleNode""
  }
}
 JSON Copy { ""cluster_id"": ""1234-567890-pouch123"" }
 To create a job or submit a run with a new cluster using a policy, set policy_id to the policy ID: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy 
{
    ""num_workers"": null,
    ""autoscale"": {
        ""min_workers"": 2,
        ""max_workers"": 8
    },
    ""cluster_name"": ""my-cluster"",
    ""spark_version"": ""7.3.x-scala2.12"",
    ""spark_conf"": {},
    ""node_type_id"": ""Standard_D3_v2"",
    ""custom_tags"": {},
    ""spark_env_vars"": {
        ""PYSPARK_PYTHON"": ""/databricks/python3/bin/python3""
    },
    ""autotermination_minutes"": 120,
    ""init_scripts"": [],
    ""policy_id"": ""C65B864F02000008""
}
 To create a new cluster, define the cluster’s properties in new_cluster: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/job/create \
--data @create-job.json
 create-job.json: JSON Copy {
  ""run_name"": ""my spark task"",
  ""new_cluster"": {
    ""spark_version"": ""7.3.x-scala2.12"",
    ""node_type_id"": ""Standard_D3_v2"",
    ""num_workers"": 10,
    ""policy_id"": ""ABCD000000000000""
  },
  ""libraries"": [
    {
      ""jar"": ""dbfs:/my-jar.jar""
    },
    {
      ""maven"": {
        ""coordinates"": ""org.jsoup:jsoup:1.7.2""
      }
    }
  ],
  ""spark_jar_task"": {
    ""main_class_name"": ""com.databricks.ComputeModels""
  }
}
 Request structure of the cluster definition Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. This field is required. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. This field is required. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. custom_tags ClusterTag An object containing a set of tags for cluster resources. Databricks tags all cluster resources (such as VMs) with these tags in addition to default_tags. Note: * Azure Databricks allows at most 43 custom tags. * If the cluster is created on an instance pool, the cluster tags are not copied to the cluster resources. To tag resources for an instance pool, see the custom_tags field in the Instance Pools API 2.0. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of scripts can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. driver_instance_pool_id STRING The ID of the instance pool to use for drivers. You must also specify instance_pool_id. Refer to Instance Pools API 2.0 for details. instance_pool_id STRING The optional ID of the instance pool to use for cluster nodes. If driver_instance_pool_id is present, instance_pool_id is used for worker nodes only. Otherwise, it is used for both the driver and the worker nodes. Refer to Instance Pools API 2.0 for details. idempotency_token STRING An optional token that can be used to guarantee the idempotency of cluster creation requests. If the idempotency token is assigned to a cluster that is not in the TERMINATED state, the request does not create a new cluster but instead returns the ID of the existing cluster. Otherwise, a new cluster is created. The idempotency token is cleared when the cluster is terminated If you specify the idempotency token, upon failure you can retry until the request succeeds. Azure Databricks will guarantee that exactly one cluster will be launched with that idempotency token. This token should have at most 64 characters. apply_policy_default_values BOOL Whether to use policy default values for missing cluster attributes. enable_local_disk_encryption BOOL Whether encryption of disks locally attached to the cluster is enabled. azure_attributes AzureAttributes Attributes related to clusters running on Azure. If not specified at cluster creation, a set of default values is used. runtime_engine STRING The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: * PHOTON: Use the Photon runtime engine type. * STANDARD: Use the standard runtime engine type. This field is optional. Response structure Field Name Type Description cluster_id STRING Canonical identifier for the cluster. Edit Endpoint HTTP Method 2.0/clusters/edit POST Edit the configuration of a cluster to match the provided attributes and size. You can edit a cluster if it is in a RUNNING or TERMINATED state. If you edit a cluster while it is in a RUNNING state, it will be restarted so that the new attributes can take effect. If you edit a cluster while it is in a TERMINATED state, it will remain TERMINATED. The next time it is started using the clusters/start API, the new attributes will take effect. An attempt to edit a cluster in any other state will be rejected with an INVALID_STATE error code. Clusters created by the Databricks Jobs service cannot be edited. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/edit \
--data @edit-cluster.json
 edit-cluster.json: JSON Copy {
  ""cluster_id"": ""1202-211320-brick1"",
  ""num_workers"": 10,
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2""
}
 JSON Copy {}
 Request structure Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING Canonical identifier for the cluster. This field is required. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. This field is required. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. This field is required. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default Databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. apply_policy_default_values BOOL Whether to use policy default values for missing cluster attributes. enable_local_disk_encryption BOOL Whether encryption of disks locally attached to the cluster is enabled. azure_attributes AzureAttributes Attributes related to clusters running on Azure. If not specified at cluster creation, a set of default values is used. runtime_engine STRING The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: * PHOTON: Use the Photon runtime engine type. * STANDARD: Use the standard runtime engine type. This field is optional. Start Endpoint HTTP Method 2.0/clusters/start POST Start a terminated cluster given its ID. This is similar to createCluster, except: The terminated cluster ID and attributes are preserved. The cluster starts with the last specified cluster size. If the terminated cluster is an autoscaling cluster, the cluster starts with the minimum number of nodes. If the cluster is in the RESTARTING state, a 400 error is returned. You cannot start a cluster launched to run a job. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/start \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be started. This field is required. Restart Endpoint HTTP Method 2.0/clusters/restart POST Restart a cluster given its ID. The cluster must be in the RUNNING state. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/restart \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be started. This field is required. Resize Endpoint HTTP Method 2.0/clusters/resize POST Resize a cluster to have a desired number of workers. The cluster must be in the RUNNING state. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/resize \
--data '{ ""cluster_id"": ""1234-567890-reef123"", ""num_workers"": 30 }'
 JSON Copy {}
 Request structure Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING The cluster to be resized. This field is required. Delete (terminate) Endpoint HTTP Method 2.0/clusters/delete POST Terminate a cluster given its ID. The cluster is removed asynchronously. Once the termination has completed, the cluster will be in the TERMINATED state. If the cluster is already in a TERMINATING or TERMINATED state, nothing will happen. Unless a cluster is pinned, 30 days after the cluster is terminated, it is permanently deleted. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/delete \
--data '{ ""cluster_id"": ""1234-567890-frays123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be terminated. This field is required. Permanent delete Endpoint HTTP Method 2.0/clusters/permanent-delete POST Permanently delete a cluster. If the cluster is running, it is terminated and its resources are asynchronously removed. If the cluster is terminated, then it is immediately removed. You cannot perform any action, including retrieve the cluster’s permissions, on a permanently deleted cluster. A permanently deleted cluster is also no longer returned in the cluster list. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/permanent-delete \
--data '{ ""cluster_id"": ""1234-567890-frays123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be permanently deleted. This field is required. Get Endpoint HTTP Method 2.0/clusters/get GET Retrieve the information for a cluster given its identifier. Clusters can be described while they are running or up to 30 days after they are terminated. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/get \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }' \
| jq .
 JSON Copy {
  ""cluster_id"": ""1234-567890-reef123"",
  ""driver"": {
    ""node_id"": ""dced0ce388954c38abef081f54c18afd"",
    ""instance_id"": ""c69c0b119a2a499d8a2843c4d256136a"",
    ""start_timestamp"": 1619718438896,
    ""host_private_ip"": ""10.0.0.1"",
    ""private_ip"": ""10.0.0.2""
  },
  ""spark_context_id"": 5631707659504820000,
  ""jdbc_port"": 10000,
  ""cluster_name"": ""my-cluster"",
  ""spark_version"": ""8.2.x-scala2.12"",
  ""node_type_id"": ""Standard_L4s"",
  ""driver_node_type_id"": ""Standard_L4s"",
  ""custom_tags"": {
    ""ResourceClass"": ""SingleNode""
  },
  ""autotermination_minutes"": 0,
  ""enable_elastic_disk"": true,
  ""disk_spec"": {},
  ""cluster_source"": ""UI"",
  ""enable_local_disk_encryption"": false,
  ""azure_attributes"": {
    ""first_on_demand"": 1,
    ""availability"": ""ON_DEMAND_AZURE"",
    ""spot_bid_max_price"": -1
  },
  ""instance_source"": {
    ""node_type_id"": ""Standard_L4s""
  },
  ""driver_instance_source"": {
    ""node_type_id"": ""Standard_L4s""
  },
  ""state"": ""RUNNING"",
  ""state_message"": """",
  ""start_time"": 1610745129764,
  ""last_state_loss_time"": 1619718513513,
  ""num_workers"": 0,
  ""cluster_memory_mb"": 32768,
  ""cluster_cores"": 4,
  ""default_tags"": {
    ""Vendor"": ""Databricks"",
    ""Creator"": ""someone@example.com"",
    ""ClusterName"": ""my-cluster"",
    ""ClusterId"": ""1234-567890-reef123""
  },
  ""creator_user_name"": ""someone@example.com"",
  ""pinned_by_user_name"": ""3401478490056118"",
  ""init_scripts_safe_mode"": false
}
 Request structure Field Name Type Description cluster_id STRING The cluster about which to retrieve information. This field is required. Response structure Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING Canonical identifier for the cluster. This ID is retained during cluster restarts and resizes, while each new cluster has a globally unique ID. creator_user_name STRING Creator user name. The field won’t be included in the response if the user has already been deleted. driver SparkNode Node on which the Spark driver resides. The driver node contains the Spark master and the Databricks application that manages the per-notebook Spark REPLs. executors An array of SparkNode Nodes on which the Spark executors reside. spark_context_id INT64 A canonical SparkContext identifier. This value does change when the Spark driver restarts. The pair (cluster_id, spark_context_id) is a globally unique identifier over all Spark contexts. jdbc_port INT32 Port on which Spark JDBC server is listening in the driver node. No service will be listening on on this port in executor nodes. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. This field is required. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. custom_tags ClusterTag An object containing a set of tags for cluster resources. Databricks tags all cluster resources with these tags in addition to default_tags. Note: * Tags are not supported on legacy node types such as compute-optimized and memory-optimized. * Databricks allows at most 45 custom tags. * If the cluster is created on an instance pool, the cluster tags are not copied to the cluster resources. To tag resources for an instance pool, see the custom_tags field in the Instance Pools API 2.0. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default Databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. enable_elastic_disk BOOL Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space. See Autoscaling local storage for details. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. state ClusterState State of the cluster. state_message STRING A message associated with the most recent state transition (for example, the reason why the cluster entered the TERMINATED state). start_time INT64 Time (in epoch milliseconds) when the cluster creation request was received (when the cluster entered the PENDING state). terminated_time INT64 Time (in epoch milliseconds) when the cluster was terminated, if applicable. last_state_loss_time INT64 Time when the cluster driver last lost its state (due to a restart or driver failure). last_activity_time INT64 Time (in epoch milliseconds) when the cluster was last active. A cluster is active if there is at least one command that has not finished on the cluster. This field is available after the cluster has reached the RUNNING state. Updates to this field are made as best-effort attempts. Certain versions of Spark do not support reporting of cluster activity. Refer to Automatic termination for details. cluster_memory_mb INT64 Total amount of cluster memory, in megabytes. cluster_cores FLOAT Number of CPU cores available for this cluster. This can be fractional since certain node types are configured to share cores between Spark nodes on the same instance. default_tags ClusterTag An object containing a set of tags that are added by Azure Databricks regardless of any custom_tags, including: * Vendor: Databricks * Creator: * ClusterName: * ClusterId: * Name: On job clusters: * RunName: * JobId: On resources used by Databricks SQL: * SqlWarehouseId: cluster_log_status LogSyncStatus Cluster log delivery status. termination_reason TerminationReason Information about why the cluster was terminated. This field appears only when the cluster is in the TERMINATING or TERMINATED state. Pin Note You must be an Azure Databricks administrator to invoke this API. Endpoint HTTP Method 2.0/clusters/pin POST Ensure that an all-purpose cluster configuration is retained even after a cluster has been terminated for more than 30 days. Pinning ensures that the cluster is always returned by the List API. Pinning a cluster that is already pinned has no effect. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/pin \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to pin. This field is required. Unpin Note You must be an Azure Databricks administrator to invoke this API. Endpoint HTTP Method 2.0/clusters/unpin POST Allows the cluster to eventually be removed from the list returned by the List API. Unpinning a cluster that is not pinned has no effect. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/unpin \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to unpin. This field is required. List Endpoint HTTP Method 2.0/clusters/list GET Return information about all pinned clusters, active clusters, up to 200 of the most recently terminated all-purpose clusters in the past 30 days, and up to 30 of the most recently terminated job clusters in the past 30 days. For example, if there is 1 pinned cluster, 4 active clusters, 45 terminated all-purpose clusters in the past 30 days, and 50 terminated job clusters in the past 30 days, then this API returns the 1 pinned cluster, 4 active clusters, all 45 terminated all-purpose clusters, and the 30 most recently terminated job clusters. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list \
| jq .
 JSON Copy {
  ""clusters"": [
    {
      ""cluster_id"": ""1234-567890-reef123"",
      ""driver"": {
        ""node_id"": ""dced0ce388954c38abef081f54c18afd"",
        ""instance_id"": ""c69c0b119a2a499d8a2843c4d256136a"",
        ""start_timestamp"": 1619718438896,
        ""host_private_ip"": ""10.0.0.1"",
        ""private_ip"": ""10.0.0.2""
      },
      ""spark_context_id"": 5631707659504820000,
      ""jdbc_port"": 10000,
      ""cluster_name"": ""my-cluster"",
      ""spark_version"": ""8.2.x-scala2.12"",
      ""node_type_id"": ""Standard_L4s"",
      ""driver_node_type_id"": ""Standard_L4s"",
      ""custom_tags"": {
        ""ResourceClass"": ""SingleNode""
      },
      ""autotermination_minutes"": 0,
      ""enable_elastic_disk"": true,
      ""disk_spec"": {},
      ""cluster_source"": ""UI"",
      ""enable_local_disk_encryption"": false,
      ""azure_attributes"": {
        ""first_on_demand"": 1,
        ""availability"": ""ON_DEMAND_AZURE"",
        ""spot_bid_max_price"": -1
      },
      ""instance_source"": {
        ""node_type_id"": ""Standard_L4s""
      },
      ""driver_instance_source"": {
        ""node_type_id"": ""Standard_L4s""
      },
      ""state"": ""RUNNING"",
      ""state_message"": """",
      ""start_time"": 1610745129764,
      ""last_state_loss_time"": 1619718513513,
      ""num_workers"": 0,
      ""cluster_memory_mb"": 32768,
      ""cluster_cores"": 4,
      ""default_tags"": {
        ""Vendor"": ""Databricks"",
        ""Creator"": ""someone@example.com"",
        ""ClusterName"": ""my-cluster"",
        ""ClusterId"": ""1234-567890-reef123""
      },
      ""creator_user_name"": ""someone@example.com"",
      ""pinned_by_user_name"": ""3401478490056118"",
      ""init_scripts_safe_mode"": false
    },
    {
      ""...""
    }
  ]
}
 Response structure Field Name Type Description clusters An array of ClusterInfo A list of clusters. List node types Endpoint HTTP Method 2.0/clusters/list-node-types GET Return a list of supported Spark node types. These node types can be used to launch a cluster. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list-node-types \
| jq .
 JSON Copy {
  ""node_types"": [
    {
      ""node_type_id"": ""Standard_L80s_v2"",
      ""memory_mb"": 655360,
      ""num_cores"": 80,
      ""description"": ""Standard_L80s_v2"",
      ""instance_type_id"": ""Standard_L80s_v2"",
      ""is_deprecated"": false,
      ""category"": ""Storage Optimized"",
      ""support_ebs_volumes"": true,
      ""support_cluster_tags"": true,
      ""num_gpus"": 0,
      ""node_instance_type"": {
        ""instance_type_id"": ""Standard_L80s_v2"",
        ""local_disks"": 1,
        ""local_disk_size_gb"": 800,
        ""instance_family"": ""Standard LSv2 Family vCPUs"",
        ""local_nvme_disk_size_gb"": 1788,
        ""local_nvme_disks"": 10,
        ""swap_size"": ""10g""
      },
      ""is_hidden"": false,
      ""support_port_forwarding"": true,
      ""display_order"": 0,
      ""is_io_cache_enabled"": true,
      ""node_info"": {
        ""available_core_quota"": 350,
        ""total_core_quota"": 350
      }
    },
    {
      ""...""
    }
  ]
}
 Response structure Field Name Type Description node_types An array of NodeType The list of available Spark node types. Runtime versions Endpoint HTTP Method 2.0/clusters/spark-versions GET Return the list of available runtime versions. These versions can be used to launch a cluster. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/spark-versions \
| jq .
 JSON Copy {
  ""versions"": [
    {
      ""key"": ""8.2.x-scala2.12"",
      ""name"": ""8.2 (includes Apache Spark 3.1.1, Scala 2.12)""
    },
    {
      ""...""
    }
  ]
}
 Response structure Field Name Type Description versions An array of SparkVersion All the available runtime versions. Events Endpoint HTTP Method 2.0/clusters/events POST Retrieve a list of events about the activity of a cluster. You can retrieve events from active clusters (running, pending, or reconfiguring) and terminated clusters within 30 days of their last termination. This API is paginated. If there are more events to read, the response includes all the parameters necessary to request the next page of events. Example: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/events \
--data @list-events.json \
| jq .
 list-events.json: JSON Copy {
  ""cluster_id"": ""1234-567890-reef123"",
  ""start_time"": 1617238800000,
  ""end_time"": 1619485200000,
  ""order"": ""DESC"",
  ""offset"": 5,
  ""limit"": 5,
  ""event_type"": ""RUNNING""
}
 JSON Copy {
  ""events"": [
    {
      ""cluster_id"": ""1234-567890-reef123"",
      ""timestamp"": 1619471498409,
      ""type"": ""RUNNING"",
      ""details"": {
        ""current_num_workers"": 2,
        ""target_num_workers"": 2
      }
    },
    {
      ""...""
    }
  ],
  ""next_page"": {
    ""cluster_id"": ""1234-567890-reef123"",
    ""start_time"": 1617238800000,
    ""end_time"": 1619485200000,
    ""order"": ""DESC"",
    ""offset"": 10,
    ""limit"": 5
  },
  ""total_count"": 25
}
 Example request to retrieve the next page of events: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/events \
--data @list-events.json \
| jq .
 list-events.json: JSON Copy {
  ""cluster_id"": ""1234-567890-reef123"",
  ""start_time"": 1617238800000,
  ""end_time"": 1619485200000,
  ""order"": ""DESC"",
  ""offset"": 10,
  ""limit"": 5,
  ""event_type"": ""RUNNING""
}
 JSON Copy {
  ""events"": [
    {
      ""cluster_id"": ""1234-567890-reef123"",
      ""timestamp"": 1618330776302,
      ""type"": ""RUNNING"",
      ""details"": {
        ""current_num_workers"": 2,
        ""target_num_workers"": 2
      }
    },
    {
      ""...""
    }
  ],
  ""next_page"": {
    ""cluster_id"": ""1234-567890-reef123"",
    ""start_time"": 1617238800000,
    ""end_time"": 1619485200000,
    ""order"": ""DESC"",
    ""offset"": 15,
    ""limit"": 5
  },
  ""total_count"": 25
}
 Request structure Retrieve events pertaining to a specific cluster. Field Name Type Description cluster_id STRING The ID of the cluster to retrieve events about. This field is required. start_time INT64 The start time in epoch milliseconds. If empty, returns events starting from the beginning of time. end_time INT64 The end time in epoch milliseconds. If empty, returns events up to the current time. order ListOrder The order to list events in; either ASC or DESC. Defaults to DESC. event_types An array of ClusterEventType An optional set of event types to filter on. If empty, all event types are returned. offset INT64 The offset in the result set. Defaults to 0 (no offset). When an offset is specified and the results are requested in descending order, the end_time field is required. limit INT64 The maximum number of events to include in a page of events. Defaults to 50, and maximum allowed value is 500. Response structure Field Name Type Description events An array of ClusterEvent This list of matching events. next_page Request structure The parameters required to retrieve the next page of events. Omitted if there are no more events to read. total_count INT64 The total number of events filtered by the start_time, end_time, and event_types. Data structures In this section: AutoScale ClusterInfo ClusterEvent ClusterEventType EventDetails ClusterAttributes ClusterSize ListOrder ResizeCause ClusterLogConf InitScriptInfo ClusterTag DbfsStorageInfo FileStorageInfo DockerImage DockerBasicAuth LogSyncStatus NodeType ClusterCloudProviderNodeInfo ClusterCloudProviderNodeStatus ParameterPair SparkConfPair SparkEnvPair SparkNode SparkVersion TerminationReason PoolClusterTerminationCode ClusterSource ClusterState TerminationCode TerminationType TerminationParameter AzureAttributes AzureAvailability AutoScale Range defining the min and max number of cluster workers. Field Name Type Description min_workers INT32 The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation. max_workers INT32 The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers. ClusterInfo Metadata about a cluster. Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING Canonical identifier for the cluster. This ID is retained during cluster restarts and resizes, while each new cluster has a globally unique ID. creator_user_name STRING Creator user name. The field won’t be included in the response if the user has already been deleted. driver SparkNode Node on which the Spark driver resides. The driver node contains the Spark master and the Databricks application that manages the per-notebook Spark REPLs. executors An array of SparkNode Nodes on which the Spark executors reside. spark_context_id INT64 A canonical SparkContext identifier. This value does change when the Spark driver restarts. The pair (cluster_id, spark_context_id) is a globally unique identifier over all Spark contexts. jdbc_port INT32 Port on which Spark JDBC server is listening in the driver node. No service will be listening on on this port in executor nodes. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the List node types API call. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. To specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. enable_elastic_disk BOOL Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space. See Autoscaling local storage for details. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. state ClusterState State of the cluster. state_message STRING A message associated with the most recent state transition (for example, the reason why the cluster entered a TERMINATED state). start_time INT64 Time (in epoch milliseconds) when the cluster creation request was received (when the cluster entered a PENDING state). terminated_time INT64 Time (in epoch milliseconds) when the cluster was terminated, if applicable. last_state_loss_time INT64 Time when the cluster driver last lost its state (due to a restart or driver failure). last_activity_time INT64 Time (in epoch milliseconds) when the cluster was last active. A cluster is active if there is at least one command that has not finished on the cluster. This field is available after the cluster has reached a RUNNING state. Updates to this field are made as best-effort attempts. Certain versions of Spark do not support reporting of cluster activity. Refer to Automatic termination for details. cluster_memory_mb INT64 Total amount of cluster memory, in megabytes. cluster_cores FLOAT Number of CPU cores available for this cluster. This can be fractional since certain node types are configured to share cores between Spark nodes on the same instance. default_tags ClusterTag An object containing a set of tags that are added by Azure Databricks regardless of any custom_tags, including: * Vendor: Databricks * Creator: * ClusterName: * ClusterId: * Name: On job clusters: * RunName: * JobId: On resources used by Databricks SQL: * SqlWarehouseId: cluster_log_status LogSyncStatus Cluster log delivery status. termination_reason TerminationReason Information about why the cluster was terminated. This field only appears when the cluster is in a TERMINATING or TERMINATED state. ClusterEvent Cluster event information. Field Name Type Description cluster_id STRING Canonical identifier for the cluster. This field is required. timestamp INT64 The timestamp when the event occurred, stored as the number of milliseconds since the unix epoch. Assigned by the Timeline service. type ClusterEventType The event type. This field is required. details EventDetails The event details. This field is required. ClusterEventType Type of a cluster event. Event Type Description CREATING Indicates that the cluster is being created. DID_NOT_EXPAND_DISK Indicates that a disk is low on space, but adding disks would put it over the max capacity. EXPANDED_DISK Indicates that a disk was low on space and the disks were expanded. FAILED_TO_EXPAND_DISK Indicates that a disk was low on space and disk space could not be expanded. INIT_SCRIPTS_STARTING Indicates that the cluster scoped init script has started. INIT_SCRIPTS_FINISHED Indicates that the cluster scoped init script has finished. STARTING Indicates that the cluster is being started. RESTARTING Indicates that the cluster is being started. TERMINATING Indicates that the cluster is being terminated. EDITED Indicates that the cluster has been edited. RUNNING Indicates the cluster has finished being created. Includes the number of nodes in the cluster and a failure reason if some nodes could not be acquired. RESIZING Indicates a change in the target size of the cluster (upsize or downsize). UPSIZE_COMPLETED Indicates that nodes finished being added to the cluster. Includes the number of nodes in the cluster and a failure reason if some nodes could not be acquired. NODES_LOST Indicates that some nodes were lost from the cluster. DRIVER_HEALTHY Indicates that the driver is healthy and the cluster is ready for use. DRIVER_UNAVAILABLE Indicates that the driver is unavailable. SPARK_EXCEPTION Indicates that a Spark exception was thrown from the driver. DRIVER_NOT_RESPONDING Indicates that the driver is up but is not responsive, likely due to GC. DBFS_DOWN Indicates that the driver is up but DBFS is down. METASTORE_DOWN Indicates that the driver is up but the metastore is down. NODE_BLACKLISTED Indicates that a node is not allowed by Spark. PINNED Indicates that the cluster was pinned. UNPINNED Indicates that the cluster was unpinned. EventDetails Details about a cluster event. Field Name Type Description current_num_workers INT32 The number of nodes in the cluster. target_num_workers INT32 The targeted number of nodes in the cluster. previous_attributes ClusterAttributes The cluster attributes before a cluster was edited. attributes ClusterAttributes * For created clusters, the attributes of the cluster. * For edited clusters, the new attributes of the cluster. previous_cluster_size ClusterSize The size of the cluster before an edit or resize. cluster_size ClusterSize The cluster size that was set in the cluster creation or edit. cause ResizeCause The cause of a change in target size. reason TerminationReason A termination reason: * On a TERMINATED event, the reason for the termination. * On a RESIZE_COMPLETE event, indicates the reason that we failed to acquire some nodes. user STRING The user that caused the event to occur. (Empty if it was done by Azure Databricks.) ClusterAttributes Common set of attributes set during cluster creation. These attributes cannot be changed over the lifetime of a cluster. Field Name Type Description cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster, for example “5.0.x-scala2.11”. You can retrieve a list of available runtime versions by using the Runtime versions API call. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. ssh_public_keys An array of STRING SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. Up to 10 keys can be specified. custom_tags ClusterTag An object containing a set of tags for cluster resources. Databricks tags all cluster resources with these tags in addition to default_tags. Note: * Tags are not supported on legacy node types such as compute-optimized and memory-optimized. * Databricks allows at most 45 custom tags. * If the cluster is created on an instance pool, the cluster tags are not copied to the cluster resources. To tag resources for an instance pool, see the custom_tags field in the Instance Pools API 2.0. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. enable_elastic_disk BOOL Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space. See Autoscaling local storage for details. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. cluster_source ClusterSource Determines whether the cluster was created by a user through the UI, created by the Databricks Jobs scheduler, or through an API request. policy_id STRING A cluster policy ID. azure_attributes AzureAttributes Defines attributes such as the instance availability type, node placement, and max bid price. If not specified during cluster creation, a set of default values is used. ClusterSize Cluster size specification. Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field is updated to reflect the target size of 10 workers, whereas the workers listed in executors gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. ListOrder Generic ordering enum for list-based queries. Order Description DESC Descending order. ASC Ascending order. ResizeCause Reason why a cluster was resized. Cause Description AUTOSCALE Automatically resized based on load. USER_REQUEST User requested a new size. AUTORECOVERY Autorecovery monitor resized the cluster after it lost a node. ClusterLogConf Path to cluster log. Field Name Type Description dbfs DbfsStorageInfo DBFS location of cluster log. Destination must be provided. For example, { ""dbfs"" : { ""destination"" : ""dbfs:/home/cluster_log"" } } InitScriptInfo Path to an init script. For instructions on using init scripts with Databricks Container Services, see Use an init script. Note The file storage type is only available for clusters set up using Databricks Container Services. Field Name Type Description dbfs OR file DbfsStorageInfo FileStorageInfo DBFS location of init script. Destination must be provided. For example, { ""dbfs"" : { ""destination"" : ""dbfs:/home/init_script"" } } File location of init script. Destination must be provided. For example, { ""file"" : { ""destination"" : ""file:/my/local/file.sh"" } } ClusterTag Cluster tag definition. Type Description STRING The key of the tag. The key must: * Be between 1 and 512 characters long * Not contain any of the characters <>%*&+?\\/ * Not begin with azure, microsoft, or windows STRING The value of the tag. The value length must be less than or equal to 256 UTF-8 characters. DbfsStorageInfo DBFS storage information. Field Name Type Description destination STRING DBFS destination. Example: dbfs:/my/path FileStorageInfo File storage information. Note This location type is only available for clusters set up using Databricks Container Services. Field Name Type Description destination STRING File destination. Example: file:/my/file.sh DockerImage Docker image connection information. Field Type Description url string URL for the Docker image. basic_auth DockerBasicAuth Basic authentication information for Docker repository. DockerBasicAuth Docker repository basic authentication information. Field Description username User name for the Docker repository. password Password for the Docker repository. LogSyncStatus Log delivery status. Field Name Type Description last_attempted INT64 The timestamp of last attempt. If the last attempt fails, last_exception contains the exception in the last attempt. last_exception STRING The exception thrown in the last attempt, it would be null (omitted in the response) if there is no exception in last attempted. NodeType Description of a Spark node type including both the dimensions of the node and the instance type on which it will be hosted. Field Name Type Description node_type_id STRING Unique identifier for this node type. This field is required. memory_mb INT32 Memory (in MB) available for this node type. This field is required. num_cores FLOAT Number of CPU cores available for this node type. This can be fractional if the number of cores on a machine instance is not divisible by the number of Spark nodes on that machine. This field is required. description STRING A string description associated with this node type. This field is required. instance_type_id STRING An identifier for the type of hardware that this node runs on. This field is required. is_deprecated BOOL Whether the node type is deprecated. Non-deprecated node types offer greater performance. node_info ClusterCloudProviderNodeInfo Node type info reported by the cloud provider. ClusterCloudProviderNodeInfo Information about an instance supplied by a cloud provider. Field Name Type Description status ClusterCloudProviderNodeStatus Status as reported by the cloud provider. available_core_quota INT32 Available CPU core quota. total_core_quota INT32 Total CPU core quota. ClusterCloudProviderNodeStatus Status of an instance supplied by a cloud provider. Status Description NotEnabledOnSubscription Node type not available for subscription. NotAvailableInRegion Node type not available in region. ParameterPair Parameter that provides additional information about why a cluster was terminated. Type Description TerminationParameter Type of termination information. STRING The termination information. SparkConfPair Spark configuration key-value pairs. Type Description STRING A configuration property name. STRING The configuration property value. SparkEnvPair Spark environment variable key-value pairs. Important When specifying environment variables in a job cluster, the fields in this data structure accept only Latin characters (ASCII character set). Using non-ASCII characters will return an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis. Type Description STRING An environment variable name. STRING The environment variable value. SparkNode Spark driver or executor configuration. Field Name Type Description private_ip STRING Private IP address (typically a 10.x.x.x address) of the Spark node. This is different from the private IP address of the host instance. public_dns STRING Public DNS address of this node. This address can be used to access the Spark JDBC server on the driver node. node_id STRING Globally unique identifier for this node. instance_id STRING Globally unique identifier for the host instance from the cloud provider. start_timestamp INT64 The timestamp (in millisecond) when the Spark node is launched. host_private_ip STRING The private IP address of the host instance. SparkVersion Databricks Runtime version of the cluster. Field Name Type Description key STRING Databricks Runtime version key, for example 7.3.x-scala2.12. The value that should be provided as the spark_version when creating a new cluster. The exact runtime version may change over time for a “wildcard” version (that is, 7.3.x-scala2.12 is a “wildcard” version) with minor bug fixes. name STRING A descriptive name for the runtime version, for example “Databricks Runtime 7.3 LTS”. TerminationReason Reason why a cluster was terminated. Field Name Type Description code TerminationCode Status code indicating why a cluster was terminated. type TerminationType Reason indicating why a cluster was terminated. parameters ParameterPair Object containing a set of parameters that provide information about why a cluster was terminated. PoolClusterTerminationCode Status code indicating why the cluster was terminated due to a pool failure. Code Description INSTANCE_POOL_MAX_CAPACITY_FAILURE The pool max capacity has been reached. INSTANCE_POOL_NOT_FOUND_FAILURE The pool specified by the cluster is no longer active or doesn’t exist. ClusterSource Service that created the cluster. Service Description UI Cluster created through the UI. JOB Cluster created by the Databricks job scheduler. API Cluster created through an API call. ClusterState State of a cluster. The allowable state transitions are as follows: PENDING -> RUNNING PENDING -> TERMINATING RUNNING -> RESIZING RUNNING -> RESTARTING RUNNING -> TERMINATING RESTARTING -> RUNNING RESTARTING -> TERMINATING RESIZING -> RUNNING RESIZING -> TERMINATING TERMINATING -> TERMINATED State Description PENDING Indicates that a cluster is in the process of being created. RUNNING Indicates that a cluster has been started and is ready for use. RESTARTING Indicates that a cluster is in the process of restarting. RESIZING Indicates that a cluster is in the process of adding or removing nodes. TERMINATING Indicates that a cluster is in the process of being destroyed. TERMINATED Indicates that a cluster has been successfully destroyed. ERROR This state is no longer used. It was used to indicate a cluster that failed to be created. TERMINATING and TERMINATED are used instead. UNKNOWN Indicates that a cluster is in an unknown state. A cluster should never be in this state. TerminationCode Status code indicating why the cluster was terminated. Code Description USER_REQUEST A user terminated the cluster directly. Parameters should include a username field that indicates the specific user who terminated the cluster. JOB_FINISHED The cluster was launched by a job, and terminated when the job completed. INACTIVITY The cluster was terminated since it was idle. CLOUD_PROVIDER_SHUTDOWN The instance that hosted the Spark driver was terminated by the cloud provider. COMMUNICATION_LOST Azure Databricks lost connection to services on the driver instance. For example, this can happen when problems arise in cloud networking infrastructure, or when the instance itself becomes unhealthy. CLOUD_PROVIDER_LAUNCH_FAILURE Azure Databricks experienced a cloud provider failure when requesting instances to launch clusters. SPARK_STARTUP_FAILURE The cluster failed to initialize. Possible reasons may include failure to create the environment for Spark or issues launching the Spark master and worker processes. INVALID_ARGUMENT Cannot launch the cluster because the user specified an invalid argument. For example, the user might specify an invalid runtime version for the cluster. UNEXPECTED_LAUNCH_FAILURE While launching this cluster, Azure Databricks failed to complete critical setup steps, terminating the cluster. INTERNAL_ERROR Azure Databricks encountered an unexpected error that forced the running cluster to be terminated. Contact Azure Databricks support for additional details. SPARK_ERROR The Spark driver failed to start. Possible reasons may include incompatible libraries and initialization scripts that corrupted the Spark container. METASTORE_COMPONENT_UNHEALTHY The cluster failed to start because the external metastore could not be reached. Refer to Troubleshooting. DBFS_COMPONENT_UNHEALTHY The cluster failed to start because Databricks File System (DBFS) could not be reached. AZURE_RESOURCE_PROVIDER_THROTTLING Azure Databricks reached the Azure Resource Provider request limit. Specifically, the API request rate to the specific resource type (compute, network, etc.) can’t exceed the limit. Retry might help to resolve the issue. For further information, seehttps://docs.microsoft.com/azure/virtual-machines/troubleshooting/troubleshooting-throttling-errors. AZURE_RESOURCE_MANAGER_THROTTLING Azure Databricks reached the Azure Resource Manager request limit which will prevent the Azure SDK from issuing any read or write request to the Azure Resource Manager. The request limit is applied to each subscription every hour. Retry after an hour or changing to a smaller cluster size might help to resolve the issue. For further information, seehttps://docs.microsoft.com/azure/azure-resource-manager/resource-manager-request-limits. NETWORK_CONFIGURATION_FAILURE The cluster was terminated due to an error in the network configuration. For example, a workspace with VNet injection had incorrect DNS settings that blocked access to worker artifacts. DRIVER_UNREACHABLE Azure Databricks was not able to access the Spark driver, because it was not reachable. DRIVER_UNRESPONSIVE Azure Databricks was not able to access the Spark driver, because it was unresponsive. INSTANCE_UNREACHABLE Azure Databricks was not able to access instances in order to start the cluster. This can be a transient networking issue. If the problem persists, this usually indicates a networking environment misconfiguration. CONTAINER_LAUNCH_FAILURE Azure Databricks was unable to launch containers on worker nodes for the cluster. Have your admin check your network configuration. INSTANCE_POOL_CLUSTER_FAILURE Pool backed cluster specific failure. See Pools for details. REQUEST_REJECTED Azure Databricks cannot handle the request at this moment. Try again later and contact Azure Databricks if the problem persists. INIT_SCRIPT_FAILURE Azure Databricks cannot load and run a cluster-scoped init script on one of the cluster’s nodes, or the init script terminates with a non-zero exit code. See Init script logs. TRIAL_EXPIRED The Azure Databricks trial subscription expired. BOOTSTRAP_TIMEOUT The cluster failed to start because of user network configuration issues. Possible reasons include misconfiguration of firewall settings, UDR entries, DNS, or route tables. TerminationType Reason why the cluster was terminated. Type Description SUCCESS Termination succeeded. CLIENT_ERROR Non-retriable. Client must fix parameters before reattempting the cluster creation. SERVICE_FAULT Azure Databricks service issue. Client can retry. CLOUD_FAILURE Cloud provider infrastructure issue. Client can retry after the underlying issue is resolved. TerminationParameter Key that provides additional information about why a cluster was terminated. Key Description username The username of the user who terminated the cluster. databricks_error_message Additional context that may explain the reason for cluster termination. inactivity_duration_min An idle cluster was shut down after being inactive for this duration. instance_id The ID of the instance that was hosting the Spark driver. azure_error_code The Azure provided error code describing why cluster nodes could not be provisioned. For reference, see: https://docs.microsoft.com/azure/virtual-machines/windows/error-messages. azure_error_message Human-readable context of various failures from Azure. This field is unstructured, and its exact format is subject to change. instance_pool_id The ID of the instance pool the cluster is using. instance_pool_error_code The error code for cluster failures specific to a pool. AzureAttributes Attributes set during cluster creation related to Azure. Field Name Type Description first_on_demand INT32 The first first_on_demand nodes of the cluster will be placed on on-demand instances. This value must be greater than 0, or else cluster creation validation fails. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. availability AzureAvailability Availability type used for all subsequent nodes past the first_on_demand ones. spot_bid_max_price DOUBLE The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to -1 (the default), which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance. You can view historical pricing and eviction rates in the Azure portal. AzureAvailability The Azure instance availability type behavior. Type Description SPOT_AZURE Use spot instances. ON_DEMAND_AZURE Use on-demand instances. SPOT_WITH_FALLBACK_AZURE Preferably use spot instances, but fall back to on-demand instances if spot instances cannot be acquired (for example, if Azure spot prices are too high or out of quota). Does not apply to pool availability.",Clusters API 2.0
44,ARR - Jobs timing out,"Run result unavailable: job failed with error message Unexpected failure while waiting for the cluster (0620-160006-xiafh9uu) to be ready.Cause Unexpected state for cluster (0620-160006-xiafh9uu): CONTAINER_LAUNCH_FAILURE(SERVICE_FAULT): instance_id:4476bf78f35a438bbf1ea3dece4fee53,databricks_error_message:Failed to launch spark container on instance 4476bf78f35a438bbf1ea3dece4fee53. Exception: Unexpected internal error, please contact Databricks support",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/clusters,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Clusters API 2.0 Article 07/14/2022 46 minutes to read 6 contributors In this article Create Edit Start Restart Resize Delete (terminate) Permanent delete Get Pin Unpin List List node types Runtime versions Events Data structures The Clusters API allows you to create, start, edit, list, terminate, and delete clusters. The maximum allowed size of a request to the Clusters API is 10MB. Cluster lifecycle methods require a cluster ID, which is returned from Create. To obtain a list of clusters, invoke List. Azure Databricks maps cluster node instance types to compute units known as DBUs. See the instance type pricing page for a list of the supported instance types and their corresponding DBUs. For instance provider information, see Azure instance type specifications and pricing. Azure Databricks always provides one year’s deprecation notice before ceasing support for an instance type. Important To access Databricks REST APIs, you must authenticate. Create Endpoint HTTP Method 2.0/clusters/create POST Create a new Apache Spark cluster. This method acquires new instances from the cloud provider if necessary. This method is asynchronous; the returned cluster_id can be used to poll the cluster state. When this method returns, the cluster is in a PENDING state. The cluster is usable once it enters a RUNNING state. See ClusterState. Note Azure Databricks may not be able to acquire some of the requested nodes, due to cloud provider limitations or transient network issues. If it is unable to acquire a sufficient number of the requested nodes, cluster creation will terminate with an informative error message. Examples Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy {
  ""cluster_name"": ""my-cluster"",
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2"",
  ""spark_conf"": {
    ""spark.speculation"": true
  },
  ""num_workers"": 25
}
 JSON Copy { ""cluster_id"": ""1234-567890-undid123"" }
 Here is an example for an autoscaling cluster. This cluster will start with two nodes, the minimum. Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy {
  ""cluster_name"": ""autoscaling-cluster"",
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2"",
  ""autoscale"" : {
    ""min_workers"": 2,
    ""max_workers"": 50
  }
}
 JSON Copy { ""cluster_id"": ""1234-567890-hared123"" }
 This example creates a Single Node cluster. To create a Single Node cluster: Set spark_conf and custom_tags to the exact values in the example. Set num_workers to 0. Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy {
  ""cluster_name"": ""single-node-cluster"",
  ""spark_version"": ""7.6.x-scala2.12"",
  ""node_type_id"": ""Standard_DS3_v2"",
  ""num_workers"": 0,
  ""spark_conf"": {
    ""spark.databricks.cluster.profile"": ""singleNode"",
    ""spark.master"": ""local[*]""
  },
  ""custom_tags"": {
    ""ResourceClass"": ""SingleNode""
  }
}
 JSON Copy { ""cluster_id"": ""1234-567890-pouch123"" }
 To create a job or submit a run with a new cluster using a policy, set policy_id to the policy ID: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/create \
--data @create-cluster.json
 create-cluster.json: JSON Copy 
{
    ""num_workers"": null,
    ""autoscale"": {
        ""min_workers"": 2,
        ""max_workers"": 8
    },
    ""cluster_name"": ""my-cluster"",
    ""spark_version"": ""7.3.x-scala2.12"",
    ""spark_conf"": {},
    ""node_type_id"": ""Standard_D3_v2"",
    ""custom_tags"": {},
    ""spark_env_vars"": {
        ""PYSPARK_PYTHON"": ""/databricks/python3/bin/python3""
    },
    ""autotermination_minutes"": 120,
    ""init_scripts"": [],
    ""policy_id"": ""C65B864F02000008""
}
 To create a new cluster, define the cluster’s properties in new_cluster: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/job/create \
--data @create-job.json
 create-job.json: JSON Copy {
  ""run_name"": ""my spark task"",
  ""new_cluster"": {
    ""spark_version"": ""7.3.x-scala2.12"",
    ""node_type_id"": ""Standard_D3_v2"",
    ""num_workers"": 10,
    ""policy_id"": ""ABCD000000000000""
  },
  ""libraries"": [
    {
      ""jar"": ""dbfs:/my-jar.jar""
    },
    {
      ""maven"": {
        ""coordinates"": ""org.jsoup:jsoup:1.7.2""
      }
    }
  ],
  ""spark_jar_task"": {
    ""main_class_name"": ""com.databricks.ComputeModels""
  }
}
 Request structure of the cluster definition Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. This field is required. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. This field is required. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. custom_tags ClusterTag An object containing a set of tags for cluster resources. Databricks tags all cluster resources (such as VMs) with these tags in addition to default_tags. Note: * Azure Databricks allows at most 43 custom tags. * If the cluster is created on an instance pool, the cluster tags are not copied to the cluster resources. To tag resources for an instance pool, see the custom_tags field in the Instance Pools API 2.0. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of scripts can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. driver_instance_pool_id STRING The ID of the instance pool to use for drivers. You must also specify instance_pool_id. Refer to Instance Pools API 2.0 for details. instance_pool_id STRING The optional ID of the instance pool to use for cluster nodes. If driver_instance_pool_id is present, instance_pool_id is used for worker nodes only. Otherwise, it is used for both the driver and the worker nodes. Refer to Instance Pools API 2.0 for details. idempotency_token STRING An optional token that can be used to guarantee the idempotency of cluster creation requests. If the idempotency token is assigned to a cluster that is not in the TERMINATED state, the request does not create a new cluster but instead returns the ID of the existing cluster. Otherwise, a new cluster is created. The idempotency token is cleared when the cluster is terminated If you specify the idempotency token, upon failure you can retry until the request succeeds. Azure Databricks will guarantee that exactly one cluster will be launched with that idempotency token. This token should have at most 64 characters. apply_policy_default_values BOOL Whether to use policy default values for missing cluster attributes. enable_local_disk_encryption BOOL Whether encryption of disks locally attached to the cluster is enabled. azure_attributes AzureAttributes Attributes related to clusters running on Azure. If not specified at cluster creation, a set of default values is used. runtime_engine STRING The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: * PHOTON: Use the Photon runtime engine type. * STANDARD: Use the standard runtime engine type. This field is optional. Response structure Field Name Type Description cluster_id STRING Canonical identifier for the cluster. Edit Endpoint HTTP Method 2.0/clusters/edit POST Edit the configuration of a cluster to match the provided attributes and size. You can edit a cluster if it is in a RUNNING or TERMINATED state. If you edit a cluster while it is in a RUNNING state, it will be restarted so that the new attributes can take effect. If you edit a cluster while it is in a TERMINATED state, it will remain TERMINATED. The next time it is started using the clusters/start API, the new attributes will take effect. An attempt to edit a cluster in any other state will be rejected with an INVALID_STATE error code. Clusters created by the Databricks Jobs service cannot be edited. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/edit \
--data @edit-cluster.json
 edit-cluster.json: JSON Copy {
  ""cluster_id"": ""1202-211320-brick1"",
  ""num_workers"": 10,
  ""spark_version"": ""7.3.x-scala2.12"",
  ""node_type_id"": ""Standard_D3_v2""
}
 JSON Copy {}
 Request structure Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING Canonical identifier for the cluster. This field is required. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. This field is required. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. This field is required. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default Databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. apply_policy_default_values BOOL Whether to use policy default values for missing cluster attributes. enable_local_disk_encryption BOOL Whether encryption of disks locally attached to the cluster is enabled. azure_attributes AzureAttributes Attributes related to clusters running on Azure. If not specified at cluster creation, a set of default values is used. runtime_engine STRING The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: * PHOTON: Use the Photon runtime engine type. * STANDARD: Use the standard runtime engine type. This field is optional. Start Endpoint HTTP Method 2.0/clusters/start POST Start a terminated cluster given its ID. This is similar to createCluster, except: The terminated cluster ID and attributes are preserved. The cluster starts with the last specified cluster size. If the terminated cluster is an autoscaling cluster, the cluster starts with the minimum number of nodes. If the cluster is in the RESTARTING state, a 400 error is returned. You cannot start a cluster launched to run a job. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/start \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be started. This field is required. Restart Endpoint HTTP Method 2.0/clusters/restart POST Restart a cluster given its ID. The cluster must be in the RUNNING state. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/restart \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be started. This field is required. Resize Endpoint HTTP Method 2.0/clusters/resize POST Resize a cluster to have a desired number of workers. The cluster must be in the RUNNING state. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/resize \
--data '{ ""cluster_id"": ""1234-567890-reef123"", ""num_workers"": 30 }'
 JSON Copy {}
 Request structure Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING The cluster to be resized. This field is required. Delete (terminate) Endpoint HTTP Method 2.0/clusters/delete POST Terminate a cluster given its ID. The cluster is removed asynchronously. Once the termination has completed, the cluster will be in the TERMINATED state. If the cluster is already in a TERMINATING or TERMINATED state, nothing will happen. Unless a cluster is pinned, 30 days after the cluster is terminated, it is permanently deleted. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/delete \
--data '{ ""cluster_id"": ""1234-567890-frays123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be terminated. This field is required. Permanent delete Endpoint HTTP Method 2.0/clusters/permanent-delete POST Permanently delete a cluster. If the cluster is running, it is terminated and its resources are asynchronously removed. If the cluster is terminated, then it is immediately removed. You cannot perform any action, including retrieve the cluster’s permissions, on a permanently deleted cluster. A permanently deleted cluster is also no longer returned in the cluster list. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/permanent-delete \
--data '{ ""cluster_id"": ""1234-567890-frays123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to be permanently deleted. This field is required. Get Endpoint HTTP Method 2.0/clusters/get GET Retrieve the information for a cluster given its identifier. Clusters can be described while they are running or up to 30 days after they are terminated. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/get \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }' \
| jq .
 JSON Copy {
  ""cluster_id"": ""1234-567890-reef123"",
  ""driver"": {
    ""node_id"": ""dced0ce388954c38abef081f54c18afd"",
    ""instance_id"": ""c69c0b119a2a499d8a2843c4d256136a"",
    ""start_timestamp"": 1619718438896,
    ""host_private_ip"": ""10.0.0.1"",
    ""private_ip"": ""10.0.0.2""
  },
  ""spark_context_id"": 5631707659504820000,
  ""jdbc_port"": 10000,
  ""cluster_name"": ""my-cluster"",
  ""spark_version"": ""8.2.x-scala2.12"",
  ""node_type_id"": ""Standard_L4s"",
  ""driver_node_type_id"": ""Standard_L4s"",
  ""custom_tags"": {
    ""ResourceClass"": ""SingleNode""
  },
  ""autotermination_minutes"": 0,
  ""enable_elastic_disk"": true,
  ""disk_spec"": {},
  ""cluster_source"": ""UI"",
  ""enable_local_disk_encryption"": false,
  ""azure_attributes"": {
    ""first_on_demand"": 1,
    ""availability"": ""ON_DEMAND_AZURE"",
    ""spot_bid_max_price"": -1
  },
  ""instance_source"": {
    ""node_type_id"": ""Standard_L4s""
  },
  ""driver_instance_source"": {
    ""node_type_id"": ""Standard_L4s""
  },
  ""state"": ""RUNNING"",
  ""state_message"": """",
  ""start_time"": 1610745129764,
  ""last_state_loss_time"": 1619718513513,
  ""num_workers"": 0,
  ""cluster_memory_mb"": 32768,
  ""cluster_cores"": 4,
  ""default_tags"": {
    ""Vendor"": ""Databricks"",
    ""Creator"": ""someone@example.com"",
    ""ClusterName"": ""my-cluster"",
    ""ClusterId"": ""1234-567890-reef123""
  },
  ""creator_user_name"": ""someone@example.com"",
  ""pinned_by_user_name"": ""3401478490056118"",
  ""init_scripts_safe_mode"": false
}
 Request structure Field Name Type Description cluster_id STRING The cluster about which to retrieve information. This field is required. Response structure Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING Canonical identifier for the cluster. This ID is retained during cluster restarts and resizes, while each new cluster has a globally unique ID. creator_user_name STRING Creator user name. The field won’t be included in the response if the user has already been deleted. driver SparkNode Node on which the Spark driver resides. The driver node contains the Spark master and the Databricks application that manages the per-notebook Spark REPLs. executors An array of SparkNode Nodes on which the Spark executors reside. spark_context_id INT64 A canonical SparkContext identifier. This value does change when the Spark driver restarts. The pair (cluster_id, spark_context_id) is a globally unique identifier over all Spark contexts. jdbc_port INT32 Port on which Spark JDBC server is listening in the driver node. No service will be listening on on this port in executor nodes. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. This field is required. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. custom_tags ClusterTag An object containing a set of tags for cluster resources. Databricks tags all cluster resources with these tags in addition to default_tags. Note: * Tags are not supported on legacy node types such as compute-optimized and memory-optimized. * Databricks allows at most 45 custom tags. * If the cluster is created on an instance pool, the cluster tags are not copied to the cluster resources. To tag resources for an instance pool, see the custom_tags field in the Instance Pools API 2.0. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default Databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. enable_elastic_disk BOOL Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space. See Autoscaling local storage for details. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. state ClusterState State of the cluster. state_message STRING A message associated with the most recent state transition (for example, the reason why the cluster entered the TERMINATED state). start_time INT64 Time (in epoch milliseconds) when the cluster creation request was received (when the cluster entered the PENDING state). terminated_time INT64 Time (in epoch milliseconds) when the cluster was terminated, if applicable. last_state_loss_time INT64 Time when the cluster driver last lost its state (due to a restart or driver failure). last_activity_time INT64 Time (in epoch milliseconds) when the cluster was last active. A cluster is active if there is at least one command that has not finished on the cluster. This field is available after the cluster has reached the RUNNING state. Updates to this field are made as best-effort attempts. Certain versions of Spark do not support reporting of cluster activity. Refer to Automatic termination for details. cluster_memory_mb INT64 Total amount of cluster memory, in megabytes. cluster_cores FLOAT Number of CPU cores available for this cluster. This can be fractional since certain node types are configured to share cores between Spark nodes on the same instance. default_tags ClusterTag An object containing a set of tags that are added by Azure Databricks regardless of any custom_tags, including: * Vendor: Databricks * Creator: * ClusterName: * ClusterId: * Name: On job clusters: * RunName: * JobId: On resources used by Databricks SQL: * SqlWarehouseId: cluster_log_status LogSyncStatus Cluster log delivery status. termination_reason TerminationReason Information about why the cluster was terminated. This field appears only when the cluster is in the TERMINATING or TERMINATED state. Pin Note You must be an Azure Databricks administrator to invoke this API. Endpoint HTTP Method 2.0/clusters/pin POST Ensure that an all-purpose cluster configuration is retained even after a cluster has been terminated for more than 30 days. Pinning ensures that the cluster is always returned by the List API. Pinning a cluster that is already pinned has no effect. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/pin \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to pin. This field is required. Unpin Note You must be an Azure Databricks administrator to invoke this API. Endpoint HTTP Method 2.0/clusters/unpin POST Allows the cluster to eventually be removed from the list returned by the List API. Unpinning a cluster that is not pinned has no effect. Example Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/unpin \
--data '{ ""cluster_id"": ""1234-567890-reef123"" }'
 JSON Copy {}
 Request structure Field Name Type Description cluster_id STRING The cluster to unpin. This field is required. List Endpoint HTTP Method 2.0/clusters/list GET Return information about all pinned clusters, active clusters, up to 200 of the most recently terminated all-purpose clusters in the past 30 days, and up to 30 of the most recently terminated job clusters in the past 30 days. For example, if there is 1 pinned cluster, 4 active clusters, 45 terminated all-purpose clusters in the past 30 days, and 50 terminated job clusters in the past 30 days, then this API returns the 1 pinned cluster, 4 active clusters, all 45 terminated all-purpose clusters, and the 30 most recently terminated job clusters. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list \
| jq .
 JSON Copy {
  ""clusters"": [
    {
      ""cluster_id"": ""1234-567890-reef123"",
      ""driver"": {
        ""node_id"": ""dced0ce388954c38abef081f54c18afd"",
        ""instance_id"": ""c69c0b119a2a499d8a2843c4d256136a"",
        ""start_timestamp"": 1619718438896,
        ""host_private_ip"": ""10.0.0.1"",
        ""private_ip"": ""10.0.0.2""
      },
      ""spark_context_id"": 5631707659504820000,
      ""jdbc_port"": 10000,
      ""cluster_name"": ""my-cluster"",
      ""spark_version"": ""8.2.x-scala2.12"",
      ""node_type_id"": ""Standard_L4s"",
      ""driver_node_type_id"": ""Standard_L4s"",
      ""custom_tags"": {
        ""ResourceClass"": ""SingleNode""
      },
      ""autotermination_minutes"": 0,
      ""enable_elastic_disk"": true,
      ""disk_spec"": {},
      ""cluster_source"": ""UI"",
      ""enable_local_disk_encryption"": false,
      ""azure_attributes"": {
        ""first_on_demand"": 1,
        ""availability"": ""ON_DEMAND_AZURE"",
        ""spot_bid_max_price"": -1
      },
      ""instance_source"": {
        ""node_type_id"": ""Standard_L4s""
      },
      ""driver_instance_source"": {
        ""node_type_id"": ""Standard_L4s""
      },
      ""state"": ""RUNNING"",
      ""state_message"": """",
      ""start_time"": 1610745129764,
      ""last_state_loss_time"": 1619718513513,
      ""num_workers"": 0,
      ""cluster_memory_mb"": 32768,
      ""cluster_cores"": 4,
      ""default_tags"": {
        ""Vendor"": ""Databricks"",
        ""Creator"": ""someone@example.com"",
        ""ClusterName"": ""my-cluster"",
        ""ClusterId"": ""1234-567890-reef123""
      },
      ""creator_user_name"": ""someone@example.com"",
      ""pinned_by_user_name"": ""3401478490056118"",
      ""init_scripts_safe_mode"": false
    },
    {
      ""...""
    }
  ]
}
 Response structure Field Name Type Description clusters An array of ClusterInfo A list of clusters. List node types Endpoint HTTP Method 2.0/clusters/list-node-types GET Return a list of supported Spark node types. These node types can be used to launch a cluster. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list-node-types \
| jq .
 JSON Copy {
  ""node_types"": [
    {
      ""node_type_id"": ""Standard_L80s_v2"",
      ""memory_mb"": 655360,
      ""num_cores"": 80,
      ""description"": ""Standard_L80s_v2"",
      ""instance_type_id"": ""Standard_L80s_v2"",
      ""is_deprecated"": false,
      ""category"": ""Storage Optimized"",
      ""support_ebs_volumes"": true,
      ""support_cluster_tags"": true,
      ""num_gpus"": 0,
      ""node_instance_type"": {
        ""instance_type_id"": ""Standard_L80s_v2"",
        ""local_disks"": 1,
        ""local_disk_size_gb"": 800,
        ""instance_family"": ""Standard LSv2 Family vCPUs"",
        ""local_nvme_disk_size_gb"": 1788,
        ""local_nvme_disks"": 10,
        ""swap_size"": ""10g""
      },
      ""is_hidden"": false,
      ""support_port_forwarding"": true,
      ""display_order"": 0,
      ""is_io_cache_enabled"": true,
      ""node_info"": {
        ""available_core_quota"": 350,
        ""total_core_quota"": 350
      }
    },
    {
      ""...""
    }
  ]
}
 Response structure Field Name Type Description node_types An array of NodeType The list of available Spark node types. Runtime versions Endpoint HTTP Method 2.0/clusters/spark-versions GET Return the list of available runtime versions. These versions can be used to launch a cluster. Example Bash Copy curl --netrc -X GET \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/spark-versions \
| jq .
 JSON Copy {
  ""versions"": [
    {
      ""key"": ""8.2.x-scala2.12"",
      ""name"": ""8.2 (includes Apache Spark 3.1.1, Scala 2.12)""
    },
    {
      ""...""
    }
  ]
}
 Response structure Field Name Type Description versions An array of SparkVersion All the available runtime versions. Events Endpoint HTTP Method 2.0/clusters/events POST Retrieve a list of events about the activity of a cluster. You can retrieve events from active clusters (running, pending, or reconfiguring) and terminated clusters within 30 days of their last termination. This API is paginated. If there are more events to read, the response includes all the parameters necessary to request the next page of events. Example: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/events \
--data @list-events.json \
| jq .
 list-events.json: JSON Copy {
  ""cluster_id"": ""1234-567890-reef123"",
  ""start_time"": 1617238800000,
  ""end_time"": 1619485200000,
  ""order"": ""DESC"",
  ""offset"": 5,
  ""limit"": 5,
  ""event_type"": ""RUNNING""
}
 JSON Copy {
  ""events"": [
    {
      ""cluster_id"": ""1234-567890-reef123"",
      ""timestamp"": 1619471498409,
      ""type"": ""RUNNING"",
      ""details"": {
        ""current_num_workers"": 2,
        ""target_num_workers"": 2
      }
    },
    {
      ""...""
    }
  ],
  ""next_page"": {
    ""cluster_id"": ""1234-567890-reef123"",
    ""start_time"": 1617238800000,
    ""end_time"": 1619485200000,
    ""order"": ""DESC"",
    ""offset"": 10,
    ""limit"": 5
  },
  ""total_count"": 25
}
 Example request to retrieve the next page of events: Bash Copy curl --netrc -X POST \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/events \
--data @list-events.json \
| jq .
 list-events.json: JSON Copy {
  ""cluster_id"": ""1234-567890-reef123"",
  ""start_time"": 1617238800000,
  ""end_time"": 1619485200000,
  ""order"": ""DESC"",
  ""offset"": 10,
  ""limit"": 5,
  ""event_type"": ""RUNNING""
}
 JSON Copy {
  ""events"": [
    {
      ""cluster_id"": ""1234-567890-reef123"",
      ""timestamp"": 1618330776302,
      ""type"": ""RUNNING"",
      ""details"": {
        ""current_num_workers"": 2,
        ""target_num_workers"": 2
      }
    },
    {
      ""...""
    }
  ],
  ""next_page"": {
    ""cluster_id"": ""1234-567890-reef123"",
    ""start_time"": 1617238800000,
    ""end_time"": 1619485200000,
    ""order"": ""DESC"",
    ""offset"": 15,
    ""limit"": 5
  },
  ""total_count"": 25
}
 Request structure Retrieve events pertaining to a specific cluster. Field Name Type Description cluster_id STRING The ID of the cluster to retrieve events about. This field is required. start_time INT64 The start time in epoch milliseconds. If empty, returns events starting from the beginning of time. end_time INT64 The end time in epoch milliseconds. If empty, returns events up to the current time. order ListOrder The order to list events in; either ASC or DESC. Defaults to DESC. event_types An array of ClusterEventType An optional set of event types to filter on. If empty, all event types are returned. offset INT64 The offset in the result set. Defaults to 0 (no offset). When an offset is specified and the results are requested in descending order, the end_time field is required. limit INT64 The maximum number of events to include in a page of events. Defaults to 50, and maximum allowed value is 500. Response structure Field Name Type Description events An array of ClusterEvent This list of matching events. next_page Request structure The parameters required to retrieve the next page of events. Omitted if there are no more events to read. total_count INT64 The total number of events filtered by the start_time, end_time, and event_types. Data structures In this section: AutoScale ClusterInfo ClusterEvent ClusterEventType EventDetails ClusterAttributes ClusterSize ListOrder ResizeCause ClusterLogConf InitScriptInfo ClusterTag DbfsStorageInfo FileStorageInfo DockerImage DockerBasicAuth LogSyncStatus NodeType ClusterCloudProviderNodeInfo ClusterCloudProviderNodeStatus ParameterPair SparkConfPair SparkEnvPair SparkNode SparkVersion TerminationReason PoolClusterTerminationCode ClusterSource ClusterState TerminationCode TerminationType TerminationParameter AzureAttributes AzureAvailability AutoScale Range defining the min and max number of cluster workers. Field Name Type Description min_workers INT32 The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation. max_workers INT32 The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers. ClusterInfo Metadata about a cluster. Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. Note: When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field will immediately be updated to reflect the target size of 10 workers, whereas the workers listed in executors will gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. cluster_id STRING Canonical identifier for the cluster. This ID is retained during cluster restarts and resizes, while each new cluster has a globally unique ID. creator_user_name STRING Creator user name. The field won’t be included in the response if the user has already been deleted. driver SparkNode Node on which the Spark driver resides. The driver node contains the Spark master and the Databricks application that manages the per-notebook Spark REPLs. executors An array of SparkNode Nodes on which the Spark executors reside. spark_context_id INT64 A canonical SparkContext identifier. This value does change when the Spark driver restarts. The pair (cluster_id, spark_context_id) is a globally unique identifier over all Spark contexts. jdbc_port INT32 Port on which Spark JDBC server is listening in the driver node. No service will be listening on on this port in executor nodes. cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster. You can retrieve a list of available runtime versions by using the Runtime versions API call. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads. A list of available node types can be retrieved by using the List node types API call. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. To specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. enable_elastic_disk BOOL Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space. See Autoscaling local storage for details. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. state ClusterState State of the cluster. state_message STRING A message associated with the most recent state transition (for example, the reason why the cluster entered a TERMINATED state). start_time INT64 Time (in epoch milliseconds) when the cluster creation request was received (when the cluster entered a PENDING state). terminated_time INT64 Time (in epoch milliseconds) when the cluster was terminated, if applicable. last_state_loss_time INT64 Time when the cluster driver last lost its state (due to a restart or driver failure). last_activity_time INT64 Time (in epoch milliseconds) when the cluster was last active. A cluster is active if there is at least one command that has not finished on the cluster. This field is available after the cluster has reached a RUNNING state. Updates to this field are made as best-effort attempts. Certain versions of Spark do not support reporting of cluster activity. Refer to Automatic termination for details. cluster_memory_mb INT64 Total amount of cluster memory, in megabytes. cluster_cores FLOAT Number of CPU cores available for this cluster. This can be fractional since certain node types are configured to share cores between Spark nodes on the same instance. default_tags ClusterTag An object containing a set of tags that are added by Azure Databricks regardless of any custom_tags, including: * Vendor: Databricks * Creator: * ClusterName: * ClusterId: * Name: On job clusters: * RunName: * JobId: On resources used by Databricks SQL: * SqlWarehouseId: cluster_log_status LogSyncStatus Cluster log delivery status. termination_reason TerminationReason Information about why the cluster was terminated. This field only appears when the cluster is in a TERMINATING or TERMINATED state. ClusterEvent Cluster event information. Field Name Type Description cluster_id STRING Canonical identifier for the cluster. This field is required. timestamp INT64 The timestamp when the event occurred, stored as the number of milliseconds since the unix epoch. Assigned by the Timeline service. type ClusterEventType The event type. This field is required. details EventDetails The event details. This field is required. ClusterEventType Type of a cluster event. Event Type Description CREATING Indicates that the cluster is being created. DID_NOT_EXPAND_DISK Indicates that a disk is low on space, but adding disks would put it over the max capacity. EXPANDED_DISK Indicates that a disk was low on space and the disks were expanded. FAILED_TO_EXPAND_DISK Indicates that a disk was low on space and disk space could not be expanded. INIT_SCRIPTS_STARTING Indicates that the cluster scoped init script has started. INIT_SCRIPTS_FINISHED Indicates that the cluster scoped init script has finished. STARTING Indicates that the cluster is being started. RESTARTING Indicates that the cluster is being started. TERMINATING Indicates that the cluster is being terminated. EDITED Indicates that the cluster has been edited. RUNNING Indicates the cluster has finished being created. Includes the number of nodes in the cluster and a failure reason if some nodes could not be acquired. RESIZING Indicates a change in the target size of the cluster (upsize or downsize). UPSIZE_COMPLETED Indicates that nodes finished being added to the cluster. Includes the number of nodes in the cluster and a failure reason if some nodes could not be acquired. NODES_LOST Indicates that some nodes were lost from the cluster. DRIVER_HEALTHY Indicates that the driver is healthy and the cluster is ready for use. DRIVER_UNAVAILABLE Indicates that the driver is unavailable. SPARK_EXCEPTION Indicates that a Spark exception was thrown from the driver. DRIVER_NOT_RESPONDING Indicates that the driver is up but is not responsive, likely due to GC. DBFS_DOWN Indicates that the driver is up but DBFS is down. METASTORE_DOWN Indicates that the driver is up but the metastore is down. NODE_BLACKLISTED Indicates that a node is not allowed by Spark. PINNED Indicates that the cluster was pinned. UNPINNED Indicates that the cluster was unpinned. EventDetails Details about a cluster event. Field Name Type Description current_num_workers INT32 The number of nodes in the cluster. target_num_workers INT32 The targeted number of nodes in the cluster. previous_attributes ClusterAttributes The cluster attributes before a cluster was edited. attributes ClusterAttributes * For created clusters, the attributes of the cluster. * For edited clusters, the new attributes of the cluster. previous_cluster_size ClusterSize The size of the cluster before an edit or resize. cluster_size ClusterSize The cluster size that was set in the cluster creation or edit. cause ResizeCause The cause of a change in target size. reason TerminationReason A termination reason: * On a TERMINATED event, the reason for the termination. * On a RESIZE_COMPLETE event, indicates the reason that we failed to acquire some nodes. user STRING The user that caused the event to occur. (Empty if it was done by Azure Databricks.) ClusterAttributes Common set of attributes set during cluster creation. These attributes cannot be changed over the lifetime of a cluster. Field Name Type Description cluster_name STRING Cluster name requested by the user. This doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string. spark_version STRING The runtime version of the cluster, for example “5.0.x-scala2.11”. You can retrieve a list of available runtime versions by using the Runtime versions API call. spark_conf SparkConfPair An object containing a set of optional, user-specified Spark configuration key-value pairs. You can also pass in a string of extra JVM options to the driver and the executors via spark.driver.extraJavaOptions and spark.executor.extraJavaOptions respectively. Example Spark confs: {""spark.speculation"": true, ""spark.streaming.ui.retainedBatches"": 5} or {""spark.driver.extraJavaOptions"": ""-verbose:gc -XX:+PrintGCDetails""} node_type_id STRING This field encodes, through a single value, the resources available to each of the Spark nodes in this cluster. For example, the Spark nodes can be provisioned and optimized for memory or compute intensive workloads A list of available node types can be retrieved by using the List node types API call. driver_node_type_id STRING The node type of the Spark driver. This field is optional; if unset, the driver node type will be set as the same value as node_type_id defined above. ssh_public_keys An array of STRING SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. Up to 10 keys can be specified. custom_tags ClusterTag An object containing a set of tags for cluster resources. Databricks tags all cluster resources with these tags in addition to default_tags. Note: * Tags are not supported on legacy node types such as compute-optimized and memory-optimized. * Databricks allows at most 45 custom tags. * If the cluster is created on an instance pool, the cluster tags are not copied to the cluster resources. To tag resources for an instance pool, see the custom_tags field in the Instance Pools API 2.0. cluster_log_conf ClusterLogConf The configuration for delivering Spark logs to a long-term storage destination. Only one destination can be specified for one cluster. If the conf is given, the logs will be delivered to the destination every 5 mins. The destination of driver logs is <destination>/<cluster-ID>/driver, while the destination of executor logs is <destination>/<cluster-ID>/executor. init_scripts An array of InitScriptInfo The configuration for storing init scripts. Any number of destinations can be specified. The scripts are executed sequentially in the order provided. If cluster_log_conf is specified, init script logs are sent to <destination>/<cluster-ID>/init_scripts. docker_image DockerImage Docker image for a custom container. spark_env_vars SparkEnvPair An object containing a set of optional, user-specified environment variable key-value pairs. Key-value pairs of the form (X,Y) are exported as is (that is, export X='Y') while launching the driver and workers. In order to specify an additional set of SPARK_DAEMON_JAVA_OPTS, we recommend appending them to $SPARK_DAEMON_JAVA_OPTS as shown in the following example. This ensures that all default databricks managed environmental variables are included as well. Example Spark environment variables: {""SPARK_WORKER_MEMORY"": ""28000m"", ""SPARK_LOCAL_DIRS"": ""/local_disk0""} or {""SPARK_DAEMON_JAVA_OPTS"": ""$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true""} autotermination_minutes INT32 Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. enable_elastic_disk BOOL Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk space when its Spark workers are running low on disk space. See Autoscaling local storage for details. instance_pool_id STRING The optional ID of the instance pool to which the cluster belongs. Refer to Pools for details. cluster_source ClusterSource Determines whether the cluster was created by a user through the UI, created by the Databricks Jobs scheduler, or through an API request. policy_id STRING A cluster policy ID. azure_attributes AzureAttributes Defines attributes such as the instance availability type, node placement, and max bid price. If not specified during cluster creation, a set of default values is used. ClusterSize Cluster size specification. Field Name Type Description num_workers OR autoscale INT32 OR AutoScale If num_workers, number of worker nodes that this cluster should have. A cluster has one Spark driver and num_workers executors for a total of num_workers + 1 Spark nodes. When reading the properties of a cluster, this field reflects the desired number of workers rather than the actual number of workers. For instance, if a cluster is resized from 5 to 10 workers, this field is updated to reflect the target size of 10 workers, whereas the workers listed in executors gradually increase from 5 to 10 as the new nodes are provisioned. If autoscale, parameters needed in order to automatically scale clusters up and down based on load. ListOrder Generic ordering enum for list-based queries. Order Description DESC Descending order. ASC Ascending order. ResizeCause Reason why a cluster was resized. Cause Description AUTOSCALE Automatically resized based on load. USER_REQUEST User requested a new size. AUTORECOVERY Autorecovery monitor resized the cluster after it lost a node. ClusterLogConf Path to cluster log. Field Name Type Description dbfs DbfsStorageInfo DBFS location of cluster log. Destination must be provided. For example, { ""dbfs"" : { ""destination"" : ""dbfs:/home/cluster_log"" } } InitScriptInfo Path to an init script. For instructions on using init scripts with Databricks Container Services, see Use an init script. Note The file storage type is only available for clusters set up using Databricks Container Services. Field Name Type Description dbfs OR file DbfsStorageInfo FileStorageInfo DBFS location of init script. Destination must be provided. For example, { ""dbfs"" : { ""destination"" : ""dbfs:/home/init_script"" } } File location of init script. Destination must be provided. For example, { ""file"" : { ""destination"" : ""file:/my/local/file.sh"" } } ClusterTag Cluster tag definition. Type Description STRING The key of the tag. The key must: * Be between 1 and 512 characters long * Not contain any of the characters <>%*&+?\\/ * Not begin with azure, microsoft, or windows STRING The value of the tag. The value length must be less than or equal to 256 UTF-8 characters. DbfsStorageInfo DBFS storage information. Field Name Type Description destination STRING DBFS destination. Example: dbfs:/my/path FileStorageInfo File storage information. Note This location type is only available for clusters set up using Databricks Container Services. Field Name Type Description destination STRING File destination. Example: file:/my/file.sh DockerImage Docker image connection information. Field Type Description url string URL for the Docker image. basic_auth DockerBasicAuth Basic authentication information for Docker repository. DockerBasicAuth Docker repository basic authentication information. Field Description username User name for the Docker repository. password Password for the Docker repository. LogSyncStatus Log delivery status. Field Name Type Description last_attempted INT64 The timestamp of last attempt. If the last attempt fails, last_exception contains the exception in the last attempt. last_exception STRING The exception thrown in the last attempt, it would be null (omitted in the response) if there is no exception in last attempted. NodeType Description of a Spark node type including both the dimensions of the node and the instance type on which it will be hosted. Field Name Type Description node_type_id STRING Unique identifier for this node type. This field is required. memory_mb INT32 Memory (in MB) available for this node type. This field is required. num_cores FLOAT Number of CPU cores available for this node type. This can be fractional if the number of cores on a machine instance is not divisible by the number of Spark nodes on that machine. This field is required. description STRING A string description associated with this node type. This field is required. instance_type_id STRING An identifier for the type of hardware that this node runs on. This field is required. is_deprecated BOOL Whether the node type is deprecated. Non-deprecated node types offer greater performance. node_info ClusterCloudProviderNodeInfo Node type info reported by the cloud provider. ClusterCloudProviderNodeInfo Information about an instance supplied by a cloud provider. Field Name Type Description status ClusterCloudProviderNodeStatus Status as reported by the cloud provider. available_core_quota INT32 Available CPU core quota. total_core_quota INT32 Total CPU core quota. ClusterCloudProviderNodeStatus Status of an instance supplied by a cloud provider. Status Description NotEnabledOnSubscription Node type not available for subscription. NotAvailableInRegion Node type not available in region. ParameterPair Parameter that provides additional information about why a cluster was terminated. Type Description TerminationParameter Type of termination information. STRING The termination information. SparkConfPair Spark configuration key-value pairs. Type Description STRING A configuration property name. STRING The configuration property value. SparkEnvPair Spark environment variable key-value pairs. Important When specifying environment variables in a job cluster, the fields in this data structure accept only Latin characters (ASCII character set). Using non-ASCII characters will return an error. Examples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis. Type Description STRING An environment variable name. STRING The environment variable value. SparkNode Spark driver or executor configuration. Field Name Type Description private_ip STRING Private IP address (typically a 10.x.x.x address) of the Spark node. This is different from the private IP address of the host instance. public_dns STRING Public DNS address of this node. This address can be used to access the Spark JDBC server on the driver node. node_id STRING Globally unique identifier for this node. instance_id STRING Globally unique identifier for the host instance from the cloud provider. start_timestamp INT64 The timestamp (in millisecond) when the Spark node is launched. host_private_ip STRING The private IP address of the host instance. SparkVersion Databricks Runtime version of the cluster. Field Name Type Description key STRING Databricks Runtime version key, for example 7.3.x-scala2.12. The value that should be provided as the spark_version when creating a new cluster. The exact runtime version may change over time for a “wildcard” version (that is, 7.3.x-scala2.12 is a “wildcard” version) with minor bug fixes. name STRING A descriptive name for the runtime version, for example “Databricks Runtime 7.3 LTS”. TerminationReason Reason why a cluster was terminated. Field Name Type Description code TerminationCode Status code indicating why a cluster was terminated. type TerminationType Reason indicating why a cluster was terminated. parameters ParameterPair Object containing a set of parameters that provide information about why a cluster was terminated. PoolClusterTerminationCode Status code indicating why the cluster was terminated due to a pool failure. Code Description INSTANCE_POOL_MAX_CAPACITY_FAILURE The pool max capacity has been reached. INSTANCE_POOL_NOT_FOUND_FAILURE The pool specified by the cluster is no longer active or doesn’t exist. ClusterSource Service that created the cluster. Service Description UI Cluster created through the UI. JOB Cluster created by the Databricks job scheduler. API Cluster created through an API call. ClusterState State of a cluster. The allowable state transitions are as follows: PENDING -> RUNNING PENDING -> TERMINATING RUNNING -> RESIZING RUNNING -> RESTARTING RUNNING -> TERMINATING RESTARTING -> RUNNING RESTARTING -> TERMINATING RESIZING -> RUNNING RESIZING -> TERMINATING TERMINATING -> TERMINATED State Description PENDING Indicates that a cluster is in the process of being created. RUNNING Indicates that a cluster has been started and is ready for use. RESTARTING Indicates that a cluster is in the process of restarting. RESIZING Indicates that a cluster is in the process of adding or removing nodes. TERMINATING Indicates that a cluster is in the process of being destroyed. TERMINATED Indicates that a cluster has been successfully destroyed. ERROR This state is no longer used. It was used to indicate a cluster that failed to be created. TERMINATING and TERMINATED are used instead. UNKNOWN Indicates that a cluster is in an unknown state. A cluster should never be in this state. TerminationCode Status code indicating why the cluster was terminated. Code Description USER_REQUEST A user terminated the cluster directly. Parameters should include a username field that indicates the specific user who terminated the cluster. JOB_FINISHED The cluster was launched by a job, and terminated when the job completed. INACTIVITY The cluster was terminated since it was idle. CLOUD_PROVIDER_SHUTDOWN The instance that hosted the Spark driver was terminated by the cloud provider. COMMUNICATION_LOST Azure Databricks lost connection to services on the driver instance. For example, this can happen when problems arise in cloud networking infrastructure, or when the instance itself becomes unhealthy. CLOUD_PROVIDER_LAUNCH_FAILURE Azure Databricks experienced a cloud provider failure when requesting instances to launch clusters. SPARK_STARTUP_FAILURE The cluster failed to initialize. Possible reasons may include failure to create the environment for Spark or issues launching the Spark master and worker processes. INVALID_ARGUMENT Cannot launch the cluster because the user specified an invalid argument. For example, the user might specify an invalid runtime version for the cluster. UNEXPECTED_LAUNCH_FAILURE While launching this cluster, Azure Databricks failed to complete critical setup steps, terminating the cluster. INTERNAL_ERROR Azure Databricks encountered an unexpected error that forced the running cluster to be terminated. Contact Azure Databricks support for additional details. SPARK_ERROR The Spark driver failed to start. Possible reasons may include incompatible libraries and initialization scripts that corrupted the Spark container. METASTORE_COMPONENT_UNHEALTHY The cluster failed to start because the external metastore could not be reached. Refer to Troubleshooting. DBFS_COMPONENT_UNHEALTHY The cluster failed to start because Databricks File System (DBFS) could not be reached. AZURE_RESOURCE_PROVIDER_THROTTLING Azure Databricks reached the Azure Resource Provider request limit. Specifically, the API request rate to the specific resource type (compute, network, etc.) can’t exceed the limit. Retry might help to resolve the issue. For further information, seehttps://docs.microsoft.com/azure/virtual-machines/troubleshooting/troubleshooting-throttling-errors. AZURE_RESOURCE_MANAGER_THROTTLING Azure Databricks reached the Azure Resource Manager request limit which will prevent the Azure SDK from issuing any read or write request to the Azure Resource Manager. The request limit is applied to each subscription every hour. Retry after an hour or changing to a smaller cluster size might help to resolve the issue. For further information, seehttps://docs.microsoft.com/azure/azure-resource-manager/resource-manager-request-limits. NETWORK_CONFIGURATION_FAILURE The cluster was terminated due to an error in the network configuration. For example, a workspace with VNet injection had incorrect DNS settings that blocked access to worker artifacts. DRIVER_UNREACHABLE Azure Databricks was not able to access the Spark driver, because it was not reachable. DRIVER_UNRESPONSIVE Azure Databricks was not able to access the Spark driver, because it was unresponsive. INSTANCE_UNREACHABLE Azure Databricks was not able to access instances in order to start the cluster. This can be a transient networking issue. If the problem persists, this usually indicates a networking environment misconfiguration. CONTAINER_LAUNCH_FAILURE Azure Databricks was unable to launch containers on worker nodes for the cluster. Have your admin check your network configuration. INSTANCE_POOL_CLUSTER_FAILURE Pool backed cluster specific failure. See Pools for details. REQUEST_REJECTED Azure Databricks cannot handle the request at this moment. Try again later and contact Azure Databricks if the problem persists. INIT_SCRIPT_FAILURE Azure Databricks cannot load and run a cluster-scoped init script on one of the cluster’s nodes, or the init script terminates with a non-zero exit code. See Init script logs. TRIAL_EXPIRED The Azure Databricks trial subscription expired. BOOTSTRAP_TIMEOUT The cluster failed to start because of user network configuration issues. Possible reasons include misconfiguration of firewall settings, UDR entries, DNS, or route tables. TerminationType Reason why the cluster was terminated. Type Description SUCCESS Termination succeeded. CLIENT_ERROR Non-retriable. Client must fix parameters before reattempting the cluster creation. SERVICE_FAULT Azure Databricks service issue. Client can retry. CLOUD_FAILURE Cloud provider infrastructure issue. Client can retry after the underlying issue is resolved. TerminationParameter Key that provides additional information about why a cluster was terminated. Key Description username The username of the user who terminated the cluster. databricks_error_message Additional context that may explain the reason for cluster termination. inactivity_duration_min An idle cluster was shut down after being inactive for this duration. instance_id The ID of the instance that was hosting the Spark driver. azure_error_code The Azure provided error code describing why cluster nodes could not be provisioned. For reference, see: https://docs.microsoft.com/azure/virtual-machines/windows/error-messages. azure_error_message Human-readable context of various failures from Azure. This field is unstructured, and its exact format is subject to change. instance_pool_id The ID of the instance pool the cluster is using. instance_pool_error_code The error code for cluster failures specific to a pool. AzureAttributes Attributes set during cluster creation related to Azure. Field Name Type Description first_on_demand INT32 The first first_on_demand nodes of the cluster will be placed on on-demand instances. This value must be greater than 0, or else cluster creation validation fails. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, first_on_demand nodes will be placed on on-demand instances and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. availability AzureAvailability Availability type used for all subsequent nodes past the first_on_demand ones. spot_bid_max_price DOUBLE The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to -1 (the default), which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance. You can view historical pricing and eviction rates in the Azure portal. AzureAvailability The Azure instance availability type behavior. Type Description SPOT_AZURE Use spot instances. ON_DEMAND_AZURE Use on-demand instances. SPOT_WITH_FALLBACK_AZURE Preferably use spot instances, but fall back to on-demand instances if spot instances cannot be acquired (for example, if Azure spot prices are too high or out of quota). Does not apply to pool availability.",Clusters API 2.0
45,Billing: Need help to understand Untagged cluster costs,"Issue context: I have a customer that would like to know what jobs/clusters are making up their costs so far this month. Particularly, they would like to know the jobs/clusters that are making up for their untagged costs. 

On Jun 4, customer accumulated $48.74 in costs and would like to know what jobs/clusters makes up for that cost (see screenshot attached).

Workspace details
workspaceID: 4591240762141326
Region: eastus2
Azure SubscriptionID: d72fadc4-9df6-4271-bf73-c5edb2d0bd50
VNET Injected Workspace: True

 ASK: Are you able to see from your side what jobs/clusters that are making up for their untagged costs in the month of June and the users that trigger those jobs?",https://docs.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/usage-detail-tags-azure,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Monitor usage using cluster, pool, and workspace tags Article 06/30/2022 2 minutes to read 4 contributors In this article Tagged objects and resources Tag propagation To monitor cost and accurately attribute Azure Databricks usage to your organization’s business units and teams (for chargebacks, for example), you can tag workspaces (resource groups), clusters, and pools. These tags propagate to detailed cost analysis reports that you can access in the Azure portal. For example, here is a cost analysis invoice details report in the Azure portal that details cost by clusterid tag over a one-month period: Tagged objects and resources You can add custom tags for the following objects managed by Azure Databricks: Object Tagging interface (UI) Tagging interface (API) Workspace Azure Portal Azure Resources API Pool Pool UI in the Azure Databricks workspace Instance Pool API Cluster Cluster UI in the Azure Databricks workspace Clusters API Azure Databricks adds the following default tags to all pools and clusters: Pool tag key name Value Vendor Constant “Databricks” DatabricksInstancePoolCreatorId Azure Databricks internal identifier of the user who created the pool DatabricksInstancePoolId Azure Databricks internal identifier of the pool Cluster tag key name Value Vendor Constant “Databricks” ClusterId Azure Databricks internal identifier of the cluster ClusterName Name of the cluster Creator Username (email address) of the user who created the cluster On job clusters, Azure Databricks also applies the following default tags: Cluster tag key name Value RunName Job name JobId Job ID On resources used by Databricks SQL, Azure Databricks also applies the following default tag: Cluster tag key name Value SqlWarehouseId Azure Databricks internal identifier of the SQL warehouse Tag propagation Workspace, pool, and cluster tags are aggregated by Azure Databricks and propagated to Azure VMs for cost analysis reporting. But pool and cluster tags are propagated differently from each other. Workspace and pool tags are aggregated and assigned as resource tags of the Azure VMs that host the pools. Workspace and cluster tags are aggregated and assigned as resource tags of the Azure VMs that host the clusters. When clusters are created from pools, only workspace tags and pool tags are propagated to the VMs. Cluster tags are not propagated, in order to preserve pool cluster startup performance. Tag conflict resolution If a custom cluster tag, pool tag, or workspace tag has the same name as a Azure Databricks default cluster or pool tag, the custom tag is prefixed with an x_ when it is propagated. For example, if a workspace is tagged with vendor = Azure Databricks, that tag will conflict with the default cluster tag vendor = Databricks. The tags will therefore be propagated as x_vendor = Azure Databricks and vendor = Databricks. Limitations It can take up to one hour for custom workspace tags to propagate to Azure Databricks after any change. No more than 50 tags can be assigned to an Azure resource. If the overall count of aggregated tags exceeds this limit, x_-prefixed tags are evaluated in alphabetical order and those that exceed the limit are ignored. If all x_-prefixed tags are ignored and the count is till over the limit, the remaining tags are evaluated in alphabetical order and those that exceed the limit are ignored. Tag keys and values can contain only characters from the ISO 8859-1 (latin1) set. Tags containing other characters are ignored. If you change tag key names or values, these changes apply only after cluster restart or pool expansion.","Monitor usage using cluster, pool, and workspace tags"
46,Daily DBU usage on Azure Databricks,"Cx shared a screenshot(attached) from AWS Databricks service that they can find daily detail usage report and asked if there is a view to monitor daily DBU usage on each workspace in Azure as well. I think Azure doesn't provide a daily usage report on DBU.

The background of asking this question is that customer is using more than 1000 clusters and they are planning to purchase the resource in DBCU (Databricks Commit Unit) in advance, not paying after using the resource. Therefore, they would like to understand the current DBU usage, so that they can estimate how much they need to purchase. I think we can calculate the DBU by checking each DBU/hour from the cluster and multiplying the used time, but since there are too many clusters, it's not an easy situation. Can you help identify the daily dbu usage on customers' every workspace and cluster?


FYI. customer can check the daily cost(billing) usage by referencing the following link, but no DBU usage view is provided.
https://docs.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/usage-detail-tags-azure",https://docs.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/usage-detail-tags-azure,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Monitor usage using cluster, pool, and workspace tags Article 06/30/2022 2 minutes to read 4 contributors In this article Tagged objects and resources Tag propagation To monitor cost and accurately attribute Azure Databricks usage to your organization’s business units and teams (for chargebacks, for example), you can tag workspaces (resource groups), clusters, and pools. These tags propagate to detailed cost analysis reports that you can access in the Azure portal. For example, here is a cost analysis invoice details report in the Azure portal that details cost by clusterid tag over a one-month period: Tagged objects and resources You can add custom tags for the following objects managed by Azure Databricks: Object Tagging interface (UI) Tagging interface (API) Workspace Azure Portal Azure Resources API Pool Pool UI in the Azure Databricks workspace Instance Pool API Cluster Cluster UI in the Azure Databricks workspace Clusters API Azure Databricks adds the following default tags to all pools and clusters: Pool tag key name Value Vendor Constant “Databricks” DatabricksInstancePoolCreatorId Azure Databricks internal identifier of the user who created the pool DatabricksInstancePoolId Azure Databricks internal identifier of the pool Cluster tag key name Value Vendor Constant “Databricks” ClusterId Azure Databricks internal identifier of the cluster ClusterName Name of the cluster Creator Username (email address) of the user who created the cluster On job clusters, Azure Databricks also applies the following default tags: Cluster tag key name Value RunName Job name JobId Job ID On resources used by Databricks SQL, Azure Databricks also applies the following default tag: Cluster tag key name Value SqlWarehouseId Azure Databricks internal identifier of the SQL warehouse Tag propagation Workspace, pool, and cluster tags are aggregated by Azure Databricks and propagated to Azure VMs for cost analysis reporting. But pool and cluster tags are propagated differently from each other. Workspace and pool tags are aggregated and assigned as resource tags of the Azure VMs that host the pools. Workspace and cluster tags are aggregated and assigned as resource tags of the Azure VMs that host the clusters. When clusters are created from pools, only workspace tags and pool tags are propagated to the VMs. Cluster tags are not propagated, in order to preserve pool cluster startup performance. Tag conflict resolution If a custom cluster tag, pool tag, or workspace tag has the same name as a Azure Databricks default cluster or pool tag, the custom tag is prefixed with an x_ when it is propagated. For example, if a workspace is tagged with vendor = Azure Databricks, that tag will conflict with the default cluster tag vendor = Databricks. The tags will therefore be propagated as x_vendor = Azure Databricks and vendor = Databricks. Limitations It can take up to one hour for custom workspace tags to propagate to Azure Databricks after any change. No more than 50 tags can be assigned to an Azure resource. If the overall count of aggregated tags exceeds this limit, x_-prefixed tags are evaluated in alphabetical order and those that exceed the limit are ignored. If all x_-prefixed tags are ignored and the count is till over the limit, the remaining tags are evaluated in alphabetical order and those that exceed the limit are ignored. Tag keys and values can contain only characters from the ISO 8859-1 (latin1) set. Tags containing other characters are ignored. If you change tag key names or values, these changes apply only after cluster restart or pool expansion.","Monitor usage using cluster, pool, and workspace tags"
47,Databricks Delta - Excessive file accumulation -2206130010002322,"I have been requested to open a case seeking guidance on
how to address an excessive number of files related to our Databricks Delta
database environment.  In short, we neglected to implement a maintenance plan
that would remove files that are not managed by Delta, or no longer in the
latest state of the transaction log and are older than a retention threshold.
 We have no need for 'time travel' functionality and simply wish to maintain
the current version of records.



The cumulative size of the files is ~220 TB.  We are concerned that if we were
to simply issue a 'Vacuum' command against all delta tables, the process may
exceed an acceptable outage window and/or cause additional unknown problems.



As stated, we would like guidance on establishing a course of action that would
address the file growth issue and alleviate concerns with respect to downtime.



Problem start date and time
Thu, Jun 9, 2022, 12:00 AM (UTC-06:00) Central Time (US & Canada)


<Start:Agent_Additional_Properties_Do_Not_Edit>
- ProblemStartTime: 06/09/2022 05:00:00
- Cloud: Azure
- AzureProductSubscriptionID: 661fdd96-f214-4915-8380-8a67ae74924c
- AzureProductSubscriptionName: Amadeus Hospitality - Customer Facing
- PUID: 1003200142C819B4
- Tenant Id: 40e336d2-01d8-4231-bea7-1876d1513843
- Object Id: 
- SubscriptionType: UnifiedEnterprise
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Unified Enterprise
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: False

- Location: eastus2
- ResourceUri: /subscriptions/661fdd96-f214-4915-8380-8a67ae74924c/resourceGroups/prod0hidpuse02resgrp002/providers/Microsoft.Storage/storageAccounts/prod0hidpuse02stgact010

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/delta-vacuum,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents VACUUM Article 05/19/2022 2 minutes to read 5 contributors In this article Vacuum a Delta table (Delta Lake on Azure Databricks) Vacuum a Spark table (Apache Spark) Remove unused files from a table directory. Note This command works differently depending on whether you’re working on a Delta or Apache Spark table. Vacuum a Delta table (Delta Lake on Azure Databricks) Recursively vacuum directories associated with the Delta table. VACUUM removes all files from the table directory that are not managed by Delta, as well as data files that are no longer in the latest state of the transaction log for the table and are older than a retention threshold. VACUUM will skip all directories that begin with an underscore (_), which includes the _delta_log. Partitioning your table on a column that begins with an underscore is an exception to this rule; VACUUM scans all valid partitions included in the target Delta table. Delta table data files are deleted according to the time they have been logically removed from Delta’s transaction log plus retention hours, not their modification timestamps on the storage system. The default threshold is 7 days. On Delta tables, Azure Databricks does not automatically trigger VACUUM operations. See Remove files no longer referenced by a Delta table. If you run VACUUM on a Delta table, you lose the ability to time travel back to a version older than the specified data retention period. Warning It is recommended that you set a retention interval to be at least 7 days, because old snapshots and uncommitted files can still be in use by concurrent readers or writers to the table. If VACUUM cleans up active files, concurrent readers can fail or, worse, tables can be corrupted when VACUUM deletes files that have not yet been committed. You must choose an interval that is longer than the longest running concurrent transaction and the longest period that any stream can lag behind the most recent update to the table. Delta Lake has a safety check to prevent you from running a dangerous VACUUM command. If you are certain that there are no operations being performed on this table that take longer than the retention interval you plan to specify, you can turn off this safety check by setting the Spark configuration property spark.databricks.delta.retentionDurationCheck.enabled to false. Copy VACUUM table_name [RETAIN num HOURS] [DRY RUN]
 Parameters table_name Identifies an existing Delta table. The name must not include a temporal specification. RETAIN num HOURS The retention threshold. DRY RUN Return a list of files to be deleted. Vacuum a Spark table (Apache Spark) Recursively vacuums directories associated with the Spark table and remove uncommitted files older than a retention threshold. The default threshold is 7 days. On Spark tables, Azure Databricks automatically triggers VACUUM operations as data is written. See Clean up uncommitted files. Syntax Copy VACUUM table_name [RETAIN num HOURS]
 Parameters table_name Identifies an existing table by name or path. RETAIN num HOURS The retention threshold.",VACUUM
48,Unable to mount new containers / access exixting mount points,"Earlier customer was able to list all the mount points , But from last Monday onwards cx is unable to list the mount points which already exists. They are getting the below error:
	
	HTTP Error 401; url=&#39;https://login.microsoftonline.com/46d4eb5d-8290-43d3-bf6f-e6a48a2627ae/oauth2/token&#39; AADToken: HTTP connection to https://login.microsoftonline.com/46d4eb5d-8290-43d3-bf6f-e6a48a2627ae/oauth2/token failed for getting token from AzureAD.; requestId=&#39;dea68559-d3c8-4656-9af2-5d7b2b2f1500&#39;; contentType=&#39;application/json; charset=utf-8&#39;; response &#39;{&#34;error&#34;:&#34;invalid_client&#34;,&#34;error_description&#34;:&#34;AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app 
	 
	
CX is facing this issue in a workspace where unity catalogue feature is enabled 
	
CX is able to list all the mountpoints present in the workspace in which the unity catalogue feature is not enabled
	
CX was unable to create new mount points as well.
	
Using wasbs cx is not able to create mount points, But using abfss they are able to create new mount points


Queries CX have:


	
why it is not possible to list the mountpoints present?
	
On a cluster, in the cluster configuration only SQL commands can be used?
	
Why CX is not able to create new mount points.
	
Why using wasbs it is not possible to create mount points and why it is possible using abfss, what is the difference?
	
Could you please help me on this",https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/get-started,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Get started using Unity Catalog Article 07/12/2022 10 minutes to read 5 contributors In this article Requirements Configure and grant access to Azure storage for your metastore Create your first metastore and attach a workspace Add users and groups Create a compute resource Create your first table (Optional) Link the metastore to additional workspaces. (Optional) Unlink the metastore from workspaces. Example notebook (Recommended) Transfer ownership of your metastore to a group (Optional) Install the Unity Catalog CLI Next steps Important Unity Catalog is in Public Preview. To participate in the preview, contact your Azure Databricks representative. This guide helps you get started with Unity Catalog, the Azure Databricks data governance framework. Requirements You must be an Azure Databricks account admin. The first Azure Databricks account admin must be an Azure Active Directory Global Administrator or a member of the root management group, which is usually named Tenant root group. That user can assign users with any level of Azure tenant permission as subsequent Azure Databricks account admins (who can themselves assign more account admins). Your Azure Databricks account must be on the Premium plan. In your Azure tenant, you must have permission to create: A storage account to use with Azure Data Lake Storage Gen2. See Create a storage account to use with Azure Data Lake Storage Gen2. A new resource to hold a system-assigned managed identity. This requires that you be a Contributor or Owner of a resource group in any subscription in the tenant. Configure and grant access to Azure storage for your metastore In this step, you create a storage account and container for the metadata and tables that will be managed by the Unity Catalog metastore, create an Azure connector that generates a system-assigned managed identity, and give that managed identity access to the storage container. Create a storage account for Azure Data Lake Storage Gen2. This storage account will contain metadata related to Unity Catalog metastores and their objects, as well as the data for managed tables in Unity Catalog. See Create a storage account to use with Azure Data Lake Storage Gen2. Make a note of the region where you created the storage account. Create a storage container that will hold your Unity Catalog metastore’s metadata and managed tables. You can create no more than one metastore per region. It is recommended that you use the same region for your metastore and storage container. Make a note of the ADLSv2 URI for the container, which is in the following format: Copy abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<metastore-name>
 In the steps that follow, replace <storage-container> with this URI. In Azure, create an Azure Databricks access connector that holds a managed identity and give it access to the storage container. See Use Azure managed identities in Unity Catalog to access storage. Create your first metastore and attach a workspace A metastore is the top-level container for data in Unity Catalog. Each metastore exposes a 3-level namespace (catalog.schema.table) by which data can be organized. A single metastore can be shared across multiple Azure Databricks workspaces in an account. Each linked workspace has the same view of the data in the metastore, and data access control can be managed across workspaces. Databricks allows one metastore per region. If you have a multi-region Databricks deployment, you may want separate metastores for each region, but it is good practice to use a small number of metastores unless your organization requires hard isolation boundaries between sets of data. Data cannot easily be joined or queried across metastores. To create a metastore: Make sure that you have the path to the storage container and the resource ID of the Azure Databricks access connector that you created in the previous task. Log in to the Azure Databricks account console. Click Data. Click Create Metastore. Enter values for the following fields Name for the metastore. Region where the metastore will be deployed. For best performance, co-locate the access connector, workspaces, metastore and cloud storage location in the same cloud region. ADLS Gen 2 path: Enter the path to the storage container that you will use as root storage for the metastore. The abfss:// prefix is added automatically. Access Connector ID: Enter the Azure Databricks access connector’s resource ID in the format: Copy /subscriptions/12f34567-8ace-9c10-111c-aea8eba12345c/resourceGroups/<resource_group>/providers/Microsoft.Databricks/accessConnectors/<connector-name>
 Click Create. If the request fails, retry using a different metastore name. When prompted, select workspaces to link to the metastore. The account-level user who creates a metastore is its owner and metastore admin. Any account admin can manage permissions for a metastore and its objects. To transfer ownership of a metastore to a different account-level user or a group, see (Recommended) Transfer ownership of your metastore to a group. Add users and groups A Unity Catalog metastore can be shared across multiple Databricks workspaces. So that Databricks has a consistent view of users and groups across all workspaces, you can now add Azure Active Directory users and groups as account-level identities. Follow these steps to add account-level identities. Note Users and groups must be added as account-level identities before they can access Unity Catalog. The initial account-level admin must be a Contributor in the Azure Active Directory root management group, which is named Tenant root group by default. An Azure Active Directory Global Administrator can add themselves to this group. Grant yourself this role, or ask an Azure Active Directory Global Administrator to grant it to you. The initial account-level admin can add users or groups to the account console, and can designate other account-level admins by granting the Admin role to users. All Azure Active Directory users who have been added to workspaces in your Azure tenant are automatically added as account-level identities. To designate additional account-level admins, you grant users the Admin role. Note It is not possible to grant the Admin role to a group. Log in to the account console by clicking Settings, then clicking Manage account.. Click Users and Groups. A list of Azure Active Directory users appears. Only users and groups who have been added to workspaces are shown. Click the name of a user. Click Roles. Enable Admin. To get started, create a group called data-consumers. This group is used later in this walk-through. Create a compute resource Tables defined in Unity Catalog are protected by fine-grained access controls. To ensure that access controls are enforced, Unity Catalog requires clusters to conform to a secure configuration. Unity Catalog is secure by default, meaning that non-conforming clusters cannot access tables in Unity Catalog. To create a compute resource that can access data in Unity Catalog: Create a cluster To create a Data Science & Engineering cluster that can access Unity Catalog: Log in to the workspace as a workspace-level admin. Click Compute. Click Create cluster. Enter a name for the cluster. Set Databricks runtime version to Runtime: 10.3 (Scala 2.12, Spark 3.2.1) or higher. Click Advanced Options. Set Security Mode to User Isolation or Single User. User Isolation clusters can be shared by multiple users, but only SQL workloads are supported. Some advanced cluster features such as library installation, init scripts, and the DBFS Fuse mount are also disabled to ensure security isolation among cluster users. To use those advanced cluster features or languages or to run workloads using Python, Scala and R, set the cluster mode to Single User. Single User cluster can also run SQL workloads. The cluster can be used exclusively by a single user (by default the owner of the cluster); other users cannot attach to the cluster. Automated jobs should run in this mode, and the job’s owner should be the cluster’s owner. In this mode, view security cannot be enforced. A user selecting from a view executes with their own permissions. For more information about the features available in each security mode, see Cluster security mode. Click Create Cluster. Create a SQL warehouse To create a SQL warehouse that can access Unity Catalog data: Log in to the workspace as a workspace-level admin. From the persona switcher, select SQL. Click Create, then select SQL Warehouse. Under Advanced Settings set Channel to Preview. SQL warehouses are automatically created with the correct security mode, with no configuration required. Create your first table In Unity Catalog, metastores contain catalogs that contain schemas (databases), and you always create a table in a schema. You can refer to a table using three-level notation: Copy <catalog>.<schema>.<table>
 A newly-created metastore contains a catalog named main with an empty schema named default. In this example, you will create a table named department in the default schema in the main catalog. To create a table, you must be an account admin, metastore admin, or a user with the CREATE permission on the parent schema and the USAGE permission on the parent catalog and schema. Follow these steps to create a table manually. You can also import an example notebook and run it to create a catalog, schema, and table, along with managing permissions on each. Create a notebook and attach it to the cluster you created in Create a compute resource. For the notebook language, select SQL. Grant permission to create tables on the default schema. To create tables, users require the CREATE and USAGE permissions on the schema in addition to the USAGE permission on the catalog. All users receive the USAGE privilege on the main catalog and the main.default schema when a metastore is created. Account admins, metastore admins, and the owner of the schema main.default can use the following command to GRANT the CREATE privilege to a user or group: SQL Copy GRANT CREATE ON SCHEMA <catalog-name>.<schema-name> TO `<EMAIL_ADDRESS>`;
 For example, to allow members of the group data-consumers to create tables in main.default: SQL Copy GRANT CREATE ON SCHEMA main.default to `data-consumers`;
 Run the cell. Create a new table called department. Add a new cell to the notebook. Paste in the following SQL, which specifies the table name, its columns, and inserts five rows into it. SQL Copy CREATE TABLE main.default.department
 (
    deptcode   INT,
    deptname  STRING,
    location  STRING
 );

INSERT INTO main.default.department VALUES
  (10, 'FINANCE', 'EDINBURGH'),
  (20, 'SOFTWARE', 'PADDINGTON'),
  (30, 'SALES', 'MAIDSTONE'),
  (40, 'MARKETING', 'DARLINGTON'),
  (50, 'ADMIN', 'BIRMINGHAM');
 Run the cell. Query the table. Add a new cell to the notebook. Paste in the following SQL, then run the cell. SQL Copy SELECT * from main.default.department;
 Grant the ability to read and query the table to the data-consumers group that you created in Add users and groups. Add a new cell to the notebook and paste in the following SQL: SQL Copy GRANT SELECT ON main.default.department TO `data-consumers`;
 Note To grant read access to all account-level users instead of only data-consumers, use the group name account users instead. Run the cell. (Optional) Link the metastore to additional workspaces. A key benefit of Unity Catalog is the ability to share a single metastore among multiple workspaces. You can then run different types of workloads against the same data without the need to move or copy data amongst workspaces. Each workspace can have a maximum of one Unity Catalog metastore assigned to it. To link the metastore to additional workspaces: Log in to the account console. Click Data. Click the name of a metastore to open its properties. Click the Workspaces tab. Click Assign to workspaces. Select one or more workspaces. You can type part of the workspace name to filter the list. Click Assign. When the assignment is complete, the workspace appears in the metastore’s Workspaces tab. Users in each of the workspaces you selected can now access data in the metastore. (Optional) Unlink the metastore from workspaces. A key benefit of Unity Catalog is the ability to share a single metastore among multiple workspaces. To remove a workspace’s access to data in a metastore, you can unlink the metatore from the workspace. Log in to the account console. Click Data. Click the name of a metastore to open its properties. Click the Workspaces tab. Deselect one or more workspaces. You can type part of the workspace name to filter the list. Click Assign. When the assignment is complete, the workspace no longer appears in the metastore’s Workspaces tab. Users in each of the workspaces you selected can no longer access data in the metastore. Example notebook You can use the following example notebook to create a catalog, schema, and table, as well as manage permissions on each. Create and manage a Unity Catalog table Get notebook (Recommended) Transfer ownership of your metastore to a group When possible, Databricks recommends group ownership over single-user ownership. The user who creates a metastore is its initial owner. A metastore owner can manage the privileges for all securable objects within a metastore, as well as create catalogs, external locations, and storage credentials. Log in to the account console. Click Data. Click the name of a metastore to open its properties. Under Owner, click Edit. Select a group from the drop-down. You can enter text in the field to search for options. Click Save. (Optional) Install the Unity Catalog CLI The Unity Catalog CLI is part of the Databricks CLI. To use the Unity Catalog CLI, do the following: Set up the CLI. Set up authentication. Optionally, create one or more connection profiles to use with the CLI. Learn how to use the Databricks CLI in general. Begin using the Unity Catalog CLI. Next steps Learn more about key concepts of Unity Catalog Create tables Create views",Get started using Unity Catalog
49,Unity Catalog Failed to acquire a SAS token,"Question: What time did the problem begin?
Answer: Tue, May 31, 2022, 12:00 AM (UTC-06:00) Central Time (US & Canada)

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer: 

Question: Is this a new problem, or it has happened before?
Answer: Never worked

Question: Cluster URL
Answer: https://adb-8904912125056473.13.azuredatabricks.net/?o=8904912125056473#setting/clusters/0606-215648-qep941zx/

Question: Notebook URL if available
Answer: https://adb-8904912125056473.13.azuredatabricks.net/?o=8904912125056473#notebook/3290234135689076/command/3290234135689208

Question: Additional details about the issue
Answer: Please direct this to Databricks Engineering as I've already discussed with out Databricks account rep

I'm trying to create a table in Unity Catalog following the Quickstart from their git repo. We are running into this error: Error in SQL statement: ExecutionException: Failed to acquire a SAS token for list on /c8b36b47-2cdf-43d5-861b-85352e46b1ea/tables/f7e89a9b-f1f7-49e3-877c-ef1b2103588e/_delta_log due to com.google.common.util.concurrent.UncheckedExecutionException: com.databricks.common.client.DatabricksServiceHttpClientException: PERMISSION_DENIED: request not authorized
com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: java.util.concurrent.ExecutionException: Failed to acquire a SAS token for list on /c8b36b47-2cdf-43d5-861b-85352e46b1ea/tables/f7e89a9b-f1f7-49e3-877c-ef1b2103588e/_delta_log due to com.google.common.util.concurrent.UncheckedExecutionException: com.databricks.common.client.DatabricksServiceHttpClientException: PERMISSION_DENIED: request not authorized

Our storage is on a virtual network and usual commands can work. I'm suspecting something specific to thier control plane.



<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Databricks:
What time did the problem begin? - 2022-05-31T05:00:00.000Z;
Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank - ;
Is this a new problem, or it has happened before? - Never worked;
Cluster URL - https://adb-8904912125056473.13.azuredatabricks.net/?o=8904912125056473#setting/clusters/0606-215648-qep941zx/;
Notebook URL if available - https://adb-8904912125056473.13.azuredatabricks.net/?o=8904912125056473#notebook/3290234135689076/command/3290234135689208;
Additional details about the issue - Please direct this to Databricks Engineering as I've already discussed with out Databricks account rep

I'm trying to create a table in Unity Catalog following the Quickstart from their git repo. We are running into this error: Error in SQL statement: ExecutionException: Failed to acquire a SAS token for list on /c8b36b47-2cdf-43d5-861b-85352e46b1ea/tables/f7e89a9b-f1f7-49e3-877c-ef1b2103588e/_delta_log due to com.google.common.util.concurrent.UncheckedExecutionException: com.databricks.common.client.DatabricksServiceHttpClientException: PERMISSION_DENIED: request not authorized
com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: java.util.concurrent.ExecutionException: Failed to acquire a SAS token for list on /c8b36b47-2cdf-43d5-861b-85352e46b1ea/tables/f7e89a9b-f1f7-49e3-877c-ef1b2103588e/_delta_log due to com.google.common.util.concurrent.UncheckedExecutionException: com.databricks.common.client.DatabricksServiceHttpClientException: PERMISSION_DENIED: request not authorized

Our storage is on a virtual network and usual commands can work. I'm suspecting something specific to thier control plane.;

- ProblemStartTime: 05/31/2022 05:00:00
- Cloud: Azure
- AzureProductSubscriptionID: 7efc9570-44d0-4d53-aac1-7e190bfb21e5
- AzureProductSubscriptionName: sb-gci-it-eda-dev-1
- PUID: 100320018E551EAA
- Tenant Id: a69a0a8f-0927-4e85-a33c-0d445c1ae881
- Object Id: 464f8f10-8f7d-4739-a820-8f8d5952a462
- SubscriptionType: Unified
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Unified Support - Advanced
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: True

- Location: westus2
- ResourceUri: /subscriptions/7efc9570-44d0-4d53-aac1-7e190bfb21e5/resourceGroups/rg-gci-it-eda-usagedev-1/providers/Microsoft.Databricks/workspaces/dbw-gci-it-eda-usagedev-1

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/get-started,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Get started using Unity Catalog Article 07/12/2022 10 minutes to read 5 contributors In this article Requirements Configure and grant access to Azure storage for your metastore Create your first metastore and attach a workspace Add users and groups Create a compute resource Create your first table (Optional) Link the metastore to additional workspaces. (Optional) Unlink the metastore from workspaces. Example notebook (Recommended) Transfer ownership of your metastore to a group (Optional) Install the Unity Catalog CLI Next steps Important Unity Catalog is in Public Preview. To participate in the preview, contact your Azure Databricks representative. This guide helps you get started with Unity Catalog, the Azure Databricks data governance framework. Requirements You must be an Azure Databricks account admin. The first Azure Databricks account admin must be an Azure Active Directory Global Administrator or a member of the root management group, which is usually named Tenant root group. That user can assign users with any level of Azure tenant permission as subsequent Azure Databricks account admins (who can themselves assign more account admins). Your Azure Databricks account must be on the Premium plan. In your Azure tenant, you must have permission to create: A storage account to use with Azure Data Lake Storage Gen2. See Create a storage account to use with Azure Data Lake Storage Gen2. A new resource to hold a system-assigned managed identity. This requires that you be a Contributor or Owner of a resource group in any subscription in the tenant. Configure and grant access to Azure storage for your metastore In this step, you create a storage account and container for the metadata and tables that will be managed by the Unity Catalog metastore, create an Azure connector that generates a system-assigned managed identity, and give that managed identity access to the storage container. Create a storage account for Azure Data Lake Storage Gen2. This storage account will contain metadata related to Unity Catalog metastores and their objects, as well as the data for managed tables in Unity Catalog. See Create a storage account to use with Azure Data Lake Storage Gen2. Make a note of the region where you created the storage account. Create a storage container that will hold your Unity Catalog metastore’s metadata and managed tables. You can create no more than one metastore per region. It is recommended that you use the same region for your metastore and storage container. Make a note of the ADLSv2 URI for the container, which is in the following format: Copy abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<metastore-name>
 In the steps that follow, replace <storage-container> with this URI. In Azure, create an Azure Databricks access connector that holds a managed identity and give it access to the storage container. See Use Azure managed identities in Unity Catalog to access storage. Create your first metastore and attach a workspace A metastore is the top-level container for data in Unity Catalog. Each metastore exposes a 3-level namespace (catalog.schema.table) by which data can be organized. A single metastore can be shared across multiple Azure Databricks workspaces in an account. Each linked workspace has the same view of the data in the metastore, and data access control can be managed across workspaces. Databricks allows one metastore per region. If you have a multi-region Databricks deployment, you may want separate metastores for each region, but it is good practice to use a small number of metastores unless your organization requires hard isolation boundaries between sets of data. Data cannot easily be joined or queried across metastores. To create a metastore: Make sure that you have the path to the storage container and the resource ID of the Azure Databricks access connector that you created in the previous task. Log in to the Azure Databricks account console. Click Data. Click Create Metastore. Enter values for the following fields Name for the metastore. Region where the metastore will be deployed. For best performance, co-locate the access connector, workspaces, metastore and cloud storage location in the same cloud region. ADLS Gen 2 path: Enter the path to the storage container that you will use as root storage for the metastore. The abfss:// prefix is added automatically. Access Connector ID: Enter the Azure Databricks access connector’s resource ID in the format: Copy /subscriptions/12f34567-8ace-9c10-111c-aea8eba12345c/resourceGroups/<resource_group>/providers/Microsoft.Databricks/accessConnectors/<connector-name>
 Click Create. If the request fails, retry using a different metastore name. When prompted, select workspaces to link to the metastore. The account-level user who creates a metastore is its owner and metastore admin. Any account admin can manage permissions for a metastore and its objects. To transfer ownership of a metastore to a different account-level user or a group, see (Recommended) Transfer ownership of your metastore to a group. Add users and groups A Unity Catalog metastore can be shared across multiple Databricks workspaces. So that Databricks has a consistent view of users and groups across all workspaces, you can now add Azure Active Directory users and groups as account-level identities. Follow these steps to add account-level identities. Note Users and groups must be added as account-level identities before they can access Unity Catalog. The initial account-level admin must be a Contributor in the Azure Active Directory root management group, which is named Tenant root group by default. An Azure Active Directory Global Administrator can add themselves to this group. Grant yourself this role, or ask an Azure Active Directory Global Administrator to grant it to you. The initial account-level admin can add users or groups to the account console, and can designate other account-level admins by granting the Admin role to users. All Azure Active Directory users who have been added to workspaces in your Azure tenant are automatically added as account-level identities. To designate additional account-level admins, you grant users the Admin role. Note It is not possible to grant the Admin role to a group. Log in to the account console by clicking Settings, then clicking Manage account.. Click Users and Groups. A list of Azure Active Directory users appears. Only users and groups who have been added to workspaces are shown. Click the name of a user. Click Roles. Enable Admin. To get started, create a group called data-consumers. This group is used later in this walk-through. Create a compute resource Tables defined in Unity Catalog are protected by fine-grained access controls. To ensure that access controls are enforced, Unity Catalog requires clusters to conform to a secure configuration. Unity Catalog is secure by default, meaning that non-conforming clusters cannot access tables in Unity Catalog. To create a compute resource that can access data in Unity Catalog: Create a cluster To create a Data Science & Engineering cluster that can access Unity Catalog: Log in to the workspace as a workspace-level admin. Click Compute. Click Create cluster. Enter a name for the cluster. Set Databricks runtime version to Runtime: 10.3 (Scala 2.12, Spark 3.2.1) or higher. Click Advanced Options. Set Security Mode to User Isolation or Single User. User Isolation clusters can be shared by multiple users, but only SQL workloads are supported. Some advanced cluster features such as library installation, init scripts, and the DBFS Fuse mount are also disabled to ensure security isolation among cluster users. To use those advanced cluster features or languages or to run workloads using Python, Scala and R, set the cluster mode to Single User. Single User cluster can also run SQL workloads. The cluster can be used exclusively by a single user (by default the owner of the cluster); other users cannot attach to the cluster. Automated jobs should run in this mode, and the job’s owner should be the cluster’s owner. In this mode, view security cannot be enforced. A user selecting from a view executes with their own permissions. For more information about the features available in each security mode, see Cluster security mode. Click Create Cluster. Create a SQL warehouse To create a SQL warehouse that can access Unity Catalog data: Log in to the workspace as a workspace-level admin. From the persona switcher, select SQL. Click Create, then select SQL Warehouse. Under Advanced Settings set Channel to Preview. SQL warehouses are automatically created with the correct security mode, with no configuration required. Create your first table In Unity Catalog, metastores contain catalogs that contain schemas (databases), and you always create a table in a schema. You can refer to a table using three-level notation: Copy <catalog>.<schema>.<table>
 A newly-created metastore contains a catalog named main with an empty schema named default. In this example, you will create a table named department in the default schema in the main catalog. To create a table, you must be an account admin, metastore admin, or a user with the CREATE permission on the parent schema and the USAGE permission on the parent catalog and schema. Follow these steps to create a table manually. You can also import an example notebook and run it to create a catalog, schema, and table, along with managing permissions on each. Create a notebook and attach it to the cluster you created in Create a compute resource. For the notebook language, select SQL. Grant permission to create tables on the default schema. To create tables, users require the CREATE and USAGE permissions on the schema in addition to the USAGE permission on the catalog. All users receive the USAGE privilege on the main catalog and the main.default schema when a metastore is created. Account admins, metastore admins, and the owner of the schema main.default can use the following command to GRANT the CREATE privilege to a user or group: SQL Copy GRANT CREATE ON SCHEMA <catalog-name>.<schema-name> TO `<EMAIL_ADDRESS>`;
 For example, to allow members of the group data-consumers to create tables in main.default: SQL Copy GRANT CREATE ON SCHEMA main.default to `data-consumers`;
 Run the cell. Create a new table called department. Add a new cell to the notebook. Paste in the following SQL, which specifies the table name, its columns, and inserts five rows into it. SQL Copy CREATE TABLE main.default.department
 (
    deptcode   INT,
    deptname  STRING,
    location  STRING
 );

INSERT INTO main.default.department VALUES
  (10, 'FINANCE', 'EDINBURGH'),
  (20, 'SOFTWARE', 'PADDINGTON'),
  (30, 'SALES', 'MAIDSTONE'),
  (40, 'MARKETING', 'DARLINGTON'),
  (50, 'ADMIN', 'BIRMINGHAM');
 Run the cell. Query the table. Add a new cell to the notebook. Paste in the following SQL, then run the cell. SQL Copy SELECT * from main.default.department;
 Grant the ability to read and query the table to the data-consumers group that you created in Add users and groups. Add a new cell to the notebook and paste in the following SQL: SQL Copy GRANT SELECT ON main.default.department TO `data-consumers`;
 Note To grant read access to all account-level users instead of only data-consumers, use the group name account users instead. Run the cell. (Optional) Link the metastore to additional workspaces. A key benefit of Unity Catalog is the ability to share a single metastore among multiple workspaces. You can then run different types of workloads against the same data without the need to move or copy data amongst workspaces. Each workspace can have a maximum of one Unity Catalog metastore assigned to it. To link the metastore to additional workspaces: Log in to the account console. Click Data. Click the name of a metastore to open its properties. Click the Workspaces tab. Click Assign to workspaces. Select one or more workspaces. You can type part of the workspace name to filter the list. Click Assign. When the assignment is complete, the workspace appears in the metastore’s Workspaces tab. Users in each of the workspaces you selected can now access data in the metastore. (Optional) Unlink the metastore from workspaces. A key benefit of Unity Catalog is the ability to share a single metastore among multiple workspaces. To remove a workspace’s access to data in a metastore, you can unlink the metatore from the workspace. Log in to the account console. Click Data. Click the name of a metastore to open its properties. Click the Workspaces tab. Deselect one or more workspaces. You can type part of the workspace name to filter the list. Click Assign. When the assignment is complete, the workspace no longer appears in the metastore’s Workspaces tab. Users in each of the workspaces you selected can no longer access data in the metastore. Example notebook You can use the following example notebook to create a catalog, schema, and table, as well as manage permissions on each. Create and manage a Unity Catalog table Get notebook (Recommended) Transfer ownership of your metastore to a group When possible, Databricks recommends group ownership over single-user ownership. The user who creates a metastore is its initial owner. A metastore owner can manage the privileges for all securable objects within a metastore, as well as create catalogs, external locations, and storage credentials. Log in to the account console. Click Data. Click the name of a metastore to open its properties. Under Owner, click Edit. Select a group from the drop-down. You can enter text in the field to search for options. Click Save. (Optional) Install the Unity Catalog CLI The Unity Catalog CLI is part of the Databricks CLI. To use the Unity Catalog CLI, do the following: Set up the CLI. Set up authentication. Optionally, create one or more connection profiles to use with the CLI. Learn how to use the Databricks CLI in general. Begin using the Unity Catalog CLI. Next steps Learn more about key concepts of Unity Catalog Create tables Create views",Get started using Unity Catalog
50,account console cannot log in,"Hi Databricks team,

Our cx is struggling with configuring unity& catalog, after he's accessing the account console he's getting this error message:""There was an error logging in. Your user does not belong to a Databricks account. Contact your Databricks account administrator.""

He fulfilled all requirements which are described here:
https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/get-started#requirements

Could you please help us resolve this issue?

Thank you,
Michal",https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/get-started,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Get started using Unity Catalog Article 07/12/2022 10 minutes to read 5 contributors In this article Requirements Configure and grant access to Azure storage for your metastore Create your first metastore and attach a workspace Add users and groups Create a compute resource Create your first table (Optional) Link the metastore to additional workspaces. (Optional) Unlink the metastore from workspaces. Example notebook (Recommended) Transfer ownership of your metastore to a group (Optional) Install the Unity Catalog CLI Next steps Important Unity Catalog is in Public Preview. To participate in the preview, contact your Azure Databricks representative. This guide helps you get started with Unity Catalog, the Azure Databricks data governance framework. Requirements You must be an Azure Databricks account admin. The first Azure Databricks account admin must be an Azure Active Directory Global Administrator or a member of the root management group, which is usually named Tenant root group. That user can assign users with any level of Azure tenant permission as subsequent Azure Databricks account admins (who can themselves assign more account admins). Your Azure Databricks account must be on the Premium plan. In your Azure tenant, you must have permission to create: A storage account to use with Azure Data Lake Storage Gen2. See Create a storage account to use with Azure Data Lake Storage Gen2. A new resource to hold a system-assigned managed identity. This requires that you be a Contributor or Owner of a resource group in any subscription in the tenant. Configure and grant access to Azure storage for your metastore In this step, you create a storage account and container for the metadata and tables that will be managed by the Unity Catalog metastore, create an Azure connector that generates a system-assigned managed identity, and give that managed identity access to the storage container. Create a storage account for Azure Data Lake Storage Gen2. This storage account will contain metadata related to Unity Catalog metastores and their objects, as well as the data for managed tables in Unity Catalog. See Create a storage account to use with Azure Data Lake Storage Gen2. Make a note of the region where you created the storage account. Create a storage container that will hold your Unity Catalog metastore’s metadata and managed tables. You can create no more than one metastore per region. It is recommended that you use the same region for your metastore and storage container. Make a note of the ADLSv2 URI for the container, which is in the following format: Copy abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<metastore-name>
 In the steps that follow, replace <storage-container> with this URI. In Azure, create an Azure Databricks access connector that holds a managed identity and give it access to the storage container. See Use Azure managed identities in Unity Catalog to access storage. Create your first metastore and attach a workspace A metastore is the top-level container for data in Unity Catalog. Each metastore exposes a 3-level namespace (catalog.schema.table) by which data can be organized. A single metastore can be shared across multiple Azure Databricks workspaces in an account. Each linked workspace has the same view of the data in the metastore, and data access control can be managed across workspaces. Databricks allows one metastore per region. If you have a multi-region Databricks deployment, you may want separate metastores for each region, but it is good practice to use a small number of metastores unless your organization requires hard isolation boundaries between sets of data. Data cannot easily be joined or queried across metastores. To create a metastore: Make sure that you have the path to the storage container and the resource ID of the Azure Databricks access connector that you created in the previous task. Log in to the Azure Databricks account console. Click Data. Click Create Metastore. Enter values for the following fields Name for the metastore. Region where the metastore will be deployed. For best performance, co-locate the access connector, workspaces, metastore and cloud storage location in the same cloud region. ADLS Gen 2 path: Enter the path to the storage container that you will use as root storage for the metastore. The abfss:// prefix is added automatically. Access Connector ID: Enter the Azure Databricks access connector’s resource ID in the format: Copy /subscriptions/12f34567-8ace-9c10-111c-aea8eba12345c/resourceGroups/<resource_group>/providers/Microsoft.Databricks/accessConnectors/<connector-name>
 Click Create. If the request fails, retry using a different metastore name. When prompted, select workspaces to link to the metastore. The account-level user who creates a metastore is its owner and metastore admin. Any account admin can manage permissions for a metastore and its objects. To transfer ownership of a metastore to a different account-level user or a group, see (Recommended) Transfer ownership of your metastore to a group. Add users and groups A Unity Catalog metastore can be shared across multiple Databricks workspaces. So that Databricks has a consistent view of users and groups across all workspaces, you can now add Azure Active Directory users and groups as account-level identities. Follow these steps to add account-level identities. Note Users and groups must be added as account-level identities before they can access Unity Catalog. The initial account-level admin must be a Contributor in the Azure Active Directory root management group, which is named Tenant root group by default. An Azure Active Directory Global Administrator can add themselves to this group. Grant yourself this role, or ask an Azure Active Directory Global Administrator to grant it to you. The initial account-level admin can add users or groups to the account console, and can designate other account-level admins by granting the Admin role to users. All Azure Active Directory users who have been added to workspaces in your Azure tenant are automatically added as account-level identities. To designate additional account-level admins, you grant users the Admin role. Note It is not possible to grant the Admin role to a group. Log in to the account console by clicking Settings, then clicking Manage account.. Click Users and Groups. A list of Azure Active Directory users appears. Only users and groups who have been added to workspaces are shown. Click the name of a user. Click Roles. Enable Admin. To get started, create a group called data-consumers. This group is used later in this walk-through. Create a compute resource Tables defined in Unity Catalog are protected by fine-grained access controls. To ensure that access controls are enforced, Unity Catalog requires clusters to conform to a secure configuration. Unity Catalog is secure by default, meaning that non-conforming clusters cannot access tables in Unity Catalog. To create a compute resource that can access data in Unity Catalog: Create a cluster To create a Data Science & Engineering cluster that can access Unity Catalog: Log in to the workspace as a workspace-level admin. Click Compute. Click Create cluster. Enter a name for the cluster. Set Databricks runtime version to Runtime: 10.3 (Scala 2.12, Spark 3.2.1) or higher. Click Advanced Options. Set Security Mode to User Isolation or Single User. User Isolation clusters can be shared by multiple users, but only SQL workloads are supported. Some advanced cluster features such as library installation, init scripts, and the DBFS Fuse mount are also disabled to ensure security isolation among cluster users. To use those advanced cluster features or languages or to run workloads using Python, Scala and R, set the cluster mode to Single User. Single User cluster can also run SQL workloads. The cluster can be used exclusively by a single user (by default the owner of the cluster); other users cannot attach to the cluster. Automated jobs should run in this mode, and the job’s owner should be the cluster’s owner. In this mode, view security cannot be enforced. A user selecting from a view executes with their own permissions. For more information about the features available in each security mode, see Cluster security mode. Click Create Cluster. Create a SQL warehouse To create a SQL warehouse that can access Unity Catalog data: Log in to the workspace as a workspace-level admin. From the persona switcher, select SQL. Click Create, then select SQL Warehouse. Under Advanced Settings set Channel to Preview. SQL warehouses are automatically created with the correct security mode, with no configuration required. Create your first table In Unity Catalog, metastores contain catalogs that contain schemas (databases), and you always create a table in a schema. You can refer to a table using three-level notation: Copy <catalog>.<schema>.<table>
 A newly-created metastore contains a catalog named main with an empty schema named default. In this example, you will create a table named department in the default schema in the main catalog. To create a table, you must be an account admin, metastore admin, or a user with the CREATE permission on the parent schema and the USAGE permission on the parent catalog and schema. Follow these steps to create a table manually. You can also import an example notebook and run it to create a catalog, schema, and table, along with managing permissions on each. Create a notebook and attach it to the cluster you created in Create a compute resource. For the notebook language, select SQL. Grant permission to create tables on the default schema. To create tables, users require the CREATE and USAGE permissions on the schema in addition to the USAGE permission on the catalog. All users receive the USAGE privilege on the main catalog and the main.default schema when a metastore is created. Account admins, metastore admins, and the owner of the schema main.default can use the following command to GRANT the CREATE privilege to a user or group: SQL Copy GRANT CREATE ON SCHEMA <catalog-name>.<schema-name> TO `<EMAIL_ADDRESS>`;
 For example, to allow members of the group data-consumers to create tables in main.default: SQL Copy GRANT CREATE ON SCHEMA main.default to `data-consumers`;
 Run the cell. Create a new table called department. Add a new cell to the notebook. Paste in the following SQL, which specifies the table name, its columns, and inserts five rows into it. SQL Copy CREATE TABLE main.default.department
 (
    deptcode   INT,
    deptname  STRING,
    location  STRING
 );

INSERT INTO main.default.department VALUES
  (10, 'FINANCE', 'EDINBURGH'),
  (20, 'SOFTWARE', 'PADDINGTON'),
  (30, 'SALES', 'MAIDSTONE'),
  (40, 'MARKETING', 'DARLINGTON'),
  (50, 'ADMIN', 'BIRMINGHAM');
 Run the cell. Query the table. Add a new cell to the notebook. Paste in the following SQL, then run the cell. SQL Copy SELECT * from main.default.department;
 Grant the ability to read and query the table to the data-consumers group that you created in Add users and groups. Add a new cell to the notebook and paste in the following SQL: SQL Copy GRANT SELECT ON main.default.department TO `data-consumers`;
 Note To grant read access to all account-level users instead of only data-consumers, use the group name account users instead. Run the cell. (Optional) Link the metastore to additional workspaces. A key benefit of Unity Catalog is the ability to share a single metastore among multiple workspaces. You can then run different types of workloads against the same data without the need to move or copy data amongst workspaces. Each workspace can have a maximum of one Unity Catalog metastore assigned to it. To link the metastore to additional workspaces: Log in to the account console. Click Data. Click the name of a metastore to open its properties. Click the Workspaces tab. Click Assign to workspaces. Select one or more workspaces. You can type part of the workspace name to filter the list. Click Assign. When the assignment is complete, the workspace appears in the metastore’s Workspaces tab. Users in each of the workspaces you selected can now access data in the metastore. (Optional) Unlink the metastore from workspaces. A key benefit of Unity Catalog is the ability to share a single metastore among multiple workspaces. To remove a workspace’s access to data in a metastore, you can unlink the metatore from the workspace. Log in to the account console. Click Data. Click the name of a metastore to open its properties. Click the Workspaces tab. Deselect one or more workspaces. You can type part of the workspace name to filter the list. Click Assign. When the assignment is complete, the workspace no longer appears in the metastore’s Workspaces tab. Users in each of the workspaces you selected can no longer access data in the metastore. Example notebook You can use the following example notebook to create a catalog, schema, and table, as well as manage permissions on each. Create and manage a Unity Catalog table Get notebook (Recommended) Transfer ownership of your metastore to a group When possible, Databricks recommends group ownership over single-user ownership. The user who creates a metastore is its initial owner. A metastore owner can manage the privileges for all securable objects within a metastore, as well as create catalogs, external locations, and storage credentials. Log in to the account console. Click Data. Click the name of a metastore to open its properties. Under Owner, click Edit. Select a group from the drop-down. You can enter text in the field to search for options. Click Save. (Optional) Install the Unity Catalog CLI The Unity Catalog CLI is part of the Databricks CLI. To use the Unity Catalog CLI, do the following: Set up the CLI. Set up authentication. Optionally, create one or more connection profiles to use with the CLI. Learn how to use the Databricks CLI in general. Begin using the Unity Catalog CLI. Next steps Learn more about key concepts of Unity Catalog Create tables Create views",Get started using Unity Catalog
51,Customer is not able to see Spark UI details for a particular stage,"Customer cannot see details in Stage 48 of Job 28 in https://adb-4873923760910439.19.azuredatabricks.net/?o=4873923760910439#setting/clusters/0613-050015-v7o1fsdv/sparkUi (28 (2713550408901295349_8042653318192595971_job-38-run-3517018-action-7535103123442445) in job-38-run-3517018).

The stage completed successfully in 26 minutes but when you click on it no details are shown. This particular stage executed 25600 tasks. Please provide RCA. Also, if you can recover the stage details please provide them.",https://docs.microsoft.com/en-us/azure/databricks/kb/clusters/replay-cluster-spark-events,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Replay Apache Spark events in a cluster Article 03/11/2022 2 minutes to read 2 contributors In this article Enable cluster log delivery Confirm cluster logs exist Launch a single node cluster Run the Event Log Replay notebook Prevent items getting dropped from the UI The Spark UI is commonly used as a debugging tool for Spark jobs. If the Spark UI is inaccessible, you can load the event logs in another cluster and use the Event Log Replay notebook to replay the Spark events. Important Cluster log delivery is not enabled by default. You must enable cluster log delivery before starting your cluster, otherwise there will be no logs to replay. Enable cluster log delivery Follow the documentation to configure Cluster log delivery on your cluster. The location of the cluster logs depends on the Cluster Log Path that you set during cluster configuration. For example, if the log path is dbfs:/cluster-logs, the log files for a specific cluster will be stored in dbfs:/cluster-logs/<cluster-name> and the individual event logs will be stored in dbfs:/cluster-logs/<cluster-name>/eventlog/<cluster-name-cluster-ip>/<log-id>/. Confirm cluster logs exist Review the cluster log path and verify that logs are being written for your chosen cluster. Log files are written every five minutes. Launch a single node cluster Launch a single node cluster. You will replay the logs on this cluster. Select the instance type based on the size of the event logs that you want to replay. Run the Event Log Replay notebook Attach the Event Log Replay notebook to the single node cluster. Enter the path to your chosen cluster event logs in the event_log_path field in the notebook. Run the notebook. Event Log Replay notebook Get notebook Prevent items getting dropped from the UI If you have a long-running cluster, it is possible for some jobs and/or stages to get dropped from the Spark UI. This happens due to default UI limits that are intended to prevent the UI from using up too much memory and causing an out-of-memory error on the cluster. If you are using a single node cluster to replay the event logs, you can increase the default UI limits and devote more memory to the Spark UI. This prevents items from getting dropped. You can adjust these values during cluster creation by editing the Spark Config. This example contains the default values for these properties. text Copy spark.ui.retainedJobs 1000
spark.ui.retainedStages 1000
spark.ui.retainedTasks 100000
spark.sql.ui.retainedExecutions 1000",Replay Apache Spark events in a cluster
52,Unable to connect to,"Hello,
It seems that we can't connect with username / password in powerBI to the PPROD EU data catalog (cf attached), can you please check ?

Thanks

Regards",https://docs.microsoft.com/en-us/azure/databricks/integrations/bi/power-bi,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Power BI Article 07/07/2022 6 minutes to read 6 contributors In this article Requirements Connect with Power BI Desktop using Partner Connect Connect with Power BI Desktop manually Unity Catalog (Preview) Integration Using a custom SQL query Access Azure Databricks data source using the Power BI service Automated HTTP proxy detection Power BI Delta Sharing Connector Limitations Microsoft Power BI is a business analytics service that provides interactive visualizations with self-service business intelligence capabilities, enabling end users to create reports and dashboards by themselves without having to depend on information technology staff or database administrators. When you use Azure Databricks as a data source with Power BI, you can bring the advantages of Azure Databricks performance and technology beyond data scientists and data engineers to all business users. You can connect Power BI Desktop to your Azure Databricks clusters and Databricks SQL warehouses by using the built-in Azure Databricks connector. You can also publish Power BI reports to the Power BI service and enable users to access the underlying Azure Databricks data using single sign-on (SSO), passing along the same Azure Active Directory credentials they use to access the report. Requirements Power BI Desktop 2.85.681.0 or above. Download the latest version. Note Power BI Desktop requires Windows. An alternative for other operating systems is to run Power BI Desktop on a physical host or a Windows-based virtual machine and then connect to it from your operating system. If you use a version of Power BI Desktop below 2.85.681.0, you also need to install the Databricks ODBC driver in the same environment as Power BI Desktop. An Azure Active Directory token (recommended), an Azure Databricks personal access token, or your Azure Active Directory account credentials. For large imports that take longer than one hour, Databricks recommends that you use Azure Databricks personal access token authentication as there is a known token refresh issue when using Azure Active Directory. An Azure Databricks cluster or Databricks SQL warehouse. Connect with Power BI Desktop using Partner Connect You can use Databricks Partner Connect to connect a cluster or SQL warehouse with Power BI Desktop in just a few clicks. Make sure your Azure Databricks account, workspace, and the signed-in user meet the requirements for Partner Connect. In the sidebar, click Partner Connect. Click the Power BI tile. In the Connect to partner dialog, for Compute, choose the name of the Azure Databricks compute resource that you want to connect. If your selected persona is Data Science & Engineering or Databricks Machine Learning, choose a cluster or SQL warehouse. If your selected persona is Databricks SQL, choose a SQL warehouse. Choose Download connection file. Open the downloaded connection file, which starts Power BI Desktop. In Power BI Desktop, enter your authentication credentials: Personal Access Token: Enter your Azure Active Directory token or Azure Databricks personal access token from the Requirements. Azure Active Directory: Click Sign in and then follow the on-screen instructions. Username / Password: Not applicable. Click Connect. Select the Azure Databricks data to query from the Power BI Navigator. Connect with Power BI Desktop manually Follow these instructions to connect to a cluster or SQL warehouse with Power BI Desktop. Note To connect faster with Power BI Desktop, use Partner Connect. Get the Server Hostname and HTTP Path. Start Power BI Desktop. Click Get data or File > Get data. Click Get data to get started. Search for Databricks, choose the Azure Databricks connector, and click Connect. Enter the Server Hostname and HTTP Path. Optionally, enter the default database and catalog to use for the connection. Select your Data Connectivity mode. For information about the difference between Import and DirectQuery, see Use DirectQuery in Power BI Desktop. Click OK. Enter your authentication credentials: Personal Access Token: Enter your Azure Active Directory token or Azure Databricks personal access token from the Requirements. Azure Active Directory: Click Sign in and then follow the on-screen instructions. Username / Password: Not applicable. Click Connect. Select the Azure Databricks data to query from the Power BI Navigator. Unity Catalog (Preview) Integration The Azure Databricks connector allows you to select a catalog before navigating through databases and tables. This feature is available in Power BI Desktop version 2.98.683.0 and above (October 2021 release). Using a custom SQL query The Databricks connector provides the Databricks.Query data source that allows a user to provide a custom SQL query. This feature will be available in the Power BI February 2022 release. Follow the steps described in Connect with Power BI Desktop to create a connection, using Import as the data connectivity mode. In the Navigator, right click the top-most item containing the selected host name and HTTP path and click Transform Data to open the Power Query Editor. In the function bar, replace the function name Databricks.Catalogs with Databricks.Query and apply the change. This creates a Power Query function that takes a SQL query as parameter. Enter the desired SQL query in the parameter field and click Invoke. This executes the query and a new table is created with the query results as its contents. Access Azure Databricks data source using the Power BI service When you publish a report to the Power BI service, you can enable users to access the report and underlying Azure Databricks data source using SSO: Publish your Power BI report from Power BI Desktop to the Power BI service. Enable single sign on (SSO) access to the report and underlying data source. Go to the underlying Azure Databricks dataset for the report in the Power BI service, expand Data source credentials, and click Edit credentials. On the configuration dialog, select Report viewers can only access this data source with their own Power BI identities using Direct Query and click Sign in. With this option selected, access to the data source is handled using DirectQuery and managed using the Azure AD identity of the user who is accessing the report. If you don’t select this option, only you, as the user who published the report, have access to the Azure Databricks data source. Automated HTTP proxy detection Power BI Desktop version 2.104.941.0 and above (May 2022 release) has built-in support for detecting Windows system-wide HTTP proxy configuration. Download the latest version. Power BI Desktop can automatically detect and use your Windows system-wide HTTP proxy configuration. If the proxy server does not provide a CRL distribution point (CDP), Power BI may show the error message Details: ""ODBC: ERROR [HY000] [Microsoft][DriverSupport] (1200) -The revocation status of the certificate or one of the certificates in the certificate chain is unknown."". To fix this error, complete the following steps: Create the file C:\Program Files\Microsoft Power BI Desktop\bin\ODBC Drivers\Simba Spark ODBC Driver\microsoft.sparkodbc.ini if it does not exist. Add the following config to your microsoft.sparkodbc.ini file: ini Copy [Driver]
CheckCertRevocation=0
 Power BI Delta Sharing Connector The Power BI Delta Sharing Connector allows users to discover, analyze and visualize datasets shared with them through the Delta Sharing open protocol. The protocol enables secure exchange of datasets across products and platforms by leveraging REST and cloud storage. Requirements Power BI Desktop 2.99.621.0 or above. Download the latest version. After you download the credentials file, open it with a text editor to retrieve the warehouse URL and the token. Connect to Databricks To connect to Azure Databricks by using the Delta Sharing Connector, complete the following steps: Open Power BI Desktop. On the Get Data menu, search for Delta Sharing. Select the connector and click Connect. Enter the warehouse URL that you copied from the credentials file into the Delta Sharing Server URL field. Optionally, in the Advanced Options tab, set a Row Limit for the maximum number of rows that you can download. This is set to 1 million rows by default. Click OK. For Authentication, copy the token that you retrieved from the credentials file into Bearer Token. Click Connect. Limitations The Azure Databricks connector does not support web proxy. In the Azure Databricks connector, the Databricks.Query data source is not supported in combination with DirectQuery mode. The Delta Sharing Connector is not yet available in the Power BI service. The data that the Delta Sharing Connector loads must fit into the memory of your machine. To ensure this, the connector limits the number of imported rows to the Row Limit that was set earlier.",Power BI
53,Errors writing to SQLMI - Databricks Notebook,"Question: What time did the problem begin?
Answer: Not sure, use current time

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer: 

Question: Is this a new problem, or it has happened before?
Answer: New problem, worked before

Question: Any changes made?
Answer: 

ISSUE: Running databrocks notebook with some Longitude / Latitude values, when converted to decimals, the data is not written to the SQL Server MI and we are getting different errors. Also the same notebook was running until yesterday, but I started getting the below error (see attaced document for details) since this morning. 

 
 


Also when trying to display the dataframe I am getting the below error
 

Can you please have a look and advise on the issue. 

Thanks
Muhammad





Question: Cluster URL
Answer: https://adb-1851547605975220.0.azuredatabricks.net/login.html?o=1851547605975220#setting/clusters/0413-214226-ei6g26iz/configuration

Question: Notebook URL if available
Answer: 

Question: Additional details about the issue
Answer: ISSUE: Running databrocks notebook with some Longitude / Latitude values, when converted to decimals, the data is not written to the SQL Server MI and we are getting different errors. Also the same notebook was running until yesterday, but I started getting the below error (see attaced document for details) since this morning. 

 
 
Also when trying to display the dataframe I am getting the below error
 

Can you please have a look and advise on the issue. 

Thanks
Muhammad


OFFICIAL





<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Databricks:
What time did the problem begin? - 2022-06-07T04:43:00.830Z;
Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank - ;
Is this a new problem, or it has happened before? - New problem, worked before;
Any changes made? - 

ISSUE: Running databrocks notebook with some Longitude / Latitude values, when converted to decimals, the data is not written to the SQL Server MI and we are getting different errors. Also the same notebook was running until yesterday, but I started getting the below error (see attaced document for details) since this morning. 

 
 


Also when trying to display the dataframe I am getting the below error
 

Can you please have a look and advise on the issue. 

Thanks
Muhammad



;
Cluster URL - https://adb-1851547605975220.0.azuredatabricks.net/login.html?o=1851547605975220#setting/clusters/0413-214226-ei6g26iz/configuration;
Notebook URL if available - ;
Additional details about the issue - ISSUE: Running databrocks notebook with some Longitude / Latitude values, when converted to decimals, the data is not written to the SQL Server MI and we are getting different errors. Also the same notebook was running until yesterday, but I started getting the below error (see attaced document for details) since this morning. 

 
 
Also when trying to display the dataframe I am getting the below error
 

Can you please have a look and advise on the issue. 

Thanks
Muhammad


OFFICIAL

;

Customer has uploaded a file. /?workspaceid=1e391fd2-0ba1-4011-a36e-df53651e9f59
- ProblemStartTime: 06/07/2022 04:43:00
- Cloud: Azure
- AzureProductSubscriptionID: 036ef1cc-dad3-4789-9057-be37d92a6507
- AzureProductSubscriptionName: protected-prd
- PUID: 100320019ECBB892
- Tenant Id: aa21b640-bac2-456d-8505-f2cc07f51784
- Object Id: 92a6a52f-8836-4c2c-9090-7e0aacb70943
- SubscriptionType: UnifiedEnterprise
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Unified Enterprise
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: True

- Location: australiacentral2
- ResourceUri: /subscriptions/036ef1cc-dad3-4789-9057-be37d92a6507/resourceGroups/radar-petl-p-auc2-rg/providers/Microsoft.Databricks/workspaces/prdpetl-databricks-workspace

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/kb/scala/decimal-is-fractional-error,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Decimal$DecimalIsFractional assertion error Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are running a job on Databricks Runtime 7.x or above when you get a java.lang.AssertionError: assertion failed: Decimal$DecimalIsFractional error message. Example stack trace: Console Copy java.lang.AssertionError: assertion failed:
 Decimal$DecimalIsFractional
  while compiling: <notebook>
   during phase: globalPhase=terminal, enteringPhase=jvm
  library version: version 2.12.10
 compiler version: version 2.12.10
reconstructed args: -deprecation -classpath .....
*** WARNING: skipped 126593 bytes of output ***
 This error message only occurs on the first run of your notebook. Subsequent runs complete without error. Cause There are two common use cases that can trigger this error message. Cause 1: You are trying to use the round() function on a decimal column that contains null values in a notebook. Cause 2: You are casting a double column to a decimal column in a notebook. This example code can be used to reproduce the error: Scala Copy import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.apache.spark.sql.Column

//Sample data with decimal values

val updateData =  Seq(
                     Row(BigDecimal.decimal(123.456), 123.456),
                     Row(BigDecimal.decimal(123.456), 123.456))

val updateSchema = List(
                    StructField(""amt_decimal"", DecimalType(14,3), true),
                    StructField(""amt_double"", DoubleType, true))

val testDF =  spark.createDataFrame(
  spark.sparkContext.parallelize(updateData),
  StructType(updateSchema)
)

// Cause 1:
// round() on the Decimal column reproduces the error

testDF.withColumn(""round_amt_decimal"",round(col(""amt_decimal""),2)).show()

// Cause 2:
// CAST() on the Double column to Decimal reproduces the error

testDF.createOrReplaceTempView(""dec_table"")
spark.sql(""select CAST(amt_double AS DECIMAL(3,3)) AS dec_col from dec_table"").show()
 Solution This is a known issue and can be safely ignored. The error message does not halt the notebook run and it should not result in any data loss.",Decimal$DecimalIsFractional assertion error
54,ARR | 2205210030000268 - ADLS Service principal based access in Databricks for Shell usage,"Customer is having a Databricks cluster in the 9.1 LTS runtime.

Their application team is not able to run the shell-based commands and script what they need to execute which is placed in ADLS gen 2.

They are using Credentials pass through authentication mechanism.

Got following links helpful in customer use case:

https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough#mount-dbfs

https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access

I am attaching all the steps and error customer is getting which could be helpful to understand further about the issue.

While investigating the issue, I found following helpful:

https://stackoverflow.com/questions/58996954/mount-error-when-trying-to-access-the-azure-dbfs-file-system-in-azure-databricks

https://kb.databricks.com/notebooks/failure-when-mounting-storage.html

https://docs.microsoft.com/en-us/azure/databricks/kb/notebooks/failure-when-mounting-storage

https://docs.microsoft.com/en-us/azure/databricks/data/databricks-file-system

Please assign this case to an IST engineer and kindly assist.",https://docs.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/service-principals,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Manage workspace-level service principals Article 07/07/2022 4 minutes to read 4 contributors In this article What is a service principal? Create a service principal Manage access tokens for a service principal Manage entitlements for a service principal Important This feature is in Public Preview. This article explains how to create and manage service principals for your workspaces. What is a service principal? A service principal is an identity created for use with automated tools, running jobs, and applications. You can restrict a service principal’s access to resources using permissions, in the same way as an Azure Databricks user. You can add entitlements to a service principal. You can add a service principal to a group, including the admins group. Unlike an Azure Databricks user, a service principal is an API-only identity; it cannot be used to access the Azure Databricks UI. For security reasons, Databricks recommends using service principals to give automated tools and scripts API-only access to Azure Databricks resources. Create a service principal To use service principals on Azure Databricks, an admin user must create a new Azure Active Directory (Azure AD) application and then add it to the Azure Databricks workspace to use as a service principal. Service principals cannot be created directly within Azure Databricks at this time. To create an Azure AD service principal, follow these instructions: Sign in to the Azure portal. Note The portal to use is different depending on whether your Azure AD application runs in the Azure public cloud or in a national or sovereign cloud. For more information, see National clouds. If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to provision the service principal. Search for and select Azure Active Directory. Within Manage, click App registrations > New registration. For Name, enter a name for the application. In the Supported account types section, select Accounts in this organizational directory only (Single tenant). Click Register. Within Manage, click Certificates & secrets. On the Client secrets tab, click New client secret. In the Add a client secret pane, for Description, enter a description for the client secret. For Expires, select an expiry time period for the client secret, and then click Add. Copy and store the client secret’s Value in a secure place, as this client secret is the password for your application. On the application page’s Overview page, in the Essentials section, copy the following values: Application (client) ID Directory (tenant) ID Finally, add the Azure Active Directory application to the Azure Databricks workspace by using the Add service principal endpoint of the SCIM API 2.0 (ServicePrincipals) API. Manage access tokens for a service principal To authenticate a service principal to APIs on Azure Databricks, an administrator can create an Azure AD access token on behalf of the service principal. The Azure AD access token can be used in place of a user’s Databricks Access Token to call Databricks REST APIs. Create an Azure AD access token by following these instructions: Gather the following information: Parameter Description Tenant ID The Directory (tenant) ID for the application registered in Azure AD. Client ID The Application (client) ID for the application registered in Azure AD. Client secret The Value of the client secret for the application registered in Azure AD. Use the preceding information along with curl to get the Azure AD access token. Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token \
-d 'client_id=<client-id>' \
-d 'grant_type=client_credentials' \
-d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default' \
-d 'client_secret=<client-secret>'
 Replace: <tenant-id> with the registered application’s tenant ID. <client-id> with the registered application’s client ID. <client-secret> with the registered application’s client secret value. Do not change the value of the scope parameter. It represents the programmatic ID for Azure Databricks (2ff814a6-3304-4ab8-85cb-cd0e6f879c1d) along with the default scope (/.default, URL-encoded as %2f.default). For example: Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/a1bc2d34-5e67-8f89-01ab-c2345d6c78de/oauth2/v2.0/token \
-d 'client_id=12a34b56-789c-0d12-e3fa-b456789c0123' \
-d 'grant_type=client_credentials' \
-d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default' \
-d 'client_secret=abc1D~Ef...2ghIJKlM3'
 The Azure AD access token is in the access_token value within the output of the call. Note It’s not possible to create, list, or manage a token for a service principal from within the Azure Databricks UI. Manage entitlements for a service principal An entitlement is a property that allows a user, service principal, or group to interact with Azure Databricks in a specified way. In the following table, each entitlement’s UI and API name is shown. Entitlement name (UI) Entitlement name (API) Default Description Workspace access workspace-access Granted by default. When granted to a user or service principal, they can access the Data Science & Engineering workspace and Databricks Machine Learning. Can’t be removed from workspace administrators. Databricks SQL access databricks-sql-access Granted by default. When granted to a user or service principal, they can access Databricks SQL. Allow unrestricted cluster creation allow-cluster-create Not granted to users or service principals by default. When granted to a user or service principal, they can create clusters. You can restrict access to existing clusters using cluster-level permissions. Can’t be removed from admin users. allow-instance-pool-create allow-instance-pool-create Can’t be granted to individual users or service principals. When granted to a group, its members can create instance pools. Can’t be removed from workspace administrators. To add or remove an entitlement for a service principal, use the SCIM API 2.0 (ServicePrincipals) API.",Manage workspace-level service principals
55,Databricks Workspace - Security Monitoring of misconfigurations,">>Cx has few queries on extending Databricks API support. Below are the cx queries:

1.	What you are saying is that the “Access Control” settings can’t be retrieved via Databricks API, right? If yes – do you have any plans to extend Databricks API to make it available?
 
2.	Do you have any plans to support Databricks API calls via Azure Policy engine in the future?

3.    Regarding the third one, let me expand on that a bit:
To call Databricks API we should pass authentication and authorization. According to the Security Requirements, we can’t use Personal Access Tokens – instead of that we should use Azure AD. So we have hundreds of the deployed Databricks Workspaces. Now we are thinking about scaling of the security monitoring based on Databricks API to all such instances. So the main question here: could we grant read-only permissions for a service principal to provide access to Databricks API of Workspaces with the least privilege principle? And if yes – could you share a guideline?

>>Please check this and let us know about this. Thanks in advance.",https://docs.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/service-principals,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Manage workspace-level service principals Article 07/07/2022 4 minutes to read 4 contributors In this article What is a service principal? Create a service principal Manage access tokens for a service principal Manage entitlements for a service principal Important This feature is in Public Preview. This article explains how to create and manage service principals for your workspaces. What is a service principal? A service principal is an identity created for use with automated tools, running jobs, and applications. You can restrict a service principal’s access to resources using permissions, in the same way as an Azure Databricks user. You can add entitlements to a service principal. You can add a service principal to a group, including the admins group. Unlike an Azure Databricks user, a service principal is an API-only identity; it cannot be used to access the Azure Databricks UI. For security reasons, Databricks recommends using service principals to give automated tools and scripts API-only access to Azure Databricks resources. Create a service principal To use service principals on Azure Databricks, an admin user must create a new Azure Active Directory (Azure AD) application and then add it to the Azure Databricks workspace to use as a service principal. Service principals cannot be created directly within Azure Databricks at this time. To create an Azure AD service principal, follow these instructions: Sign in to the Azure portal. Note The portal to use is different depending on whether your Azure AD application runs in the Azure public cloud or in a national or sovereign cloud. For more information, see National clouds. If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to provision the service principal. Search for and select Azure Active Directory. Within Manage, click App registrations > New registration. For Name, enter a name for the application. In the Supported account types section, select Accounts in this organizational directory only (Single tenant). Click Register. Within Manage, click Certificates & secrets. On the Client secrets tab, click New client secret. In the Add a client secret pane, for Description, enter a description for the client secret. For Expires, select an expiry time period for the client secret, and then click Add. Copy and store the client secret’s Value in a secure place, as this client secret is the password for your application. On the application page’s Overview page, in the Essentials section, copy the following values: Application (client) ID Directory (tenant) ID Finally, add the Azure Active Directory application to the Azure Databricks workspace by using the Add service principal endpoint of the SCIM API 2.0 (ServicePrincipals) API. Manage access tokens for a service principal To authenticate a service principal to APIs on Azure Databricks, an administrator can create an Azure AD access token on behalf of the service principal. The Azure AD access token can be used in place of a user’s Databricks Access Token to call Databricks REST APIs. Create an Azure AD access token by following these instructions: Gather the following information: Parameter Description Tenant ID The Directory (tenant) ID for the application registered in Azure AD. Client ID The Application (client) ID for the application registered in Azure AD. Client secret The Value of the client secret for the application registered in Azure AD. Use the preceding information along with curl to get the Azure AD access token. Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token \
-d 'client_id=<client-id>' \
-d 'grant_type=client_credentials' \
-d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default' \
-d 'client_secret=<client-secret>'
 Replace: <tenant-id> with the registered application’s tenant ID. <client-id> with the registered application’s client ID. <client-secret> with the registered application’s client secret value. Do not change the value of the scope parameter. It represents the programmatic ID for Azure Databricks (2ff814a6-3304-4ab8-85cb-cd0e6f879c1d) along with the default scope (/.default, URL-encoded as %2f.default). For example: Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/a1bc2d34-5e67-8f89-01ab-c2345d6c78de/oauth2/v2.0/token \
-d 'client_id=12a34b56-789c-0d12-e3fa-b456789c0123' \
-d 'grant_type=client_credentials' \
-d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default' \
-d 'client_secret=abc1D~Ef...2ghIJKlM3'
 The Azure AD access token is in the access_token value within the output of the call. Note It’s not possible to create, list, or manage a token for a service principal from within the Azure Databricks UI. Manage entitlements for a service principal An entitlement is a property that allows a user, service principal, or group to interact with Azure Databricks in a specified way. In the following table, each entitlement’s UI and API name is shown. Entitlement name (UI) Entitlement name (API) Default Description Workspace access workspace-access Granted by default. When granted to a user or service principal, they can access the Data Science & Engineering workspace and Databricks Machine Learning. Can’t be removed from workspace administrators. Databricks SQL access databricks-sql-access Granted by default. When granted to a user or service principal, they can access Databricks SQL. Allow unrestricted cluster creation allow-cluster-create Not granted to users or service principals by default. When granted to a user or service principal, they can create clusters. You can restrict access to existing clusters using cluster-level permissions. Can’t be removed from admin users. allow-instance-pool-create allow-instance-pool-create Can’t be granted to individual users or service principals. When granted to a group, its members can create instance pools. Can’t be removed from workspace administrators. To add or remove an entitlement for a service principal, use the SCIM API 2.0 (ServicePrincipals) API.",Manage workspace-level service principals
56,Best approach to read append blobs,"The customer is trying to read append blobs with spark, but as these files cannot be read directly through the API, they would like to know which is the best approach to achieve this.",https://docs.microsoft.com/en-us/azure/databricks/kb/data-sources/wasb-check-blob-types,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Unable to read files and list directories in a WASB filesystem Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem When you try reading a file on WASB with Spark, you get the following exception: Console Copy org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 19, 10.139.64.5, executor 0): shaded.databricks.org.apache.hadoop.fs.azure.AzureException: com.microsoft.azure.storage.StorageException: Incorrect Blob type, please use the correct Blob type to access a blob on the server. Expected BLOCK_BLOB, actual APPEND_BLOB.
 When you try listing files in WASB using dbutils.fs.ls or the Hadoop API, you get the following exception: Console Copy java.io.FileNotFoundException: File/<some-directory> does not exist.
 Cause The WASB filesystem supports three types of blobs: block, page, and append. Block blobs are optimized for upload of large blocks of data (the default in Hadoop). Page blobs are optimized for random read and write operations. Append blobs are optimized for append operations. See Understanding block blobs, append blobs, and page blobs for details. The errors described above occur if you try to read an append blob or list a directory that contains only append blobs. The Azure Databricks and Hadoop Azure WASB implementations do not support reading append blobs. Similarly when listing a directory, append blobs are ignored. There is no workaround to enable reading append blobs or listing a directory that contains only append blobs. However, you can use either Azure CLI or Azure Storage SDK for Python to identify if a directory contains append blobs or a file is an append blob. You can verify whether a directory contains append blobs by running the following Azure CLI command: PowerShell Copy az storage blob list \
  --auth-mode key \
  --account-name <account-name> \
  --container-name <container-name> \
  --prefix <path>
 The result is returned as a JSON document, in which you can easily find the blob type for each file. If directory is large, you can limit number of results with the flag --num-results <num>. You can also use Azure Storage SDK for Python to list and explore files in a WASB filesystem: Python Copy iter = service.list_blobs(""container"")
for blob in iter:
  if blob.properties.blob_type == ""AppendBlob"":
    print(""\t Blob name: %s, %s"" % (blob.name, blob.properties.blob_type))
 Azure Databricks does support accessing append blobs using the Hadoop API, but only when appending to a file. Solution There is no workaround for this issue. Use Azure CLI or Azure Storage SDK for Python to identify if the directory contains append blobs or the object is an append blob. You can implement either a Spark SQL UDF or custom function using RDD API to load, read, or convert blobs using Azure Storage SDK for Python.",Unable to read files and list directories in a WASB filesystem
57,[ARR] [Sev B] SR-2206130030001250 SQL endpoint throwing request not authorized to perform on delta table after re-mounting the mount point for the delta table.,"[Issue]
SQL endpoint throwing request not authorized to perform on delta table after re-mounting the mount point for the delta table.

Detailed steps:
#1 unmount 
#2 rotate key of storage account (The storage account is mounted through Access Key, only my way not the cx's. Cx's way of mounting is unknown)
#3 remount using new key
#4 create table using query
CREATE TABLE sample_3 USING CSV LOCATION '/mnt/tables/sample3.csv'
#5 error occurred 'hadoop_azure_shaded.com.microsoft.azure.storage.StorageException: Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature.'

The above error is reproduced in my environment, however, the ex's error message is a little different which is 'shaded.databricks.org.apache.hadoop.fs.azure.AzureException: hadoop_azure_shaded.com.microsoft.azure.storage.StorageException: This request is not authorized to perform this operation using this permission.'

The cx also mentioned error occurred after remounting. We are waiting for cx's reply on how they do remount.

After restarting the endpoint, mine and cx's error are both gone.

[ask]
Why the endpoint side doesn't take effect immediately after remount, it needs to restart the endpoint?",https://docs.microsoft.com/en-us/azure/databricks/kb/dbfs/remount-storage-after-rotate-access-key,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Remount a storage account after rotating access keys Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You have blob storage associated with a storage account mounted, but are unable to access it after access keys are rotated. Cause There are multiple mount points using the same storage account. Remounting some, but not all, of the mount points with new access keys results in access issues. Solution Use dbutils.fs.mounts() to check all mount points. Review the dbutils.fs.mounts() documentation for usage details. Use dbutils.fs.unmount() to unmount all storage accounts. Review the dbutils.fs.unmount() documentation for usage details. Restart the cluster. Remount the storage account with new keys. Review the mount an Azure Blob storage container documentation for usage details.",Remount a storage account after rotating access keys
58,GUI is not working properly,"Hello Team,

Greetings!

Issue: GUI is not working properly.

While using Databricks portal, the GUI for workspace is not working properly. Customer is not able to see any folder, notebook etc.

The issue is still ongoing.
They have provisioned Databricks on 14th June 2022 and GUI is not working since then.
GUI is not working properly in databricks portal, It did not work for me from begining.
When I access customer workspace from backend I am able to view notebooks and folders.
Tried checking in different browsers as well but din't work.
Collected har logs of the issue.
It is continuously in connecting and it doesn't show any folders or notebooks.

Thanks in advance.",https://docs.microsoft.com/en-us/azure/databricks/security/network/firewall-rules,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure domain name firewall rules Article 01/26/2022 2 minutes to read 3 contributors In this article Option 1: Allow traffic to *.azuredatabricks.net Option 2: Allow traffic to your Azure Databricks workspaces only If your corporate firewall blocks traffic based on domain names, you must allow HTTPS and WebSocket traffic to Azure Databricks domain names to ensure access to Azure Databricks resources. You can choose between two options, one more permissive but easier to configure, the other specific to your workspace domains. Option 1: Allow traffic to *.azuredatabricks.net Update your firewall rules to allow HTTPS and WebSocket traffic to *.azuredatabricks.net (or *.databricks.azure.us if your workspace is an Azure Government resource). This is more permissive than option 2, but it saves you the effort of updating firewall rules for each Azure Databricks workspace in your account. Option 2: Allow traffic to your Azure Databricks workspaces only If you choose to configure firewall rules for each workspace in your account, you must: Identify your workspace domains. Every Azure Databricks resource has two unique domain names. You can find the first by going to the Azure Databricks resource in the Azure Portal. The URL field displays a URL in the format https://adb-<digits>.<digits>.azuredatabricks.net, for example https://adb-1666506161514800.0.azuredatabricks.net. Remove https:// to get the first domain name. The second domain name is exactly the same as the first, except that it has an adb-dp- prefix instead of adb-. For example, if your first domain name is adb-1666506161514800.0.azuredatabricks.net, the second domain name is adb-dp-1666506161514800.0.azuredatabricks.net. Update your firewall rules. Update your firewall rules to allow HTTPS and WebSocket traffic to the two domains identified in step 1.",Configure domain name firewall rules
59,Error in Databricks while creating table or updating partitions,"Hello Team,

Issue: Error in Databricks while creating table or updating partitions.

Cx statement:

>>We are having an error while trying to update partitions or create new tables from databricks on a ADLS Gen2 location.

>>The workspace is DatabricksBITtst and the cluster it's processos-bi which is a cluster without passthrough authentication. Notebook with all process we are doing is already available in Notebook URL field. Authentication to ADLS is being made using the Dataframe/Dataset API

>>Error is:
com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: Failure to initialize configurationInvalid configuration value detected for fs.azure.account.key
at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:51)

My observation:

Suggested the Cx to add the below configuration in cluster configs:

spark.hadoop.fs.azure.account.auth.type.<storage name>.dfs.core.windows.net OAuth
spark.hadoop.fs.azure.account.oauth2.client.secret.<storage name>.dfs.core.windows.net {{secrets/<scope_name>/<secrets_name>}}
spark.hadoop.fs.azure.account.oauth2.client.id.<storage name>.dfs.core.windows.net <application ID>
spark.hadoop.fs.azure.account.oauth2.client.endpoint.<storage name>.dfs.core.windows.net https://login.microsoftonline.com/<tenant id>/oauth2/token
spark.hadoop.fs.azure.account.oauth.provider.type.<storage name>.dfs.core.windows.net org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider.

>>After adding these configs also Cx facing the below problem. 

Error in SQL statement: AbfsRestOperationException: HTTP Error 400; url='https://login.microsoftonline.com/[REDACTED]/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/[REDACTED]/oauth2/token failed for getting token from AzureAD.; requestId='08a388db-81b3-4903-8a5d-714bfdf92f00'; contentType='application/json; charset=utf-8'; response '{""error"":""unauthorized_client"",""error_description"":""AADSTS700016: Application with identifier '[REDACTED]' was not found in the directory 'SONAE'. This can happen if the application has not been installed by the administrator of the tenant or consented to by any user in the tenant. You may have sent your authentication request to the wrong tenant.\r\nTrace ID: 08a388db-81b3-4903-8a5d-714bfdf92f00\r\nCorrelation ID: 29582cd0-9dd6-411c-aedc-d11d3d4e4291\r\nTimestamp: 2022-06-15 12:12:04Z"",""error_codes"":[700016],""timestamp"":""2022-06-15 12:12:04Z"",""trace_id"":""08a388db-81b3-4903-8a5d-714bfdf92f00"",""correlation_id"":""29582cd0-9dd6-411c-aedc-d11d3d4e4291"",""error_uri"":""https://login.microsoftonline.com/error?code=700016""}'

>>Went on a call with Cx  to check  if the tenant ID, service principal ID and secret is correct.

 URL:  https://adb-3566258379046974.14.azuredatabricks.net/?o=3566258379046974#job/363958141796404/run/770612

Thank you",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/aad/service-prin-aad-token,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Get Azure AD tokens by using a service principal Article 04/29/2022 7 minutes to read 4 contributors In this article Provision a service principal in Azure portal Get an Azure AD access token Use the service principal’s Azure AD access token to access the Databricks REST API This article describes how a service principal defined in Azure Active Directory (Azure AD) can also act as a principal on which authentication and authorization policies can be enforced in Azure Databricks. Service principals in an Azure Databricks workspace can have different fine-grained access control than regular users (user principals). A service principal acts as a client role and uses the OAuth 2.0 client credentials flow to authorize access to Azure Databricks resources. You can manage service principals by using the Databricks SCIM API 2.0 (ServicePrincipals) API or by using the following procedure from the Azure portal. You can also use the Microsoft Authentication Library (MSAL) to programmatically get an Azure AD access token for a user instead of a service principal. See Get Azure AD tokens by using the Microsoft Authentication Library. Provision a service principal in Azure portal Sign in to the Azure portal. Note The portal to use is different depending on whether your Azure AD application runs in the Azure public cloud or in a national or sovereign cloud. For more information, see National clouds. If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to provision the service principal. Search for and select Azure Active Directory. Within Manage, click App registrations > New registration. For Name, enter a name for the application. In the Supported account types section, select Accounts in this organizational directory only (Single tenant). Click Register. Within Manage, click Certificates & secrets. On the Client secrets tab, click New client secret. In the Add a client secret pane, for Description, enter a description for the client secret. For Expires, select an expiry time period for the client secret, and then click Add. Copy and store the client secret’s Value in a secure place, as this client secret is the password for your application. On the application page’s Overview page, in the Essentials section, copy the following values: Application (client) ID Directory (tenant) ID Get an Azure AD access token To access the Databricks REST API with the service principal, you get and then use an Azure AD access token for the service principal. For more information, see First case: Access token request with a shared secret. Gather the following information: Parameter Description Tenant ID The Directory (tenant) ID for the application registered in Azure AD. Client ID The Application (client) ID for the application registered in Azure AD. Client secret The Value of the client secret for the application registered in Azure AD. Use the preceding information along with curl to get the Azure AD access token. Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token \
-d 'client_id=<client-id>' \
-d 'grant_type=client_credentials' \
-d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default' \
-d 'client_secret=<client-secret>'
 Replace: <tenant-id> with the registered application’s tenant ID. <client-id> with the registered application’s client ID. <client-secret> with the registered application’s client secret value. Do not change the value of the scope parameter. It represents the programmatic ID for Azure Databricks (2ff814a6-3304-4ab8-85cb-cd0e6f879c1d) along with the default scope (/.default, URL-encoded as %2f.default). For example: Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/a1bc2d34-5e67-8f89-01ab-c2345d6c78de/oauth2/v2.0/token \
-d 'client_id=12a34b56-789c-0d12-e3fa-b456789c0123' \
-d 'grant_type=client_credentials' \
-d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default' \
-d 'client_secret=abc1D~Ef...2ghIJKlM3'
 The Azure AD access token is in the access_token value within the output of the call. Use the service principal’s Azure AD access token to access the Databricks REST API The approach you take to use the service principal and the Azure AD access token to access the Databricks REST API depends on whether the service principal is added to the target Azure Databricks workspace and whether the service principal has admin access to the workspace. API access for service principals that are Azure Databricks workspace users and admins To complete this procedure, you must first add the service principal to the Azure Databricks workspace by using the Add service principal endpoint, for example: Bash Copy curl -X POST \
-H 'Authorization: Bearer dapi1234...ab123c45' \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/preview/scim/v2/ServicePrincipals \
-H 'Content-type: application/scim+json' \
-d @create-service-principal.json
 create-service-principal.json: JSON Copy {
  ""displayName"": ""My Service Principal"",
  ""applicationId"": ""12a34b56-789c-0d12-e3fa-b456789c0123"",
  ""entitlements"": [
    {
      ""value"": ""allow-cluster-create""
    }
  ],
  ""schemas"": [
    ""urn:ietf:params:scim:schemas:core:2.0:ServicePrincipal""
  ],
  ""active"": true
}
 Skip ahead to API access for service principals that are not workspace users if either of the following are true: You do not want to add the service principal to the Azure Databricks workspace. The service principal has been added to the Azure Databricks workspace; however, the Databricks REST API that you want to call requires admin access, and the service principal does not currently have admin access to the workspace. Gather the following information. Parameter Description Azure AD access token The Azure AD access token returned from the request in Get an Azure AD access token. Use the Azure AD access token along with curl to call the Databricks REST API. For example: Bash Copy curl -X GET \
-H 'Authorization: Bearer <access-token>' \
https://<databricks-instance>/api/2.0/clusters/list
 Replace: <access-token> with the Azure AD access token. <databricks-instance> with the per-workspace URL of your Azure Databricks deployment. GET and /api/2.0/clusters/list with the appropriate HTTP operation and endpoint for the target Databricks REST API. For example: Bash Copy curl -X GET \
-H 'Authorization: Bearer abC1dE...fgHi23jk' \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list
 API access for service principals that are not workspace users Follow this procedure if either of the following are true: The service principal has not already been added to the target Azure Databricks workspace by using the Add service principal endpoint. The service principal has been added to the Azure Databricks workspace; however, the Databricks REST API that you want to call requires admin access, and the service principal does not currently have admin access to the workspace. Otherwise, skip back to API access for service principals that are Azure Databricks workspace users and admins. Make sure that the service principal is assigned the Contributor or Owner role on the target workspace resource in Azure. See Assign Azure roles using the Azure portal. To get to this resource, see Open resources. To open the target resource, you can search on the Azure Databricks service type and any other information in Azure that you know about the target Azure Databricks workspace. Gather the following information: Parameter Description Tenant ID The Directory (tenant) ID for the application registered in Azure AD in Provision a service principal in Azure portal. Client ID The Application (client) ID for the application registered in Azure AD. Client secret The Value of the client secret for the application registered in Azure AD, which you created in Provision a service principal in Azure portal. Azure AD access token The Azure AD access token returned from the request in Get an Azure AD access token. Subscription ID The ID (not the name) of the Azure subscription that is associated with the target Azure Databricks workspace. To get to this and the following information, see Open resources. To open the target resource, you can search on the Azure Databricks service type and any other information in Azure that you know about the target Azure Databricks workspace. Resource group name The name of the Azure resource group that is associated with the target Azure Databricks workspace. Workspace name The name in Azure of the target Azure Databricks workspace. Use some of the preceding information along with curl to get an Azure AD management endpoint access token. Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/<tenant-id>/oauth2/token \
-d 'client_id=<client-id>' \
-d 'grant_type=client_credentials' \
-d 'resource=https%3A%2F%2Fmanagement.core.windows.net%2F' \
-d 'client_secret=<client-secret>'
 Replace: <tenant-id> with the registered application’s tenant ID. <client-id> with the registered application’s client ID. <client-secret> with the registered application’s client secret value. Do not change the value of the resource parameter. It represents the Azure AD management endpoint (https://management.core.windows.net/, URL-encoded as https%3A%2F%2Fmanagement.core.windows.net%2F). For example: Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/a1bc2d34-5e67-8f89-01ab-c2345d6c78de/oauth2/token \
-d 'client_id=12a34b56-789c-0d12-e3fa-b456789c0123' \
-d 'grant_type=client_credentials' \
-d 'resource=https%3A%2F%2Fmanagement.core.windows.net%2F' \
-d 'client_secret=abc1D~Ef...2ghIJKlM3'
 The Azure AD management endpoint access token is in the access_token value within the output of the call. Use the Azure AD management endpoint access token along with the rest of the preceding information and curl to call the Databricks REST API, for example: Bash Copy curl -X GET \
-H 'Authorization: Bearer <access-token>' \
-H 'X-Databricks-Azure-SP-Management-Token: <management-access-token>' \
-H 'X-Databricks-Azure-Workspace-Resource-Id: /subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.Databricks/workspaces/<workspace-name>' \
https://<databricks-instance>/api/2.0/clusters/list
 Replace: <access-token> with the Azure AD access token. <management-access-token> with the Azure AD management endpoint access token. <subscription-id> with the ID of the subscription that is associated with the target Azure Databricks workspace. <resource-group-name> with the name of the resource group that is associated with the target Azure Databricks workspace. <workspace-name> with the name of the target Azure Databricks workspace. <databricks-instance> with the per-workspace URL of your Azure Databricks deployment. GET and /api/2.0/clusters/list with the appropriate HTTP operation and endpoint for the target Databricks REST API. For example: Bash Copy curl -X GET \
-H 'Authorization:Bearer abC1dE...fgHi23jk' \
-H 'X-Databricks-Azure-SP-Management-Token: abC1dE...ghIj23kl' \
-H 'X-Databricks-Azure-Workspace-Resource-Id: /subscriptions/12a345...bcd6789e/resourceGroups/my-resource-group/providers/Microsoft.Databricks/workspaces/my-workspace' \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list",Get Azure AD tokens by using a service principal
60,Error in Databricks while creating table or updating partitions,"Hello Team,

Issue: Error in Databricks while creating table or updating partitions.

Cx statement:

>>We are having an error while trying to update partitions or create new tables from databricks on a ADLS Gen2 location.

>>The workspace is DatabricksBITtst and the cluster it's processos-bi which is a cluster without passthrough authentication. Notebook with all process we are doing is already available in Notebook URL field. Authentication to ADLS is being made using the Dataframe/Dataset API

>>Error is:
com.databricks.backend.common.rpc.DatabricksExceptions$SQLExecutionException: Failure to initialize configurationInvalid configuration value detected for fs.azure.account.key
at shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.services.SimpleKeyProvider.getStorageAccountKey(SimpleKeyProvider.java:51)

My observation:

Suggested the Cx to add the below configuration in cluster configs:

spark.hadoop.fs.azure.account.auth.type.<storage name>.dfs.core.windows.net OAuth
spark.hadoop.fs.azure.account.oauth2.client.secret.<storage name>.dfs.core.windows.net {{secrets/<scope_name>/<secrets_name>}}
spark.hadoop.fs.azure.account.oauth2.client.id.<storage name>.dfs.core.windows.net <application ID>
spark.hadoop.fs.azure.account.oauth2.client.endpoint.<storage name>.dfs.core.windows.net https://login.microsoftonline.com/<tenant id>/oauth2/token
spark.hadoop.fs.azure.account.oauth.provider.type.<storage name>.dfs.core.windows.net org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider.

>>After adding these configs also Cx facing the below problem. 

Error in SQL statement: AbfsRestOperationException: HTTP Error 400; url='https://login.microsoftonline.com/[REDACTED]/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/[REDACTED]/oauth2/token failed for getting token from AzureAD.; requestId='08a388db-81b3-4903-8a5d-714bfdf92f00'; contentType='application/json; charset=utf-8'; response '{""error"":""unauthorized_client"",""error_description"":""AADSTS700016: Application with identifier '[REDACTED]' was not found in the directory 'SONAE'. This can happen if the application has not been installed by the administrator of the tenant or consented to by any user in the tenant. You may have sent your authentication request to the wrong tenant.\r\nTrace ID: 08a388db-81b3-4903-8a5d-714bfdf92f00\r\nCorrelation ID: 29582cd0-9dd6-411c-aedc-d11d3d4e4291\r\nTimestamp: 2022-06-15 12:12:04Z"",""error_codes"":[700016],""timestamp"":""2022-06-15 12:12:04Z"",""trace_id"":""08a388db-81b3-4903-8a5d-714bfdf92f00"",""correlation_id"":""29582cd0-9dd6-411c-aedc-d11d3d4e4291"",""error_uri"":""https://login.microsoftonline.com/error?code=700016""}'

>>Went on a call with Cx  to check  if the tenant ID, service principal ID and secret is correct.

 URL:  https://adb-3566258379046974.14.azuredatabricks.net/?o=3566258379046974#job/363958141796404/run/770612

Thank you",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/aad/service-prin-aad-token,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Get Azure AD tokens by using a service principal Article 04/29/2022 7 minutes to read 4 contributors In this article Provision a service principal in Azure portal Get an Azure AD access token Use the service principal’s Azure AD access token to access the Databricks REST API This article describes how a service principal defined in Azure Active Directory (Azure AD) can also act as a principal on which authentication and authorization policies can be enforced in Azure Databricks. Service principals in an Azure Databricks workspace can have different fine-grained access control than regular users (user principals). A service principal acts as a client role and uses the OAuth 2.0 client credentials flow to authorize access to Azure Databricks resources. You can manage service principals by using the Databricks SCIM API 2.0 (ServicePrincipals) API or by using the following procedure from the Azure portal. You can also use the Microsoft Authentication Library (MSAL) to programmatically get an Azure AD access token for a user instead of a service principal. See Get Azure AD tokens by using the Microsoft Authentication Library. Provision a service principal in Azure portal Sign in to the Azure portal. Note The portal to use is different depending on whether your Azure AD application runs in the Azure public cloud or in a national or sovereign cloud. For more information, see National clouds. If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to provision the service principal. Search for and select Azure Active Directory. Within Manage, click App registrations > New registration. For Name, enter a name for the application. In the Supported account types section, select Accounts in this organizational directory only (Single tenant). Click Register. Within Manage, click Certificates & secrets. On the Client secrets tab, click New client secret. In the Add a client secret pane, for Description, enter a description for the client secret. For Expires, select an expiry time period for the client secret, and then click Add. Copy and store the client secret’s Value in a secure place, as this client secret is the password for your application. On the application page’s Overview page, in the Essentials section, copy the following values: Application (client) ID Directory (tenant) ID Get an Azure AD access token To access the Databricks REST API with the service principal, you get and then use an Azure AD access token for the service principal. For more information, see First case: Access token request with a shared secret. Gather the following information: Parameter Description Tenant ID The Directory (tenant) ID for the application registered in Azure AD. Client ID The Application (client) ID for the application registered in Azure AD. Client secret The Value of the client secret for the application registered in Azure AD. Use the preceding information along with curl to get the Azure AD access token. Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token \
-d 'client_id=<client-id>' \
-d 'grant_type=client_credentials' \
-d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default' \
-d 'client_secret=<client-secret>'
 Replace: <tenant-id> with the registered application’s tenant ID. <client-id> with the registered application’s client ID. <client-secret> with the registered application’s client secret value. Do not change the value of the scope parameter. It represents the programmatic ID for Azure Databricks (2ff814a6-3304-4ab8-85cb-cd0e6f879c1d) along with the default scope (/.default, URL-encoded as %2f.default). For example: Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/a1bc2d34-5e67-8f89-01ab-c2345d6c78de/oauth2/v2.0/token \
-d 'client_id=12a34b56-789c-0d12-e3fa-b456789c0123' \
-d 'grant_type=client_credentials' \
-d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default' \
-d 'client_secret=abc1D~Ef...2ghIJKlM3'
 The Azure AD access token is in the access_token value within the output of the call. Use the service principal’s Azure AD access token to access the Databricks REST API The approach you take to use the service principal and the Azure AD access token to access the Databricks REST API depends on whether the service principal is added to the target Azure Databricks workspace and whether the service principal has admin access to the workspace. API access for service principals that are Azure Databricks workspace users and admins To complete this procedure, you must first add the service principal to the Azure Databricks workspace by using the Add service principal endpoint, for example: Bash Copy curl -X POST \
-H 'Authorization: Bearer dapi1234...ab123c45' \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/preview/scim/v2/ServicePrincipals \
-H 'Content-type: application/scim+json' \
-d @create-service-principal.json
 create-service-principal.json: JSON Copy {
  ""displayName"": ""My Service Principal"",
  ""applicationId"": ""12a34b56-789c-0d12-e3fa-b456789c0123"",
  ""entitlements"": [
    {
      ""value"": ""allow-cluster-create""
    }
  ],
  ""schemas"": [
    ""urn:ietf:params:scim:schemas:core:2.0:ServicePrincipal""
  ],
  ""active"": true
}
 Skip ahead to API access for service principals that are not workspace users if either of the following are true: You do not want to add the service principal to the Azure Databricks workspace. The service principal has been added to the Azure Databricks workspace; however, the Databricks REST API that you want to call requires admin access, and the service principal does not currently have admin access to the workspace. Gather the following information. Parameter Description Azure AD access token The Azure AD access token returned from the request in Get an Azure AD access token. Use the Azure AD access token along with curl to call the Databricks REST API. For example: Bash Copy curl -X GET \
-H 'Authorization: Bearer <access-token>' \
https://<databricks-instance>/api/2.0/clusters/list
 Replace: <access-token> with the Azure AD access token. <databricks-instance> with the per-workspace URL of your Azure Databricks deployment. GET and /api/2.0/clusters/list with the appropriate HTTP operation and endpoint for the target Databricks REST API. For example: Bash Copy curl -X GET \
-H 'Authorization: Bearer abC1dE...fgHi23jk' \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list
 API access for service principals that are not workspace users Follow this procedure if either of the following are true: The service principal has not already been added to the target Azure Databricks workspace by using the Add service principal endpoint. The service principal has been added to the Azure Databricks workspace; however, the Databricks REST API that you want to call requires admin access, and the service principal does not currently have admin access to the workspace. Otherwise, skip back to API access for service principals that are Azure Databricks workspace users and admins. Make sure that the service principal is assigned the Contributor or Owner role on the target workspace resource in Azure. See Assign Azure roles using the Azure portal. To get to this resource, see Open resources. To open the target resource, you can search on the Azure Databricks service type and any other information in Azure that you know about the target Azure Databricks workspace. Gather the following information: Parameter Description Tenant ID The Directory (tenant) ID for the application registered in Azure AD in Provision a service principal in Azure portal. Client ID The Application (client) ID for the application registered in Azure AD. Client secret The Value of the client secret for the application registered in Azure AD, which you created in Provision a service principal in Azure portal. Azure AD access token The Azure AD access token returned from the request in Get an Azure AD access token. Subscription ID The ID (not the name) of the Azure subscription that is associated with the target Azure Databricks workspace. To get to this and the following information, see Open resources. To open the target resource, you can search on the Azure Databricks service type and any other information in Azure that you know about the target Azure Databricks workspace. Resource group name The name of the Azure resource group that is associated with the target Azure Databricks workspace. Workspace name The name in Azure of the target Azure Databricks workspace. Use some of the preceding information along with curl to get an Azure AD management endpoint access token. Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/<tenant-id>/oauth2/token \
-d 'client_id=<client-id>' \
-d 'grant_type=client_credentials' \
-d 'resource=https%3A%2F%2Fmanagement.core.windows.net%2F' \
-d 'client_secret=<client-secret>'
 Replace: <tenant-id> with the registered application’s tenant ID. <client-id> with the registered application’s client ID. <client-secret> with the registered application’s client secret value. Do not change the value of the resource parameter. It represents the Azure AD management endpoint (https://management.core.windows.net/, URL-encoded as https%3A%2F%2Fmanagement.core.windows.net%2F). For example: Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/a1bc2d34-5e67-8f89-01ab-c2345d6c78de/oauth2/token \
-d 'client_id=12a34b56-789c-0d12-e3fa-b456789c0123' \
-d 'grant_type=client_credentials' \
-d 'resource=https%3A%2F%2Fmanagement.core.windows.net%2F' \
-d 'client_secret=abc1D~Ef...2ghIJKlM3'
 The Azure AD management endpoint access token is in the access_token value within the output of the call. Use the Azure AD management endpoint access token along with the rest of the preceding information and curl to call the Databricks REST API, for example: Bash Copy curl -X GET \
-H 'Authorization: Bearer <access-token>' \
-H 'X-Databricks-Azure-SP-Management-Token: <management-access-token>' \
-H 'X-Databricks-Azure-Workspace-Resource-Id: /subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.Databricks/workspaces/<workspace-name>' \
https://<databricks-instance>/api/2.0/clusters/list
 Replace: <access-token> with the Azure AD access token. <management-access-token> with the Azure AD management endpoint access token. <subscription-id> with the ID of the subscription that is associated with the target Azure Databricks workspace. <resource-group-name> with the name of the resource group that is associated with the target Azure Databricks workspace. <workspace-name> with the name of the target Azure Databricks workspace. <databricks-instance> with the per-workspace URL of your Azure Databricks deployment. GET and /api/2.0/clusters/list with the appropriate HTTP operation and endpoint for the target Databricks REST API. For example: Bash Copy curl -X GET \
-H 'Authorization:Bearer abC1dE...fgHi23jk' \
-H 'X-Databricks-Azure-SP-Management-Token: abC1dE...ghIj23kl' \
-H 'X-Databricks-Azure-Workspace-Resource-Id: /subscriptions/12a345...bcd6789e/resourceGroups/my-resource-group/providers/Microsoft.Databricks/workspaces/my-workspace' \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list",Get Azure AD tokens by using a service principal
61,Follow-up of SF: 00158271,">> CX wanted to enable Unity Catalog.
>> It is enabled.
>> When cx is trying to create a metastore they are facing the below error :

2606.a7bef970.chunk.js:1 GET https://adb-1101798370485247.7.azuredatabricks.net/ajax-api/2.0/unity-catalog/catalogs 404
(anonymous) @ 2606.a7bef970.chunk.js:1
e.exports @ 2606.a7bef970.chunk.js:1
e.exports @ 2606.a7bef970.chunk.js:1
Promise.then (async)
s.request @ 2606.a7bef970.chunk.js:1
i.forEach.s.<computed> @ 2606.a7bef970.chunk.js:1
(anonymous) @ 2606.a7bef970.chunk.js:1
(anonymous) @ 2828.8b3bdbe2.chunk.js:1
(anonymous) @ 2828.8b3bdbe2.chunk.js:1
Ui @ 7043.2d6bf56d.chunk.js:1
n.unstable_runWithPriority @ 7043.2d6bf56d.chunk.js:1
$l @ 7043.2d6bf56d.chunk.js:1
Di @ 7043.2d6bf56d.chunk.js:1
(anonymous) @ 7043.2d6bf56d.chunk.js:1
F @ 7043.2d6bf56d.chunk.js:1
w.port1.onmessage @ 7043.2d6bf56d.chunk.js:1

>> They referred to the below documentation : https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/get-started

Kindly provide some insights on this.",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/aad/service-prin-aad-token,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Get Azure AD tokens by using a service principal Article 04/29/2022 7 minutes to read 4 contributors In this article Provision a service principal in Azure portal Get an Azure AD access token Use the service principal’s Azure AD access token to access the Databricks REST API This article describes how a service principal defined in Azure Active Directory (Azure AD) can also act as a principal on which authentication and authorization policies can be enforced in Azure Databricks. Service principals in an Azure Databricks workspace can have different fine-grained access control than regular users (user principals). A service principal acts as a client role and uses the OAuth 2.0 client credentials flow to authorize access to Azure Databricks resources. You can manage service principals by using the Databricks SCIM API 2.0 (ServicePrincipals) API or by using the following procedure from the Azure portal. You can also use the Microsoft Authentication Library (MSAL) to programmatically get an Azure AD access token for a user instead of a service principal. See Get Azure AD tokens by using the Microsoft Authentication Library. Provision a service principal in Azure portal Sign in to the Azure portal. Note The portal to use is different depending on whether your Azure AD application runs in the Azure public cloud or in a national or sovereign cloud. For more information, see National clouds. If you have access to multiple tenants, subscriptions, or directories, click the Directories + subscriptions (directory with filter) icon in the top menu to switch to the directory in which you want to provision the service principal. Search for and select Azure Active Directory. Within Manage, click App registrations > New registration. For Name, enter a name for the application. In the Supported account types section, select Accounts in this organizational directory only (Single tenant). Click Register. Within Manage, click Certificates & secrets. On the Client secrets tab, click New client secret. In the Add a client secret pane, for Description, enter a description for the client secret. For Expires, select an expiry time period for the client secret, and then click Add. Copy and store the client secret’s Value in a secure place, as this client secret is the password for your application. On the application page’s Overview page, in the Essentials section, copy the following values: Application (client) ID Directory (tenant) ID Get an Azure AD access token To access the Databricks REST API with the service principal, you get and then use an Azure AD access token for the service principal. For more information, see First case: Access token request with a shared secret. Gather the following information: Parameter Description Tenant ID The Directory (tenant) ID for the application registered in Azure AD. Client ID The Application (client) ID for the application registered in Azure AD. Client secret The Value of the client secret for the application registered in Azure AD. Use the preceding information along with curl to get the Azure AD access token. Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token \
-d 'client_id=<client-id>' \
-d 'grant_type=client_credentials' \
-d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default' \
-d 'client_secret=<client-secret>'
 Replace: <tenant-id> with the registered application’s tenant ID. <client-id> with the registered application’s client ID. <client-secret> with the registered application’s client secret value. Do not change the value of the scope parameter. It represents the programmatic ID for Azure Databricks (2ff814a6-3304-4ab8-85cb-cd0e6f879c1d) along with the default scope (/.default, URL-encoded as %2f.default). For example: Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/a1bc2d34-5e67-8f89-01ab-c2345d6c78de/oauth2/v2.0/token \
-d 'client_id=12a34b56-789c-0d12-e3fa-b456789c0123' \
-d 'grant_type=client_credentials' \
-d 'scope=2ff814a6-3304-4ab8-85cb-cd0e6f879c1d%2F.default' \
-d 'client_secret=abc1D~Ef...2ghIJKlM3'
 The Azure AD access token is in the access_token value within the output of the call. Use the service principal’s Azure AD access token to access the Databricks REST API The approach you take to use the service principal and the Azure AD access token to access the Databricks REST API depends on whether the service principal is added to the target Azure Databricks workspace and whether the service principal has admin access to the workspace. API access for service principals that are Azure Databricks workspace users and admins To complete this procedure, you must first add the service principal to the Azure Databricks workspace by using the Add service principal endpoint, for example: Bash Copy curl -X POST \
-H 'Authorization: Bearer dapi1234...ab123c45' \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/preview/scim/v2/ServicePrincipals \
-H 'Content-type: application/scim+json' \
-d @create-service-principal.json
 create-service-principal.json: JSON Copy {
  ""displayName"": ""My Service Principal"",
  ""applicationId"": ""12a34b56-789c-0d12-e3fa-b456789c0123"",
  ""entitlements"": [
    {
      ""value"": ""allow-cluster-create""
    }
  ],
  ""schemas"": [
    ""urn:ietf:params:scim:schemas:core:2.0:ServicePrincipal""
  ],
  ""active"": true
}
 Skip ahead to API access for service principals that are not workspace users if either of the following are true: You do not want to add the service principal to the Azure Databricks workspace. The service principal has been added to the Azure Databricks workspace; however, the Databricks REST API that you want to call requires admin access, and the service principal does not currently have admin access to the workspace. Gather the following information. Parameter Description Azure AD access token The Azure AD access token returned from the request in Get an Azure AD access token. Use the Azure AD access token along with curl to call the Databricks REST API. For example: Bash Copy curl -X GET \
-H 'Authorization: Bearer <access-token>' \
https://<databricks-instance>/api/2.0/clusters/list
 Replace: <access-token> with the Azure AD access token. <databricks-instance> with the per-workspace URL of your Azure Databricks deployment. GET and /api/2.0/clusters/list with the appropriate HTTP operation and endpoint for the target Databricks REST API. For example: Bash Copy curl -X GET \
-H 'Authorization: Bearer abC1dE...fgHi23jk' \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list
 API access for service principals that are not workspace users Follow this procedure if either of the following are true: The service principal has not already been added to the target Azure Databricks workspace by using the Add service principal endpoint. The service principal has been added to the Azure Databricks workspace; however, the Databricks REST API that you want to call requires admin access, and the service principal does not currently have admin access to the workspace. Otherwise, skip back to API access for service principals that are Azure Databricks workspace users and admins. Make sure that the service principal is assigned the Contributor or Owner role on the target workspace resource in Azure. See Assign Azure roles using the Azure portal. To get to this resource, see Open resources. To open the target resource, you can search on the Azure Databricks service type and any other information in Azure that you know about the target Azure Databricks workspace. Gather the following information: Parameter Description Tenant ID The Directory (tenant) ID for the application registered in Azure AD in Provision a service principal in Azure portal. Client ID The Application (client) ID for the application registered in Azure AD. Client secret The Value of the client secret for the application registered in Azure AD, which you created in Provision a service principal in Azure portal. Azure AD access token The Azure AD access token returned from the request in Get an Azure AD access token. Subscription ID The ID (not the name) of the Azure subscription that is associated with the target Azure Databricks workspace. To get to this and the following information, see Open resources. To open the target resource, you can search on the Azure Databricks service type and any other information in Azure that you know about the target Azure Databricks workspace. Resource group name The name of the Azure resource group that is associated with the target Azure Databricks workspace. Workspace name The name in Azure of the target Azure Databricks workspace. Use some of the preceding information along with curl to get an Azure AD management endpoint access token. Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/<tenant-id>/oauth2/token \
-d 'client_id=<client-id>' \
-d 'grant_type=client_credentials' \
-d 'resource=https%3A%2F%2Fmanagement.core.windows.net%2F' \
-d 'client_secret=<client-secret>'
 Replace: <tenant-id> with the registered application’s tenant ID. <client-id> with the registered application’s client ID. <client-secret> with the registered application’s client secret value. Do not change the value of the resource parameter. It represents the Azure AD management endpoint (https://management.core.windows.net/, URL-encoded as https%3A%2F%2Fmanagement.core.windows.net%2F). For example: Bash Copy curl -X POST -H 'Content-Type: application/x-www-form-urlencoded' \
https://login.microsoftonline.com/a1bc2d34-5e67-8f89-01ab-c2345d6c78de/oauth2/token \
-d 'client_id=12a34b56-789c-0d12-e3fa-b456789c0123' \
-d 'grant_type=client_credentials' \
-d 'resource=https%3A%2F%2Fmanagement.core.windows.net%2F' \
-d 'client_secret=abc1D~Ef...2ghIJKlM3'
 The Azure AD management endpoint access token is in the access_token value within the output of the call. Use the Azure AD management endpoint access token along with the rest of the preceding information and curl to call the Databricks REST API, for example: Bash Copy curl -X GET \
-H 'Authorization: Bearer <access-token>' \
-H 'X-Databricks-Azure-SP-Management-Token: <management-access-token>' \
-H 'X-Databricks-Azure-Workspace-Resource-Id: /subscriptions/<subscription-id>/resourceGroups/<resource-group-name>/providers/Microsoft.Databricks/workspaces/<workspace-name>' \
https://<databricks-instance>/api/2.0/clusters/list
 Replace: <access-token> with the Azure AD access token. <management-access-token> with the Azure AD management endpoint access token. <subscription-id> with the ID of the subscription that is associated with the target Azure Databricks workspace. <resource-group-name> with the name of the resource group that is associated with the target Azure Databricks workspace. <workspace-name> with the name of the target Azure Databricks workspace. <databricks-instance> with the per-workspace URL of your Azure Databricks deployment. GET and /api/2.0/clusters/list with the appropriate HTTP operation and endpoint for the target Databricks REST API. For example: Bash Copy curl -X GET \
-H 'Authorization:Bearer abC1dE...fgHi23jk' \
-H 'X-Databricks-Azure-SP-Management-Token: abC1dE...ghIj23kl' \
-H 'X-Databricks-Azure-Workspace-Resource-Id: /subscriptions/12a345...bcd6789e/resourceGroups/my-resource-group/providers/Microsoft.Databricks/workspaces/my-workspace' \
https://adb-1234567890123456.7.azuredatabricks.net/api/2.0/clusters/list",Get Azure AD tokens by using a service principal
62,Databricks Secrets are not redacted in all cluster,"We have created a secret scope in databricks. We are using this secret in diff cluster.
1. Redact secrets in all cluster. 
 In one cluster the secret: password is redacted and safe. In another cluster is exposed and not safe. Could you please look into this concern and suggest to make this secret Redacted in cluster.
2. Set Access limitations to these secrets. For example a set of people shall only able to access this secret.",https://docs.gcp.databricks.com/security/access-control/secret-acl.html,databricks_gcp_docs,gcp_docs,"Secret access control Note This article mentions the CLI, which is not available in this release of Databricks on Google Cloud. For secrets you can also use the Secrets API 2.0. By default, all users in all pricing plans can create secrets and secret scopes. Using secret access control, available with the Databricks Premium Plan, you can configure fine-grained permissions for managing access control. This guide describes how to set up these controls. Note This article describes how to manage secret access control using the Databricks CLI (version 0.7.1 and above). Alternatively, you can use the Secrets API 2.0. Secret access control Access control for secrets is managed at the secret scope level. An access control list (ACL) defines a relationship between a Databricks principal (user or group), secret scope, and permission level. In general, a user will use the most powerful permission available to them (see Permission Levels). When a secret is read via a notebook using the Secrets utility (dbutils.secrets), the user’s permission will be applied based on who is executing the command, and they must at least have READ permission. When a scope is created, an initial MANAGE permission level ACL is applied to the scope. Subsequent access control configurations can be performed by that principal. Permission levels The secret access permissions are as follows: MANAGE - Allowed to change ACLs, and read and write to this secret scope. WRITE - Allowed to read and write to this secret scope. READ - Allowed to read this secret scope and list what secrets are available. Each permission level is a subset of the previous level’s permissions (that is, a principal with WRITE permission for a given scope can perform all actions that require READ permission). Note Databricks admins have MANAGE permissions to all secret scopes in the workspace. Create a secret ACL To create a secret ACL for a given secret scope using the Databricks CLI (version 0.7.1 and above): databricks secrets put-acl --scope <scope-name> --principal <principal> --permission <permission>
 Making a put request for a principal that already has an applied permission overwrites the existing permission level. View secret ACLs To view all secret ACLs for a given secret scope: databricks secrets list-acls --scope <scope-name>
 To get the secret ACL applied to a principal for a given secret scope: databricks secrets get-acl --scope <scope-name> --principal <principal>
 If no ACL exists for the given principal and scope, this request will fail. Delete a secret ACL To delete a secret ACL applied to a principal for a given secret scope: databricks secrets delete-acl --scope <scope-name> --principal <principal>
 Terraform integration You can manage permissions in a fully automated setup using Databricks Terraform provider and databricks_secret_acl: resource ""databricks_group"" ""ds"" {
  display_name = ""data-scientists""
}

resource ""databricks_secret_scope"" ""app"" {
  name = ""app-secret-scope""
}

resource ""databricks_secret_acl"" ""my_secret_acl"" {
  principal = databricks_group.ds.display_name
  permission = ""READ""
  scope = databricks_secret_scope.app.name
}

resource ""databricks_secret"" ""publishing_api"" {
  key = ""publishing_api""
  string_value = ""SECRET_API_TOKEN_HERE""
  scope = databricks_secret_scope.app.name
}",Secret access control
63,2206160050000293  - Databricks SQL Endpoint wont start due to auth problem,"Cs statement:

SQL Endpoint is not able to run queries:  Error message is:

Error 401; url='https://login.microsoftonline.com/fed95e69-8d73-43fe-affb-a7d85ede36fb/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/fed95e69-8d73-43fe-affb-a7d85ede36fb/oauth2/token failed for getting token from AzureAD.; requestId='6e836726-206b-425f-9269-352ab2954200'; contentType='application/json; charset=utf-8'; response '{'error':'invalid_client','error_description':'AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app 'cc3c15ea-5968-4716-9a40-18483b5bc229'.\\r\\nTrace ID: 6e836726-206b-425f-9269-352ab2954200\\r\\nCorrelation ID: 37c33837-163b-4b83-b04b-aab049c7e936\\r\\nTimestamp: 2022-06-16 08:42:45Z','error_codes':[7000215],'timestamp':'2022-06-16 08:42:45Z','trace_id':'6e836726-206b-425f-9269-352ab2954200','correlation_id':'37c33837-163b-4b83-b04b-aab049c7e936','error_uri':'https://login.microsoftonline.com/error?code=7000215'}'shaded.databricks.azurebfs.org.apache.hadoop.fs.azurebfs.oauth2.AzureADAuthenticator$HttpException: HTTP Error 401; url='https://login.microsoftonline.com/fed95e69-8d73-43fe-affb-a7d85ede36fb/oauth2/token' AADToken: HTTP connection to https://login.microsoftonline.com/fed95e69-8d73-43fe-affb-a7d85ede36fb/oauth2/token failed for getting token from AzureAD.; requestId='6e836726-206b-425f-9269-352ab2954200'; contentType='application/json; charset=utf-8'; response '{'error':'invalid_client','error_description':'AADSTS7000215: Invalid client secret provided. Ensure the secret being sent in the request is the client secret value, not the client secret ID, for a secret added to app 'cc3c15ea-5968-4716-9a40-18483b5bc229'.\\r\\nTrace ID: 6e836726-206b-425f-9269-352ab2954200\\r\\nCorrelation ID: 37c33837-163b-4b83-b04b-aab049c7e936\\r\\nTimestamp: 2022-06-16 08:42:45Z','error_codes':[7000215],'timestamp':'2022-06-16 08:42:45Z','trace_id':'6e836726-206b-425f-9269-352ab2954200','correlation_id':'37c33837-163b-4b83-b04b-aab049c7e936','error_uri':'https://login.microsoftonline.com/error?code=7000215'}'

MSFT Engineer notes:
Connected with cx through screenshare 
Attempted testing the connection to login.microsoftonline.com via telnet on a notebook and it ran fine.
They even tried creating a new SQL Endpoint and the issue is still happening.
Also hive metastore is not loading any tables",https://docs.microsoft.com/en-us/azure/databricks/sql/admin/data-access-configuration,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Data access configuration Article 07/12/2022 2 minutes to read 5 contributors In this article Requirements This article describes the data access configurations performed by Azure Databricks administrators for all SQL warehouses (formerly SQL endpoints) using the UI. To configure all SQL warehouses using the REST API, see Global SQL Warehouses API. Important Changing these settings restarts all running SQL warehouses. For a general overview of how to enable access to data, see Databricks SQL security model and data access overview. Requirements You must be an Azure Databricks administrator to configure settings for all SQL warehouses. Configure a service principal To configure all warehouses to use an Azure service principal to access an Azure Data Lake Storage Gen2 storage account: Create an Azure AD application and service principal that can access resources. Record the following properties: application-id: An ID that uniquely identifies the Azure Active Directory application. directory-id: An ID that uniquely identifies the Azure Active Directory instance (called directory (tenant) ID in Azure Databricks). client secret: The client secret created for the service principal. Create an Azure Key Vault-backed secret scope and save the client secret in the Azure key vault. Record the following properties: scope-name: The name of the created secret scope. secret-name: The name of the created secret. Click Settings at the bottom of the sidebar and select SQL Admin Console. Click the SQL Warehouse Settings tab. In the Data Access Configuration field, click the Add Service Principal button. Configure the properties for your Azure Data Lake Storage Gen2 storage account. Click Add. Click Save. Configure data access properties for SQL warehouses To configure all warehouses with data access properties: Click Settings at the bottom of the sidebar and select SQL Admin Console. Click the SQL Warehouse Settings tab. In the Data Access Configuration textbox, specify key-value pairs containing metastore properties. Important To set a Spark configuration property to the value of a secret without exposing the secret value to Spark, set the value to {{secrets/<secret-scope>/<secret-name>}}. Replace <secret-scope> with the secret scope and <secret-name> with the secret name. The value must start with {{secrets/ and end with }}. For more information about this syntax, see Syntax for referencing secrets in a Spark configuration property or environment variable. Click Save. Supported properties The following properties are supported for SQL warehouses. For an entry that ends with *, all properties within that prefix are supported. For example, spark.sql.hive.metastore.* indicates that both spark.sql.hive.metastore.jars and spark.sql.hive.metastore.version are supported, as well as any other properties that start with spark.sql.hive.metastore. For properties whose values contain sensitive information, you can store the sensitive information in a secret and set the property’s value to the secret name using the following syntax: secrets/<secret-scope>/<secret-name>. spark.sql.hive.metastore.* spark.sql.warehouse.dir spark.hadoop.datanucleus.* spark.hadoop.fs.* spark.hadoop.hive.* spark.hadoop.javax.jdo.option.* spark.hive.* For more information about how to set these properties, see External Hive metastore.",Data access configuration
64,cluster is not starting its failing.,"Hello Team,

Greetings!

Issue: cluster is not starting its failing.

Customer is not able to start the cluster below is the cluster URL.

https://adb-4540170653755408.8.azuredatabricks.net/?o=4540170653755408#setting/clusters/0601-104716-ezqj9r5h/events

Attached screenshot error.

Had call with customer and checked they are getting below error message.

Time
2022-06-09 17:23:02 IST
Message
Cluster terminated. Reason: Npip Tunnel Setup Failure

Help
NPIP tunnel setup failure during launch. Please try again later and contact Databricks if the problem persists. 
Instance bootstrap failed command: WaitForNgrokTunnel
Failure message (may be truncated): Timed out waiting for ngrok tunnel to be up.

Even referred below UDR document and suggested even to whitelist the ip's referring the UDR document according to the region.
https://docs.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/udr
Even checked in ASC they haven't enable service endpoint storage, Later even we have enabled the Microsoft.Sql and Microsoft.Storage but even though cluster is failing.
Even checked with networking team and informed that the issue is not at their side.
Checked in ASC this is the NPIP enabled cluster even added the scc relay IP but still cluster is not starting.

Thanks in advance.",https://docs.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Deploy Azure Databricks in your Azure virtual network (VNet injection) Article 06/30/2022 15 minutes to read 6 contributors In this article Virtual network requirements Create an Azure Databricks workspace using Azure portal Advanced configuration using Azure Resource Manager templates Network security group rules Troubleshooting The default deployment of Azure Databricks is a fully managed service on Azure: all data plane resources, including a VNet that all clusters will be associated with, are deployed to a locked resource group. If you require network customization, however, you can deploy Azure Databricks data plane resources in your own virtual network (sometimes called VNet injection), enabling you to: Connect Azure Databricks to other Azure services (such as Azure Storage) in a more secure manner using service endpoints or private endpoints. Connect to on-premises data sources for use with Azure Databricks, taking advantage of user-defined routes. Connect Azure Databricks to a network virtual appliance to inspect all outbound traffic and take actions according to allow and deny rules, by using user-defined routes. Configure Azure Databricks to use custom DNS. Configure network security group (NSG) rules to specify egress traffic restrictions. Deploy Azure Databricks clusters in your existing VNet. Deploying Azure Databricks data plane resources to your own VNet also lets you take advantage of flexible CIDR ranges (anywhere between /16-/24 for the VNet and up to /26 for the subnets). Important You cannot replace the VNet for an existing workspace. If your current workspace cannot accommodate the required number of active cluster nodes, we recommend that you create another workspace in a larger VNet. Follow these detailed migration steps to copy resources (notebooks, cluster configurations, jobs) from the old to new workspace. Virtual network requirements The VNet that you deploy your Azure Databricks workspace to must meet the following requirements: Region: The VNet must reside in the same region as the Azure Databricks workspace. Subscription: The VNet must be in the same subscription as the Azure Databricks workspace. Address space: A CIDR block between /16 and /24 for the VNet and a CIDR block up to /26 for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see Address space and maximum cluster nodes. Subnets: The VNet must include two subnets dedicated to your Azure Databricks workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses secure cluster connectivity, both the container subnet and host subnet are private. It is unsupported to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your Azure Databricks workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see Address space and maximum cluster nodes. If you Create an Azure Databricks workspace using Azure portal, you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges. If you use the All-in-one ARM template or the VNet-only ARM template, the templates create the subnets for you. In both cases, the subnets are delegated to the Microsoft.Databricks/workspaces resource provider before workspace deployment, which allows Azure Databricks to create Network security group rules. Azure Databricks will give advance notice if we need to add or update the scope of an Azure Databricks-managed NSG rule. If you use the Workspace ARM template or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the same network security group and are properly delegated. For delegation instructions, see Upgrade your VNet Injection preview workspace to GA or Add or remove a subnet delegation. Important There is a one-to-one relationship between these subnets and an Azure Databricks workspace. You cannot share multiple workspaces across a single subnet. It is unsupported to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your Azure Databricks workspace. For more information about templates to configure your VNet and deploy your workspace, see Azure-Databricks-supplied Azure Resource Manager templates. Address space and maximum cluster nodes A workspace with a smaller virtual network can run out of IP addresses (network space) more quickly than a workspace with a larger virtual network. Use a CIDR block between /16 and /24 for the VNet and a CIDR block up to /26 for the two subnets (the container subnet and the host subnet). The CIDR range for your VNet address space affects the maximum number of cluster nodes that your workspace can use: An Azure Databricks workspace requires two subnets in the VNet: a container subnet (also known as private subnet) and a host subnet (also known as public subnet). If the workspace uses secure cluster connectivity, both container and host subnets are private. Azure reserves five IPs in each subnet. Within each subnet, Azure Databricks requires one IP address per cluster node. In total, there are two IP for each cluster node: one IP address for the host in the host subnet and one IP address for the container in the container subnet. You may not want to use all the address space of your VNet. For example, you might want to create multiple workspaces in one VNet. Because you cannot share subnets across workspaces, you may want subnets that do not use the total VNet address space. You must allocate address space for two new subnets that are within the VNet’s address space and don’t overlap address space of current or future subnets in that VNet. The following table shows maximum subnet size based on network size. This table assumes no additional subnets exist that take up address space. Use smaller subnets if you have pre-existing subnets or if you want to reserve address space for other subnets: VNet address space (CIDR) Maximum Azure Databricks subnet size (CIDR) assuming no other subnets /16 /17 /17 /18 /18 /19 /20 /21 /21 /22 /22 /23 /23 /24 /24 /25 To find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the five Azure-reserved IP addresses. The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size. Subnet size (CIDR) IP addresses per subnet Maximum Azure Databricks cluster nodes /17 32768 32763 /18 16384 16379 /19 8192 8187 /20 4096 4091 /21 2048 2043 /22 1024 1019 /23 512 507 /24 256 251 /25 128 123 /26 64 59 Create an Azure Databricks workspace using Azure portal This section describes how to create an Azure Databricks workspace in the Azure portal and deploy it in your own existing VNet. Azure Databricks updates the VNet with two new subnets if those do not exist yet, using CIDR ranges that you specify. The service also updates the subnets with a new network security group, configuring inbound and outbound rules, and finally deploys the workspace to the updated VNet. For more control over the configuration of the VNet, use Azure-Databricks-supplied Azure Resource Manager (ARM) templates instead of the portal UI. For example, use existing network security groups or create your own security rules. See Advanced configuration using Azure Resource Manager templates. Important The user who creates the workspace must be assigned the Network contributor role or a custom role that’s assigned the Microsoft.Network/virtualNetworks/subnets/join/action action. You must configure a VNet to which you will deploy the Azure Databricks workspace. You can use an existing VNet or create a new one, but the VNet must be in the same region and same subscription as the Azure Databricks workspace that you plan to create. The VNet must be sized with a CIDR range between /16 and /24. For more requirements, see Virtual network requirements. You can either use existing subnets or specify names and IP ranges for new subnets when you configure your workspace. In the Azure portal, select + Create a resource > Analytics > Azure Databricks or search for Azure Databricks and click Create or + Add to launch the Azure Databricks Service dialog. Follow the configuration steps described in the Create an Azure Databricks workspace in your own VNet quickstart. In the Networking tab, select the VNet that you want to use in the Virtual network field. Important If you do not see the network name in the picker, confirm that the Azure region that you specified for the workspace matches the Azure region of the desired VNet. Name your subnets and provide CIDR ranges in a block up to size /26. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see Address space and maximum cluster nodes. To specify existing subnets, specify the exact names of the existing subnets. When using existing subnets, also set the IP ranges in the workspace creation form to exactly match the IP ranges of the existing subnets. To create new subnets, specify subnet names that do not already exist in that VNet. The subnets are created with the specified IP ranges. You must specify IP ranges within the IP range of your VNet and not already allocated to existing subnets. Important Azure Databricks requires subnet names to be no longer than 30 characters. This is shorter than the maximum length allowed for subnets in the Azure portal. Before you use an existing subnet, rename it if its name is longer than 30 characters. The subnets get associated network security group rules that include the rule to allow cluster-internal communication. Azure Databricks will have delegated permissions to update both subnets via the Microsoft.Databricks/workspaces resource provider. These permissions apply only to network security group rules that are required by Azure Databricks, not to other network security group rules that you add or to the default network security group rules included with all network security groups. Click Create to deploy the Azure Databricks workspace to the VNet. Note When a workspace deployment fails, the workspace is still created but has a failed state. Delete the failed workspace and create a new workspace that resolves the deployment errors. When you delete the failed workspace, the managed resource group and any successfully deployed resources are also deleted. Advanced configuration using Azure Resource Manager templates If you want more control over the configuration of the VNet, you can use the following Azure Resource Manager (ARM) templates instead of the portal-UI-based automatic VNet configuration and workspace deployment. For example, use existing subnets, an existing network security group, or add your own security rules. If you are using a custom Azure Resource Manager template or the Workspace Template for Azure Databricks VNet Injection to deploy a workspace to an existing VNet, you must create host and container subnets, attach a network security group to each subnet, and delegate the subnets to the Microsoft.Databricks/workspaces resource provider before deploying the workspace. You must have a separate pair of subnets for each workspace that you deploy. All-in-one template To create a VNet and Azure Databricks workspace using one template, use the All-in-one Template for Azure Databricks VNet Injected Workspaces. Virtual network template To create a VNet with the proper subnets using a template, use the VNet Template for Databricks VNet Injection. Azure Databricks workspace template To deploy an Azure Databricks workspace to an existing VNet with a template, use the Workspace Template for Azure Databricks VNet Injection. The workspace template allows you to specify an existing VNet and use existing subnets: You must have a separate pair of host/container subnets for each workspace that you deploy. It is unsupported to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your Azure Databricks workspace. Your VNet’s host and container subnets must have network security groups attached and must be delegated to the Microsoft.Databricks/workspaces service before you use this Azure Resource Manager template to deploy a workspace. To create a VNet with properly delegated subnets, use the VNet Template for Databricks VNet Injection. To use an existing VNet when you have not yet delegated the host and container subnets, see Add or remove a subnet delegation or Upgrade your VNet Injection preview workspace to GA. Network security group rules The following tables display the current network security group rules used by Azure Databricks. If Azure Databricks needs to add a rule or change the scope of an existing rule on this list, you will receive advance notice. This article and the tables will be updated whenever such a modification occurs. In this section: How Azure Databricks manages network security group rules Network security group rules for workspaces created after January 13, 2020 Network security group rules for workspaces created before January 13, 2020 How Azure Databricks manages network security group rules The NSG rules listed in the following sections represent those that Azure Databricks auto-provisions and manages in your NSG, by virtue of the delegation of your VNet’s host and container subnets to the Microsoft.Databricks/workspaces service. You do not have permission to update or delete these NSG rules; any attempt to do so is blocked by the subnet delegation. Azure Databricks must own these rules in order to ensure that Microsoft can reliably operate and support the Azure Databricks service in your VNet. Some of these NSG rules have VirtualNetwork assigned as the source and destination. This has been implemented to simplify the design in the absence of a subnet-level service tag in Azure. All clusters are protected by a second layer of network policy internally, such that cluster A cannot connect to cluster B in the same workspace. This also applies across multiple workspaces if your workspaces are deployed into a different pair of subnets in the same customer-managed VNet. Important If the workspace VNet is peered to another customer-managed network, or if non-Azure Databricks resources are provisioned in other subnets, Databricks recommends that you add Deny inbound rules to the NSGs attached to the other networks and subnets to block source traffic from Azure Databricks clusters. You do not need to add such rules for resources that you want your Azure Databricks clusters to connect to. Network security group rules for workspaces created after January 13, 2020 The information in this section applies only to Azure Databricks workspaces created after January 13, 2020. If your workspace was created before the release of secure cluster connectivity (SCC) on January 13, 2020, see the next section. Important This table lists two inbound security group rules that are included only if secure cluster connectivity (SCC) is disabled. Direction Protocol Source Source Port Destination Dest Port Used Inbound Any VirtualNetwork Any VirtualNetwork Any Default Inbound TCP AzureDatabricks (service tag) Only if SCC is disabled Any VirtualNetwork 22 Public IP Inbound TCP AzureDatabricks (service tag) Only if SCC is disabled Any VirtualNetwork 5557 Public IP Outbound TCP VirtualNetwork Any AzureDatabricks (service tag) 443 Default Outbound TCP VirtualNetwork Any SQL 3306 Default Outbound TCP VirtualNetwork Any Storage 443 Default Outbound Any VirtualNetwork Any VirtualNetwork Any Default Outbound TCP VirtualNetwork Any EventHub 9093 Default Outbound Any VirtualNetwork Any NFS 111 Default Outbound Any VirtualNetwork Any NFS 2049 Default Network security group rules for workspaces created before January 13, 2020 The information in this section applies only to Azure Databricks workspaces created before January 13, 2020. If your workspace was created on or after January 13, 2020, see the previous section. Direction Protocol Source Source Port Destination Dest Port Used Inbound Any VirtualNetwork Any VirtualNetwork Any Default Inbound TCP ControlPlane IP Any VirtualNetwork 22 Public IP Inbound TCP ControlPlane IP Any VirtualNetwork 5557 Public IP Outbound TCP VirtualNetwork Any Webapp IP 443 Default Outbound TCP VirtualNetwork Any SQL 3306 Default Outbound TCP VirtualNetwork Any Storage 443 Default Outbound Any VirtualNetwork Any VirtualNetwork Any Default Outbound TCP VirtualNetwork Any EventHub 9093 Default Important Azure Databricks is a Microsoft Azure first-party service that is deployed on the Global Azure Public Cloud infrastructure. All communications between components of the service, including between the public IPs in the control plane and the customer data plane, remain within the Microsoft Azure network backbone. See also Microsoft global network. Troubleshooting Workspace creation errors Subnet requires any of the following delegation(s) [Microsoft.Databricks/workspaces] to reference service association link Possible cause: you are creating a workspace in a VNet whose host and container subnets have not been delegated to the Microsoft.Databricks/workspaces service. Each subnet must have a network security group attached and must be properly delegated. See Virtual network requirements for more information. The subnet is already in use by workspace Possible cause: you are creating a workspace in a VNet with host and container subnets that are already being used by an existing Azure Databricks workspace. You cannot share multiple workspaces across a single subnet. You must have a new pair of host and container subnets for each workspace you deploy. Troubleshooting Instances Unreachable: Resources were not reachable via SSH. Possible cause: traffic from control plane to workers is blocked. If you are deploying to an existing VNet connected to your on-premises network, review your setup using the information supplied in Connect your Azure Databricks workspace to your on-premises network. Unexpected Launch Failure: An unexpected error was encountered while setting up the cluster. Please retry and contact Azure Databricks if the problem persists. Internal error message: Timeout while placing node. Possible cause: traffic from workers to Azure Storage endpoints is blocked. If you are using custom DNS servers, also check the status of the DNS servers in your VNet. Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster. See the Azure Databricks guide for more information. Azure error code: AuthorizationFailed/InvalidResourceReference. Possible cause: the VNet or subnets do not exist any more. Make sure the VNet and subnets exist. Cluster terminated. Reason: Spark Startup Failure: Spark was not able to start in time. This issue can be caused by a malfunctioning Hive metastore, invalid Spark configurations, or malfunctioning init scripts. See the Spark driver logs to troubleshoot this issue, and contact Databricks if the problem persists. Internal error message: Spark failed to start: Driver failed to start in time. Possible cause: Container cannot talk to hosting instance or DBFS storage account. Fix by adding a custom route to the subnets for the DBFS storage account with the next hop being Internet.",Deploy Azure Databricks in your Azure virtual network (VNet injection)
65,AML DatabricksStep failed to submit job to databricks,"Question: When did the problem start?
Answer: Not sure, use current time

Question: What is the name of the Compute resource?
Answer: aa-zhancan-db

Question: In case of virtual network creation, can you provide NSG rules?
Answer: 

Question: What version of the Azure ML SDK are you using?
Answer: 1.42.0

Question: Details
Answer: When running a pipeline with DatabricksStep, the job fails with error message:

Failed to submit job because of error: Microsoft.RelInfra.Extensions.HttpRequestDetailException: Response status code does not indicate success: 403 (Forbidden).
{'error_code':'403','message':'Unauthorized access to Org:'}

However the pipeline is able to run after multiple retries and sometimes it can take up to 20 retries.  The retries are failed with the same error. 



<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Machine Learning:
When did the problem start? - 2022-06-16T20:51:00.053Z;
What is the name of the Compute resource? - aa-zhancan-db;
In case of virtual network creation, can you provide NSG rules? - ;
What version of the Azure ML SDK are you using? - 1.42.0;
Details - When running a pipeline with DatabricksStep, the job fails with error message:

Failed to submit job because of error: Microsoft.RelInfra.Extensions.HttpRequestDetailException: Response status code does not indicate success: 403 (Forbidden).
{'error_code':'403','message':'Unauthorized access to Org:'}

However the pipeline is able to run after multiple retries and sometimes it can take up to 20 retries.  The retries are failed with the same error. ;

Customer has uploaded a file. /?workspaceid=169026db-7eaf-4585-a8ed-5425d29e2a4e
- ProblemStartTime: 06/16/2022 20:51:00
- Cloud: Azure
- AzureProductSubscriptionID: 5fea1bce-43ac-496d-b826-b0d56cd9b209
- AzureProductSubscriptionName: MFC-Global-Canada-DataOffice-CDO-Production-S1
- PUID: 10032001303B1723
- Tenant Id: 5d3e2773-e07f-4432-a630-1a0f68a28a05
- Object Id: 1854e5be-cb8c-4dab-a3c7-cae4261f6df9
- SubscriptionType: Premier
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Premier with Azure Rapid response
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: False

- Location: canadacentral
- ResourceUri: /subscriptions/5fea1bce-43ac-496d-b826-b0d56cd9b209/resourceGroups/aalab-mlworkspace-insurancepii/providers/Microsoft.MachineLearningServices/workspaces/aalabmlworkspaceprodinspii

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/vnet-inject,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Deploy Azure Databricks in your Azure virtual network (VNet injection) Article 06/30/2022 15 minutes to read 6 contributors In this article Virtual network requirements Create an Azure Databricks workspace using Azure portal Advanced configuration using Azure Resource Manager templates Network security group rules Troubleshooting The default deployment of Azure Databricks is a fully managed service on Azure: all data plane resources, including a VNet that all clusters will be associated with, are deployed to a locked resource group. If you require network customization, however, you can deploy Azure Databricks data plane resources in your own virtual network (sometimes called VNet injection), enabling you to: Connect Azure Databricks to other Azure services (such as Azure Storage) in a more secure manner using service endpoints or private endpoints. Connect to on-premises data sources for use with Azure Databricks, taking advantage of user-defined routes. Connect Azure Databricks to a network virtual appliance to inspect all outbound traffic and take actions according to allow and deny rules, by using user-defined routes. Configure Azure Databricks to use custom DNS. Configure network security group (NSG) rules to specify egress traffic restrictions. Deploy Azure Databricks clusters in your existing VNet. Deploying Azure Databricks data plane resources to your own VNet also lets you take advantage of flexible CIDR ranges (anywhere between /16-/24 for the VNet and up to /26 for the subnets). Important You cannot replace the VNet for an existing workspace. If your current workspace cannot accommodate the required number of active cluster nodes, we recommend that you create another workspace in a larger VNet. Follow these detailed migration steps to copy resources (notebooks, cluster configurations, jobs) from the old to new workspace. Virtual network requirements The VNet that you deploy your Azure Databricks workspace to must meet the following requirements: Region: The VNet must reside in the same region as the Azure Databricks workspace. Subscription: The VNet must be in the same subscription as the Azure Databricks workspace. Address space: A CIDR block between /16 and /24 for the VNet and a CIDR block up to /26 for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see Address space and maximum cluster nodes. Subnets: The VNet must include two subnets dedicated to your Azure Databricks workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses secure cluster connectivity, both the container subnet and host subnet are private. It is unsupported to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your Azure Databricks workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see Address space and maximum cluster nodes. If you Create an Azure Databricks workspace using Azure portal, you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges. If you use the All-in-one ARM template or the VNet-only ARM template, the templates create the subnets for you. In both cases, the subnets are delegated to the Microsoft.Databricks/workspaces resource provider before workspace deployment, which allows Azure Databricks to create Network security group rules. Azure Databricks will give advance notice if we need to add or update the scope of an Azure Databricks-managed NSG rule. If you use the Workspace ARM template or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the same network security group and are properly delegated. For delegation instructions, see Upgrade your VNet Injection preview workspace to GA or Add or remove a subnet delegation. Important There is a one-to-one relationship between these subnets and an Azure Databricks workspace. You cannot share multiple workspaces across a single subnet. It is unsupported to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your Azure Databricks workspace. For more information about templates to configure your VNet and deploy your workspace, see Azure-Databricks-supplied Azure Resource Manager templates. Address space and maximum cluster nodes A workspace with a smaller virtual network can run out of IP addresses (network space) more quickly than a workspace with a larger virtual network. Use a CIDR block between /16 and /24 for the VNet and a CIDR block up to /26 for the two subnets (the container subnet and the host subnet). The CIDR range for your VNet address space affects the maximum number of cluster nodes that your workspace can use: An Azure Databricks workspace requires two subnets in the VNet: a container subnet (also known as private subnet) and a host subnet (also known as public subnet). If the workspace uses secure cluster connectivity, both container and host subnets are private. Azure reserves five IPs in each subnet. Within each subnet, Azure Databricks requires one IP address per cluster node. In total, there are two IP for each cluster node: one IP address for the host in the host subnet and one IP address for the container in the container subnet. You may not want to use all the address space of your VNet. For example, you might want to create multiple workspaces in one VNet. Because you cannot share subnets across workspaces, you may want subnets that do not use the total VNet address space. You must allocate address space for two new subnets that are within the VNet’s address space and don’t overlap address space of current or future subnets in that VNet. The following table shows maximum subnet size based on network size. This table assumes no additional subnets exist that take up address space. Use smaller subnets if you have pre-existing subnets or if you want to reserve address space for other subnets: VNet address space (CIDR) Maximum Azure Databricks subnet size (CIDR) assuming no other subnets /16 /17 /17 /18 /18 /19 /20 /21 /21 /22 /22 /23 /23 /24 /24 /25 To find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the five Azure-reserved IP addresses. The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size. Subnet size (CIDR) IP addresses per subnet Maximum Azure Databricks cluster nodes /17 32768 32763 /18 16384 16379 /19 8192 8187 /20 4096 4091 /21 2048 2043 /22 1024 1019 /23 512 507 /24 256 251 /25 128 123 /26 64 59 Create an Azure Databricks workspace using Azure portal This section describes how to create an Azure Databricks workspace in the Azure portal and deploy it in your own existing VNet. Azure Databricks updates the VNet with two new subnets if those do not exist yet, using CIDR ranges that you specify. The service also updates the subnets with a new network security group, configuring inbound and outbound rules, and finally deploys the workspace to the updated VNet. For more control over the configuration of the VNet, use Azure-Databricks-supplied Azure Resource Manager (ARM) templates instead of the portal UI. For example, use existing network security groups or create your own security rules. See Advanced configuration using Azure Resource Manager templates. Important The user who creates the workspace must be assigned the Network contributor role or a custom role that’s assigned the Microsoft.Network/virtualNetworks/subnets/join/action action. You must configure a VNet to which you will deploy the Azure Databricks workspace. You can use an existing VNet or create a new one, but the VNet must be in the same region and same subscription as the Azure Databricks workspace that you plan to create. The VNet must be sized with a CIDR range between /16 and /24. For more requirements, see Virtual network requirements. You can either use existing subnets or specify names and IP ranges for new subnets when you configure your workspace. In the Azure portal, select + Create a resource > Analytics > Azure Databricks or search for Azure Databricks and click Create or + Add to launch the Azure Databricks Service dialog. Follow the configuration steps described in the Create an Azure Databricks workspace in your own VNet quickstart. In the Networking tab, select the VNet that you want to use in the Virtual network field. Important If you do not see the network name in the picker, confirm that the Azure region that you specified for the workspace matches the Azure region of the desired VNet. Name your subnets and provide CIDR ranges in a block up to size /26. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see Address space and maximum cluster nodes. To specify existing subnets, specify the exact names of the existing subnets. When using existing subnets, also set the IP ranges in the workspace creation form to exactly match the IP ranges of the existing subnets. To create new subnets, specify subnet names that do not already exist in that VNet. The subnets are created with the specified IP ranges. You must specify IP ranges within the IP range of your VNet and not already allocated to existing subnets. Important Azure Databricks requires subnet names to be no longer than 30 characters. This is shorter than the maximum length allowed for subnets in the Azure portal. Before you use an existing subnet, rename it if its name is longer than 30 characters. The subnets get associated network security group rules that include the rule to allow cluster-internal communication. Azure Databricks will have delegated permissions to update both subnets via the Microsoft.Databricks/workspaces resource provider. These permissions apply only to network security group rules that are required by Azure Databricks, not to other network security group rules that you add or to the default network security group rules included with all network security groups. Click Create to deploy the Azure Databricks workspace to the VNet. Note When a workspace deployment fails, the workspace is still created but has a failed state. Delete the failed workspace and create a new workspace that resolves the deployment errors. When you delete the failed workspace, the managed resource group and any successfully deployed resources are also deleted. Advanced configuration using Azure Resource Manager templates If you want more control over the configuration of the VNet, you can use the following Azure Resource Manager (ARM) templates instead of the portal-UI-based automatic VNet configuration and workspace deployment. For example, use existing subnets, an existing network security group, or add your own security rules. If you are using a custom Azure Resource Manager template or the Workspace Template for Azure Databricks VNet Injection to deploy a workspace to an existing VNet, you must create host and container subnets, attach a network security group to each subnet, and delegate the subnets to the Microsoft.Databricks/workspaces resource provider before deploying the workspace. You must have a separate pair of subnets for each workspace that you deploy. All-in-one template To create a VNet and Azure Databricks workspace using one template, use the All-in-one Template for Azure Databricks VNet Injected Workspaces. Virtual network template To create a VNet with the proper subnets using a template, use the VNet Template for Databricks VNet Injection. Azure Databricks workspace template To deploy an Azure Databricks workspace to an existing VNet with a template, use the Workspace Template for Azure Databricks VNet Injection. The workspace template allows you to specify an existing VNet and use existing subnets: You must have a separate pair of host/container subnets for each workspace that you deploy. It is unsupported to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your Azure Databricks workspace. Your VNet’s host and container subnets must have network security groups attached and must be delegated to the Microsoft.Databricks/workspaces service before you use this Azure Resource Manager template to deploy a workspace. To create a VNet with properly delegated subnets, use the VNet Template for Databricks VNet Injection. To use an existing VNet when you have not yet delegated the host and container subnets, see Add or remove a subnet delegation or Upgrade your VNet Injection preview workspace to GA. Network security group rules The following tables display the current network security group rules used by Azure Databricks. If Azure Databricks needs to add a rule or change the scope of an existing rule on this list, you will receive advance notice. This article and the tables will be updated whenever such a modification occurs. In this section: How Azure Databricks manages network security group rules Network security group rules for workspaces created after January 13, 2020 Network security group rules for workspaces created before January 13, 2020 How Azure Databricks manages network security group rules The NSG rules listed in the following sections represent those that Azure Databricks auto-provisions and manages in your NSG, by virtue of the delegation of your VNet’s host and container subnets to the Microsoft.Databricks/workspaces service. You do not have permission to update or delete these NSG rules; any attempt to do so is blocked by the subnet delegation. Azure Databricks must own these rules in order to ensure that Microsoft can reliably operate and support the Azure Databricks service in your VNet. Some of these NSG rules have VirtualNetwork assigned as the source and destination. This has been implemented to simplify the design in the absence of a subnet-level service tag in Azure. All clusters are protected by a second layer of network policy internally, such that cluster A cannot connect to cluster B in the same workspace. This also applies across multiple workspaces if your workspaces are deployed into a different pair of subnets in the same customer-managed VNet. Important If the workspace VNet is peered to another customer-managed network, or if non-Azure Databricks resources are provisioned in other subnets, Databricks recommends that you add Deny inbound rules to the NSGs attached to the other networks and subnets to block source traffic from Azure Databricks clusters. You do not need to add such rules for resources that you want your Azure Databricks clusters to connect to. Network security group rules for workspaces created after January 13, 2020 The information in this section applies only to Azure Databricks workspaces created after January 13, 2020. If your workspace was created before the release of secure cluster connectivity (SCC) on January 13, 2020, see the next section. Important This table lists two inbound security group rules that are included only if secure cluster connectivity (SCC) is disabled. Direction Protocol Source Source Port Destination Dest Port Used Inbound Any VirtualNetwork Any VirtualNetwork Any Default Inbound TCP AzureDatabricks (service tag) Only if SCC is disabled Any VirtualNetwork 22 Public IP Inbound TCP AzureDatabricks (service tag) Only if SCC is disabled Any VirtualNetwork 5557 Public IP Outbound TCP VirtualNetwork Any AzureDatabricks (service tag) 443 Default Outbound TCP VirtualNetwork Any SQL 3306 Default Outbound TCP VirtualNetwork Any Storage 443 Default Outbound Any VirtualNetwork Any VirtualNetwork Any Default Outbound TCP VirtualNetwork Any EventHub 9093 Default Outbound Any VirtualNetwork Any NFS 111 Default Outbound Any VirtualNetwork Any NFS 2049 Default Network security group rules for workspaces created before January 13, 2020 The information in this section applies only to Azure Databricks workspaces created before January 13, 2020. If your workspace was created on or after January 13, 2020, see the previous section. Direction Protocol Source Source Port Destination Dest Port Used Inbound Any VirtualNetwork Any VirtualNetwork Any Default Inbound TCP ControlPlane IP Any VirtualNetwork 22 Public IP Inbound TCP ControlPlane IP Any VirtualNetwork 5557 Public IP Outbound TCP VirtualNetwork Any Webapp IP 443 Default Outbound TCP VirtualNetwork Any SQL 3306 Default Outbound TCP VirtualNetwork Any Storage 443 Default Outbound Any VirtualNetwork Any VirtualNetwork Any Default Outbound TCP VirtualNetwork Any EventHub 9093 Default Important Azure Databricks is a Microsoft Azure first-party service that is deployed on the Global Azure Public Cloud infrastructure. All communications between components of the service, including between the public IPs in the control plane and the customer data plane, remain within the Microsoft Azure network backbone. See also Microsoft global network. Troubleshooting Workspace creation errors Subnet requires any of the following delegation(s) [Microsoft.Databricks/workspaces] to reference service association link Possible cause: you are creating a workspace in a VNet whose host and container subnets have not been delegated to the Microsoft.Databricks/workspaces service. Each subnet must have a network security group attached and must be properly delegated. See Virtual network requirements for more information. The subnet is already in use by workspace Possible cause: you are creating a workspace in a VNet with host and container subnets that are already being used by an existing Azure Databricks workspace. You cannot share multiple workspaces across a single subnet. You must have a new pair of host and container subnets for each workspace you deploy. Troubleshooting Instances Unreachable: Resources were not reachable via SSH. Possible cause: traffic from control plane to workers is blocked. If you are deploying to an existing VNet connected to your on-premises network, review your setup using the information supplied in Connect your Azure Databricks workspace to your on-premises network. Unexpected Launch Failure: An unexpected error was encountered while setting up the cluster. Please retry and contact Azure Databricks if the problem persists. Internal error message: Timeout while placing node. Possible cause: traffic from workers to Azure Storage endpoints is blocked. If you are using custom DNS servers, also check the status of the DNS servers in your VNet. Cloud Provider Launch Failure: A cloud provider error was encountered while setting up the cluster. See the Azure Databricks guide for more information. Azure error code: AuthorizationFailed/InvalidResourceReference. Possible cause: the VNet or subnets do not exist any more. Make sure the VNet and subnets exist. Cluster terminated. Reason: Spark Startup Failure: Spark was not able to start in time. This issue can be caused by a malfunctioning Hive metastore, invalid Spark configurations, or malfunctioning init scripts. See the Spark driver logs to troubleshoot this issue, and contact Databricks if the problem persists. Internal error message: Spark failed to start: Driver failed to start in time. Possible cause: Container cannot talk to hosting instance or DBFS storage account. Fix by adding a custom route to the subnets for the DBFS storage account with the next hop being Internet.",Deploy Azure Databricks in your Azure virtual network (VNet injection)
66,Azure Databrticks Provisioning,"Question: What time did the problem begin?
Answer: Fri, Jun 17, 2022, 12:00 AM (UTC-08:00) Pacific Time (US & Canada)

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer: 

Question: Is this a new problem, or it has happened before?
Answer: New problem, worked before

Question: Any changes made?
Answer: Changed users 'User Prinncipal Name' in Azure AD

Question: Cluster URL
Answer: n/a

Question: Notebook URL if available
Answer: 

Question: Additional details about the issue
Answer: Azure Databricks is currently displaying each users username matching the user principal name they had prior to us modyfing it. I tried re-provisioning account but since the attribute 'username' in databricks does not exist in Azure AD , it is not seeing any new changes in order for databricks to update account name used during logon



<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Databricks:
What time did the problem begin? - 2022-06-17T07:00:00.000Z;
Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank - ;
Is this a new problem, or it has happened before? - New problem, worked before;
Any changes made? - Changed users 'User Prinncipal Name' in Azure AD;
Cluster URL - n/a;
Notebook URL if available - ;
Additional details about the issue - Azure Databricks is currently displaying each users username matching the user principal name they had prior to us modyfing it. I tried re-provisioning account but since the attribute 'username' in databricks does not exist in Azure AD , it is not seeing any new changes in order for databricks to update account name used during logon;

- ProblemStartTime: 06/17/2022 07:00:00
- Cloud: Azure
- AzureProductSubscriptionID: ade57aeb-b898-4f04-9448-dd917962e164
- AzureProductSubscriptionName: BI
- Tenant Id: 28891a93-888f-489f-9930-e78b8f733ca6
- Object Id: 066caae9-6f35-4a2d-9ddc-4d642b0a4efa
- SubscriptionType: Unified
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Unified Support – Performance
- GrantPermission: False
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: False

- Location: westus
- ResourceUri: /subscriptions/ade57aeb-b898-4f04-9448-dd917962e164/resourceGroups/rg_saas_bi_test_edwhpd/providers/Microsoft.Databricks/workspaces/dbw-thpd

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/scim/aad,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure SCIM provisioning for Microsoft Azure Active Directory Article 05/25/2022 6 minutes to read 5 contributors In this article Requirements Use an Azure Active Directory enterprise application Automate SCIM provisioning using Microsoft Graph Provisioning tips Troubleshooting Important This feature is in Public Preview. To enable provisioning to Azure Databricks using Azure Active Directory (Azure AD) you must create an enterprise application for each Azure Databricks workspace. Note The way provisioning is configured is entirely separate from configuring authentication and conditional access for Azure Databricks workspaces. Authentication for Azure Databricks is handled automatically by Azure Active Directory, using the OpenID Connect protocol flow. You configure conditional access, which lets you create rules to require multi-factor authentication or restrict logins to local networks, at the service level. Requirements Your Azure Databricks account must have the Premium Plan. Your Azure Active Directory account must be a Premium edition account. You must be a global administrator for the Azure Active Directory account. There are two ways to configure provisioning: You can create an enterprise application in the Azure portal, and use that application for provisioning. If you have an existing application, you can modify its code to automate SCIM provisioning using Microsoft Graph. This removes the need for a separate provisioning application in the Azure Portal. Use an Azure Active Directory enterprise application In the following examples, replace <workspace-url> with the workspace URL of your Azure Databricks deployment. In this section: Create the enterprise application and connect it to the Azure Databricks SCIM API Assign users and groups to the application Create the enterprise application and connect it to the Azure Databricks SCIM API Generate a personal access token in Azure Databricks and copy it. You provide this token to Azure Active Directory in a subsequent step. Important Generate this token as an Azure Databricks admin who is not managed by the Azure Active Directory enterprise application. If the Azure Databricks admin user who owns the personal access token is deprovisioned using Azure Active Directory, the SCIM provisioning application will be disabled. In your Azure portal, go to Azure Active Directory > Enterprise Applications. Click + New Application above the application list. Under Add from the gallery, search for and select Azure Databricks SCIM Provisioning Connector. Enter a Name for the application and click Add. Use a name that will help administrators find it, like <workspace-name>-provisioning. Under the Manage menu, click Provisioning. Set Provisioning Mode to Automatic. Enter the SCIM API endpoint URL. Append /api/2.0/preview/scim to your workspace URL: Copy https://<workspace-url>/api/2.0/preview/scim
 Replace <workspace-url> with the workspace URL of your Azure Databricks deployment. See Get workspace, cluster, notebook, folder, model, and job identifiers. Set Secret Token to the Azure Databricks personal access token that you generated in step 1. Click Test Connection and wait for the message that confirms that the credentials are authorized to enable provisioning. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning. Click Save. Assign users and groups to the application Go to Manage > Provisioning. Under Settings, set Scope to Sync only assigned users and groups. Databricks recommends this option, which syncs only users and groups assigned to the enterprise application. Note Azure Active Directory does not support the automatic provisioning of nested groups to Azure Databricks. Azure Active Directory can only read and provision users that are immediate members of the explicitly assigned group. As a workaround, explicitly assign (or otherwise scope in) the groups that contain the users who need to be provisioned. For more information, see this FAQ. To start synchronizing Azure Active Directory users and groups to Azure Databricks, click the Provisioning Status toggle. Click Save. Test your provisioning setup: Go to Manage > Users and groups. Add some users and groups. Click Add user, select the users and groups, and click the Assign button. Wait a few minutes and check that the users and groups exist in your Azure Databricks workspace. In the future, users and groups that you add and assign are automatically provisioned when Azure Active Directory schedules the next sync. Important Do not assign the Azure Databricks admin whose personal access token was used to configure the Azure Databricks SCIM Provisioning Connector application. Automate SCIM provisioning using Microsoft Graph Microsoft Graph includes authentication and authorization libraries that you can integrate into your application to automate provisioning of users and groups to Azure Databricks, instead of configuring a SCIM provisioning connector application. Follow the instructions for registering an application with Microsoft Graph. Make a note of the Application ID and the Tenant ID for the application Go to the applications’s Overview page. On that page: Configure a client secret for the application, and make a note of the secret. Grant the application these permissions: Application.ReadWrite.All Application.ReadWrite.OwnedBy Ask an Azure Active Directory administrator to grant admin consent. Update your application’s code to add support for Microsoft Graph. Provisioning tips Users and groups that existed in Azure Databricks prior to enabling provisioning exhibit the following behavior upon provisioning sync: Are merged if they also exist in Azure Active Directory Are ignored if they don’t exist in Azure Active Directory User permissions that are assigned individually and are duplicated through membership in a group remain after the group membership is removed for the user. Users removed from an Azure Databricks workspace directly, using the Azure Databricks Admin console: Lose access to that Azure Databricks workspace but may still have access to other Azure Databricks workspaces. Will not be synced again using Azure Active Directory provisioning, even if they remain in the enterprise application. The initial Azure Active Directory sync is triggered immediately after you enable provisioning. Subsequent syncs are triggered every 20-40 minutes, depending on the number of users and groups in the application. See Provisioning summary report in the Azure Active Directory documentation. You cannot update the username or email address of a Azure Databricks user. The admins group is a reserved group in Azure Databricks and cannot be removed. Groups cannot be renamed in Azure Databricks; do not attempt to rename them in Azure Active Directory. You can use the Azure Databricks Groups API 2.0 or the Groups UI to get a list of members of any Azure Databricks group. Troubleshooting Users and groups do not sync If you are using the Azure Databricks SCIM Provisioning Connector application: In the Azure Databricks admin console, verify that the Azure Databricks user whose personal access token is being used by the Azure Databricks SCIM Provisioning Connector application is still an admin user in Azure Databricks and that the token is still valid. Do not attempt to sync nested groups, which are not supported by Azure Active Directory automatic provisioning. For more information, see this FAQ. After initial sync, the users and groups stop syncing If you are using the Azure Databricks SCIM Provisioning Connector application: After the initial sync, Azure Active Directory does not sync immediately after you change user or group assignments. It schedules a sync with the application after a delay, based on the number of users and groups. To request an immediate sync, go to Manage > Provisioning for the enterprise application and select Clear current state and restart synchronization. Azure Active Directory provisioning service IP range not accessible The Azure Active Directory provisioning service operates under specific IP ranges. If you need to restrict network access, you must allow traffic from the IP addresses for AzureActiveDirectory in this IP range file. For more information, see IP Ranges.",Configure SCIM provisioning for Microsoft Azure Active Directory
67,Follow-up of SF: 00158271,">> CX wanted to enable Unity Catalog.
>> It is enabled.
>> When cx is trying to create a metastore they are facing the below error :

2606.a7bef970.chunk.js:1 GET https://adb-1101798370485247.7.azuredatabricks.net/ajax-api/2.0/unity-catalog/catalogs 404
(anonymous) @ 2606.a7bef970.chunk.js:1
e.exports @ 2606.a7bef970.chunk.js:1
e.exports @ 2606.a7bef970.chunk.js:1
Promise.then (async)
s.request @ 2606.a7bef970.chunk.js:1
i.forEach.s.<computed> @ 2606.a7bef970.chunk.js:1
(anonymous) @ 2606.a7bef970.chunk.js:1
(anonymous) @ 2828.8b3bdbe2.chunk.js:1
(anonymous) @ 2828.8b3bdbe2.chunk.js:1
Ui @ 7043.2d6bf56d.chunk.js:1
n.unstable_runWithPriority @ 7043.2d6bf56d.chunk.js:1
$l @ 7043.2d6bf56d.chunk.js:1
Di @ 7043.2d6bf56d.chunk.js:1
(anonymous) @ 7043.2d6bf56d.chunk.js:1
F @ 7043.2d6bf56d.chunk.js:1
w.port1.onmessage @ 7043.2d6bf56d.chunk.js:1

>> They referred to the below documentation : https://docs.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/get-started

Kindly provide some insights on this.",https://docs.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/scim/aad,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure SCIM provisioning for Microsoft Azure Active Directory Article 05/25/2022 6 minutes to read 5 contributors In this article Requirements Use an Azure Active Directory enterprise application Automate SCIM provisioning using Microsoft Graph Provisioning tips Troubleshooting Important This feature is in Public Preview. To enable provisioning to Azure Databricks using Azure Active Directory (Azure AD) you must create an enterprise application for each Azure Databricks workspace. Note The way provisioning is configured is entirely separate from configuring authentication and conditional access for Azure Databricks workspaces. Authentication for Azure Databricks is handled automatically by Azure Active Directory, using the OpenID Connect protocol flow. You configure conditional access, which lets you create rules to require multi-factor authentication or restrict logins to local networks, at the service level. Requirements Your Azure Databricks account must have the Premium Plan. Your Azure Active Directory account must be a Premium edition account. You must be a global administrator for the Azure Active Directory account. There are two ways to configure provisioning: You can create an enterprise application in the Azure portal, and use that application for provisioning. If you have an existing application, you can modify its code to automate SCIM provisioning using Microsoft Graph. This removes the need for a separate provisioning application in the Azure Portal. Use an Azure Active Directory enterprise application In the following examples, replace <workspace-url> with the workspace URL of your Azure Databricks deployment. In this section: Create the enterprise application and connect it to the Azure Databricks SCIM API Assign users and groups to the application Create the enterprise application and connect it to the Azure Databricks SCIM API Generate a personal access token in Azure Databricks and copy it. You provide this token to Azure Active Directory in a subsequent step. Important Generate this token as an Azure Databricks admin who is not managed by the Azure Active Directory enterprise application. If the Azure Databricks admin user who owns the personal access token is deprovisioned using Azure Active Directory, the SCIM provisioning application will be disabled. In your Azure portal, go to Azure Active Directory > Enterprise Applications. Click + New Application above the application list. Under Add from the gallery, search for and select Azure Databricks SCIM Provisioning Connector. Enter a Name for the application and click Add. Use a name that will help administrators find it, like <workspace-name>-provisioning. Under the Manage menu, click Provisioning. Set Provisioning Mode to Automatic. Enter the SCIM API endpoint URL. Append /api/2.0/preview/scim to your workspace URL: Copy https://<workspace-url>/api/2.0/preview/scim
 Replace <workspace-url> with the workspace URL of your Azure Databricks deployment. See Get workspace, cluster, notebook, folder, model, and job identifiers. Set Secret Token to the Azure Databricks personal access token that you generated in step 1. Click Test Connection and wait for the message that confirms that the credentials are authorized to enable provisioning. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning. Click Save. Assign users and groups to the application Go to Manage > Provisioning. Under Settings, set Scope to Sync only assigned users and groups. Databricks recommends this option, which syncs only users and groups assigned to the enterprise application. Note Azure Active Directory does not support the automatic provisioning of nested groups to Azure Databricks. Azure Active Directory can only read and provision users that are immediate members of the explicitly assigned group. As a workaround, explicitly assign (or otherwise scope in) the groups that contain the users who need to be provisioned. For more information, see this FAQ. To start synchronizing Azure Active Directory users and groups to Azure Databricks, click the Provisioning Status toggle. Click Save. Test your provisioning setup: Go to Manage > Users and groups. Add some users and groups. Click Add user, select the users and groups, and click the Assign button. Wait a few minutes and check that the users and groups exist in your Azure Databricks workspace. In the future, users and groups that you add and assign are automatically provisioned when Azure Active Directory schedules the next sync. Important Do not assign the Azure Databricks admin whose personal access token was used to configure the Azure Databricks SCIM Provisioning Connector application. Automate SCIM provisioning using Microsoft Graph Microsoft Graph includes authentication and authorization libraries that you can integrate into your application to automate provisioning of users and groups to Azure Databricks, instead of configuring a SCIM provisioning connector application. Follow the instructions for registering an application with Microsoft Graph. Make a note of the Application ID and the Tenant ID for the application Go to the applications’s Overview page. On that page: Configure a client secret for the application, and make a note of the secret. Grant the application these permissions: Application.ReadWrite.All Application.ReadWrite.OwnedBy Ask an Azure Active Directory administrator to grant admin consent. Update your application’s code to add support for Microsoft Graph. Provisioning tips Users and groups that existed in Azure Databricks prior to enabling provisioning exhibit the following behavior upon provisioning sync: Are merged if they also exist in Azure Active Directory Are ignored if they don’t exist in Azure Active Directory User permissions that are assigned individually and are duplicated through membership in a group remain after the group membership is removed for the user. Users removed from an Azure Databricks workspace directly, using the Azure Databricks Admin console: Lose access to that Azure Databricks workspace but may still have access to other Azure Databricks workspaces. Will not be synced again using Azure Active Directory provisioning, even if they remain in the enterprise application. The initial Azure Active Directory sync is triggered immediately after you enable provisioning. Subsequent syncs are triggered every 20-40 minutes, depending on the number of users and groups in the application. See Provisioning summary report in the Azure Active Directory documentation. You cannot update the username or email address of a Azure Databricks user. The admins group is a reserved group in Azure Databricks and cannot be removed. Groups cannot be renamed in Azure Databricks; do not attempt to rename them in Azure Active Directory. You can use the Azure Databricks Groups API 2.0 or the Groups UI to get a list of members of any Azure Databricks group. Troubleshooting Users and groups do not sync If you are using the Azure Databricks SCIM Provisioning Connector application: In the Azure Databricks admin console, verify that the Azure Databricks user whose personal access token is being used by the Azure Databricks SCIM Provisioning Connector application is still an admin user in Azure Databricks and that the token is still valid. Do not attempt to sync nested groups, which are not supported by Azure Active Directory automatic provisioning. For more information, see this FAQ. After initial sync, the users and groups stop syncing If you are using the Azure Databricks SCIM Provisioning Connector application: After the initial sync, Azure Active Directory does not sync immediately after you change user or group assignments. It schedules a sync with the application after a delay, based on the number of users and groups. To request an immediate sync, go to Manage > Provisioning for the enterprise application and select Clear current state and restart synchronization. Azure Active Directory provisioning service IP range not accessible The Azure Active Directory provisioning service operates under specific IP ranges. If you need to restrict network access, you must allow traffic from the IP addresses for AzureActiveDirectory in this IP range file. For more information, see IP Ranges.",Configure SCIM provisioning for Microsoft Azure Active Directory
68,"External users in AAD, display name is blank when you add a user to the Databricks Admin console","In the Databricks console, when you add some users email that is in the Azure Active Directory, for some users it is not displaying the Name field. It is blank. In the azure active directory the display name is populated. 

Within Azure Databricks, when a new user is added to the workspace through the 'Admin Console', the user's 'Name' field comes up empty. These users are part of AAD tenant. They can be added into Databricks Admin Console through their VA email. When added using their VA email, the 'Name' field doesn't get transferred from the AAD tenant.


Is reproducible in any workspace/tenant 

ICM: https://portal.microsofticm.com/imp/v3/incidents/details/315994706/home

Some images attached.",https://docs.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/scim/aad,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Configure SCIM provisioning for Microsoft Azure Active Directory Article 05/25/2022 6 minutes to read 5 contributors In this article Requirements Use an Azure Active Directory enterprise application Automate SCIM provisioning using Microsoft Graph Provisioning tips Troubleshooting Important This feature is in Public Preview. To enable provisioning to Azure Databricks using Azure Active Directory (Azure AD) you must create an enterprise application for each Azure Databricks workspace. Note The way provisioning is configured is entirely separate from configuring authentication and conditional access for Azure Databricks workspaces. Authentication for Azure Databricks is handled automatically by Azure Active Directory, using the OpenID Connect protocol flow. You configure conditional access, which lets you create rules to require multi-factor authentication or restrict logins to local networks, at the service level. Requirements Your Azure Databricks account must have the Premium Plan. Your Azure Active Directory account must be a Premium edition account. You must be a global administrator for the Azure Active Directory account. There are two ways to configure provisioning: You can create an enterprise application in the Azure portal, and use that application for provisioning. If you have an existing application, you can modify its code to automate SCIM provisioning using Microsoft Graph. This removes the need for a separate provisioning application in the Azure Portal. Use an Azure Active Directory enterprise application In the following examples, replace <workspace-url> with the workspace URL of your Azure Databricks deployment. In this section: Create the enterprise application and connect it to the Azure Databricks SCIM API Assign users and groups to the application Create the enterprise application and connect it to the Azure Databricks SCIM API Generate a personal access token in Azure Databricks and copy it. You provide this token to Azure Active Directory in a subsequent step. Important Generate this token as an Azure Databricks admin who is not managed by the Azure Active Directory enterprise application. If the Azure Databricks admin user who owns the personal access token is deprovisioned using Azure Active Directory, the SCIM provisioning application will be disabled. In your Azure portal, go to Azure Active Directory > Enterprise Applications. Click + New Application above the application list. Under Add from the gallery, search for and select Azure Databricks SCIM Provisioning Connector. Enter a Name for the application and click Add. Use a name that will help administrators find it, like <workspace-name>-provisioning. Under the Manage menu, click Provisioning. Set Provisioning Mode to Automatic. Enter the SCIM API endpoint URL. Append /api/2.0/preview/scim to your workspace URL: Copy https://<workspace-url>/api/2.0/preview/scim
 Replace <workspace-url> with the workspace URL of your Azure Databricks deployment. See Get workspace, cluster, notebook, folder, model, and job identifiers. Set Secret Token to the Azure Databricks personal access token that you generated in step 1. Click Test Connection and wait for the message that confirms that the credentials are authorized to enable provisioning. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning. Click Save. Assign users and groups to the application Go to Manage > Provisioning. Under Settings, set Scope to Sync only assigned users and groups. Databricks recommends this option, which syncs only users and groups assigned to the enterprise application. Note Azure Active Directory does not support the automatic provisioning of nested groups to Azure Databricks. Azure Active Directory can only read and provision users that are immediate members of the explicitly assigned group. As a workaround, explicitly assign (or otherwise scope in) the groups that contain the users who need to be provisioned. For more information, see this FAQ. To start synchronizing Azure Active Directory users and groups to Azure Databricks, click the Provisioning Status toggle. Click Save. Test your provisioning setup: Go to Manage > Users and groups. Add some users and groups. Click Add user, select the users and groups, and click the Assign button. Wait a few minutes and check that the users and groups exist in your Azure Databricks workspace. In the future, users and groups that you add and assign are automatically provisioned when Azure Active Directory schedules the next sync. Important Do not assign the Azure Databricks admin whose personal access token was used to configure the Azure Databricks SCIM Provisioning Connector application. Automate SCIM provisioning using Microsoft Graph Microsoft Graph includes authentication and authorization libraries that you can integrate into your application to automate provisioning of users and groups to Azure Databricks, instead of configuring a SCIM provisioning connector application. Follow the instructions for registering an application with Microsoft Graph. Make a note of the Application ID and the Tenant ID for the application Go to the applications’s Overview page. On that page: Configure a client secret for the application, and make a note of the secret. Grant the application these permissions: Application.ReadWrite.All Application.ReadWrite.OwnedBy Ask an Azure Active Directory administrator to grant admin consent. Update your application’s code to add support for Microsoft Graph. Provisioning tips Users and groups that existed in Azure Databricks prior to enabling provisioning exhibit the following behavior upon provisioning sync: Are merged if they also exist in Azure Active Directory Are ignored if they don’t exist in Azure Active Directory User permissions that are assigned individually and are duplicated through membership in a group remain after the group membership is removed for the user. Users removed from an Azure Databricks workspace directly, using the Azure Databricks Admin console: Lose access to that Azure Databricks workspace but may still have access to other Azure Databricks workspaces. Will not be synced again using Azure Active Directory provisioning, even if they remain in the enterprise application. The initial Azure Active Directory sync is triggered immediately after you enable provisioning. Subsequent syncs are triggered every 20-40 minutes, depending on the number of users and groups in the application. See Provisioning summary report in the Azure Active Directory documentation. You cannot update the username or email address of a Azure Databricks user. The admins group is a reserved group in Azure Databricks and cannot be removed. Groups cannot be renamed in Azure Databricks; do not attempt to rename them in Azure Active Directory. You can use the Azure Databricks Groups API 2.0 or the Groups UI to get a list of members of any Azure Databricks group. Troubleshooting Users and groups do not sync If you are using the Azure Databricks SCIM Provisioning Connector application: In the Azure Databricks admin console, verify that the Azure Databricks user whose personal access token is being used by the Azure Databricks SCIM Provisioning Connector application is still an admin user in Azure Databricks and that the token is still valid. Do not attempt to sync nested groups, which are not supported by Azure Active Directory automatic provisioning. For more information, see this FAQ. After initial sync, the users and groups stop syncing If you are using the Azure Databricks SCIM Provisioning Connector application: After the initial sync, Azure Active Directory does not sync immediately after you change user or group assignments. It schedules a sync with the application after a delay, based on the number of users and groups. To request an immediate sync, go to Manage > Provisioning for the enterprise application and select Clear current state and restart synchronization. Azure Active Directory provisioning service IP range not accessible The Azure Active Directory provisioning service operates under specific IP ranges. If you need to restrict network access, you must allow traffic from the IP addresses for AzureActiveDirectory in this IP range file. For more information, see IP Ranges.",Configure SCIM provisioning for Microsoft Azure Active Directory
69,ARR | 2206130050001255 | Provide read permissions to all jobs running on job clusters,"Hello team,

The cx implemented Access Control in Databricks workspace (https://docs.databricks.com/security/access-control/index.html).
Access control works fine except for job clusters. When I had a call with the cx, I got more details on what they are trying to do and the access they need.
  
The user is using DBX that he is calling from Azure DevOps, to deploy and run the jobs in Databricks. The job is not running a notebook, it runs a Python script that is stored under (dbfs:/Shared/dbx/projects/models/). Example job run that was shared during the call:
https://adb-6485290132708982.2.azuredatabricks.net/?o=6485290132708982#job/338093280997341/run/13104

This job ran the script:
dbfs:/Shared/dbx/projects/models/274d5afee9c043f9a49d09561cc317a6/artifacts/models/gstm/src/entrypoint.py (The sub-directory under /models/ changes for each run).

He is able to see the output from the job run, but the issue is when he tries to access the Driver Logs then he doesn’t have this permission as in the attached snapshot.
https://adb-6485290132708982.2.azuredatabricks.net/?o=6485290132708982#setting/clusters/0615-143254-d479bgph/driverLogs

Would you please advise which permission this access is inherited from, and how to give the user the permission to check the driver log of the mentioned jobs/job clusters?

If more info is needed, please let me know.

Thanks,
Doaa",https://docs.microsoft.com/en-us/azure/databricks/security/access-control/jobs-acl,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Jobs access control Article 06/23/2022 2 minutes to read 5 contributors In this article Job permissions Configure job permissions Terraform integration Note Access control is available only in the Premium Plan. Enabling access control for jobs allows job owners to control who can view job results or manage runs of a job. This article describes the individual permissions and how to configure jobs access control. Before you can use jobs access control, an Azure Databricks admin must enable it for the workspace. See Enable jobs access control for your workspace. Job permissions There are five permission levels for jobs: No Permissions, Can View, Can Manage Run, Is Owner, and Can Manage. Admins are granted the Can Manage permission by default, and they can assign that permission to non-admin users. Note The job owner can be changed only by an admin. The table lists the abilities for each permission. Ability No Permissions Can View Can Manage Run Is Owner Can Manage View job details and settings x x x x x View results, Spark UI, logs of a job run x x x x Run now x x x Cancel run x x x Edit job settings x x Modify permissions x x Delete job x x Change owner Note The creator of a job has Is Owner permission. A job cannot have more than one owner. A job cannot have a group as an owner. Jobs triggered through Run Now assume the permissions of the job owner and not the user who issued Run Now. For example, even if job A is configured to run on an existing cluster accessible only to the job owner (user A), a user (user B) with Can Manage Run permission can start a new run of the job. You can view notebook run results only if you have the Can View or higher permission on the job. This allows jobs access control to be intact even if the job notebook was renamed, moved, or deleted. Jobs access control applies to jobs displayed in the Databricks Jobs UI and their runs. It doesn’t apply to runs spawned by modularized or linked code in notebooks or runs submitted by API whose ACLs are bundled with the notebooks. Configure job permissions Note This section describes how to manage permissions using the UI. You can also use the Permissions API 2.0. You must have Can Manage or Is Owner permission. Go to the details page for a job. Click the Edit permissions button in the Job details panel. In the pop-up dialog box, assign job permissions via the drop-down menu beside a user’s name. Click Save Changes. Terraform integration You can manage permissions in a fully automated setup using Databricks Terraform provider and databricks_permissions: Copy resource ""databricks_group"" ""auto"" {
  display_name = ""Automation""
}

resource ""databricks_group"" ""eng"" {
  display_name = ""Engineering""
}

data ""databricks_spark_version"" ""latest"" {}

data ""databricks_node_type"" ""smallest"" {
  local_disk = true
}

resource ""databricks_job"" ""this"" {
  name                = ""Featurization""
  max_concurrent_runs = 1

  new_cluster {
    num_workers   = 300
    spark_version = data.databricks_spark_version.latest.id
    node_type_id  = data.databricks_node_type.smallest.id
  }

  notebook_task {
    notebook_path = ""/Production/MakeFeatures""
  }
}

resource ""databricks_permissions"" ""job_usage"" {
  job_id = databricks_job.this.id

  access_control {
    group_name       = ""users""
    permission_level = ""CAN_VIEW""
  }

  access_control {
    group_name       = databricks_group.auto.display_name
    permission_level = ""CAN_MANAGE_RUN""
  }

  access_control {
    group_name       = databricks_group.eng.display_name
    permission_level = ""CAN_MANAGE""
  }
}",Jobs access control
70,Databricks - Unable to register ML model,"Question: What time did the problem begin?
Answer: Sat, Jun 18, 2022, 12:00 AM (UTC-08:00) Pacific Time (US & Canada)

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer: 

Question: Is this a new problem, or it has happened before?
Answer: New problem, worked before

Question: Any changes made?
Answer: No

Question: Cluster URL
Answer: https://adb-5859819091135113.13.azuredatabricks.net/?o=5859819091135113#job/3980117562827/run/519789

Question: Notebook URL if available
Answer: https://adb-5859819091135113.13.azuredatabricks.net/?o=5859819091135113#job/3980117562827/run/519789

Question: libraries involved
Answer: 

Question: Additional details about the issue
Answer: model_details = mlflow.register_model(model_uri=model_uri, name=model_name)

Unable to register the model



<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Databricks:
What time did the problem begin? - 2022-06-18T07:00:00.000Z;
Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank - ;
Is this a new problem, or it has happened before? - New problem, worked before;
Any changes made? - No;
Cluster URL - https://adb-5859819091135113.13.azuredatabricks.net/?o=5859819091135113#job/3980117562827/run/519789;
Notebook URL if available - https://adb-5859819091135113.13.azuredatabricks.net/?o=5859819091135113#job/3980117562827/run/519789;
libraries involved - ;
Additional details about the issue - model_details = mlflow.register_model(model_uri=model_uri, name=model_name)

Unable to register the model;

- ProblemStartTime: 06/18/2022 07:00:00
- Cloud: Azure
- AzureProductSubscriptionID: 6dfbe157-2219-4313-adfc-6df4829ab651
- AzureProductSubscriptionName: Abs-ITDS-Prod
- Tenant Id: b7f604a0-00a9-4188-9248-42f3a5aac2e9
- Object Id: caa4e396-f013-4bcd-a9ef-33cd1eacad3e
- SubscriptionType: Premier
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Premier with Azure Rapid response
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: True

- Location: westus
- ResourceUri: /subscriptions/6dfbe157-2219-4313-adfc-6df4829ab651/resourceGroups/dbs-itds-prod-wus-rg/providers/Microsoft.Databricks/workspaces/dbs-itds-prod-wus-wp-02

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/token-management,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Token Management API 2.0 Article 01/26/2022 2 minutes to read 3 contributors In this article The Token Management API lets Azure Databricks administrators manage their users’ Azure Databricks personal access tokens. As an admin, you can: Monitor and revoke users’ personal access tokens. Control the lifetime of future tokens in your workspace. You can also control which users can create and use tokens via the Permissions API 2.0 or in the Admin Console. The Token Management API is provided as an OpenAPI 3.0 specification that you can download and view as a structured API reference in your favorite OpenAPI editor. Download the OpenAPI specification View in Redocly: this link immediately opens the OpenAPI specification as a structured API reference for easy viewing. View in Postman: Postman is an app that you must download to your computer. Once you do, you can import the OpenAPI spec as a file or URL. View in Swagger Editor: In the online Swagger Editor, go to the File menu and click Import file to import and view the downloaded OpenAPI specification. Important To access Databricks REST APIs, you must authenticate.",Token Management API 2.0
71,using DBconnect but got errors,"Cx used DB connect to connect Databricks clusters but got some errors.
FIrst cluster:
https://adb-640597335864734.14.azuredatabricks.net/?o=640597335864734#setting/clusters/1006-054016-viand258/configuration

Error message:
 22/06/21 16:46:20 WARN SparkServiceRPCClient: Cluster 1006-054016-viand258 in state PENDING, waiting for it to start running...
22/06/21 16:46:30 WARN SparkServiceRPCClient: Cluster 1006-054016-viand258 in state PENDING, waiting for it to start running...
22/06/21 16:46:40 ERROR SparkClientManager: Fail to get the SparkClient
java.util.concurrent.ExecutionException: com.databricks.service.SparkServiceConnectionException: Invalid cluster ID: '1006-054016-viand258'
The cluster ID you specified does not correspond to any existing cluster.
Cluster ID: The ID of the cluster on which you want to run your code
  - This should look like 0123-456789-abcd012
  - Get current value: spark.conf.get('spark.databricks.service.clusterId')
  - Set via conf: spark.conf.set('spark.databricks.service.clusterId', your cluster ID)
  - Set via environment variable: export DATABRICKS_CLUSTER_ID=your cluster ID

But the cluster was vaild.

Second cluster:
https://adb-640597335864734.14.azuredatabricks.net/?o=640597335864734#setting/clusters/0729-025817-leap125/configuration

Error message:
Invaid shared address(I will attach the screen shot)

This cluster was valid as well.

Db connect version: 7.3.*


Please kindly advise us.

Thank you in advance!",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Databricks Connect Article 05/18/2022 27 minutes to read 6 contributors In this article Overview Requirements Set up the client Set up your IDE or notebook server Run examples from your IDE Access DBUtils Access the Hadoop filesystem Set Hadoop configurations Troubleshooting Authentication using Azure Active Directory tokens Limitations Note Databricks recommends that you use dbx by Databricks Labs for local development instead of Databricks Connect. Databricks plans no new feature development for Databricks Connect at this time. Also, be aware of the limitations of Databricks Connect. Databricks Connect allows you to connect your favorite IDE (Eclipse, IntelliJ, PyCharm, RStudio, Visual Studio Code), notebook server (Jupyter Notebook, Zeppelin), and other custom applications to Azure Databricks clusters. This article explains how Databricks Connect works, walks you through the steps to get started with Databricks Connect, explains how to troubleshoot issues that may arise when using Databricks Connect, and differences between running using Databricks Connect versus running in an Azure Databricks notebook. Overview Databricks Connect is a client library for Databricks Runtime. It allows you to write jobs using Spark APIs and run them remotely on an Azure Databricks cluster instead of in the local Spark session. For example, when you run the DataFrame command spark.read.format(""parquet"").load(...).groupBy(...).agg(...).show() using Databricks Connect, the parsing and planning of the job runs on your local machine. Then, the logical representation of the job is sent to the Spark server running in Azure Databricks for execution in the cluster. With Databricks Connect, you can: Run large-scale Spark jobs from any Python, Java, Scala, or R application. Anywhere you can import pyspark, import org.apache.spark, or require(SparkR), you can now run Spark jobs directly from your application, without needing to install any IDE plugins or use Spark submission scripts. Step through and debug code in your IDE even when working with a remote cluster. Iterate quickly when developing libraries. You do not need to restart the cluster after changing Python or Java library dependencies in Databricks Connect, because each client session is isolated from each other in the cluster. Shut down idle clusters without losing work. Because the client application is decoupled from the cluster, it is unaffected by cluster restarts or upgrades, which would normally cause you to lose all the variables, RDDs, and DataFrame objects defined in a notebook. Note For Python development with SQL queries, Databricks recommends that you use the Databricks SQL Connector for Python instead of Databricks Connect. the Databricks SQL Connector for Python is easier to set up than Databricks Connect. Also, Databricks Connect parses and plans jobs runs on your local machine, while jobs run on remote compute resources. This can make it especially difficult to debug runtime errors. The Databricks SQL Connector for Python submits SQL queries directly to remote compute resources and fetches results. Requirements Only the following Databricks Runtime versions are supported: Databricks Runtime 10.4 LTS ML, Databricks Runtime 10.4 LTS Databricks Runtime 9.1 LTS ML, Databricks Runtime 9.1 LTS Databricks Runtime 7.3 LTS ML, Databricks Runtime 7.3 LTS Databricks Runtime 6.4 ML, Databricks Runtime 6.4 The minor version of your client Python installation must be the same as the minor Python version of your Azure Databricks cluster. The table shows the Python version installed with each Databricks Runtime. Databricks Runtime version Python version 10.4 LTS ML, 10.4 LTS 3.8 9.1 LTS ML, 9.1 LTS 3.8 7.3 LTS ML, 7.3 LTS 3.7 6.4 ML, 6.4 3.7 For example, if you’re using Conda on your local development environment and your cluster is running Python 3.7, you must create an environment with that version, for example: Bash Copy conda create --name dbconnect python=3.7
conda
 The Databricks Connect major and minor package version must always match your Databricks Runtime version. Databricks recommends that you always use the most recent package of Databricks Connect that matches your Databricks Runtime version. For example, when using a Databricks Runtime 7.3 LTS cluster, use the databricks-connect==7.3.* package. Note See the Databricks Connect release notes for a list of available Databricks Connect releases and maintenance updates. Java Runtime Environment (JRE) 8. The client has been tested with the OpenJDK 8 JRE. The client does not support Java 11. Note On Windows, if you see an error that Databricks Connect cannot find winutils.exe, see Cannot find winutils.exe on Windows. Set up the client Note Before you begin to set up the Databricks Connect client, you must meet the requirements for Databricks Connect. Step 1: Install the client Uninstall PySpark. This is required because the databricks-connect package conflicts with PySpark. For details, see Conflicting PySpark installations. Bash Copy pip uninstall pyspark
 Install the Databricks Connect client. Bash Copy pip install -U ""databricks-connect==7.3.*""  # or X.Y.* to match your cluster version.
 Note Always specify databricks-connect==X.Y.* instead of databricks-connect=X.Y, to make sure that the newest package is installed. Step 2: Configure connection properties Collect the following configuration properties: Azure Databricks workspace URL. Azure Databricks personal access token or an Azure Active Directory token. For Azure Data Lake Storage (ADLS) credential passthrough, you must use an Azure Active Directory token. Azure Active Directory credential passthrough is supported only on Standard clusters running Databricks Runtime 7.3 LTS and above, and is not compatible with service principal authentication. For more information about authentication with Azure Active Directory tokens, see Authentication using Azure Active Directory tokens. The ID of the cluster you created. You can obtain the cluster ID from the URL. Here the cluster ID is 1108-201635-xxxxxxxx. The unique organization ID for your workspace. See Get workspace, cluster, notebook, folder, model, and job identifiers. The port that Databricks Connect connects to. The default port is 15001. If your cluster is configured to use a different port, such as 8787 which was given in previous instructions for Azure Databricks, use the configured port number. Configure the connection. You can use the CLI, SQL configs, or environment variables. The precedence of configuration methods from highest to lowest is: SQL config keys, CLI, and environment variables. CLI Run databricks-connect. Bash Copy databricks-connect configure
 The license displays: Copy Copyright (2018) Databricks, Inc.

This library (the ""Software"") may not be used except in connection with the
Licensee's use of the Databricks Platform Services pursuant to an Agreement
  ...
 Accept the license and supply configuration values. For Databricks Host and Databricks Token, enter the workspace URL and the personal access token you noted in Step 1. Copy Do you accept the above agreement? [y/N] y
Set new config values (leave input empty to accept default):
Databricks Host [no current value, must start with https://]: <databricks-url>
Databricks Token [no current value]: <databricks-token>
Cluster ID (e.g., 0921-001415-jelly628) [no current value]: <cluster-id>
Org ID (Azure-only, see ?o=orgId in URL) [0]: <org-id>
Port [15001]: <port>
 If you get a message that the Azure Active Directory token is too long, you can leave the Databricks Token field empty and manually enter the token in ~/.databricks-connect. SQL configs or environment variables. The following table shows the SQL config keys and the environment variables that correspond to the configuration properties you noted in Step 1. To set a SQL config key, use sql(""set config=value""). For example: sql(""set spark.databricks.service.clusterId=0304-201045-abcdefgh""). Parameter SQL config key Environment variable name Databricks Host spark.databricks.service.address DATABRICKS_ADDRESS Databricks Token spark.databricks.service.token DATABRICKS_API_TOKEN Cluster ID spark.databricks.service.clusterId DATABRICKS_CLUSTER_ID Org ID spark.databricks.service.orgId DATABRICKS_ORG_ID Port spark.databricks.service.port DATABRICKS_PORT Test connectivity to Azure Databricks. Bash Copy databricks-connect test
 If the cluster you configured is not running, the test starts the cluster which will remain running until its configured autotermination time. The output should be something like: Copy * PySpark is installed at /.../3.5.6/lib/python3.5/site-packages/pyspark
* Checking java version
java version ""1.8.0_152""
Java(TM) SE Runtime Environment (build 1.8.0_152-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.152-b16, mixed mode)
* Testing scala command
18/12/10 16:38:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
18/12/10 16:38:50 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
18/12/10 16:39:53 WARN SparkServiceRPCClient: Now tracking server state for 5abb7c7e-df8e-4290-947c-c9a38601024e, invalidating prev state
18/12/10 16:39:59 WARN SparkServiceRPCClient: Syncing 129 files (176036 bytes) took 3003 ms
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.0-SNAPSHOT
      /_/

Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_152)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.range(100).reduce(_ + _)
Spark context Web UI available at https://10.8.5.214:4040
Spark context available as 'sc' (master = local[*], app id = local-1544488730553).
Spark session available as 'spark'.
View job details at <databricks-url>/?o=0#/setting/clusters/<cluster-id>/sparkUi
View job details at <databricks-url>?o=0#/setting/clusters/<cluster-id>/sparkUi
res0: Long = 4950

scala> :quit

* Testing python command
18/12/10 16:40:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
18/12/10 16:40:17 WARN MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set.
18/12/10 16:40:28 WARN SparkServiceRPCClient: Now tracking server state for 5abb7c7e-df8e-4290-947c-c9a38601024e, invalidating prev state
View job details at <databricks-url>/?o=0#/setting/clusters/<cluster-id>/sparkUi
 Set up your IDE or notebook server The section describes how to configure your preferred IDE or notebook server to use the Databricks Connect client. In this section: Jupyter notebook PyCharm SparkR and RStudio Desktop sparklyr and RStudio Desktop IntelliJ (Scala or Java) Eclipse Visual Studio Code SBT Jupyter notebook Note Databricks recommends that you use dbx by Databricks Labs for local development instead of Databricks Connect. Databricks plans no new feature development for Databricks Connect at this time. Also, be aware of the limitations of Databricks Connect. Before you begin to use Databricks Connect, you must meet the requirements and set up the client for Databricks Connect. The Databricks Connect configuration script automatically adds the package to your project configuration. To get started in a Python kernel, run: Python Copy from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
 To enable the %sql shorthand for running and visualizing SQL queries, use the following snippet: Python Copy from IPython.core.magic import line_magic, line_cell_magic, Magics, magics_class

@magics_class
class DatabricksConnectMagics(Magics):

   @line_cell_magic
   def sql(self, line, cell=None):
       if cell and line:
           raise ValueError(""Line must be empty for cell magic"", line)
       try:
           from autovizwidget.widget.utils import display_dataframe
       except ImportError:
           print(""Please run `pip install autovizwidget` to enable the visualization widget."")
           display_dataframe = lambda x: x
       return display_dataframe(self.get_spark().sql(cell or line).toPandas())

   def get_spark(self):
       user_ns = get_ipython().user_ns
       if ""spark"" in user_ns:
           return user_ns[""spark""]
       else:
           from pyspark.sql import SparkSession
           user_ns[""spark""] = SparkSession.builder.getOrCreate()
           return user_ns[""spark""]

ip = get_ipython()
ip.register_magics(DatabricksConnectMagics)
 PyCharm Note Databricks recommends that you use dbx by Databricks Labs for local development instead of Databricks Connect. Databricks plans no new feature development for Databricks Connect at this time. Also, be aware of the limitations of Databricks Connect. Before you begin to use Databricks Connect, you must meet the requirements and set up the client for Databricks Connect. The Databricks Connect configuration script automatically adds the package to your project configuration. Python 3 clusters When you create a PyCharm project, select Existing Interpreter. From the drop-down menu, select the Conda environment you created (see Requirements). Go to Run > Edit Configurations. Add PYSPARK_PYTHON=python3 as an environment variable. SparkR and RStudio Desktop Note Databricks recommends that you use dbx by Databricks Labs for local development instead of Databricks Connect. Databricks plans no new feature development for Databricks Connect at this time. Also, be aware of the limitations of Databricks Connect. Before you begin to use Databricks Connect, you must meet the requirements and set up the client for Databricks Connect. Download and unpack the open source Spark onto your local machine. Choose the same version as in your Azure Databricks cluster (Hadoop 2.7). Run databricks-connect get-jar-dir. This command returns a path like /usr/local/lib/python3.5/dist-packages/pyspark/jars. Copy the file path of one directory above the JAR directory file path, for example, /usr/local/lib/python3.5/dist-packages/pyspark, which is the SPARK_HOME directory. Configure the Spark lib path and Spark home by adding them to the top of your R script. Set <spark-lib-path> to the directory where you unpacked the open source Spark package in step 1. Set <spark-home-path> to the Databricks Connect directory from step 2. R Copy # Point to the OSS package path, e.g., /path/to/.../spark-2.4.0-bin-hadoop2.7
library(SparkR, lib.loc = .libPaths(c(file.path('<spark-lib-path>', 'R', 'lib'), .libPaths())))

# Point to the Databricks Connect PySpark installation, e.g., /path/to/.../pyspark
Sys.setenv(SPARK_HOME = ""<spark-home-path>"")
 Initiate a Spark session and start running SparkR commands. R Copy sparkR.session()

df <- as.DataFrame(faithful)
head(df)

df1 <- dapply(df, function(x) { x }, schema(df))
collect(df1)
 sparklyr and RStudio Desktop Important This feature is in Public Preview. Note Databricks recommends that you use dbx by Databricks Labs for local development instead of Databricks Connect. Databricks plans no new feature development for Databricks Connect at this time. Also, be aware of the limitations of Databricks Connect. Before you begin to use Databricks Connect, you must meet the requirements and set up the client for Databricks Connect. You can copy sparklyr-dependent code that you’ve developed locally using Databricks Connect and run it in an Azure Databricks notebook or hosted RStudio Server in your Azure Databricks workspace with minimal or no code changes. In this section: Requirements Install, configure, and use sparklyr Resources sparklyr and RStudio Desktop limitations Requirements sparklyr 1.2 or above. Databricks Runtime 6.4 or above with matching Databricks Connect. Install, configure, and use sparklyr In RStudio Desktop, install sparklyr 1.2 or above from CRAN or install the latest master version from GitHub. R Copy # Install from CRAN
install.packages(""sparklyr"")

# Or install the latest master version from GitHub
install.packages(""devtools"")
devtools::install_github(""sparklyr/sparklyr"")
 Activate the Python environment with Databricks Connect installed and run the following command in the terminal to get the <spark-home-path>: Bash Copy databricks-connect get-spark-home
 Initiate a Spark session and start running sparklyr commands. R Copy library(sparklyr)
sc <- spark_connect(method = ""databricks"", spark_home = ""<spark-home-path>"")

iris_tbl <- copy_to(sc, iris, overwrite = TRUE)

library(dplyr)
src_tbls(sc)

iris_tbl %>% count
 Close the connection. R Copy spark_disconnect(sc)
 Resources For more information, see the sparklyr GitHub README. For code examples, see sparklyr. sparklyr and RStudio Desktop limitations The following features are unsupported: sparklyr streaming APIs sparklyr ML APIs broom APIs csv_file serialization mode spark submit IntelliJ (Scala or Java) Note Databricks recommends that you use dbx by Databricks Labs for local development instead of Databricks Connect. Databricks plans no new feature development for Databricks Connect at this time. Also, be aware of the limitations of Databricks Connect. Before you begin to use Databricks Connect, you must meet the requirements and set up the client for Databricks Connect. Run databricks-connect get-jar-dir. Point the dependencies to the directory returned from the command. Go to File > Project Structure > Modules > Dependencies > ‘+’ sign > JARs or Directories. To avoid conflicts, we strongly recommend removing any other Spark installations from your classpath. If this is not possible, make sure that the JARs you add are at the front of the classpath. In particular, they must be ahead of any other installed version of Spark (otherwise you will either use one of those other Spark versions and run locally or throw a ClassDefNotFoundError). Check the setting of the breakout option in IntelliJ. The default is All and will cause network timeouts if you set breakpoints for debugging. Set it to Thread to avoid stopping the background network threads. Eclipse Note Databricks recommends that you use dbx by Databricks Labs for local development instead of Databricks Connect. Databricks plans no new feature development for Databricks Connect at this time. Also, be aware of the limitations of Databricks Connect. Before you begin to use Databricks Connect, you must meet the requirements and set up the client for Databricks Connect. Run databricks-connect get-jar-dir. Point the external JARs configuration to the directory returned from the command. Go to Project menu > Properties > Java Build Path > Libraries > Add External Jars. To avoid conflicts, we strongly recommend removing any other Spark installations from your classpath. If this is not possible, make sure that the JARs you add are at the front of the classpath. In particular, they must be ahead of any other installed version of Spark (otherwise you will either use one of those other Spark versions and run locally or throw a ClassDefNotFoundError). Visual Studio Code Note Databricks recommends that you use dbx by Databricks Labs for local development instead of Databricks Connect. Databricks plans no new feature development for Databricks Connect at this time. Also, be aware of the limitations of Databricks Connect. Before you begin to use Databricks Connect, you must meet the requirements and set up the client for Databricks Connect. Verify that the Python extension is installed. Open the Command Palette (Command+Shift+P on macOS and Ctrl+Shift+P on Windows/Linux). Select a Python interpreter. Go to Code > Preferences > Settings, and choose python settings. Run databricks-connect get-jar-dir. Add the directory returned from the command to the User Settings JSON under python.venvPath. This should be added to the Python Configuration. Disable the linter. Click the … on the right side and edit json settings. The modified settings are as follows: If running with a virtual environment, which is the recommended way to develop for Python in VS Code, in the Command Palette type select python interpreter and point to your environment that matches your cluster Python version. For example, if your cluster is Python 3.5, your local environment should be Python 3.5. SBT Note Databricks recommends that you use dbx by Databricks Labs for local development instead of Databricks Connect. Databricks plans no new feature development for Databricks Connect at this time. Also, be aware of the limitations of Databricks Connect. Before you begin to use Databricks Connect, you must meet the requirements and set up the client for Databricks Connect. To use SBT, you must configure your build.sbt file to link against the Databricks Connect JARs instead of the usual Spark library dependency. You do this with the unmanagedBase directive in the following example build file, which assumes a Scala app that has a com.example.Test main object: build.sbt Copy name := ""hello-world""
version := ""1.0""
scalaVersion := ""2.11.6""
// this should be set to the path returned by ``databricks-connect get-jar-dir``
unmanagedBase := new java.io.File(""/usr/local/lib/python2.7/dist-packages/pyspark/jars"")
mainClass := Some(""com.example.Test"")
 Run examples from your IDE Java Java Copy import java.util.ArrayList;
import java.util.List;
import java.sql.Date;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.types.*;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.RowFactory;
import org.apache.spark.sql.Dataset;

public class App {
    public static void main(String[] args) throws Exception {
        SparkSession spark = SparkSession
            .builder()
            .appName(""Temps Demo"")
            .config(""spark.master"", ""local"")
            .getOrCreate();

        // Create a Spark DataFrame consisting of high and low temperatures
        // by airport code and date.
        StructType schema = new StructType(new StructField[] {
            new StructField(""AirportCode"", DataTypes.StringType, false, Metadata.empty()),
            new StructField(""Date"", DataTypes.DateType, false, Metadata.empty()),
            new StructField(""TempHighF"", DataTypes.IntegerType, false, Metadata.empty()),
            new StructField(""TempLowF"", DataTypes.IntegerType, false, Metadata.empty()),
        });

        List<Row> dataList = new ArrayList<Row>();
        dataList.add(RowFactory.create(""BLI"", Date.valueOf(""2021-04-03""), 52, 43));
        dataList.add(RowFactory.create(""BLI"", Date.valueOf(""2021-04-02""), 50, 38));
        dataList.add(RowFactory.create(""BLI"", Date.valueOf(""2021-04-01""), 52, 41));
        dataList.add(RowFactory.create(""PDX"", Date.valueOf(""2021-04-03""), 64, 45));
        dataList.add(RowFactory.create(""PDX"", Date.valueOf(""2021-04-02""), 61, 41));
        dataList.add(RowFactory.create(""PDX"", Date.valueOf(""2021-04-01""), 66, 39));
        dataList.add(RowFactory.create(""SEA"", Date.valueOf(""2021-04-03""), 57, 43));
        dataList.add(RowFactory.create(""SEA"", Date.valueOf(""2021-04-02""), 54, 39));
        dataList.add(RowFactory.create(""SEA"", Date.valueOf(""2021-04-01""), 56, 41));

        Dataset<Row> temps = spark.createDataFrame(dataList, schema);

        // Create a table on the Databricks cluster and then fill
        // the table with the DataFrame's contents.
        // If the table already exists from a previous run,
        // delete it first.
        spark.sql(""USE default"");
        spark.sql(""DROP TABLE IF EXISTS demo_temps_table"");
        temps.write().saveAsTable(""demo_temps_table"");

        // Query the table on the Databricks cluster, returning rows
        // where the airport code is not BLI and the date is later
        // than 2021-04-01. Group the results and order by high
        // temperature in descending order.
        Dataset<Row> df_temps = spark.sql(""SELECT * FROM demo_temps_table "" +
            ""WHERE AirportCode != 'BLI' AND Date > '2021-04-01' "" +
            ""GROUP BY AirportCode, Date, TempHighF, TempLowF "" +
            ""ORDER BY TempHighF DESC"");
        df_temps.show();

        // Results:
        //
        // +-----------+----------+---------+--------+
        // |AirportCode|      Date|TempHighF|TempLowF|
        // +-----------+----------+---------+--------+
        // |        PDX|2021-04-03|       64|      45|
        // |        PDX|2021-04-02|       61|      41|
        // |        SEA|2021-04-03|       57|      43|
        // |        SEA|2021-04-02|       54|      39|
        // +-----------+----------+---------+--------+

        // Clean up by deleting the table from the Databricks cluster.
        spark.sql(""DROP TABLE demo_temps_table"");
    }
}
 Python Python Copy from pyspark.sql import SparkSession
from pyspark.sql.types import *
from datetime import date

spark = SparkSession.builder.appName('temps-demo').getOrCreate()

# Create a Spark DataFrame consisting of high and low temperatures
# by airport code and date.
schema = StructType([
    StructField('AirportCode', StringType(), False),
    StructField('Date', DateType(), False),
    StructField('TempHighF', IntegerType(), False),
    StructField('TempLowF', IntegerType(), False)
])

data = [
    [ 'BLI', date(2021, 4, 3), 52, 43],
    [ 'BLI', date(2021, 4, 2), 50, 38],
    [ 'BLI', date(2021, 4, 1), 52, 41],
    [ 'PDX', date(2021, 4, 3), 64, 45],
    [ 'PDX', date(2021, 4, 2), 61, 41],
    [ 'PDX', date(2021, 4, 1), 66, 39],
    [ 'SEA', date(2021, 4, 3), 57, 43],
    [ 'SEA', date(2021, 4, 2), 54, 39],
    [ 'SEA', date(2021, 4, 1), 56, 41]
]

temps = spark.createDataFrame(data, schema)

# Create a table on the Databricks cluster and then fill
# the table with the DataFrame's contents.
# If the table already exists from a previous run,
# delete it first.
spark.sql('USE default')
spark.sql('DROP TABLE IF EXISTS demo_temps_table')
temps.write.saveAsTable('demo_temps_table')

# Query the table on the Databricks cluster, returning rows
# where the airport code is not BLI and the date is later
# than 2021-04-01. Group the results and order by high
# temperature in descending order.
df_temps = spark.sql(""SELECT * FROM demo_temps_table "" \
    ""WHERE AirportCode != 'BLI' AND Date > '2021-04-01' "" \
    ""GROUP BY AirportCode, Date, TempHighF, TempLowF "" \
    ""ORDER BY TempHighF DESC"")
df_temps.show()

# Results:
#
# +-----------+----------+---------+--------+
# |AirportCode|      Date|TempHighF|TempLowF|
# +-----------+----------+---------+--------+
# |        PDX|2021-04-03|       64|      45|
# |        PDX|2021-04-02|       61|      41|
# |        SEA|2021-04-03|       57|      43|
# |        SEA|2021-04-02|       54|      39|
# +-----------+----------+---------+--------+

# Clean up by deleting the table from the Databricks cluster.
spark.sql('DROP TABLE demo_temps_table')
 Scala Scala Copy import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
import java.sql.Date

object Demo {
  def main(args: Array[String]) {
      val spark = SparkSession.builder.master(""local"").getOrCreate()

      // Create a Spark DataFrame consisting of high and low temperatures
      // by airport code and date.
      val schema = StructType(Array(
        StructField(""AirportCode"", StringType, false),
        StructField(""Date"", DateType, false),
        StructField(""TempHighF"", IntegerType, false),
        StructField(""TempLowF"", IntegerType, false)
      ))

      val data = List(
        Row(""BLI"", Date.valueOf(""2021-04-03""), 52, 43),
        Row(""BLI"", Date.valueOf(""2021-04-02""), 50, 38),
        Row(""BLI"", Date.valueOf(""2021-04-01""), 52, 41),
        Row(""PDX"", Date.valueOf(""2021-04-03""), 64, 45),
        Row(""PDX"", Date.valueOf(""2021-04-02""), 61, 41),
        Row(""PDX"", Date.valueOf(""2021-04-01""), 66, 39),
        Row(""SEA"", Date.valueOf(""2021-04-03""), 57, 43),
        Row(""SEA"", Date.valueOf(""2021-04-02""), 54, 39),
        Row(""SEA"", Date.valueOf(""2021-04-01""), 56, 41)
      )

      val rdd = spark.sparkContext.makeRDD(data)
      val temps = spark.createDataFrame(rdd, schema)

      // Create a table on the Databricks cluster and then fill
      // the table with the DataFrame's contents.
      // If the table already exists from a previous run,
      // delete it first.
      spark.sql(""USE default"")
      spark.sql(""DROP TABLE IF EXISTS demo_temps_table"")
      temps.write.saveAsTable(""demo_temps_table"")

      // Query the table on the Databricks cluster, returning rows
      // where the airport code is not BLI and the date is later
      // than 2021-04-01. Group the results and order by high
      // temperature in descending order.
      val df_temps = spark.sql(""SELECT * FROM demo_temps_table "" +
        ""WHERE AirportCode != 'BLI' AND Date > '2021-04-01' "" +
        ""GROUP BY AirportCode, Date, TempHighF, TempLowF "" +
        ""ORDER BY TempHighF DESC"")
      df_temps.show()

      // Results:
      //
      // +-----------+----------+---------+--------+
      // |AirportCode|      Date|TempHighF|TempLowF|
      // +-----------+----------+---------+--------+
      // |        PDX|2021-04-03|       64|      45|
      // |        PDX|2021-04-02|       61|      41|
      // |        SEA|2021-04-03|       57|      43|
      // |        SEA|2021-04-02|       54|      39|
      // +-----------+----------+---------+--------+

      // Clean up by deleting the table from the Databricks cluster.
      spark.sql(""DROP TABLE demo_temps_table"")
  }
}
 Work with dependencies Typically your main class or Python file will have other dependency JARs and files. You can add such dependency JARs and files by calling sparkContext.addJar(""path-to-the-jar"") or sparkContext.addPyFile(""path-to-the-file""). You can also add Egg files and zip files with the addPyFile() interface. Every time you run the code in your IDE, the dependency JARs and files are installed on the cluster. Python Python Copy from lib import Foo
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

sc = spark.sparkContext
#sc.setLogLevel(""INFO"")

print(""Testing simple count"")
print(spark.range(100).count())

print(""Testing addPyFile isolation"")
sc.addPyFile(""lib.py"")
print(sc.parallelize(range(10)).map(lambda i: Foo(2)).collect())

class Foo(object):
  def __init__(self, x):
    self.x = x
 Python + Java UDFs Python Copy from pyspark.sql import SparkSession
from pyspark.sql.column import _to_java_column, _to_seq, Column

## In this example, udf.jar contains compiled Java / Scala UDFs:
#package com.example
#
#import org.apache.spark.sql._
#import org.apache.spark.sql.expressions._
#import org.apache.spark.sql.functions.udf
#
#object Test {
#  val plusOne: UserDefinedFunction = udf((i: Long) => i + 1)
#}

spark = SparkSession.builder \
  .config(""spark.jars"", ""/path/to/udf.jar"") \
  .getOrCreate()
sc = spark.sparkContext

def plus_one_udf(col):
  f = sc._jvm.com.example.Test.plusOne()
  return Column(f.apply(_to_seq(sc, [col], _to_java_column)))

sc._jsc.addJar(""/path/to/udf.jar"")
spark.range(100).withColumn(""plusOne"", plus_one_udf(""id"")).show()
 Scala Scala Copy package com.example

import org.apache.spark.sql.SparkSession

case class Foo(x: String)

object Test {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      ...
      .getOrCreate();
    spark.sparkContext.setLogLevel(""INFO"")

    println(""Running simple show query..."")
    spark.read.format(""parquet"").load(""/tmp/x"").show()

    println(""Running simple UDF query..."")
    spark.sparkContext.addJar(""./target/scala-2.11/hello-world_2.11-1.0.jar"")
    spark.udf.register(""f"", (x: Int) => x + 1)
    spark.range(10).selectExpr(""f(id)"").show()

    println(""Running custom objects query..."")
    val objs = spark.sparkContext.parallelize(Seq(Foo(""bye""), Foo(""hi""))).collect()
    println(objs.toSeq)
  }
}
 Access DBUtils You can use dbutils.fs and dbutils.secrets utilities of the Databricks Utilities module. Supported commands are dbutils.fs.cp, dbutils.fs.head, dbutils.fs.ls, dbutils.fs.mkdirs, dbutils.fs.mv, dbutils.fs.put, dbutils.fs.rm, dbutils.secrets.get, dbutils.secrets.getBytes, dbutils.secrets.list, dbutils.secrets.listScopes. See File system utility (dbutils.fs) or run dbutils.fs.help() and Secrets utility (dbutils.secrets) or run dbutils.secrets.help(). Python Python Copy from pyspark.sql import SparkSession
from pyspark.dbutils import DBUtils

spark = SparkSession.builder.getOrCreate()

dbutils = DBUtils(spark)
print(dbutils.fs.ls(""dbfs:/""))
print(dbutils.secrets.listScopes())
 When using Databricks Runtime 7.3 LTS or above, to access the DBUtils module in a way that works both locally and in Azure Databricks clusters, use the following get_dbutils(): Python Copy def get_dbutils(spark):
  from pyspark.dbutils import DBUtils
  return DBUtils(spark)
 Otherwise, use the following get_dbutils(): Python Copy def get_dbutils(spark):
  if spark.conf.get(""spark.databricks.service.client.enabled"") == ""true"":
    from pyspark.dbutils import DBUtils
    return DBUtils(spark)
  else:
    import IPython
    return IPython.get_ipython().user_ns[""dbutils""]
 Scala Scala Copy val dbutils = com.databricks.service.DBUtils
println(dbutils.fs.ls(""dbfs:/""))
println(dbutils.secrets.listScopes())
 Copying files between local and remote filesystems You can use dbutils.fs to copy files between your client and remote filesystems. Scheme file:/ refers to the local filesystem on the client. Python Copy from pyspark.dbutils import DBUtils
dbutils = DBUtils(spark)

dbutils.fs.cp('file:/home/user/data.csv', 'dbfs:/uploads')
dbutils.fs.cp('dbfs:/output/results.csv', 'file:/home/user/downloads/')
 The maximum file size that can be transferred that way is 250 MB. Enable dbutils.secrets.get Because of security restrictions, the ability to call dbutils.secrets.get is disabled by default. Contact Azure Databricks support to enable this feature for your workspace. Access the Hadoop filesystem You can also access DBFS directly using the standard Hadoop filesystem interface: Scala Copy > import org.apache.hadoop.fs._

// get new DBFS connection
> val dbfs = FileSystem.get(spark.sparkContext.hadoopConfiguration)
dbfs: org.apache.hadoop.fs.FileSystem = com.databricks.backend.daemon.data.client.DBFS@2d036335

// list files
> dbfs.listStatus(new Path(""dbfs:/""))
res1: Array[org.apache.hadoop.fs.FileStatus] = Array(FileStatus{path=dbfs:/$; isDirectory=true; ...})

// open file
> val stream = dbfs.open(new Path(""dbfs:/path/to/your_file""))
stream: org.apache.hadoop.fs.FSDataInputStream = org.apache.hadoop.fs.FSDataInputStream@7aa4ef24

// get file contents as string
> import org.apache.commons.io._
> println(new String(IOUtils.toByteArray(stream)))
 Set Hadoop configurations On the client you can set Hadoop configurations using the spark.conf.set API, which applies to SQL and DataFrame operations. Hadoop configurations set on the sparkContext must be set in the cluster configuration or using a notebook. This is because configurations set on sparkContext are not tied to user sessions but apply to the entire cluster. Troubleshooting Run databricks-connect test to check for connectivity issues. This section describes some common issues you may encounter and how to resolve them. Python version mismatch Check the Python version you are using locally has at least the same minor release as the version on the cluster (for example, 3.5.1 versus 3.5.2 is OK, 3.5 versus 3.6 is not). If you have multiple Python versions installed locally, ensure that Databricks Connect is using the right one by setting the PYSPARK_PYTHON environment variable (for example, PYSPARK_PYTHON=python3). Server not enabled Ensure the cluster has the Spark server enabled with spark.databricks.service.server.enabled true. You should see the following lines in the driver log if it is: Copy 18/10/25 21:39:18 INFO SparkConfUtils$: Set spark config:
spark.databricks.service.server.enabled -> true
...
18/10/25 21:39:21 INFO SparkContext: Loading Spark Service RPC Server
18/10/25 21:39:21 INFO SparkServiceRPCServer:
Starting Spark Service RPC Server
18/10/25 21:39:21 INFO Server: jetty-9.3.20.v20170531
18/10/25 21:39:21 INFO AbstractConnector: Started ServerConnector@6a6c7f42
{HTTP/1.1,[http/1.1]}{0.0.0.0:15001}
18/10/25 21:39:21 INFO Server: Started @5879ms
 Conflicting PySpark installations The databricks-connect package conflicts with PySpark. Having both installed will cause errors when initializing the Spark context in Python. This can manifest in several ways, including “stream corrupted” or “class not found” errors. If you have PySpark installed in your Python environment, ensure it is uninstalled before installing databricks-connect. After uninstalling PySpark, make sure to fully re-install the Databricks Connect package: Bash Copy pip uninstall pyspark
pip uninstall databricks-connect
pip install -U ""databricks-connect==9.1.*""  # or X.Y.* to match your cluster version.
 Conflicting SPARK_HOME If you have previously used Spark on your machine, your IDE may be configured to use one of those other versions of Spark rather than the Databricks Connect Spark. This can manifest in several ways, including “stream corrupted” or “class not found” errors. You can see which version of Spark is being used by checking the value of the SPARK_HOME environment variable: Java Java Copy System.out.println(System.getenv(""SPARK_HOME""));
 Python Python Copy import os
print(os.environ['SPARK_HOME'])
 Scala Scala Copy println(sys.env.get(""SPARK_HOME""))
 Resolution If SPARK_HOME is set to a version of Spark other than the one in the client, you should unset the SPARK_HOME variable and try again. Check your IDE environment variable settings, your .bashrc, .zshrc, or .bash_profile file, and anywhere else environment variables might be set. You will most likely have to quit and restart your IDE to purge the old state, and you may even need to create a new project if the problem persists. You should not need to set SPARK_HOME to a new value; unsetting it should be sufficient. Conflicting or Missing PATH entry for binaries It is possible your PATH is configured so that commands like spark-shell will be running some other previously installed binary instead of the one provided with Databricks Connect. This can cause databricks-connect test to fail. You should make sure either the Databricks Connect binaries take precedence, or remove the previously installed ones. If you can’t run commands like spark-shell, it is also possible your PATH was not automatically set up by pip install and you’ll need to add the installation bin dir to your PATH manually. It’s possible to use Databricks Connect with IDEs even if this isn’t set up. However, the databricks-connect test command will not work. Conflicting serialization settings on the cluster If you see “stream corrupted” errors when running databricks-connect test, this may be due to incompatible cluster serialization configs. For example, setting the spark.io.compression.codec config can cause this issue. To resolve this issue, consider removing these configs from the cluster settings, or setting the configuration in the Databricks Connect client. Cannot find winutils.exe on Windows If you are using Databricks Connect on Windows and see: Copy ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
 Follow the instructions to configure the Hadoop path on Windows. The filename, directory name, or volume label syntax is incorrect on Windows If you are using Databricks Connect on Windows and see: Copy The filename, directory name, or volume label syntax is incorrect.
 Either Java or Databricks Connect was installed into a directory with a space in your path. You can work around this by either installing into a directory path without spaces, or configuring your path using the short name form. Authentication using Azure Active Directory tokens When you use Databricks Connect, you can authenticate by using an Azure Active Directory token instead of a personal access token. Azure Active Directory tokens have a limited lifetime. When the Azure Active Directory token expires, Databricks Connect fails with an Invalid Token error. In Databricks Connect 7.3.5 and above, you can provide the Azure Active Directory token in your running Databricks Connect application. Your application needs to obtain the new access token, and set it to the spark.databricks.service.token SQL config key. Python Python Copy spark.conf.set(""spark.databricks.service.token"", new_aad_token)
 Scala Scala Copy spark.conf.set(""spark.databricks.service.token"", newAADToken)
 After you update the token, the application can continue to use the same SparkSession and any objects and state that are created in the context of the session. To avoid intermittent errors, Databricks recommends that you provide a new token before the old token expires. You can extend the lifetime of the Azure Active Directory token to persist during the execution of your application. To do that, attach a TokenLifetimePolicy with an appropriately long lifetime to the Azure Active Directory authorization application that you used to acquire the access token. Note Azure Active Directory passthrough uses two tokens: the Azure Active Directory access token that was previously described that you configure in Databricks Connect, and the ADLS passthrough token for the specific resource that Databricks generates while Databricks processes the request. You cannot extend the lifetime of ADLS passthrough tokens by using Azure Active Directory token lifetime policies. If you send a command to the cluster that takes longer than an hour, it will fail if the command accesses an ADLS resource after the one hour mark. Limitations Databricks Connect does not support the following Azure Databricks features and third-party platforms: Structured Streaming. Running arbitrary code that is not a part of a Spark job on the remote cluster. Native Scala, Python, and R APIs for Delta table operations (for example, DeltaTable.forPath) are not supported. However, the SQL API (spark.sql(...)) with Delta Lake operations and the Spark API (for example, spark.read.load) on Delta tables are both supported. Copy into. Apache Zeppelin 0.7.x and below. Connecting to clusters with table access control. Connecting to clusters with process isolation enabled (in other words, where spark.databricks.pyspark.enableProcessIsolation is set to true). Delta CLONE SQL command. Global temporary views. Koalas. Azure Active Directory credential passthrough is supported only on standard clusters running Databricks Runtime 7.3 LTS and above, and is not compatible with service principal authentication. The following Databricks Utilities: library notebook workflow widgets",Databricks Connect
72,using DBconnect but got errors,"Cx used DB connect to connect Databricks clusters but got some errors.
FIrst cluster:
https://adb-640597335864734.14.azuredatabricks.net/?o=640597335864734#setting/clusters/1006-054016-viand258/configuration

Error message:
 22/06/21 16:46:20 WARN SparkServiceRPCClient: Cluster 1006-054016-viand258 in state PENDING, waiting for it to start running...
22/06/21 16:46:30 WARN SparkServiceRPCClient: Cluster 1006-054016-viand258 in state PENDING, waiting for it to start running...
22/06/21 16:46:40 ERROR SparkClientManager: Fail to get the SparkClient
java.util.concurrent.ExecutionException: com.databricks.service.SparkServiceConnectionException: Invalid cluster ID: '1006-054016-viand258'
The cluster ID you specified does not correspond to any existing cluster.
Cluster ID: The ID of the cluster on which you want to run your code
  - This should look like 0123-456789-abcd012
  - Get current value: spark.conf.get('spark.databricks.service.clusterId')
  - Set via conf: spark.conf.set('spark.databricks.service.clusterId', your cluster ID)
  - Set via environment variable: export DATABRICKS_CLUSTER_ID=your cluster ID

But the cluster was vaild.

Second cluster:
https://adb-640597335864734.14.azuredatabricks.net/?o=640597335864734#setting/clusters/0729-025817-leap125/configuration

Error message:
Invaid shared address(I will attach the screen shot)

This cluster was valid as well.

Db connect version: 7.3.*


Please kindly advise us.

Thank you in advance!",https://docs.microsoft.com/en-us/azure/databricks/release-notes/dbconnect/,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Databricks Connect release notes Article 07/12/2022 12 minutes to read 5 contributors In this article Databricks Connect for Databricks Runtime 10.4 LTS Databricks Connect for Databricks Runtime 9.1 LTS Databricks Connect for Databricks Runtime 8.1 Databricks Connect for Databricks Runtime 7.3 LTS Databricks Connect for Databricks Runtime 7.1 (Unsupported) Databricks Connect for Databricks Runtime 6.4 This page lists releases and maintenance updates issued for Databricks Connect. The Databricks Connect major and minor package version must always match your Databricks Runtime version. Databricks recommends that you always use the most recent patch version of Databricks Connect that matches your Databricks Runtime version. For example, when you use a Databricks Runtime 7.3 cluster, use the latest databricks-connect==7.3.* package. Databricks Connect for Databricks Runtime 10.4 LTS Databricks Connect 10.4.6 June 16, 2022 Databricks Connect 10.4 is now GA. Databricks Connect 10.4.0b0 March 29, 2022 Initial Databricks Connect client release to support Databricks Runtime 10.4 LTS. The Python and Java integrations are functioning correctly There is a known issue in the R integration (some Spark plans will fail to execute) Databricks Connect for Databricks Runtime 9.1 LTS Databricks Connect 9.1.18 June 16, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of June 12, 2022. Databricks Connect 9.1.17 June 7, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of June 2, 2022. Databricks Connect 9.1.16 May 20, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of May 20, 2022. Databricks Connect 9.1.15 May 5, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of May 5, 2022. Databricks Connect 9.1.14 April 21, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of April 21, 2022. Databricks Connect 9.1.13 April 7, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of April 7, 2022. Databricks Connect 9.1.12 March 29, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of March 29, 2022. Databricks Connect 9.1.11 March 10, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of March 10, 2022. Fix ClassNotFoundException error message Databricks Connect 9.1.10 February 23, 2022 Security enhancements Databricks Connect 9.1.9 February 8, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of February 8, 2022. Databricks Connect 9.1.8 January 26, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of January 26, 2022. Databricks Connect 9.1.7 January 19, 2022 [ES-216554] Fixed an issue where cluster-attached libraries were not loaded correctly server-side in some cases Databricks Connect 9.1.5 December 7, 2021 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of December 7, 2021. Databricks Connect 9.1.4 November 16, 2021 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of November 16, 2021. Databricks Connect 9.1.3 November 4, 2021 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of November 4, 2021. Databricks Connect 9.1.2 October 20, 2021 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of October 20, 2021. Databricks Connect 9.1.1 October 5, 2021 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of October 5, 2021. Databricks Connect 9.1.0 September 23, 2021 Databricks Connect 9.1 is now GA. Databricks Connect for Databricks Runtime 8.1 Databricks Connect 8.1.14 September 23, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of September 23, 2021. Databricks Connect 8.1.13 September 15, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of September 15, 2021. Databricks Connect 8.1.12 September 8, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of September 8, 2021. Databricks Connect 8.1.11 August 25, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of August 25, 2021. Databricks Connect 8.1.10 August 11, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of August 11, 2021. Databricks Connect 8.1.9 July 30, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of July 30, 2021. Databricks Connect 8.1.8 July 14, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of July 14, 2021. Databricks Connect 8.1.7 June 29, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of June 29, 2021. Databricks Connect 8.1.6 June 15, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of June 15, 2021. Databricks Connect 8.1.5 June 7, 2021 [ES-102916] Support no_proxy environment variable in DBConnnect client. Set it to a comma-separated list of fully qualified domain names for which the proxy set in https_proxy or http_proxy environment variable should not be used. Databricks Connect 8.1.4 May 26, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of May 26, 2021. Databricks Connect 8.1.3 April 30, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of April 30, 2021. Databricks Connect 8.1.2 April 27, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of April 27, 2021. Databricks Connect 8.1.1 April 7, 2021 Improved robustness of Databricks Connect startup process. The previous mechanism for accessing secrets over Databricks Connect (using a dkea token) has been deprecated. Contact support to enable the new access mechanism for your workspace. Fixed a bug that occasionally caused Spark configurations to fail to be picked up correctly. [ES-77737] Fixed a bug preventing proper installation of cluster-wide libraries on Spark executors. Databricks Connect 8.1.0 March 22, 2021 Databricks Connect 8.1 is now GA. Improved support of Catalog API V2 commands Improved support of SQL expressions in ANSI mode Databricks Connect for Databricks Runtime 7.3 LTS Databricks Connect 7.3.43 June 16, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of June 12, 2022. Databricks Connect 7.3.42 June 7, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of June 2, 2022. Databricks Connect 7.3.41 May 20, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of May 20, 2022. Databricks Connect 7.3.40 May 5, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of May 5, 2022. Databricks Connect 7.3.39 April 21, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of April 21, 2022. Databricks Connect 7.3.38 April 7, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of April 7, 2022. Databricks Connect 7.3.37 March 29, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of March 29, 2022. Databricks Connect 7.3.36 March 10, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of March 10, 2022. Databricks Connect 7.3.35 February 23, 2022 Security enhancements Databricks Connect 7.3.34 February 8, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of February 8, 2022. Databricks Connect 7.3.33 January 26, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of January 26, 2022. Databricks Connect 7.3.32 January 19, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of January 19, 2022. Databricks Connect 7.3.30 December 7, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of December 7, 2021. Databricks Connect 7.3.29 November 16, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of November 16, 2021. Databricks Connect 7.3.28 November 4, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of November 4, 2021. Databricks Connect 7.3.27 October 20, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of October 20, 2021. Databricks Connect 7.3.26 October 5, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of October 5, 2021. Databricks Connect 7.3.25 September 23, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of September 23, 2021. Databricks Connect 7.3.24 September 15, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of September 15, 2021. Databricks Connect 7.3.23 September 8, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of September 8, 2021. Databricks Connect 7.3.22 August 25, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of August 25, 2021. Databricks Connect 7.3.21 August 11, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of August 11, 2021. Databricks Connect 7.3.20 July 30, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of July 30, 2021. Databricks Connect 7.3.19 July 14, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of July 14, 2021. Databricks Connect 7.3.18 June 29, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of June 29, 2021. Databricks Connect 7.3.17 June 15, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of June 15, 2021. Databricks Connect 7.3.16 June 7, 2021 [ES-102916] Support no_proxy environment variable in DBConnnect client. Set it to a comma-separated list of fully qualified domain names for which the proxy set in https_proxy or http_proxy environment variable should not be used. Databricks Connect 7.3.15 May 26, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of May 26, 2021. Databricks Connect 7.3.14 April 30, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of April 30, 2021. Databricks Connect 7.3.13 April 27, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of April 27, 2021. Databricks Connect 7.3.12 April 7, 2021 Improved robustness of Databricks Connect startup process. The previous mechanism for accessing secrets over Databricks Connect (using a dkea token) has been deprecated. Contact support to enable the new access mechanism for your workspace. Fixed a bug that occasionally caused Spark configurations to fail to be picked up correctly. [ES-77737] Fixed a bug preventing proper installation of cluster-wide libraries on Spark executors. Databricks Connect 7.3.11 March 24, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of March 24, 2021 Databricks Connect 7.3.10 March 9, 2021 [ES-69946] Fix path separator on Windows for databricks-connect get-jar-dir [ES-68776] Update Azure Blob File System client Add support for FlatMapCoGroupsInPandas Make initial connection process during cluster startup more robust Note The Databricks Runtime cluster must be running the latest maintenance release, and the Databricks Connect client must be updated to version 7.3.10 to continue to use the Azure Blob File System and mapPartitions and mapElements RDD operations with Databricks Connect. A restart of the Databricks Runtime cluster is required to update it to the latest maintenance release. Databricks Connect 7.3.9 February 23, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of February 23, 2021 Databricks Connect 7.3.8 February 4, 2021 [ES-58153] Fix Analysis Exception when creating tables or views based on a query with aggregations. Databricks Connect 7.3.7 January 20, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of January 20, 2021 Databricks Connect 7.3.6 January 12, 2021 [ES-59054] Fix Delta Optimize Table command through Databricks Connect [ES-53497] Fix Databricks Connect on Databricks Container Services (DCS) Databricks Connect 7.3.5 December 9, 2020 Databricks Connect 7.3 is now GA. [ES-49865] Prevent 500 Internal Server Errors due to automatic synchronization of dependent libraries when using Databricks Connect with SBT. Databricks Connect 7.3.4 Beta November 24, 2020 [ES-44382] Set spark.databricks.service.allowTerminatedClusterStart=false to prevent Databricks Connect from starting a terminated Databricks Runtime cluster. Fix an error where client could not synchronize state with server correctly if INFO logging level was disabled. Databricks Connect 7.3.3 Beta November 3, 2020 Initial Databricks Connect release for Databricks Runtime 7.3. This release includes: Support for Azure Active Directory credential passthrough. You can now use Azure Active Directory credential passthrough with Standard clusters using Databricks Connect. This enables you to authenticate automatically to Azure Data Lake Storage Gen1 and Azure Data Lake Storage Gen2 from Databricks Connect using the same Azure Active Directory identity that you use to authenticate to Azure Databricks. Support for Delta Lake time travel. Easy transition between Databricks Connect clients and Databricks Runtime jobs or notebooks when using DBUtils. See Access DBUtils. Known issues: Databricks Runtime version validation is disabled in this beta release. Make sure to connect this client to a 7.3 Databricks Runtime cluster. Databricks Connect for Databricks Runtime 7.1 (Unsupported) Databricks Connect 7.1.14 February 4, 2021 Databricks Connect client update to support Databricks Runtime 7.1 maintenance release of February 4, 2021 Databricks Connect 7.1.13 January 20, 2021 Databricks Connect client update to support Databricks Runtime 7.1 maintenance release of January 20, 2021 Databricks Connect 7.1.12 January 12, 2021 [ES-59054] Fix Delta Optimize Table command through Databricks Connect [ES-53497] Fix Databricks Connect on Databricks Container Services (DCS) Databricks Connect 7.1.11 December 9, 2020 [ES-49865] Prevent 500 Internal Server Errors due to automatic synchronization of dependent libraries when using Databricks Connect with SBT. Databricks Connect 7.1.10 November 24, 2020 Prevent status polls from restarting the cluster. [ES-43656] Fix serialization error related to Azure Data Lake Storage Gen2 support. [ES-30594] Allow larger HTTP headers and enlarge buffers in Databricks Connect. [ES-34186] Avoid out of memory exception related to logging. [ES-39493] Prevent recursive load error in databricks-connect test. [ES-44382] Set spark.databricks.service.allowTerminatedClusterStart=false to prevent Databricks Connect from starting a Databricks Runtime cluster that is terminated. Fix an error in which client could not synchronize state with server correctly if INFO logging level was disabled. Minor fixes. Databricks Connect 7.1.1 September 23, 2020 [ES-32536] Fix an issue with serialization of nested timezone-aware expressions. [ES-33705] Support Azure Data Lake Storage Gen2 filesystem in DButils. Databricks Connect 7.1.0 August 13, 2020 Initial Databricks Connect release for Databricks Runtime 7.1. Support new Spark 3.0 DataSource V2 APIs. Support using multiple SparkSessions in a single Databricks Connect application. Optimize Databricks Connect state synchronization between client and server. Avoid excessive polling for status updates when the server is idle. Databricks Connect for Databricks Runtime 6.4 Databricks Connect 6.4.49 February 23, 2022 Security enhancements Databricks Connect 6.4.28 April 7, 2021 The previous mechanism for accessing secrets over Databricks Connect (using a dkea token) has been deprecated. Contact support to enable the new access mechanism for your workspace. Databricks Connect 6.4.27 March 24, 2021 Databricks Connect client update to support Databricks Runtime 6.4 maintenance release of March 24, 2021 Databricks Connect 6.4.26 March 15, 2021 New version has been released, but there are no changes compared to Databricks Connect 6.4.25 Databricks Connect 6.4.25 March 9, 2021 [ES-69946] Fix path separator on Windows for databricks-connect get-jar-dir Add support for FlatMapCoGroupsInPandas Databricks Connect 6.4.24 February 23, 2021 Databricks Connect client update to support Databricks Runtime 6.4 maintenance release of February 23, 2021 Databricks Connect 6.4.23 February 4, 2021 Databricks Connect client update to support Databricks Runtime 6.4 maintenance release of February 4, 2021 Databricks Connect 6.4.22 January 12, 2021 [ES-59054] Fix Delta Optimize Table command through Databricks Connect [ES-53497] Fix Databricks Connect on Databricks Container Services (DCS) Databricks Connect 6.4.21 December 9, 2020 [ES-25890] Fix an error where in some cases DataFrame cache() would have no effect and the query would be executed again. [ES-26196] Support using NATURAL and USING syntax in joins. [ES-33475][ES-34186] Avoid out of memory exception related to logging. [ES-39493] Prevent recursive load error in databricks-connect test. [ES-43656] Fix serialization error related to Azure Data Lake Storage Gen2 support. [ES-30594] Allow larger HTTP headers and enlarge buffers in Databricks Connect. Minor fixes Databricks Connect 6.4.2 November 5, 2020 [ES-47216] Fix version check compatibility issue with Databricks Runtime 6.4 November 3, 2020 maintenance update. Databricks Connect 6.4.1 April 8, 2020 Minor fixes and updates. Databricks Connect 6.4.0 March 7, 2020 Initial Databricks Connect release for Databricks Runtime 6.4.",Databricks Connect release notes
73,using DBconnect but got errors,"Cx used DB connect to connect Databricks clusters but got some errors.
FIrst cluster:
https://adb-640597335864734.14.azuredatabricks.net/?o=640597335864734#setting/clusters/1006-054016-viand258/configuration

Error message:
 22/06/21 16:46:20 WARN SparkServiceRPCClient: Cluster 1006-054016-viand258 in state PENDING, waiting for it to start running...
22/06/21 16:46:30 WARN SparkServiceRPCClient: Cluster 1006-054016-viand258 in state PENDING, waiting for it to start running...
22/06/21 16:46:40 ERROR SparkClientManager: Fail to get the SparkClient
java.util.concurrent.ExecutionException: com.databricks.service.SparkServiceConnectionException: Invalid cluster ID: '1006-054016-viand258'
The cluster ID you specified does not correspond to any existing cluster.
Cluster ID: The ID of the cluster on which you want to run your code
  - This should look like 0123-456789-abcd012
  - Get current value: spark.conf.get('spark.databricks.service.clusterId')
  - Set via conf: spark.conf.set('spark.databricks.service.clusterId', your cluster ID)
  - Set via environment variable: export DATABRICKS_CLUSTER_ID=your cluster ID

But the cluster was vaild.

Second cluster:
https://adb-640597335864734.14.azuredatabricks.net/?o=640597335864734#setting/clusters/0729-025817-leap125/configuration

Error message:
Invaid shared address(I will attach the screen shot)

This cluster was valid as well.

Db connect version: 7.3.*


Please kindly advise us.

Thank you in advance!",https://docs.microsoft.com/en-us/azure/databricks/release-notes/dbconnect/,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Databricks Connect release notes Article 07/12/2022 12 minutes to read 5 contributors In this article Databricks Connect for Databricks Runtime 10.4 LTS Databricks Connect for Databricks Runtime 9.1 LTS Databricks Connect for Databricks Runtime 8.1 Databricks Connect for Databricks Runtime 7.3 LTS Databricks Connect for Databricks Runtime 7.1 (Unsupported) Databricks Connect for Databricks Runtime 6.4 This page lists releases and maintenance updates issued for Databricks Connect. The Databricks Connect major and minor package version must always match your Databricks Runtime version. Databricks recommends that you always use the most recent patch version of Databricks Connect that matches your Databricks Runtime version. For example, when you use a Databricks Runtime 7.3 cluster, use the latest databricks-connect==7.3.* package. Databricks Connect for Databricks Runtime 10.4 LTS Databricks Connect 10.4.6 June 16, 2022 Databricks Connect 10.4 is now GA. Databricks Connect 10.4.0b0 March 29, 2022 Initial Databricks Connect client release to support Databricks Runtime 10.4 LTS. The Python and Java integrations are functioning correctly There is a known issue in the R integration (some Spark plans will fail to execute) Databricks Connect for Databricks Runtime 9.1 LTS Databricks Connect 9.1.18 June 16, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of June 12, 2022. Databricks Connect 9.1.17 June 7, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of June 2, 2022. Databricks Connect 9.1.16 May 20, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of May 20, 2022. Databricks Connect 9.1.15 May 5, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of May 5, 2022. Databricks Connect 9.1.14 April 21, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of April 21, 2022. Databricks Connect 9.1.13 April 7, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of April 7, 2022. Databricks Connect 9.1.12 March 29, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of March 29, 2022. Databricks Connect 9.1.11 March 10, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of March 10, 2022. Fix ClassNotFoundException error message Databricks Connect 9.1.10 February 23, 2022 Security enhancements Databricks Connect 9.1.9 February 8, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of February 8, 2022. Databricks Connect 9.1.8 January 26, 2022 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of January 26, 2022. Databricks Connect 9.1.7 January 19, 2022 [ES-216554] Fixed an issue where cluster-attached libraries were not loaded correctly server-side in some cases Databricks Connect 9.1.5 December 7, 2021 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of December 7, 2021. Databricks Connect 9.1.4 November 16, 2021 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of November 16, 2021. Databricks Connect 9.1.3 November 4, 2021 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of November 4, 2021. Databricks Connect 9.1.2 October 20, 2021 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of October 20, 2021. Databricks Connect 9.1.1 October 5, 2021 Databricks Connect client update to support Databricks Runtime 9.1 maintenance release of October 5, 2021. Databricks Connect 9.1.0 September 23, 2021 Databricks Connect 9.1 is now GA. Databricks Connect for Databricks Runtime 8.1 Databricks Connect 8.1.14 September 23, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of September 23, 2021. Databricks Connect 8.1.13 September 15, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of September 15, 2021. Databricks Connect 8.1.12 September 8, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of September 8, 2021. Databricks Connect 8.1.11 August 25, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of August 25, 2021. Databricks Connect 8.1.10 August 11, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of August 11, 2021. Databricks Connect 8.1.9 July 30, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of July 30, 2021. Databricks Connect 8.1.8 July 14, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of July 14, 2021. Databricks Connect 8.1.7 June 29, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of June 29, 2021. Databricks Connect 8.1.6 June 15, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of June 15, 2021. Databricks Connect 8.1.5 June 7, 2021 [ES-102916] Support no_proxy environment variable in DBConnnect client. Set it to a comma-separated list of fully qualified domain names for which the proxy set in https_proxy or http_proxy environment variable should not be used. Databricks Connect 8.1.4 May 26, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of May 26, 2021. Databricks Connect 8.1.3 April 30, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of April 30, 2021. Databricks Connect 8.1.2 April 27, 2021 Databricks Connect client update to support Databricks Runtime 8.1 maintenance release of April 27, 2021. Databricks Connect 8.1.1 April 7, 2021 Improved robustness of Databricks Connect startup process. The previous mechanism for accessing secrets over Databricks Connect (using a dkea token) has been deprecated. Contact support to enable the new access mechanism for your workspace. Fixed a bug that occasionally caused Spark configurations to fail to be picked up correctly. [ES-77737] Fixed a bug preventing proper installation of cluster-wide libraries on Spark executors. Databricks Connect 8.1.0 March 22, 2021 Databricks Connect 8.1 is now GA. Improved support of Catalog API V2 commands Improved support of SQL expressions in ANSI mode Databricks Connect for Databricks Runtime 7.3 LTS Databricks Connect 7.3.43 June 16, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of June 12, 2022. Databricks Connect 7.3.42 June 7, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of June 2, 2022. Databricks Connect 7.3.41 May 20, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of May 20, 2022. Databricks Connect 7.3.40 May 5, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of May 5, 2022. Databricks Connect 7.3.39 April 21, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of April 21, 2022. Databricks Connect 7.3.38 April 7, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of April 7, 2022. Databricks Connect 7.3.37 March 29, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of March 29, 2022. Databricks Connect 7.3.36 March 10, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of March 10, 2022. Databricks Connect 7.3.35 February 23, 2022 Security enhancements Databricks Connect 7.3.34 February 8, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of February 8, 2022. Databricks Connect 7.3.33 January 26, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of January 26, 2022. Databricks Connect 7.3.32 January 19, 2022 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of January 19, 2022. Databricks Connect 7.3.30 December 7, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of December 7, 2021. Databricks Connect 7.3.29 November 16, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of November 16, 2021. Databricks Connect 7.3.28 November 4, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of November 4, 2021. Databricks Connect 7.3.27 October 20, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of October 20, 2021. Databricks Connect 7.3.26 October 5, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of October 5, 2021. Databricks Connect 7.3.25 September 23, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of September 23, 2021. Databricks Connect 7.3.24 September 15, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of September 15, 2021. Databricks Connect 7.3.23 September 8, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of September 8, 2021. Databricks Connect 7.3.22 August 25, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of August 25, 2021. Databricks Connect 7.3.21 August 11, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of August 11, 2021. Databricks Connect 7.3.20 July 30, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of July 30, 2021. Databricks Connect 7.3.19 July 14, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of July 14, 2021. Databricks Connect 7.3.18 June 29, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of June 29, 2021. Databricks Connect 7.3.17 June 15, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of June 15, 2021. Databricks Connect 7.3.16 June 7, 2021 [ES-102916] Support no_proxy environment variable in DBConnnect client. Set it to a comma-separated list of fully qualified domain names for which the proxy set in https_proxy or http_proxy environment variable should not be used. Databricks Connect 7.3.15 May 26, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of May 26, 2021. Databricks Connect 7.3.14 April 30, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of April 30, 2021. Databricks Connect 7.3.13 April 27, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of April 27, 2021. Databricks Connect 7.3.12 April 7, 2021 Improved robustness of Databricks Connect startup process. The previous mechanism for accessing secrets over Databricks Connect (using a dkea token) has been deprecated. Contact support to enable the new access mechanism for your workspace. Fixed a bug that occasionally caused Spark configurations to fail to be picked up correctly. [ES-77737] Fixed a bug preventing proper installation of cluster-wide libraries on Spark executors. Databricks Connect 7.3.11 March 24, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of March 24, 2021 Databricks Connect 7.3.10 March 9, 2021 [ES-69946] Fix path separator on Windows for databricks-connect get-jar-dir [ES-68776] Update Azure Blob File System client Add support for FlatMapCoGroupsInPandas Make initial connection process during cluster startup more robust Note The Databricks Runtime cluster must be running the latest maintenance release, and the Databricks Connect client must be updated to version 7.3.10 to continue to use the Azure Blob File System and mapPartitions and mapElements RDD operations with Databricks Connect. A restart of the Databricks Runtime cluster is required to update it to the latest maintenance release. Databricks Connect 7.3.9 February 23, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of February 23, 2021 Databricks Connect 7.3.8 February 4, 2021 [ES-58153] Fix Analysis Exception when creating tables or views based on a query with aggregations. Databricks Connect 7.3.7 January 20, 2021 Databricks Connect client update to support Databricks Runtime 7.3 maintenance release of January 20, 2021 Databricks Connect 7.3.6 January 12, 2021 [ES-59054] Fix Delta Optimize Table command through Databricks Connect [ES-53497] Fix Databricks Connect on Databricks Container Services (DCS) Databricks Connect 7.3.5 December 9, 2020 Databricks Connect 7.3 is now GA. [ES-49865] Prevent 500 Internal Server Errors due to automatic synchronization of dependent libraries when using Databricks Connect with SBT. Databricks Connect 7.3.4 Beta November 24, 2020 [ES-44382] Set spark.databricks.service.allowTerminatedClusterStart=false to prevent Databricks Connect from starting a terminated Databricks Runtime cluster. Fix an error where client could not synchronize state with server correctly if INFO logging level was disabled. Databricks Connect 7.3.3 Beta November 3, 2020 Initial Databricks Connect release for Databricks Runtime 7.3. This release includes: Support for Azure Active Directory credential passthrough. You can now use Azure Active Directory credential passthrough with Standard clusters using Databricks Connect. This enables you to authenticate automatically to Azure Data Lake Storage Gen1 and Azure Data Lake Storage Gen2 from Databricks Connect using the same Azure Active Directory identity that you use to authenticate to Azure Databricks. Support for Delta Lake time travel. Easy transition between Databricks Connect clients and Databricks Runtime jobs or notebooks when using DBUtils. See Access DBUtils. Known issues: Databricks Runtime version validation is disabled in this beta release. Make sure to connect this client to a 7.3 Databricks Runtime cluster. Databricks Connect for Databricks Runtime 7.1 (Unsupported) Databricks Connect 7.1.14 February 4, 2021 Databricks Connect client update to support Databricks Runtime 7.1 maintenance release of February 4, 2021 Databricks Connect 7.1.13 January 20, 2021 Databricks Connect client update to support Databricks Runtime 7.1 maintenance release of January 20, 2021 Databricks Connect 7.1.12 January 12, 2021 [ES-59054] Fix Delta Optimize Table command through Databricks Connect [ES-53497] Fix Databricks Connect on Databricks Container Services (DCS) Databricks Connect 7.1.11 December 9, 2020 [ES-49865] Prevent 500 Internal Server Errors due to automatic synchronization of dependent libraries when using Databricks Connect with SBT. Databricks Connect 7.1.10 November 24, 2020 Prevent status polls from restarting the cluster. [ES-43656] Fix serialization error related to Azure Data Lake Storage Gen2 support. [ES-30594] Allow larger HTTP headers and enlarge buffers in Databricks Connect. [ES-34186] Avoid out of memory exception related to logging. [ES-39493] Prevent recursive load error in databricks-connect test. [ES-44382] Set spark.databricks.service.allowTerminatedClusterStart=false to prevent Databricks Connect from starting a Databricks Runtime cluster that is terminated. Fix an error in which client could not synchronize state with server correctly if INFO logging level was disabled. Minor fixes. Databricks Connect 7.1.1 September 23, 2020 [ES-32536] Fix an issue with serialization of nested timezone-aware expressions. [ES-33705] Support Azure Data Lake Storage Gen2 filesystem in DButils. Databricks Connect 7.1.0 August 13, 2020 Initial Databricks Connect release for Databricks Runtime 7.1. Support new Spark 3.0 DataSource V2 APIs. Support using multiple SparkSessions in a single Databricks Connect application. Optimize Databricks Connect state synchronization between client and server. Avoid excessive polling for status updates when the server is idle. Databricks Connect for Databricks Runtime 6.4 Databricks Connect 6.4.49 February 23, 2022 Security enhancements Databricks Connect 6.4.28 April 7, 2021 The previous mechanism for accessing secrets over Databricks Connect (using a dkea token) has been deprecated. Contact support to enable the new access mechanism for your workspace. Databricks Connect 6.4.27 March 24, 2021 Databricks Connect client update to support Databricks Runtime 6.4 maintenance release of March 24, 2021 Databricks Connect 6.4.26 March 15, 2021 New version has been released, but there are no changes compared to Databricks Connect 6.4.25 Databricks Connect 6.4.25 March 9, 2021 [ES-69946] Fix path separator on Windows for databricks-connect get-jar-dir Add support for FlatMapCoGroupsInPandas Databricks Connect 6.4.24 February 23, 2021 Databricks Connect client update to support Databricks Runtime 6.4 maintenance release of February 23, 2021 Databricks Connect 6.4.23 February 4, 2021 Databricks Connect client update to support Databricks Runtime 6.4 maintenance release of February 4, 2021 Databricks Connect 6.4.22 January 12, 2021 [ES-59054] Fix Delta Optimize Table command through Databricks Connect [ES-53497] Fix Databricks Connect on Databricks Container Services (DCS) Databricks Connect 6.4.21 December 9, 2020 [ES-25890] Fix an error where in some cases DataFrame cache() would have no effect and the query would be executed again. [ES-26196] Support using NATURAL and USING syntax in joins. [ES-33475][ES-34186] Avoid out of memory exception related to logging. [ES-39493] Prevent recursive load error in databricks-connect test. [ES-43656] Fix serialization error related to Azure Data Lake Storage Gen2 support. [ES-30594] Allow larger HTTP headers and enlarge buffers in Databricks Connect. Minor fixes Databricks Connect 6.4.2 November 5, 2020 [ES-47216] Fix version check compatibility issue with Databricks Runtime 6.4 November 3, 2020 maintenance update. Databricks Connect 6.4.1 April 8, 2020 Minor fixes and updates. Databricks Connect 6.4.0 March 7, 2020 Initial Databricks Connect release for Databricks Runtime 6.4.",Databricks Connect release notes
74,Databricks job fails abruptly,"We have a long-running Databricks Spark cluster. When certain load is applied, the container stops responding and the job fails abruptly. No errors can be found in the job logs. Here is one app instance where this happened. Please assess what is causing the job to crash.

https://adb-6017136147241804.4.azuredatabricks.net/?o=6017136147241804#job/328053899684715/run/737534",https://docs.microsoft.com/en-us/azure/databricks/kb/jobs/job-cluster-limit-nb-output,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Job cluster limits on notebook output Article 03/11/2022 2 minutes to read 2 contributors In this article Problem Cause Solution Problem You are running a notebook on a job cluster and you get an error message indicating that the output is too large. Console Copy The output of the notebook is too large. Cause: rpc response (of 20975548 bytes) exceeds limit of 20971520 bytes
 Cause This error message can occur in a job cluster whenever the notebook output is greater then 20 MB. If you are using multiple display(), displayHTML(), show() commands in your notebook, this increases the amount of output. Once the output exceeds 20 MB, the error occurs. If you are using multiple print() commands in your notebook, this can increase the output to stdout. Once the output exceeds 20 MB, the error occurs. If you are running a streaming job and enable awaitAnyTermination in the cluster’s Spark Config, it tries to fetch the entire output in a single request. If this exceeds 20 MB, the error occurs. Solution Remove any unnecessary display(), displayHTML(), print(), and show(), commands in your notebook. These can be useful for debugging, but they are not recommended for production jobs. If your job output is exceeding the 20 MB limit, try redirecting your logs to log4j or disable stdout by setting spark.databricks.driver.disableScalaOutput true in the cluster’s Spark Config. For more information, please review the documentation on output size limits.",Job cluster limits on notebook output
75,Job stuck,"Hi Team,

We have a job has been stuck over 1k hours, need to find out the root cause.

For details please check the attachments.

Regards,
Rick Xu",https://kb.databricks.com/en_US/scala/create-df-from-json-string-python-dictionary,aws_kb,aws kb,"Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Create a DataFrame from a JSON string or Python dictionary Create a DataFrame from a JSON string or Python dictionary Create an Apache Spark DataFrame from a variable containing a JSON string or a Python dictionary. Written by ram.sankarasubramanian Last published at: July 1st, 2022 In this article we are going to review how you can create an Apache Spark DataFrame from a variable containing a JSON string or a Python dictionary. Create a Spark DataFrame from a JSON string Add the JSON content from the variable to a list. %scala

import scala.collection.mutable.ListBuffer
val json_content1 = ""{'json_col1': 'hello', 'json_col2': 32}""
val json_content2 = ""{'json_col1': 'hello', 'json_col2': 'world'}""

var json_seq = new ListBuffer[String]()
json_seq += json_content1
json_seq += json_content2 Create a Spark dataset from the list. %scala

val json_ds = json_seq.toDS() Use spark.read.json to parse the Spark dataset. %scala

val df= spark.read.json(json_ds)
display(df) Combined sample code These sample code blocks combine the previous steps into individual examples. The Python and Scala samples perform the same tasks. %python

json_content1 = ""{'json_col1': 'hello', 'json_col2': 32}""
json_content2 = ""{'json_col1': 'hello', 'json_col2': 'world'}""

json_list = []
json_list.append(json_content1)
json_list.append(json_content2)

df = spark.read.json(sc.parallelize(json_list))
display(df) %scala

import scala.collection.mutable.ListBuffer
val json_content1 = ""{'json_col1': 'hello', 'json_col2': 32}""
val json_content2 = ""{'json_col1': 'hello', 'json_col2': 'world'}""

var json_seq = new ListBuffer[String]()
json_seq += json_content1
json_seq += json_content2

val json_ds = json_seq.toDS()
val df= spark.read.json(json_ds)
display(df) Extract a string column with JSON data from a DataFrame and parse it Select the JSON column from a DataFrame and convert it to an RDD of type RDD[Row]. %scala


import org.apache.spark.sql.functions._


val test_df = Seq((""1"", ""{'json_col1': 'hello', 'json_col2': 32}"", ""1.0""),(""1"", ""{'json_col1': 'hello', 'json_col2': 'world'}"", ""1.0"")).toDF(""row_number"", ""json"", ""token"")


val row_rdd = test_df.select(col(""json"")).rdd  // Selecting just the JSON column and converting it to RDD. Convert RDD[Row] to RDD[String]. %scala

val string_rdd = row_rdd.map(_.mkString("","")) Use spark.read.json to parse the RDD[String]. %scala


val df1= spark.read.json(string_rdd)
 display(df1) Combined sample code This sample code block combines the previous steps into a single example. %scala

import org.apache.spark.sql.functions._

val test_df = Seq((""1"", ""{'json_col1': 'hello', 'json_col2': 32}"", ""1.0""),(""1"", ""{'json_col1': 'hello', 'json_col2': 'world'}"", ""1.0"")).toDF(""row_number"", ""json"", ""token"")

val row_rdd = test_df.select(col(""json"")).rdd
val string_rdd = row_rdd.map(_.mkString("",""))

val df1= spark.read.json(string_rdd)
display(df1) Create a Spark DataFrame from a Python dictionary Check the data type and confirm that it is of dictionary type. %python

jsonDataDict = {""job_id"":33100,""run_id"":1048560,""number_in_job"":1,""state"":{""life_cycle_state"":""PENDING"",""state_message"":""Waiting for cluster""},""task"":{""notebook_task"":{""notebook_path"":""/Users/user@databricks.com/path/test_notebook""}},""cluster_spec"":{""new_cluster"":{""spark_version"":""4.3.x-scala2.11"",""attributes"":{""type"":""fixed_node"",""memory"":""8g""},""enable_elastic_disk"":""false"",""num_workers"":1}},""cluster_instance"":{""cluster_id"":""0000-000000-wares10""},""start_time"":1584689872601,""setup_duration"":0,""execution_duration"":0,""cleanup_duration"":0,""creator_user_name"":""user@databricks.com"",""run_name"":""my test job"",""run_page_url"":""https://testurl.databricks.com#job/33100/run/1"",""run_type"":""SUBMIT_RUN""}

type(jsonDataDict) Use json.dumpsto convert the Python dictionary into a JSON string. %python

import json
jsonData = json.dumps(jsonDataDict) Add the JSON content to a list. %python

jsonDataList = []
jsonDataList.append(jsonData) Convert the list to a RDD and parse it using spark.read.json. %python

jsonRDD = sc.parallelize(jsonDataList)
df = spark.read.json(jsonRDD)
display(df) Combined sample code These sample code block combines the previous steps into a single example. %python

jsonDataDict = {""job_id"":33100,""run_id"":1048560,""number_in_job"":1,""state"":{""life_cycle_state"":""PENDING"",""state_message"":""Waiting for cluster""},""task"":{""notebook_task"":{""notebook_path"":""/Users/user@databricks.com/path/test_notebook""}},""cluster_spec"":{""new_cluster"":{""spark_version"":""4.3.x-scala2.11"",""attributes"":{""type"":""fixed_node"",""memory"":""8g""},""enable_elastic_disk"":""false"",""num_workers"":1}},""cluster_instance"":{""cluster_id"":""0000-000000-wares10""},""start_time"":1584689872601,""setup_duration"":0,""execution_duration"":0,""cleanup_duration"":0,""creator_user_name"":""user@databricks.com"",""run_name"":""my test job"",""run_page_url"":""https://testurl.databricks.com#job/33100/run/1"",""run_type"":""SUBMIT_RUN""}

type(jsonDataDict)

import json
jsonData = json.dumps(jsonDataDict)

jsonDataList = []
jsonDataList.append(jsonData)

jsonRDD = sc.parallelize(jsonDataList)
df = spark.read.json(jsonRDD)
display(df) Example notebook Review the Parse a JSON string or Python dictionary example notebook. Was this article helpful? (14) (44) Additional Informations Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",Create a DataFrame from a JSON string or Python dictionary
76,Job stuck,"Hi Team,

We have a job has been stuck over 1k hours, need to find out the root cause.

For details please check the attachments.

Regards,
Rick Xu",https://kb.databricks.com/en_US/scala/create-df-from-json-string-python-dictionary,aws_kb,aws kb,"Databricks Knowledge Base Main Navigation Help Center Documentation Knowledge Base Community Training Feedback All Categories Amazon Azure Google Cloud Platform All articles Contact Us If you still have questions or prefer to get help directly from an agent, please submit a request. We’ll get back to you as soon as possible. Your email address Subject Description Please enter the details of your request. A member of our support staff will respond as soon as possible. Home All articles Scala with Apache Spark Create a DataFrame from a JSON string or Python dictionary Create a DataFrame from a JSON string or Python dictionary Create an Apache Spark DataFrame from a variable containing a JSON string or a Python dictionary. Written by ram.sankarasubramanian Last published at: July 1st, 2022 In this article we are going to review how you can create an Apache Spark DataFrame from a variable containing a JSON string or a Python dictionary. Create a Spark DataFrame from a JSON string Add the JSON content from the variable to a list. %scala

import scala.collection.mutable.ListBuffer
val json_content1 = ""{'json_col1': 'hello', 'json_col2': 32}""
val json_content2 = ""{'json_col1': 'hello', 'json_col2': 'world'}""

var json_seq = new ListBuffer[String]()
json_seq += json_content1
json_seq += json_content2 Create a Spark dataset from the list. %scala

val json_ds = json_seq.toDS() Use spark.read.json to parse the Spark dataset. %scala

val df= spark.read.json(json_ds)
display(df) Combined sample code These sample code blocks combine the previous steps into individual examples. The Python and Scala samples perform the same tasks. %python

json_content1 = ""{'json_col1': 'hello', 'json_col2': 32}""
json_content2 = ""{'json_col1': 'hello', 'json_col2': 'world'}""

json_list = []
json_list.append(json_content1)
json_list.append(json_content2)

df = spark.read.json(sc.parallelize(json_list))
display(df) %scala

import scala.collection.mutable.ListBuffer
val json_content1 = ""{'json_col1': 'hello', 'json_col2': 32}""
val json_content2 = ""{'json_col1': 'hello', 'json_col2': 'world'}""

var json_seq = new ListBuffer[String]()
json_seq += json_content1
json_seq += json_content2

val json_ds = json_seq.toDS()
val df= spark.read.json(json_ds)
display(df) Extract a string column with JSON data from a DataFrame and parse it Select the JSON column from a DataFrame and convert it to an RDD of type RDD[Row]. %scala


import org.apache.spark.sql.functions._


val test_df = Seq((""1"", ""{'json_col1': 'hello', 'json_col2': 32}"", ""1.0""),(""1"", ""{'json_col1': 'hello', 'json_col2': 'world'}"", ""1.0"")).toDF(""row_number"", ""json"", ""token"")


val row_rdd = test_df.select(col(""json"")).rdd  // Selecting just the JSON column and converting it to RDD. Convert RDD[Row] to RDD[String]. %scala

val string_rdd = row_rdd.map(_.mkString("","")) Use spark.read.json to parse the RDD[String]. %scala


val df1= spark.read.json(string_rdd)
 display(df1) Combined sample code This sample code block combines the previous steps into a single example. %scala

import org.apache.spark.sql.functions._

val test_df = Seq((""1"", ""{'json_col1': 'hello', 'json_col2': 32}"", ""1.0""),(""1"", ""{'json_col1': 'hello', 'json_col2': 'world'}"", ""1.0"")).toDF(""row_number"", ""json"", ""token"")

val row_rdd = test_df.select(col(""json"")).rdd
val string_rdd = row_rdd.map(_.mkString("",""))

val df1= spark.read.json(string_rdd)
display(df1) Create a Spark DataFrame from a Python dictionary Check the data type and confirm that it is of dictionary type. %python

jsonDataDict = {""job_id"":33100,""run_id"":1048560,""number_in_job"":1,""state"":{""life_cycle_state"":""PENDING"",""state_message"":""Waiting for cluster""},""task"":{""notebook_task"":{""notebook_path"":""/Users/user@databricks.com/path/test_notebook""}},""cluster_spec"":{""new_cluster"":{""spark_version"":""4.3.x-scala2.11"",""attributes"":{""type"":""fixed_node"",""memory"":""8g""},""enable_elastic_disk"":""false"",""num_workers"":1}},""cluster_instance"":{""cluster_id"":""0000-000000-wares10""},""start_time"":1584689872601,""setup_duration"":0,""execution_duration"":0,""cleanup_duration"":0,""creator_user_name"":""user@databricks.com"",""run_name"":""my test job"",""run_page_url"":""https://testurl.databricks.com#job/33100/run/1"",""run_type"":""SUBMIT_RUN""}

type(jsonDataDict) Use json.dumpsto convert the Python dictionary into a JSON string. %python

import json
jsonData = json.dumps(jsonDataDict) Add the JSON content to a list. %python

jsonDataList = []
jsonDataList.append(jsonData) Convert the list to a RDD and parse it using spark.read.json. %python

jsonRDD = sc.parallelize(jsonDataList)
df = spark.read.json(jsonRDD)
display(df) Combined sample code These sample code block combines the previous steps into a single example. %python

jsonDataDict = {""job_id"":33100,""run_id"":1048560,""number_in_job"":1,""state"":{""life_cycle_state"":""PENDING"",""state_message"":""Waiting for cluster""},""task"":{""notebook_task"":{""notebook_path"":""/Users/user@databricks.com/path/test_notebook""}},""cluster_spec"":{""new_cluster"":{""spark_version"":""4.3.x-scala2.11"",""attributes"":{""type"":""fixed_node"",""memory"":""8g""},""enable_elastic_disk"":""false"",""num_workers"":1}},""cluster_instance"":{""cluster_id"":""0000-000000-wares10""},""start_time"":1584689872601,""setup_duration"":0,""execution_duration"":0,""cleanup_duration"":0,""creator_user_name"":""user@databricks.com"",""run_name"":""my test job"",""run_page_url"":""https://testurl.databricks.com#job/33100/run/1"",""run_type"":""SUBMIT_RUN""}

type(jsonDataDict)

import json
jsonData = json.dumps(jsonDataDict)

jsonDataList = []
jsonDataList.append(jsonData)

jsonRDD = sc.parallelize(jsonDataList)
df = spark.read.json(jsonRDD)
display(df) Example notebook Review the Parse a JSON string or Python dictionary example notebook. Was this article helpful? (14) (44) Additional Informations Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... Related Articles Apache Spark job fails with Parquet column cannot be converted error Problem You are reading data in Parquet format and writing to a Delta table when ... Convert flattened DataFrame to nested JSON This article explains how to convert a flattened DataFrame to a nested structure,... Cannot import timestamp_millis or unix_millis Problem You are trying to import timestamp_millis or unix_millis into a Scala not... © Databricks 2022. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the Apache Software Foundation. Send us feedback | Privacy Policy | Terms of Use Definition by Author 0 0",Create a DataFrame from a JSON string or Python dictionary
77,Connectivity to postgresql failing,"Customer is using ADF (Azure Data Factory) to submit jobs to databricks using a job cluster.

They are running below jar in the job:
“com.hologic.etl.UnifiAnalyticsETLMain”

Also, it is using below dependent library:
dbfs:/FileStore/jars/3f9a67a6_1287_42c2_a8c1_c8d11dfede62-UnifiAnalyticsETL_V1_2_7_jar_with_dependencies_25012022-e9c38.jar (Jar)

Below is the actual error they are getting while running job:
Job aborted due to stage failure.
Caused by: PSQLException: The connection attempt failed.
Caused by: EOFException:
at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:275)
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:194)


Job run url: 
https://adb-602576115854594.14.azuredatabricks.net/?o=602576115854594#job/709653708101247/run/40086

Connection between Databricks and postgresql server is already tested and it is able to establish the connection.

Also, there is no timeout parameters and max connection properties set in postgresql server.

Can you please help us resolve this issue.

Regards,
Apolok.",https://docs.microsoft.com/en-us/azure/databricks/kb/python/import-custom-ca-cert,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to import a custom CA certificate Article 03/11/2022 2 minutes to read 5 contributors In this article When working with Python, you may want to import a custom CA certificate to avoid connection errors to your endpoints. Console Copy ConnectionError: HTTPSConnectionPool(host='my_server_endpoint', port=443): Max retries exceeded with url: /endpoint (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fb73dc3b3d0>: Failed to establish a new connection: [Errno 110] Connection timed out',))
 To import one or more custom CA certificates to your Azure Databricks cluster: Create an init script that adds the entire CA chain and sets the REQUESTS_CA_BUNDLE property. In this example, PEM format CA certificates are added to the file myca.crt which is located at /user/local/share/ca-certificates/. This file is referenced in the custom-cert.sh init script. Bash Copy dbutils.fs.put(""/databricks/init-scripts/custom-cert.sh"", """"""#!/bin/bash

cat << 'EOF' > /usr/local/share/ca-certificates/myca.crt
-----BEGIN CERTIFICATE-----
<CA CHAIN 1 CERTIFICATE CONTENT>
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
<CA CHAIN 2 CERTIFICATE CONTENT>
-----END CERTIFICATE-----
EOF

update-ca-certificates

PEM_FILE=""/etc/ssl/certs/myca.pem""
PASSWORD=""<password>""
JAVA_HOME=$(readlink -f /usr/bin/java | sed ""s:bin/java::"")
KEYSTORE=""$JAVA_HOME/lib/security/cacerts""

CERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)

# To process multiple certs with keytool, you need to extract
# each one from the PEM file and import it into the Java KeyStore.

for N in $(seq 0 $(($CERTS - 1))); do
  ALIAS=""$(basename $PEM_FILE)-$N""
  echo ""Adding to keystore with alias:$ALIAS""
  cat $PEM_FILE |
    awk ""n==$N { print }; /END CERTIFICATE/ { n++ }"" |
    keytool -noprompt -import -trustcacerts \
            -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD
done

echo ""export REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt"" >> /databricks/spark/conf/spark-env.sh
"""""")
 To use your custom CA certificates with DBFS FUSE, add this line to the bottom of your init script: Bash Copy /databricks/spark/scripts/restart_dbfs_fuse_daemon.sh
 Attach the init script to the cluster as a cluster-scoped init script. Restart the cluster.",How to import a custom CA certificate
78,Connectivity to postgresql failing,"Customer is using ADF (Azure Data Factory) to submit jobs to databricks using a job cluster.

They are running below jar in the job:
“com.hologic.etl.UnifiAnalyticsETLMain”

Also, it is using below dependent library:
dbfs:/FileStore/jars/3f9a67a6_1287_42c2_a8c1_c8d11dfede62-UnifiAnalyticsETL_V1_2_7_jar_with_dependencies_25012022-e9c38.jar (Jar)

Below is the actual error they are getting while running job:
Job aborted due to stage failure.
Caused by: PSQLException: The connection attempt failed.
Caused by: EOFException:
at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:275)
	at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49)
	at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:194)


Job run url: 
https://adb-602576115854594.14.azuredatabricks.net/?o=602576115854594#job/709653708101247/run/40086

Connection between Databricks and postgresql server is already tested and it is able to establish the connection.

Also, there is no timeout parameters and max connection properties set in postgresql server.

Can you please help us resolve this issue.

Regards,
Apolok.",https://docs.microsoft.com/en-us/azure/databricks/kb/python/import-custom-ca-cert,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents How to import a custom CA certificate Article 03/11/2022 2 minutes to read 5 contributors In this article When working with Python, you may want to import a custom CA certificate to avoid connection errors to your endpoints. Console Copy ConnectionError: HTTPSConnectionPool(host='my_server_endpoint', port=443): Max retries exceeded with url: /endpoint (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7fb73dc3b3d0>: Failed to establish a new connection: [Errno 110] Connection timed out',))
 To import one or more custom CA certificates to your Azure Databricks cluster: Create an init script that adds the entire CA chain and sets the REQUESTS_CA_BUNDLE property. In this example, PEM format CA certificates are added to the file myca.crt which is located at /user/local/share/ca-certificates/. This file is referenced in the custom-cert.sh init script. Bash Copy dbutils.fs.put(""/databricks/init-scripts/custom-cert.sh"", """"""#!/bin/bash

cat << 'EOF' > /usr/local/share/ca-certificates/myca.crt
-----BEGIN CERTIFICATE-----
<CA CHAIN 1 CERTIFICATE CONTENT>
-----END CERTIFICATE-----
-----BEGIN CERTIFICATE-----
<CA CHAIN 2 CERTIFICATE CONTENT>
-----END CERTIFICATE-----
EOF

update-ca-certificates

PEM_FILE=""/etc/ssl/certs/myca.pem""
PASSWORD=""<password>""
JAVA_HOME=$(readlink -f /usr/bin/java | sed ""s:bin/java::"")
KEYSTORE=""$JAVA_HOME/lib/security/cacerts""

CERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)

# To process multiple certs with keytool, you need to extract
# each one from the PEM file and import it into the Java KeyStore.

for N in $(seq 0 $(($CERTS - 1))); do
  ALIAS=""$(basename $PEM_FILE)-$N""
  echo ""Adding to keystore with alias:$ALIAS""
  cat $PEM_FILE |
    awk ""n==$N { print }; /END CERTIFICATE/ { n++ }"" |
    keytool -noprompt -import -trustcacerts \
            -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD
done

echo ""export REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt"" >> /databricks/spark/conf/spark-env.sh
"""""")
 To use your custom CA certificates with DBFS FUSE, add this line to the bottom of your init script: Bash Copy /databricks/spark/scripts/restart_dbfs_fuse_daemon.sh
 Attach the init script to the cluster as a cluster-scoped init script. Restart the cluster.",How to import a custom CA certificate
79,ARR | Gap| Error accessing feature store in a remote workspace | 2205090010004670,"Hi

I am working on Centralized Feature Store.

I would like to use feature stores across workspaces. There is a sample taxi feature store created in this databricks instance: https://adb-2510763650757259.19.azuredatabricks.net/?o=2510763650757259#feature-store

As per the reference notebook(https://docs.microsoft.com/en-us/azure/databricks/_static/notebooks/machine-learning/feature-store-multi-workspace.html): I have created relavant scopes and secrets. I am getting this error when I try to access feature store in a different workspace:
ValueError: The feature data table for 'feature_store_taxi_example.trip_dropoff_features_4030' could not be found.

Accessing the same feature store through postman works fine.
So at this point I am not sure if there is a setting that I am missing or some set up done incorrectly.

I am not able to create feature stores in remote workspace either.",https://docs.microsoft.com/en-us/azure/databricks/applications/machine-learning/feature-store/feature-tables,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Work with feature tables Article 07/08/2022 10 minutes to read 5 contributors In this article In this section: Create a database for feature tables Create a feature table in Databricks Feature Store Register an existing Delta table as a feature table Control access to feature tables Update a feature table Read from a feature table Search and browse feature tables Get feature table metadata Work with feature table tags Update data sources for a feature table Delete a feature table Supported data types In this section: Create a database for feature tables Create a feature table in Databricks Feature Store Register an existing Delta table as a feature table Control access to feature tables Update a feature table Read from a feature table Search and browse feature tables Get feature table metadata Work with feature table tags Update data sources for a feature table Delete a feature table Supported data types For information about tracking feature lineage and freshness, see Use the Feature Store UI. Note Database and feature table names cannot contain hyphens (-). Create a database for feature tables Before creating any feature tables, you must create a database to store them. Copy %sql CREATE DATABASE IF NOT EXISTS <database_name>
 Feature tables are stored as Delta tables. When you create a feature table with create_table (Databricks Runtime 10.2 ML or above) or create_feature_table (Databricks Runtime 10.1 ML or below), you must specify the database name. For example, this argument creates a Delta table named customer_features in the database recommender_system. name='recommender_system.customer_features' When you publish a feature table to an online store, the default table and database name are the ones specified when you created the table; you can specify different names using the publish_table method. The Databricks Feature Store UI shows the name of the table and database in the online store, along with other metadata. Create a feature table in Databricks Feature Store Note You can also register an existing Delta table as a feature table. See Register an existing Delta table as a feature table. The basic steps to creating a feature table are: Write the Python functions to compute the features. The output of each function should be an Apache Spark DataFrame with a unique primary key. The primary key can consist of one or more columns. Create a feature table by instantiating a FeatureStoreClient and using create_table (Databricks Runtime 10.2 ML or above) or create_feature_table (Databricks Runtime 10.1 ML or below). Populate the feature table using write_table. Databricks runtime 10.2 ml and above Python Copy from databricks.feature_store import feature_table

def compute_customer_features(data):
  ''' Feature computation code returns a DataFrame with 'customer_id' as primary key'''
  pass

# create feature table keyed by customer_id
# take schema from DataFrame output by compute_customer_features
from databricks.feature_store import FeatureStoreClient

customer_features_df = compute_customer_features(df)

fs = FeatureStoreClient()

customer_feature_table = fs.create_table(
  name='recommender_system.customer_features',
  primary_keys='customer_id',
  schema=customer_features_df.schema,
  description='Customer features'
)

# An alternative is to use `create_table` and specify the `df` argument.
# This code automatically saves the features to the underlying Delta table.

# customer_feature_table = fs.create_table(
#  ...
#  df=customer_features_df,
#  ...
# )

# To use a composite key, pass all keys in the create_table call

# customer_feature_table = fs.create_table(
#   ...
#   primary_keys=['customer_id', 'date'],
#   ...
# )

# Use write_table to write data to the feature table
# Overwrite mode does a full refresh of the feature table

fs.write_table(
  name='recommender_system.customer_features',
  df = customer_features_df,
  mode = 'overwrite'
)
 Databricks runtime 10.1 ml and below Python Copy from databricks.feature_store import feature_table

def compute_customer_features(data):
  ''' Feature computation code returns a DataFrame with 'customer_id' as primary key'''
  pass

# create feature table keyed by customer_id
# take schema from DataFrame output by compute_customer_features
from databricks.feature_store import FeatureStoreClient

customer_features_df = compute_customer_features(df)

fs = FeatureStoreClient()

customer_feature_table = fs.create_feature_table(
  name='recommender_system.customer_features',
  keys='customer_id',
  schema=customer_features_df.schema,
  description='Customer features'
)

# An alternative is to use `create_feature_table` and specify the `features_df` argument.
# This code automatically saves the features to the underlying Delta table.

# customer_feature_table = fs.create_feature_table(
#  ...
#  features_df=customer_features_df,
#  ...
# )

# To use a composite key, pass all keys in the create_feature_table call

# customer_feature_table = fs.create_feature_table(
#   ...
#   keys=['customer_id', 'date'],
#   ...
# )

# Use write_table to write data to the feature table
# Overwrite mode does a full refresh of the feature table

fs.write_table(
  name='recommender_system.customer_features',
  df = customer_features_df,
  mode = 'overwrite'
)from databricks.feature_store import feature_table

def compute_customer_features(data):
  ''' Feature computation code returns a DataFrame with 'customer_id' as primary key'''
  pass

# create feature table keyed by customer_id
# take schema from DataFrame output by compute_customer_features
from databricks.feature_store import FeatureStoreClient

customer_features_df = compute_customer_features(df)

fs = FeatureStoreClient()

customer_feature_table = fs.create_feature_table(
  name='recommender_system.customer_features',
  keys='customer_id',
  schema=customer_features_df.schema,
  description='Customer features'
)

# An alternative is to use `create_feature_table` and specify the `features_df` argument.
# This code automatically saves the features to the underlying Delta table.

# customer_feature_table = fs.create_feature_table(
#  ...
#  features_df=customer_features_df,
#  ...
# )

# To use a composite key, pass all keys in the create_feature_table call

# customer_feature_table = fs.create_feature_table(
#   ...
#   keys=['customer_id', 'date'],
#   ...
# )

# Use write_table to write data to the feature table
# Overwrite mode does a full refresh of the feature table

fs.write_table(
  name='recommender_system.customer_features',
  df = customer_features_df,
  mode = 'overwrite'
)
 Register an existing Delta table as a feature table With Databricks Runtime 10.4 LTS ML and above, you can register an existing Delta table as a feature table. The Delta table must exist in the metastore. Note To update a registered feature table, you must use the Feature Store Python API. Python Copy fs.register_table(
  delta_table='recommender.customer_features',
  primary_keys='customer_id',
  description='Customer features'
)
 Control access to feature tables See Control access to feature tables. Update a feature table You can update a feature table by adding new features or by modifying specific rows based on the primary key. The following feature table metadata cannot be updated: Primary key Partition key Name or type of an existing feature Add new features to an existing feature table You can add new features to an existing feature table in one of two ways: Update the existing feature computation function and run write_table with the returned DataFrame. This updates the feature table schema and merges new feature values based on the primary key. Create a new feature computation function to calculate the new feature values. The DataFrame returned by this new computation function must contain the feature tables’s primary keys and partition keys (if defined). Run write_table with the DataFrame to write the new features to the existing feature table, using the same primary key. Update only specific rows in a feature table Use mode = ""merge"" in write_table. Rows whose primary key does not exist in the DataFrame sent in the write_table call remain unchanged. Python Copy fs.write_table(
  name='recommender.customer_features',
  df = customer_features_df,
  mode = 'merge'
)
 Schedule a job to update a feature table To ensure that features in feature tables always have the most recent values, Databricks recommends that you create a job that runs a notebook to update your feature table on a regular basis, such as every day. If you already have a non-scheduled job created, you can convert it to a scheduled job to make sure the feature values are always up-to-date. Code to update a feature table uses mode='merge', as shown in the following example. Python Copy fs = FeatureStoreClient()

customer_features_df = compute_customer_features(data)

fs.write_table(
  df=customer_features_df,
  name='recommender_system.customer_features',
  mode='merge'
)
 Store past values of daily features Define a feature table with a composite primary key. Include the date in the primary key. For example, for a feature table store_purchases, you might use a composite primary key (date, user_id) and partition key date for efficient reads. You can then create code to read from the feature table filtering date to the time period of interest. To keep the feature table up to date, set up a regularly scheduled job to write features, or stream new feature values into the feature table. Create a streaming feature computation pipeline to update features To create a streaming feature computation pipeline, pass a streaming DataFrame as an argument to write_table. This method returns a StreamingQuery object. Python Copy def compute_additional_customer_features(data):
  ''' Returns Streaming DataFrame
  '''
  pass  # not shown

customer_transactions = spark.readStream.load(""dbfs:/events/customer_transactions"")
stream_df = compute_additional_customer_features(customer_transactions)

fs.write_table(
  df=stream_df,
  name='recommender_system.customer_features',
  mode='merge'
)
 Read from a feature table Use read_table to read feature values. Read data from a specific timestamp Databricks feature tables are Delta table, so you can retrieve feature values as of any timestamp. Python Copy import datetime
yesterday = datetime.date.today() - datetime.timedelta(days=1)

# read customer_features values from 1 day ago
customer_features_df = fs.read_table(
  name='recommender_system.customer_features',
  as_of_delta_timestamp=str(yesterday)
)
 Search and browse feature tables Use the Feature Store UI to search for or browse feature tables. Click Feature Store in the sidebar to display the Feature Store UI. This icon appears only when you are in the machine learning persona. In the search box, enter all or part of the name of a feature table, a feature, or a data source used for feature computation. You can also enter all or part of the key or value of a tag. Search text is case-insensitive. The following screen shot shows an example. The results from searching on “age” include a table with a feature age, a table with a feature HouseAge, and a table based on the data source raw_data.usage_logs. Get feature table metadata The API to get feature table metadata depends on the Databricks runtime version you are using. With Databricks Runtime 10.2 ML and above, use get_table. With Databricks Runtime 10.1 ML and below, use get_feature_table. Python Copy # this example works with Databricks Runtime for Machine Learning 10.2 and above
# for Databricks Runtime for Machine Learning 10.1 and below, use `get_feature_table`
from databricks.feature_store import FeatureStoreClient
fs = FeatureStoreClient()
fs.get_table(""feature_store_example.user_feature_table"")
 Work with feature table tags Tags are key-value pairs that you can create and use to search for feature tables. You can create, edit, and delete tags using the Feature Store UI or the Feature Store Python API. Work with feature table tags in the UI Use the Feature Store UI to search for or browse feature tables. To access the UI, click Feature Store in the sidebar to display the Feature Store UI. This icon appears only when you are in the machine learning persona. Add a tag using the Feature Store UI Click if it is not already open. The tags table appears. Click in the Name and Value fields and enter the key and value for your tag. Click Add. Edit or delete a tag using the Feature Store UI To edit or delete an existing tag, use the icons in the Actions column. Work with feature table tags using the Feature Store Python API On clusters running Databricks Runtime 10.5 ML and above, you can create, edit, and delete tags using the Feature Store Python API. Requirements Databricks Runtime 10.5 ML or above Create feature table with tag using the Feature Store Python API Python Copy from databricks.feature_store import FeatureStoreClient
fs = FeatureStoreClient()

customer_feature_table = fs.create_table(
  ...
  tags={""tag_key_1"": ""tag_value_1"", ""tag_key_2"": ""tag_value_2"", ...},
  ...
)
 Add, update, and delete tags using the Feature Store Python API Python Copy from databricks.feature_store import FeatureStoreClient
fs = FeatureStoreClient()

# Upsert a tag
fs.set_feature_table_tag(table_name=""my_table"", key=""quality"", value=""gold"")

# Delete a tag
fs.delete_feature_table_tag(table_name=""my_table"", key=""quality"")
 Update data sources for a feature table Feature store automatically tracks the data sources used to compute features. You can also manually update the data sources by using the Feature Store Python API. Requirements Databricks Runtime 11.1 ML or above Add data sources using the Feature Store Python API Below are some example commands. For details, see the API documentation. Python Copy from databricks.feature_store import FeatureStoreClient
fs = FeatureStoreClient()

# Use `source_type=""table""` to add a table in the metastore as data source.
fs.add_data_sources(feature_table_name=""clicks"", data_sources=""user_info.clicks"", source_type=""table"")

# Use `source_type=""path""` to add a data source in path format.
fs.add_data_sources(feature_table_name=""user_metrics"", data_sources=""dbfs:/FileStore/user_metrics.json"", source_type=""path"")

# Use `source_type=""custom""` if the source is not a table or a path.
fs.add_data_sources(feature_table_name=""user_metrics"", data_sources=""user_metrics.txt"", source_type=""custom"")
 Delete data sources using the Feature Store Python API For details, see the API documentation. Note The following command deletes data sources of all types (“table”, “path”, and “custom”) that match the source names. Python Copy from databricks.feature_store import FeatureStoreClient
fs = FeatureStoreClient()
fs.delete_data_sources(feature_table_name=""clicks"", sources_names=""user_info.clicks"")
 Delete a feature table This section covers how to delete a table using the Feature Store Python API. You can also delete a feature table from the UI. Note Deleting a feature table can lead to unexpected failures in upstream producers and downstream consumers (models, endpoints, and scheduled jobs). You must delete published online stores with your cloud provider. When you delete a feature table using the API, the underlying Delta table is also dropped. When you delete a feature table from the UI, you must drop the underlying Delta table separately. Requirements Databricks Runtime 10.5 ML or above Delete a feature table using the Feature Store Python API Use drop_table to delete a feature table. When you delete a table with drop_table, the underlying Delta table is also dropped. Python Copy fs.drop_table(
  name='recommender_system.customer_features'
)
 Supported data types Feature Store supports the following PySpark data types: IntegerType FloatType BooleanType StringType DoubleType LongType TimestampType DateType ShortType (Databricks Runtime 9.1 LTS ML and above) BinaryType (Databricks Runtime 10.1 ML and above) DecimalType (Databricks Runtime 10.1 ML and above) ArrayType (Databricks Runtime 9.1 LTS ML and above) MapType (Databricks Runtime 10.1 ML and above) The data types listed above support feature types that are common in machine learning applications. For example: You can store dense vectors, tensors, and embeddings as ArrayType. You can store sparse vectors, tensors, and embeddings as MapType. You can store text as StringType. The Feature Store UI displays metadata on feature data types: When published to online stores, ArrayType and MapType features are stored in JSON format.",Work with feature tables
80,ARR Customer Barracuda - AutoLoader in Databricks automatically defaults to ABFSS instead of WABS,"Customer is testing the autoloader, loading from Blob. 
The code they used worked before loading from ADLS (ABFSS) but not from Blob (WABS). 

Autoloader has support for WABS, however, I think the problem is that the Spark code is not recognizing the configuration with the storage key, and it’s prefixing the path with WABS instead of ABFSS.
 
For more details:
This is the spark config they are setting:
 
spark.conf.set(""fs.azure.account.key.<account_name>.blob.core.windows.net"", account_key)
 
To test the spark config, I do a read from a random file in the blob account and display the dataset with no issue.
 
display (spark.read.format('binaryFile').load(""wasb://<container_name>@<account_name>.blob.core.windows.net/<path>))
 
This is the odd part. They are able to display a df with the config above, but autoloader doesn’t seem to recognize it. I think it’s expecting ADLS configuration with a service principal instead. Another thing is that the connection string already has the SAS or key, so it doesn’t make sense why it has to fall back to the spark config. Unless, the connection string is not constructed correctly.

## conn string with SAS
# blob_conn_string1 = 'BlobEndpoint=https://<account_name>.blob.core.windows.net/;QueueEndpoint=https:// <account_name>.queue.core.windows.net/;FileEndpoint=https:// <account_name>.file.core.windows.net/;TableEndpoint=https:// <account_name>.table.core.windows.net/;SharedAccessSignature=<***>'
 
## conn string with Key
# blob_conn_string2 = 'DefaultEndpointsProtocol=https;AccountName=<account_name>;AccountKey=<***>EndpointSuffix=core.windows.net'
 
This is the code for the Autoloader. Note: I can’t specify a loading path since I’m loading data from 200+ containers. I tested this code in an ADLS account, and it worked fine.

df = (spark.readStream.format('cloudFiles')
    .option('cloudFiles.connectionString',blob_conn_string)
    .option('cloudFiles.queueName', queue_name)
    .option(""cloudFiles.resourceGroup"", blob_resource_group)
    .option(""checkpointLocation"", checkpoint_path)
    .option('cloudFiles.useNotifications', 'true')
    .option('cloudFiles.fetchParallelism', 5)
    .option(""cloudFiles.maxFilesPerTrigger"", 20)
    .option('cloudFiles.format', 'binaryfile')
    .option(""cloudFiles.includeExistingFiles"", False)
    .schema('path string, modificationTime timestamp, length long, content binary')
    .load()
    .withColumn(""decoded"",col_re_list)
    .withColumn(""val"", explode(""decoded""))
    .drop(""content"",""decoded"")
     )

Error below:
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 3.0 failed 4 times, most recent failure: Lost task 1.3 in stage 3.0 (TID 15) (10.152.49.139 executor 3): com.databricks.sql.io.FileReadException: Error while reading file abfss:REDACTED_LOCAL_PART@accountname.dfs.core.windows.net/O9GR_/oi-utyg3_gCY9xCuS910-NoY/LotAVOrbeww.3/1.
…
Caused by: Failure to initialize configuration Invalid configuration value detected for fs.azure.account.key",https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-storage,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Accessing Azure Data Lake Storage Gen2 and Blob Storage with Azure Databricks Article 07/12/2022 3 minutes to read 5 contributors In this article Deprecated patterns for storing and accessing data from Azure Databricks Direct access using ABFS URI for Blob Storage or Azure Data Lake Storage Gen2 Access Azure Data Lake Storage Gen2 or Blob Storage using OAuth 2.0 with an Azure service principal Access Azure Data Lake Storage Gen2 or Blob Storage using a SAS token Access Azure Data Lake Storage Gen2 or Blob Storage using the account key Example notebook Azure Data Lake Storage Gen2 FAQs and known issues Use the Azure Blob Filesystem driver (ABFS) to connect to Azure Blob Storage and Azure Data Lake Storage Gen2 from Azure Databricks. Databricks recommends securing access to Azure storage containers by using Azure service principals set in cluster configurations. This article details how to access Azure storage containers using: Azure service principals SAS tokens Account keys You will set Spark properties to configure these credentials for a compute environment, either: Scoped to an Azure Databricks cluster Scoped to an Azure Databricks notebook Azure service principals can also be used to access Azure storage from Databricks SQL; see Configure access to cloud storage. Databricks recommends using secret scopes for storing all credentials. Deprecated patterns for storing and accessing data from Azure Databricks Databricks no longer recommends mounting external data locations to the Databricks Filesystem; see Mounting cloud object storage on Azure Databricks. Databricks no longer recommends Access Azure Data Lake Storage using Azure Active Directory credential passthrough. The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB; see Azure documentation on ABFS. For documentation for working with the legacy WASB driver, see Connect to Azure Blob Storage with WASB (legacy). Azure has announced the pending retirement of Azure Data Lake Storage Gen1. Azure Databricks recommends migrating all Azure Data Lake Storage Gen1 to Azure Data Lake Storage Gen2. If you have not yet migrated, see Accessing Azure Data Lake Storage Gen1 from Azure Databricks. Direct access using ABFS URI for Blob Storage or Azure Data Lake Storage Gen2 If you have properly configured credentials to access your Azure storage container, you can interact with resources in the storage account using URIs. Databricks recommends using the abfss driver for greater security. Python Copy spark.read.load(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>"")

dbutils.fs.ls(""abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>"")
 SQL Copy CREATE TABLE <database-name>.<table-name>;

COPY INTO <database-name>.<table-name>
FROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder'
FILEFORMAT = CSV
COPY_OPTIONS ('mergeSchema' = 'true');
 Access Azure Data Lake Storage Gen2 or Blob Storage using OAuth 2.0 with an Azure service principal You can securely access data in an Azure storage account using OAuth 2.0 with an Azure Active Directory (Azure AD) application service principal for authentication; see Configure access to Azure storage with an Azure Active Directory service principal. Python Copy service_credential = dbutils.secrets.get(scope=""<scope>"",key=""<service-credential-key>"")

spark.conf.set(""fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net"", ""OAuth"")
spark.conf.set(""fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net"", ""org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"")
spark.conf.set(""fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net"", ""<application-id>"")
spark.conf.set(""fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net"", service_credential)
spark.conf.set(""fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net"", ""https://login.microsoftonline.com/<directory-id>/oauth2/token"")
 Replace <scope> with the Databricks secret scope name. <service-credential-key> with the name of the key containing the client secret. <storage-account> with the name of the Azure storage account. <application-id> with the Application (client) ID for the Azure Active Directory application. <directory-id> with the Directory (tenant) ID for the Azure Active Directory application. Access Azure Data Lake Storage Gen2 or Blob Storage using a SAS token You can use storage shared access signatures (SAS) to access an Azure Data Lake Storage Gen2 storage account directly. With SAS, you can restrict access to a storage account using temporary tokens with fine-grained access control. You can configure SAS tokens for multiple storage accounts in the same Spark session. Note SAS support is available in Databricks Runtime 7.5 and above. Python Copy spark.conf.set(""fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net"", ""SAS"")
spark.conf.set(""fs.azure.sas.token.provider.type.<storage-account>.dfs.core.windows.net"", ""org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider"")
spark.conf.set(""fs.azure.sas.fixed.token.<storage-account>.dfs.core.windows.net"", ""<token>"")
 Access Azure Data Lake Storage Gen2 or Blob Storage using the account key You can use storage account access keys to manage access to Azure Storage. Python Copy   spark.conf.set(
      ""fs.azure.account.key.<storage-account>.dfs.core.windows.net"",
      dbutils.secrets.get(scope=""<scope>"", key=""<storage-account-access-key>""))
 Replace <storage-account> with the Azure Storage account name. <scope> with the Azure Databricks secret scope name. <storage-account-access-key> with the name of the key containing the Azure storage account access key. Example notebook This notebook demonstrates using a service principal to: Authenticate to an ADLS Gen2 storage account. Mount a filesystem in the storage account. Write a JSON file containing Internet of things (IoT) data to the new container. List files using direct access and through the mount point. Read and display the IoT file using direct access and through the mount point. ADLS Gen2 OAuth 2.0 with Azure service principals notebook Get notebook Azure Data Lake Storage Gen2 FAQs and known issues See Azure Data Lake Storage Gen2 frequently asked questions and known issues.",Accessing Azure Data Lake Storage Gen2 and Blob Storage with Azure Databricks
81,The count result sometimes becomes 0 when SELECT query executed,"[Background]
Customer is trying to get the number of data using SQL below.
```
%sql
select 'test_csv' as TABLE_NAME, count(*) AS DATA_COUNT from default.test_csv
```

[Issue]
As shown in the attached gif, when the text in the SELECT query is changed, the count result sometimes becomes 0.
This does not depend on the content of the text and has been confirmed in runtime version 9.1 LTS not 7.3 LTS.

[Ask]
Has this problem ever happened in 9.1 LTS before?",https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/delta-fsck,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents FSCK REPAIR TABLE Article 01/26/2022 2 minutes to read 3 contributors In this article Syntax Parameters Removes the file entries from the transaction log of a Delta table that can no longer be found in the underlying file system. This can happen when these files have been manually deleted. Syntax Copy FSCK REPAIR TABLE table_name [DRY RUN]
 Parameters table_name Identifies an existing Delta table. The name must not include a temporal specification. DRY RUN Return a list of files to be removed from the transaction log.",FSCK REPAIR TABLE
82,The count result sometimes becomes 0 when SELECT query executed,"[Background]
Customer is trying to get the number of data using SQL below.
```
%sql
select 'test_csv' as TABLE_NAME, count(*) AS DATA_COUNT from default.test_csv
```

[Issue]
As shown in the attached gif, when the text in the SELECT query is changed, the count result sometimes becomes 0.
This does not depend on the content of the text and has been confirmed in runtime version 9.1 LTS not 7.3 LTS.

[Ask]
Has this problem ever happened in 9.1 LTS before?",https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/delta-fsck,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents FSCK REPAIR TABLE Article 01/26/2022 2 minutes to read 3 contributors In this article Syntax Parameters Removes the file entries from the transaction log of a Delta table that can no longer be found in the underlying file system. This can happen when these files have been manually deleted. Syntax Copy FSCK REPAIR TABLE table_name [DRY RUN]
 Parameters table_name Identifies an existing Delta table. The name must not include a temporal specification. DRY RUN Return a list of files to be removed from the transaction log.",FSCK REPAIR TABLE
83,cluster termination issue.,"Hello Team,

Greetings!

Issue: cluster is not starting its failing.

Customer is not able to start the cluster below is the cluster URL.

https://adb-4540170653755408.8.azuredatabricks.net/?o=4540170653755408#setting/clusters/0601-104716-ezqj9r5h/events

Attached screenshot error.

Had call with customer and checked they are getting below error message.

Time
2022-06-09 17:23:02 IST
Message
Cluster terminated. Reason: Npip Tunnel Setup Failure

Help
NPIP tunnel setup failure during launch. Please try again later and contact Databricks if the problem persists.
Instance bootstrap failed command: WaitForNgrokTunnel
Failure message (may be truncated): Timed out waiting for ngrok tunnel to be up.

Even referred below UDR document and suggested even to whitelist the ip's referring the UDR document according to the region.
https://docs.microsoft.com/en-us/azure/databricks/administration-guide/cloud-configurations/azure/udr
Even checked with their networkset up their SG rules were also set. 
Even checked in ASC they haven't enable service endpoint storage, Later even we have enabled the Microsoft.Sql and Microsoft.Storage but even though cluster is failing.
Even checked with networking team and informed that the issue is not at their side.
Checked in ASC this is the NPIP enabled cluster even added the scc relay IP but still cluster is not starting.

Thank you.",https://docs.microsoft.com/en-us/azure/databricks/security/secure-cluster-connectivity,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Secure cluster connectivity (No Public IP / NPIP) Article 06/30/2022 4 minutes to read 4 contributors In this article Use secure cluster connectivity With secure cluster connectivity enabled, customer virtual networks have no open ports and Databricks Runtime cluster nodes have no public IP addresses. Secure cluster connectivity is also known as No Public IP (NPIP). At a network level, each cluster initiates a connection to the control plane secure cluster connectivity relay during cluster creation. The cluster establishes this connection using port 443 (HTTPS) and uses a different IP address than is used for the Web application and REST API. When the control plane logically starts new Databricks Runtime jobs or performs other cluster administration tasks, these requests are sent to the cluster through this tunnel. The data plane (the VNet) has no open ports, and Databricks Runtime cluster nodes have no public IP addresses. Benefits: Easy network administration, with no need to configure ports on security groups or to configure network peering. With enhanced security and simple network administration, information security teams can expedite approval of Databricks as a PaaS provider. Note All Azure Databricks network traffic between the data plane VNet and the Azure Databricks control plane goes across the Microsoft network backbone, not the public Internet. This is true even if secure cluster connectivity is disabled. Use secure cluster connectivity To use secure cluster connectivity with a new Azure Databricks workspace, use any of the following options. Azure Portal: When you provision the workspace, go to the Networking tab and set the option Deploy Azure Databricks workspace with Secure Cluster Connectivity (No Public IP) to Yes. ARM Templates: For the Microsoft.Databricks/workspaces resource that creates your new workspace, set the enableNoPublicIp Boolean parameter to true. Important In either case, you must register the Azure Resource Provider Microsoft.ManagedIdentity in the Azure subscription that is used to launch workspaces with secure cluster connectivity. This is a one-time operation per subscription. For instructions, see Azure resource providers and types. You cannot add secure cluster connectivity to an existing workspace. For information about migrating your resources to the new workspaces, contact your Microsoft or Databricks account team for details. If you’re using ARM templates, add the parameter to one of the following templates, based on whether you want Azure Databricks to create a default (managed) virtual network for the workspace, or if you want to use your own virtual network, also known as VNet injection. VNet injection is an optional feature that allows you to provide your own VNet to host new Azure Databricks clusters. ARM template to set up a workspace using the default (managed) VNet. ARM template to set up a workspace using VNet injection. Egress from workspace subnets When you enable secure cluster connectivity, both of your workspace subnets are private subnets, since cluster nodes do not have public IP addresses. The implementation details of network egress vary based on whether you use the default (managed) VNet or whether you use the optional VNet injection feature to provide your own VNet in which to deploy your workspace. See the following sections for details. Important Additional costs may be incurred due to increased egress traffic when you use secure cluster connectivity. For a smaller organization that needs a cost-optimized solution, it may be acceptable to disable secure cluster connectivity when you deploy your workspace. However, for the most secure deployment, Microsoft and Databricks strongly recommend that you enable secure cluster connectivity. Egress with default (managed) VNet If you use secure cluster connectivity with the default VNet that Azure Databricks creates, Azure Databricks automatically creates a NAT gateway for outbound traffic from your workspace’s subnets to the Azure backbone and public network. The NAT gateway is created within the managed resource group managed by Azure Databricks. You cannot modify this resource group or any resources provisioned within it. The automatically-created NAT gateway incurs additional cost. Egress with VNet injection If you use secure cluster connectivity with optional VNet injection to provide your own VNet, ensure that your workspace has a stable egress public IP and choose one of the following options: For simple deployments, choose an egress load balancer, also called an outbound load balancer. The load balancer’s configuration is managed by Azure Databricks. Clusters have a stable public IP, but you cannot modify the configuration for custom egress needs. This Azure template-only solution has the following requirements: Azure Databricks expects additional fields to the ARM template that creates the workspace: loadBalancerName (load balancer name), loadBalancerBackendPoolName (load balancer backend pool name), loadBalancerFrontendConfigName (load balancer frontend configuration name) and loadBalancerPublicIpName (load balancer public IP name). Azure Databricks expects the Microsoft.Databricks/workspaces resource to have parameters loadBalancerId (load balancer ID) and loadBalancerBackendPoolName (load balancer backend pool name). Azure Databricks does not support changing the configuration of the load balancer. For deployments that need some customization, choose an Azure NAT gateway. Configure the gateway on both of the workspace’s subnets to ensure that all outbound traffic to the Azure backbone and public network transits through it. Clusters have a stable egress public IP, and you can modify the configuration for custom egress needs. You can implement this solution using either an Azure template or from the Azure portal. For deployments with complex routing requirements or deployments that use VNet injection with an egress firewall such as Azure Firewall or other custom networking architectures, you can use custom routes called user-defined routes (UDRs). UDRs ensure that network traffic is routed correctly for your workspace, either directly to the required endpoints or through an egress firewall. If you use such a solution, you must add direct routes or allowed firewall rules for the Azure Databricks secure cluster connectivity relay and other required endpoints listed at User-defined route settings for Azure Databricks.",Secure cluster connectivity (No Public IP / NPIP)
84,Databricks Workspace - Security Monitoring of misconfigurations,">>Cx has few queries on extending Databricks API support. Below are the cx queries:

1.	What you are saying is that the “Access Control” settings can’t be retrieved via Databricks API, right? If yes – do you have any plans to extend Databricks API to make it available?
 
2.	Do you have any plans to support Databricks API calls via Azure Policy engine in the future?

3.    Regarding the third one, let me expand on that a bit:
To call Databricks API we should pass authentication and authorization. According to the Security Requirements, we can’t use Personal Access Tokens – instead of that we should use Azure AD. So we have hundreds of the deployed Databricks Workspaces. Now we are thinking about scaling of the security monitoring based on Databricks API to all such instances. So the main question here: could we grant read-only permissions for a service principal to provide access to Databricks API of Workspaces with the least privilege principle? And if yes – could you share a guideline?

>>Please check this and let us know about this. Thanks in advance.",https://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/permissions,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Permissions API 2.0 Article 01/26/2022 2 minutes to read 3 contributors In this article Important This feature is in Public Preview. The Permissions API lets you manage permissions for: Tokens Clusters Pools Jobs Notebooks Folders (directories) MLflow registered models The Permissions API is provided as an OpenAPI 3.0 specification that you can download and view as a structured API reference in your favorite OpenAPI editor. Download the OpenAPI specification View in Redocly: this link immediately opens the OpenAPI specification as a structured API reference for easy viewing. View in Postman: Postman is an app that you must download to your computer. Once you do, you can import the OpenAPI spec as a file or URL. View in Swagger Editor: In the online Swagger Editor, go to the File menu and click Import file to import and view the downloaded OpenAPI specification. Important To access Databricks REST APIs, you must authenticate.",Permissions API 2.0
85,tensorboard does not work on databricks,"Question: What time did the problem begin?
Answer: Not sure, use current time

Question: Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank
Answer: Not sure, use current time

Question: Was it working in previous DBR?
Answer: Other, don't know or not applicable

Question: Notebook URL if available
Answer: 

Question: Cluster URL if notebook URL is not available
Answer: https://adb-8336238433566097.17.azuredatabricks.net/?o=8336238433566097#setting/clusters/0318-145415-tours246/configuration

Question: Additional details about the issue
Answer: I tried to use tensorboard on databricks but it just won’t load. It just shows a blank screen on the notebook, and opening in a new tab just remains stuck on loading forever.

A screenshot is attached.

Regards,
Gary



<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer answers to additional questions for Databricks:
What time did the problem begin? - 2022-06-22T19:14:00.379Z;
Approximate time when the problem stopped occurring. If the issue is ongoing, leave this field blank - 2022-06-22T19:14:00.380Z;
Was it working in previous DBR? - Other, don't know or not applicable;
Notebook URL if available - ;
Cluster URL if notebook URL is not available - https://adb-8336238433566097.17.azuredatabricks.net/?o=8336238433566097#setting/clusters/0318-145415-tours246/configuration;
Additional details about the issue - I tried to use tensorboard on databricks but it just won’t load. It just shows a blank screen on the notebook, and opening in a new tab just remains stuck on loading forever.

A screenshot is attached.

Regards,
Gary;

Customer has uploaded a file. /?workspaceid=a93e37e5-e745-4626-bc70-6090b421df1d
- ProblemStartTime: 06/22/2022 19:14:00
- Cloud: Azure
- AzureProductSubscriptionID: b7d659a5-f700-4979-bdfd-005976e63e11
- AzureProductSubscriptionName: HCE-CHQ-ForgeInsights-NonProd
- Tenant Id: 96ece526-9c7d-48b0-8daf-8b93c90a5d18
- Object Id: efaacab9-8c54-4515-92a9-7af8e87559bf
- SubscriptionType: Premier
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Premier with Azure Rapid response
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: True

- Location: eastus
- ResourceUri: /subscriptions/b7d659a5-f700-4979-bdfd-005976e63e11/resourceGroups/hce-ds2-forgeinsights-databricks-hia/providers/Microsoft.Databricks/workspaces/hce-forgeinsights-dev-hia-wks

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/applications/machine-learning/train-model/tensorflow,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents TensorFlow Article 02/02/2022 3 minutes to read 4 contributors In this article Single node and distributed training Example notebook TensorBoard TensorFlow is an open-source framework for machine learning created by Google. It supports deep-learning and general numerical computations on CPUs, GPUs, and clusters of GPUs. It is subject to the terms and conditions of the Apache License 2.0. Databricks Runtime for Machine Learning includes TensorFlow and TensorBoard, so you can use these libraries without installing any packages. For the version of TensorFlow installed in the Databricks Runtime ML version that you are using, see the release notes. Note This guide is not a comprehensive guide on TensorFlow. See the TensorFlow website. Single node and distributed training To test and migrate single-machine workflows, use a Single Node cluster. For distributed training options for deep learning, see Distributed training. Example notebook The following notebook shows how you can run TensorFlow (1.x and 2.x) with TensorBoard monitoring on a Single Node cluster. TensorFlow 1.15/2.x notebook Get notebook TensorBoard TensorBoard is a suite of visualization tools for debugging, optimizing, and understanding TensorFlow, PyTorch, and other machine learning programs. Use TensorBoard Use TensorBoard on Databricks Runtime 7.2 and above Starting TensorBoard in Azure Databricks is no different than starting it on a Jupyter notebook on your local computer. Load the %tensorboard magic command and define your log directory. Copy %load_ext tensorboard
experiment_log_dir = <log-directory>
 Invoke the %tensorboard magic command. Copy %tensorboard --logdir $experiment_log_dir
 The TensorBoard server starts and displays the user interface inline in the notebook. It also provides a link to open TensorBoard in a new tab. The following screenshot shows the TensorBoard UI started in a populated log directory. You can also start TensorBoard by using TensorBoard’s notebook module directly. Python Copy from tensorboard import notebook
notebook.start(""--logdir {}"".format(experiment_log_dir))
 Use TensorBoard on Databricks Runtime 7.1 and below To start TensorBoard from your notebook, use the dbutils.tensorboard utility. Python Copy dbutils.tensorboard.start(""/tmp/tensorflow_log_dir"")
 This command displays a link that, when clicked, opens TensorBoard in a new tab. When started using this API TensorBoard continues to run until you either stop it with dbutils.tensorboard.stop() or you shut down your cluster. Note If you attach TensorFlow to your cluster as an Azure Databricks library, you may need to reattach your notebook before starting TensorBoard. TensorBoard logs and directories TensorBoard visualizes your machine learning programs by reading logs generated by TensorBoard callbacks and functions in TensorBoard or PyTorch. To generate logs for other machine learning libraries, you can directly write logs using TensorFlow file writers (see Module: tf.summary for TensorFlow 2.x and see Module: tf.compat.v1.summary for the older API in TensorFlow 1.x ). To make sure that your experiment logs are reliably stored, Azure Databricks recommends writing logs to DBFS (that is, a log directory under /dbfs/) rather than on the ephemeral cluster file system. For each experiment, start TensorBoard in a unique directory. For each run of your machine learning code in the experiment that generates logs, set the TensorBoard callback or filewriter to write to a subdirectory of the experiment directory. That way, the data in the TensorBoard UI will be separated into runs. Read the official TensorBoard documentation to get started using TensorBoard to log information for your machine learning program. Manage TensorBoard processes The TensorBoard processes started within Azure Databricks notebook are not terminated when the notebook is detached or the REPL is restarted (for example, when you clear the state of the notebook). To manually kill a TensorBoard process, send it a termination signal using %sh kill -15 pid. Improperly killed TensorBoard processes may corrupt notebook.list(). To list the TensorBoard servers currently running on your cluster, with their corresponding log directories and process IDs, run notebook.list() from the TensorBoard notebook module. Known issues The inline TensorBoard UI is inside an iframe. Browser security features prevent external links within the UI from working unless you open the link in a new tab. The --window_title option of TensorBoard is overridden on Azure Databricks. By default, TensorBoard scans a port range for selecting a port to listen to. If there are too many TensorBoard processes running on the cluster, all ports in the port range may be unavailable. You can work around this limitation by specifying a port number with the --port argument. The specified port should be between 6006 and 6106. In order for download links to work, you should open TensorBoard in a tab. When using TensorBoard 1.15.0, the Projector tab is blank. As a workaround, to visit the projector page directly, you can replace #projector in the URL by data/plugin/projector/projector_binary.html. TensorBoard 2.4.0 has a known issue that might affect TensorBoard rendering if upgraded.",TensorFlow
86,CSS-ARR-SR#2206220030001709-Copy Into Command Not Working After Truncating Target Table,"Copy into command is not copying data from S3/adls source to target table after performing a truncate operation on the table.

Investigation Done:
1. Database and table created in Databricks SQL.
2. Copy Into command run for populating table created in Step 1.
3. Command executed in step 2 is successful and select * command in the Databricks SQL shows all the rows.
4. The table created in step 1 is truncated.
5. Again Copy Into command is executed in Databricks SQL. It shows as successfully executed. But select * command in the same table does not show data.

Help Needed: Please kindly investigate as to why even after showing successful, data is not appearing in select * command. This means that after truncating and then running Copy Into command, data is not populated in the target table.

- ProblemStartTime: 06/22/2022 12:31:00
- Cloud: Azure
- AzureProductSubscriptionID: 6591303c-bd53-453d-bea0-861efbf12822
- AzureProductSubscriptionName: Azure R&D CTG IND_Connectivity
- Tenant Id: 2638f43e-f77d-4fc7-ab92-7b753b7876fd
- Object Id: 79a94469-2f3f-4f2f-b4d9-8fe152f51d40
- SubscriptionType: UnifiedEnterprise
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Unified Enterprise
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: False",https://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/language-manual/delta-copy-into,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents COPY INTO Article 06/10/2022 13 minutes to read 5 contributors In this article Syntax Parameters Access file metadata Format options Related articles Loads data from a file location into a Delta table. This is a retriable and idempotent operation—files in the source location that have already been loaded are skipped. For examples, see Common data loading patterns with COPY INTO. Syntax SQL Copy COPY INTO target_table
  FROM { source |
         ( SELECT expression_list FROM source ) }
           [ WITH (
             [ CREDENTIAL { credential_name |
               (temporary_credential_options) } ]
             [ ENCRYPTION (encryption_options) ])
           ]
  FILEFORMAT = data_source
  [ VALIDATE [ ALL | num_rows ROWS ] ]
  [ FILES = ( file_name [, ...] ) | PATTERN = regex_pattern ]
  [ FORMAT_OPTIONS ( { data_source_reader_option = value } [, ...] ) ]
  [ COPY_OPTIONS ( { copy_option = value } [, ...] ) ]
 Parameters target_table Identifies an existing Delta table. The target_table must not include a temporal specification. If the table name is provided in the form of a location, such as: delta.`/path/to/table` , Unity Catalog can govern access to the locations that are being written to. You can write to an external location by: Defining the location as an external location and having WRITE FILES permissions on that external location. Having WRITE FILES permissions on a named storage credential that provide authorization to write to a location using: COPY INTO delta.`/some/location` WITH (CREDENTIAL <named_credential>) See Manage external locations and storage credentials for more details. Important Unity Catalog is in Public Preview. To participate in the preview, contact your Azure Databricks representative. source The file location to load the data from. Files in this location must have the format specified in FILEFORMAT. The location is provided in the form of a URI. Access to the source location can be provided through: credential_name Optional name of the credential used to access or write to the storage location. You use this credential only if the file location isn’t included in an external location. Inline temporary credentials. Defining the source location as an external location and having READ FILES permissions on the external location through Unity Catalog. Using a named storage credential with READ FILES permissions that provide authorization to read from a location through Unity Catalog. Important Unity Catalog is in Public Preview. To participate in the preview, contact your Azure Databricks representative. You don’t need to provide inline or named credentials if the path is already defined as an external location that you have permissions to use. See Manage external locations and storage credentials for more details. Note If the source file path is a root path, add a slash (/) at the end of the file path, for example, s3://my-bucket/. Accepted credential options are: AZURE_SAS_TOKEN for ADLS Gen2 and Azure Blob Storage AWS_ACCESS_KEY, AWS_SECRET_KEY, and AWS_SESSION_TOKEN for AWS S3 Accepted encryption options are: TYPE = 'AWS_SSE_C', and MASTER_KEY for AWS S3 See Use temporary credentials to load data with COPY INTO. SELECT expression_list Selects the specified columns or expressions from the source data before copying into the Delta table. The expressions can be anything you use with SELECT statements, including window operations. You can use aggregation expressions only for global aggregates–you cannot GROUP BY on columns with this syntax. FILEFORMAT = data_source The format of the source files to load. One of CSV, JSON, AVRO, ORC, PARQUET, TEXT, BINARYFILE. VALIDATE The data that is to be loaded into a table is validated but not written to the table. These validations include: Whether the data can be parsed. Whether the schema matches that of the table or if the schema needs to be evolved. Whether all nullability and check constraints are met. The default is to validate all of the data that is to be loaded. You can provide a number of rows to be validated with the ROWS keyword, such as VALIDATE 15 ROWS. The COPY INTO statement returns a preview of the data of 50 rows or less, when a number of less than 50 is used with the ROWS keyword). Note VALIDATE mode is available in Databricks Runtime 10.3 and above. FILES A list of file names to load, with length up to 1000. Cannot be specified with PATTERN. PATTERN A regex pattern that identifies the files to load from the source directory. Cannot be specified with FILES. FORMAT_OPTIONS Options to be passed to the Apache Spark data source reader for the specified format. See Format options for each file format. COPY_OPTIONS Options to control the operation of the COPY INTO command. force: boolean, default false. If set to true, idempotency is disabled and files are loaded regardless of whether they’ve been loaded before. mergeSchema: boolean, default false. If set to true, the schema can be evolved according to the incoming data. To evolve the schema of a table, you must have OWN permissions on the table. Note mergeSchema option is available in Databricks Runtime 10.3 and above. Access file metadata To learn how to access metadata for file-based data sources, see File metadata column. Format options Generic options JSON options CSV options PARQUET options AVRO options BINARYFILE options TEXT options ORC options Generic options The following options apply to all file formats. Option modifiedAfter Type: Timestamp String, for example, 2021-01-01 00:00:00.000000 UTC+0 An optional timestamp to ingest files that have a modification timestamp after the provided timestamp. Default value: None modifiedBefore Type: Timestamp String, for example, 2021-01-01 00:00:00.000000 UTC+0 An optional timestamp to ingest files that have a modification timestamp before the provided timestamp. Default value: None pathGlobFilter Type: String A potential glob pattern to provide for choosing files. Equivalent to PATTERN in COPY INTO. Default value: None recursiveFileLookup Type: Boolean Whether to load data recursively within the base directory and skip partition inference. Default value: false JSON options Option allowBackslashEscapingAnyCharacter Type: Boolean Whether to allow backslashes to escape any character that succeeds it. If not enabled, only characters that are explicitly listed by the JSON specification can be escaped. Default value: false allowComments Type: Boolean Whether to allow the use of Java, C, and C++ style comments ('/', '*', and '//' varieties) within parsed content or not. Default value: false allowNonNumericNumbers Type: Boolean Whether to allow the set of not-a-number (NaN) tokens as legal floating number values. Default value: true allowNumericLeadingZeros Type: Boolean Whether to allow integral numbers to start with additional (ignorable) zeroes (for example, 000001). Default value: false allowSingleQuotes Type: Boolean Whether to allow use of single quotes (apostrophe, character '\') for quoting strings (names and String values). Default value: true allowUnquotedControlChars Type: Boolean Whether to allow JSON strings to contain unescaped control characters (ASCII characters with value less than 32, including tab and line feed characters) or not. Default value: false allowUnquotedFieldNames Type: Boolean Whether to allow use of unquoted field names (which are allowed by JavaScript, but not by the JSON specification). Default value: false badRecordsPath Type: String The path to store files for recording the information about bad JSON records. Default value: None columnNameOfCorruptRecord Type: String The column for storing records that are malformed and cannot be parsed. If the mode for parsing is set as DROPMALFORMED, this column will be empty. Default value: _corrupt_record dateFormat Type: String The format for parsing date strings. Default value: yyyy-MM-dd dropFieldIfAllNull Type: Boolean Whether to ignore columns of all null values or empty arrays and structs during schema inference. Default value: false encoding or charset Type: String The name of the encoding of the JSON files. See java.nio.charset.Charset for list of options. You cannot use UTF-16 and UTF-32 when multiline is true. Default value: UTF-8 ignoreCorruptFile Type: Boolean Whether to ignore corrupt files. If true, the Spark jobs will continue to run when encountering corrupted files and the contents that have been read will still be returned. Observable as numSkippedCorruptFiles in the operationMetrics column of the Delta Lake history. Available in Databricks Runtime 11.0 and above. Default value: false ignoreMissingFiles Type: Boolean Whether to ignore missing files. If true, the Spark jobs will continue to run when encountering missing files and the contents that have been read will still be returned. Available in Databricks Runtime 11.0 and above. Default value: false (true for COPY INTO) inferTimestamp Type: Boolean Whether to try and infer timestamp strings as a TimestampType. When set to true, schema inference may take noticeably longer. Default value: false lineSep Type: String A string between two consecutive JSON records. Default value: None, which covers \r, \r\n, and \n locale Type: String A java.util.Locale identifier. Influences default date, timestamp, and decimal parsing within the JSON. Default value: US mode Type: String Parser mode around handling malformed records. One of 'PERMISSIVE', 'DROPMALFORMED', or 'FAILFAST'. Default value: PERMISSIVE multiLine Type: Boolean Whether the JSON records span multiple lines. Default value: false prefersDecimal Type: Boolean Whether to infer floats and doubles as DecimalType during schema inference. Default value: false primitivesAsString Type: Boolean Whether to infer primitive types like numbers and booleans as StringType. Default value: false rescuedDataColumn Type: String Whether to collect all data that can’t be parsed due to a data type mismatch or schema mismatch (including column casing) to a separate column. This column is included by default when using Auto Loader. For more details, refer to Rescued data column. Default value: None timestampFormat Type: String The format for parsing timestamp strings. Default value: yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX] timeZone Type: String The java.time.ZoneId to use when parsing timestamps and dates. Default value: None CSV options Option badRecordsPath Type: String The path to store files for recording the information about bad CSV records. Default value: None charToEscapeQuoteEscaping Type: Char The character used to escape the character used for escaping quotes. For example, for the following record: [ "" a\\"", b ]: * If the character to escape the '\' is undefined, the record won’t be parsed. The parser will read characters: [a],[\],[""],[,],[ ],[b] and throw an error because it cannot find a closing quote. * If the character to escape the '\' is defined as '\', the record will be read with 2 values: [a\] and [b]. Default value: '\0' columnNameOfCorruptRecord Type: String A column for storing records that are malformed and cannot be parsed. If the mode for parsing is set as DROPMALFORMED, this column will be empty. Default value: _corrupt_record comment Type: Char Defines the character that represents a line comment when found in the beginning of a line of text. Use '\0' to disable comment skipping. Default value: '\u0000' dateFormat Type: String The format for parsing date strings. Default value: yyyy-MM-dd emptyValue Type: String String representation of an empty value. Default value: """" encoding or charset Type: String The name of the encoding of the CSV files. See java.nio.charset.Charset for the list of options. UTF-16 and UTF-32 cannot be used when multiline is true. Default value: UTF-8 enforceSchema Type: Boolean Whether to forcibly apply the specified or inferred schema to the CSV files. If the option is enabled, headers of CSV files are ignored. This option is ignored by default when using Auto Loader to rescue data and allow schema evolution. Default value: true escape Type: Char The escape character to use when parsing the data. Default value: '\' header Type: Boolean Whether the CSV files contain a header. Auto Loader assumes that files have headers when inferring the schema. Default value: false ignoreLeadingWhiteSpace Type: Boolean Whether to ignore leading whitespaces for each parsed value. Default value: false ignoreTrailingWhiteSpace Type: Boolean Whether to ignore trailing whitespaces for each parsed value. Default value: false inferSchema Type: Boolean Whether to infer the data types of the parsed CSV records or to assume all columns are of StringType. Requires an additional pass over the data if set to true. Default value: false lineSep Type: String A string between two consecutive CSV records. Default value: None, which covers \r, \r\n, and \n locale Type: String A java.util.Locale identifier. Influences default date, timestamp, and decimal parsing within the CSV. Default value: US maxCharsPerColumn Type: Int Maximum number of characters expected from a value to parse. Can be used to avoid memory errors. Defaults to -1, which means unlimited. Default value: -1 maxColumns Type: Int The hard limit of how many columns a record can have. Default value: 20480 mergeSchema Type: Boolean Whether to infer the schema across multiple files and to merge the schema of each file. Enabled by default for Auto Loader when inferring the schema. Default value: false mode Type: String Parser mode around handling malformed records. One of 'PERMISSIVE', 'DROPMALFORMED', and 'FAILFAST'. Default value: PERMISSIVE multiLine Type: Boolean Whether the CSV records span multiple lines. Default value: false nanValue Type: String The string representation of a non-a-number value when parsing FloatType and DoubleType columns. Default value: ""NaN"" negativeInf Type: String The string representation of negative infinity when parsing FloatType or DoubleType columns. Default value: ""-Inf"" nullValue Type: String String representation of a null value. Default value: """" parserCaseSensitive (deprecated) Type: Boolean While reading files, whether to align columns declared in the header with the schema case sensitively. This is true by default for Auto Loader. Columns that differ by case will be rescued in the rescuedDataColumn if enabled. This option has been deprecated in favor of readerCaseSensitive. Default value: false positiveInf Type: String The string representation of positive infinity when parsing FloatType or DoubleType columns. Default value: ""Inf"" quote Type: Char The character used for escaping values where the field delimiter is part of the value. Default value: '\' rescuedDataColumn Type: String Whether to collect all data that can’t be parsed due to: a data type mismatch, and schema mismatch (including column casing) to a separate column. This column is included by default when using Auto Loader. For more details refer to Rescued data column. Default value: None sep or delimiter Type: String The separator string between columns. Default value: "","" timestampFormat Type: String The format for parsing timestamp strings. Default value: yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX] timeZone Type: String The java.time.ZoneId to use when parsing timestamps and dates. Default value: None unescapedQuoteHandling Type: String The strategy for handling unescaped quotes. Allowed options: * STOP_AT_CLOSING_QUOTE: If unescaped quotes are found in the input, accumulate the quote character and proceed parsing the value as a quoted value, until a closing quote is found. * BACK_TO_DELIMITER: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters of the current parsed value until the delimiter defined by sep is found. If no delimiter is found in the value, the parser will continue accumulating characters from the input until a delimiter or line ending is found. * STOP_AT_DELIMITER: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters until the delimiter defined by sep, or a line ending is found in the input. * SKIP_VALUE: If unescaped quotes are found in the input, the content parsed for the given value will be skipped (until the next delimiter is found) and the value set in nullValue will be produced instead. * RAISE_ERROR: If unescaped quotes are found in the input, a TextParsingException will be thrown. Default value: STOP_AT_DELIMITER PARQUET options Option datetimeRebaseMode Type: String Controls the rebasing of the DATE and TIMESTAMP values between Julian and Proleptic Gregorian calendars. Allowed values: EXCEPTION, LEGACY, and CORRECTED. Default value: LEGACY int96RebaseMode Type: String Controls the rebasing of the INT96 timestamp values between Julian and Proleptic Gregorian calendars. Allowed values: EXCEPTION, LEGACY, and CORRECTED. Default value: LEGACY mergeSchema Type: Boolean Whether to infer the schema across multiple files and to merge the schema of each file. Default value: false AVRO options Option avroSchema Type: String Optional schema provided by a user in Avro format. When reading Avro, this option can be set to an evolved schema, which is compatible but different with the actual Avro schema. The deserialization schema will be consistent with the evolved schema. For example, if you set an evolved schema containing one additional column with a default value, the read result will contain the new column too. Default value: None datetimeRebaseMode Type: String Controls the rebasing of the DATE and TIMESTAMP values between Julian and Proleptic Gregorian calendars. Allowed values: EXCEPTION, LEGACY, and CORRECTED. Default value: LEGACY mergeSchema Type: Boolean Whether to infer the schema across multiple files and to merge the schema of each file. mergeSchema for Avro does not relax data types. Default value: false BINARYFILE options Binary files do not have any additional configuration options. TEXT options Option encoding Type: String The name of the encoding of the TEXT files. See java.nio.charset.Charset for list of options. Default value: UTF-8 lineSep Type: String A string between two consecutive TEXT records. Default value: None, which covers \r, \r\n and \n wholeText Type: Boolean Whether to read a file as a single record. Default value: false ORC options Option mergeSchema Type: Boolean Whether to infer the schema across multiple files and to merge the schema of each file. Default value: false Related articles Credentials DELETE INSERT MERGE PARTITION query UPDATE",COPY INTO
87,postgresql performance issues,"Hi Databricks team,

our customer is struggling with performance issues during data ingestion from databricks to Azure PostgreSQL DB.
I engaged our PostgreSQL team however on the PostgreSQL these queries are taking miliseconds to complete.

In this scenario cx is trying to ingest the delta table (stored on adls gen 2) which contains 20 columns and 2000 rows, it's taking from 1-7 minutes to complete, this is intermittent issue, sometimes ingesting the same data is taking 1 minute sometimes is taking 3-5 minutes, there are even occurrences when it raised 15 minutes. Customer is using Spark JDBC connection in order to ingest the data and he's using private endpoint in order to establish a connection from databricks->postgresql.(we confirmed the IP addresses from azure portal). 
 
the read operation of 18000 rows from postgresql took 2-3 seconds and if I perform read of this volume of data and then write to postgre again the performance issue persisted which give us an impression that the issue may not lie on the reading from adls gen 2 mechanism.

we have working repro here: 
https://adb-6307235058999053.13.azuredatabricks.net/?o=6307235058999053#notebook/3721444399442691/command/3721444399442692

you can use it to see how it works, you can spin up the cluster if it's disabled, however please don't restart the cluster!

Could you please help us resolve this issue?

Thank you,
Michal",https://docs.microsoft.com/en-us/azure/databricks/kb/libraries/replace-default-jar-new-jar,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Replace a default library jar Article 03/11/2022 2 minutes to read 3 contributors In this article Identify the artifact id Use the artifact id to find the jar filename Upload the replacement jar file Create the init script Install the init script and restart Azure Databricks includes a number of default Java and Scala libraries. You can replace any of these libraries with another version by using a cluster-scoped init script to remove the default library jar and then install the version you require. Important Removing default libraries and installing new versions may cause instability or completely break your Azure Databricks cluster. You should thoroughly test any new library version in your environment before running production jobs. Identify the artifact id To identify the name of the jar file you want to remove: Click the Databricks Runtime version you are using from the list of supported releases. Navigate to the Java and Scala libraries section. Identify the Artifact ID for the library you want to remove. Use the artifact id to find the jar filename Use the ls -l command in a notebook to find the jar that contains the artifact id. For example, to find the jar filename for the spark-snowflake_2.12 artifact id in Databricks Runtime 7.0 you can use the following code: Scala Copy %sh
ls -l /databricks/jars/*spark-snowflake_2.12*
 This returns the jar filename Console Copy `----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.5.9-spark_2.4.jar`.
 Upload the replacement jar file Upload your replacement jar file to a DBFS path. Create the init script Use the following template to create a cluster-scoped init script. Scala Copy #!/bin/bash
rm -rf /databricks/jars/<jar_filename_to_remove>.jar
cp /dbfs/<path_to_replacement_jar>/<replacement_jar_filename>.jar /databricks/jars/
 Using the spark-snowflake_2.12 example from the prior step would result in an init script similar to the following: Scala Copy #!/bin/bash
rm -rf /databricks/jars/----workspace_spark_3_0--maven-trees--hive-2.3__hadoop-2.7--net.snowflake--spark-snowflake_2.12--net.snowflake__spark-snowflake_2.12__2.5.9-spark_2.4.jar
cp /dbfs/FileStore/jars/e43fe9db_c48d_412b_b142_cdde10250800-spark_snowflake_2_11_2_7_1_spark_2_4-b2adc.jar /databricks/jars/
 Install the init script and restart Install the cluster-scoped init script on the cluster, following the instructions in Configure a cluster-scoped init script. Restart the cluster.",Replace a default library jar
88,Databrick Cluster Issue,"Getting the error that Cluster is not reachable and driver of the cluster is restarted during run.
This is making our notebooks failed and all the dependencies jobs got holded becaus eof this issue affecting downstreams.

Problem start date and time
Thu, Jun 23, 2022, 11:30 PM (UTC-08:00) Pacific Time (US & Canada)


<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer has uploaded a file. /?workspaceid=3986636d-9a38-42d6-9763-188cc512d674
- ProblemStartTime: 06/24/2022 06:30:00
- Cloud: Azure
- AzureProductSubscriptionID: 8cc9b872-5ea9-4c4d-8db9-280336c41458
- AzureProductSubscriptionName: C1-SIT3-NA
- Tenant Id: bacbd475-b9da-43fa-958c-ed27e9d42d0e
- Object Id: 9eb2ec85-3769-4654-8c23-50dd2a453e02
- SubscriptionType: Unified
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Unified Support - Advanced
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: True

- Location: westus
- ResourceUri: /subscriptions/8cc9b872-5ea9-4c4d-8db9-280336c41458/resourceGroups/cai-databricks/providers/Microsoft.Databricks/workspaces/cai-databricks-ods-c1-sit3

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Manage notebooks Article 07/08/2022 10 minutes to read 7 contributors In this article Create a notebook Open a notebook Delete a notebook Copy notebook path Rename a notebook Control access to a notebook Notebook external formats Notebooks and clusters Schedule a notebook Distribute notebooks You can manage notebooks using the UI, the CLI, and by invoking the Workspace API. This article focuses on performing notebook tasks using the UI. For the other methods, see Databricks CLI and Workspace API 2.0. Create a notebook Use the Create button The easiest way to create a new notebook in your default folder is to use the Create button: Click Create in the sidebar and select Notebook from the menu. The Create Notebook dialog appears. Enter a name and select the notebook’s default language. If there are running clusters, the Cluster drop-down displays. Select the cluster you want to attach the notebook to. Click Create. Create a notebook in any folder You can create a new notebook in any folder (for example, in the Shared folder) following these steps: In the sidebar, click Workspace. Do one of the following: Next to any folder, click the on the right side of the text and select Create > Notebook. In the workspace or a user folder, click and select Create > Notebook. Follow steps 2 through 4 in Use the Create button. Open a notebook In your workspace, click a . The notebook path displays when you hover over the notebook title. Delete a notebook See Folders and Workspace object operations for information about how to access the workspace menu and delete notebooks or other items in the workspace. Copy notebook path To copy a notebook file path without opening the notebook, right-click the notebook name or click the to the right of the notebook name and select Copy File Path. Rename a notebook To change the title of an open notebook, click the title and edit inline or click File > Rename. Control access to a notebook If your Azure Databricks account has the Premium Plan, you can use Workspace access control to control who has access to a notebook. Notebook external formats Azure Databricks supports several notebook external formats: Source file: A file containing only source code statements with the extension .scala, .py, .sql, or .r. HTML: An Azure Databricks notebook with the extension .html. DBC archive: A Databricks archive. IPython notebook: A Jupyter notebook with the extension .ipynb. RMarkdown: An R Markdown document with the extension .Rmd. In this section: Import a notebook Convert a file to a notebook Export a notebook Export all notebooks in a folder Import a notebook You can import an external notebook from a URL or a file. You can also import a ZIP archive of notebooks exported in bulk from an Azure Databricks workspace. Click Workspace in the sidebar. Do one of the following: Next to any folder, click the on the right side of the text and select Import. In the Workspace or a user folder, click and select Import. Specify the URL or browse to a file containing a supported external format or a ZIP archive of notebooks exported from an Azure Databricks workspace. Click Import. If you choose a single notebook, it is exported in the current folder. If you choose a DBC or ZIP archive, its folder structure is recreated in the current folder and each notebook is imported. Convert a file to a notebook You can convert existing Python, SQL, Scala, and R scripts to single-cell notebooks by adding a comment to the first cell of the file: Python Python Copy # Databricks notebook source
 SQL SQL Copy -- Databricks notebook source
 Scala Scala Copy // Databricks notebook source
 R R Copy # Databricks notebook source
 Databricks notebooks use a special comment surrounded by whitespace to define cells: Python Python Copy # COMMAND ----------
 SQL SQL Copy -- COMMAND ----------
 Scala Scala Copy // COMMAND ----------
 R R Copy # COMMAND ----------
 Export a notebook In the notebook toolbar, select File > Export and a format. Note When you export a notebook as HTML, IPython notebook, or archive (DBC), and you have not cleared the results, the results of running the notebook are included. Export all notebooks in a folder Note When you export a notebook as HTML, IPython notebook, or archive (DBC), and you have not cleared the results, the results of running the notebook are included. To export all folders in a workspace folder as a ZIP archive: Click Workspace in the sidebar. Do one of the following: Next to any folder, click the on the right side of the text and select Export. In the Workspace or a user folder, click and select Export. Select the export format: DBC Archive: Export a Databricks archive, a binary format that includes metadata and notebook command results. Source File: Export a ZIP archive of notebook source files, which can be imported into an Azure Databricks workspace, used in a CI/CD pipeline, or viewed as source files in each notebook’s default language. Notebook command results are not included. HTML Archive: Export a ZIP archive of HTML files. Each notebook’s HTML file can be imported into an Azure Databricks workspace or viewed as HTML. Notebook command results are included. Notebooks and clusters Before you can do any work in a notebook, you must first attach the notebook to a cluster. This section describes how to attach and detach notebooks to and from clusters and what happens behind the scenes when you perform these actions. In this section: Execution contexts Attach a notebook to a cluster Detach a notebook from a cluster View all notebooks attached to a cluster Execution contexts When you attach a notebook to a cluster, Azure Databricks creates an execution context. An execution context contains the state for a REPL environment for each supported programming language: Python, R, Scala, and SQL. When you run a cell in a notebook, the command is dispatched to the appropriate language REPL environment and run. You can also use the REST 1.2 API to create an execution context and send a command to run in the execution context. Similarly, the command is dispatched to the language REPL environment and run. A cluster has a maximum number of execution contexts (145). Once the number of execution contexts has reached this threshold, you cannot attach a notebook to the cluster or create a new execution context. Idle execution contexts An execution context is considered idle when the last completed execution occurred past a set idle threshold. Last completed execution is the last time the notebook completed execution of commands. The idle threshold is the amount of time that must pass between the last completed execution and any attempt to automatically detach the notebook. The default idle threshold is 24 hours. When a cluster has reached the maximum context limit, Azure Databricks removes (evicts) idle execution contexts (starting with the least recently used) as needed. Even when a context is removed, the notebook using the context is still attached to the cluster and appears in the cluster’s notebook list. Streaming notebooks are considered actively running, and their context is never evicted until their execution has been stopped. If an idle context is evicted, the UI displays a message indicating that the notebook using the context was detached due to being idle. If you attempt to attach a notebook to cluster that has maximum number of execution contexts and there are no idle contexts (or if auto-eviction is disabled), the UI displays a message saying that the current maximum execution contexts threshold has been reached and the notebook will remain in the detached state. If you fork a process, an idle execution context is still considered idle once execution of the request that forked the process returns. Forking separate processes is not recommended with Spark. Configure context auto-eviction Auto-eviction is enabled by default. To disable auto-eviction for a cluster, set the Spark property spark.databricks.chauffeur.enableIdleContextTracking false. Attach a notebook to a cluster To attach a notebook to a cluster, you need the Can Attach To cluster-level permission. Important As long as a notebook is attached to a cluster, any user with the Can Run permission on the notebook has implicit permission to access the cluster. To attach a notebook to a cluster: In the notebook toolbar, click Detached . From the drop-down, select a cluster. Important An attached notebook has the following Apache Spark variables defined. Class Variable Name SparkContext sc SQLContext/HiveContext sqlContext SparkSession (Spark 2.x) spark Do not create a SparkSession, SparkContext, or SQLContext. Doing so will lead to inconsistent behavior. Determine Spark and Databricks Runtime version To determine the Spark version of the cluster your notebook is attached to, run: Python Copy spark.version
 To determine the Databricks Runtime version of the cluster your notebook is attached to, run: Python Copy spark.conf.get(""spark.databricks.clusterUsageTags.sparkVersion"")
 Note Both this sparkVersion tag and the spark_version property required by the endpoints in the Clusters API 2.0 and Jobs API 2.1 refer to the Databricks Runtime version, not the Spark version. Detach a notebook from a cluster In the notebook toolbar, click Attached . Select Detach. You can also detach notebooks from a cluster using the Notebooks tab on the cluster details page. When you detach a notebook from a cluster, the execution context is removed and all computed variable values are cleared from the notebook. Tip Azure Databricks recommends that you detach unused notebooks from a cluster. This frees up memory space on the driver. View all notebooks attached to a cluster The Notebooks tab on the cluster details page displays all of the notebooks that are attached to a cluster. The tab also displays the status of each attached notebook, along with the last time a command was run from the notebook. Schedule a notebook To schedule a notebook job to run periodically: In the notebook, click at the top right. If no jobs exist for this notebook, the Schedule dialog appears. If jobs already exist for the notebook, the Jobs List dialog appears. To display the Schedule dialog, click Add a schedule. In the Schedule dialog, optionally enter a name for the job. The default name is the name of the notebook. Select Manual to run your job only when manually triggered, or Scheduled to define a schedule for running the job. If you select Scheduled, use the drop-downs to specify the frequency, time, and time zone. In the Cluster drop-down, select the cluster to run the task. If you have Allow Cluster Creation permissions, by default the job runs on a new job cluster. To edit the configuration of the default job cluster, click Edit at the right of the field to display the cluster configuration dialog. If you do not have Allow Cluster Creation permissions, by default the job runs on the cluster that the notebook is attached to. If the notebook is not attached to a cluster, you must select a cluster from the Cluster drop-down. Optionally, enter any Parameters to pass to the job. Click Add and specify the key and value of each parameter. Parameters set the value of the notebook widget specified by the key of the parameter. Use Task parameter variables to pass a limited set of dynamic values as part of a parameter value. Optionally, specify email addresses to receive Alerts on job events. See Notifications. Click Submit. Manage scheduled notebook jobs To display jobs associated with this notebook, click the Schedule button. The jobs list dialog appears, showing all jobs currently defined for this notebook. To manage jobs, click at the right of a job in the list. From this menu, you can edit, clone, view, pause, resume, or delete a scheduled job. When you clone a scheduled job, a new job is created with the same parameters as the original. The new job appears in the list with the name “Clone of ”. How you edit a job depends on the complexity of the job’s schedule. Either the Schedule dialog or the Job details panel displays, allowing you to edit the schedule, cluster, parameters, and so on. Distribute notebooks To allow you to easily distribute Azure Databricks notebooks, Azure Databricks supports the Databricks archive, which is a package that can contain a folder of notebooks or a single notebook. A Databricks archive is a JAR file with extra metadata and has the extension .dbc. The notebooks contained in the archive are in an Azure Databricks internal format. Import an archive Click or to the right of a folder or notebook and select Import. Choose File or URL. Go to or drop a Databricks archive in the dropzone. Click Import. The archive is imported into Azure Databricks. If the archive contains a folder, Azure Databricks recreates that folder. Export an archive Click or to the right of a folder or notebook and select Export > DBC Archive. Azure Databricks downloads a file named <[folder|notebook]-name>.dbc.",Manage notebooks
89,Databrick Cluster Issue,"Getting the error that Cluster is not reachable and driver of the cluster is restarted during run.
This is making our notebooks failed and all the dependencies jobs got holded becaus eof this issue affecting downstreams.

Problem start date and time
Thu, Jun 23, 2022, 11:30 PM (UTC-08:00) Pacific Time (US & Canada)


<Start:Agent_Additional_Properties_Do_Not_Edit>
Customer has uploaded a file. /?workspaceid=3986636d-9a38-42d6-9763-188cc512d674
- ProblemStartTime: 06/24/2022 06:30:00
- Cloud: Azure
- AzureProductSubscriptionID: 8cc9b872-5ea9-4c4d-8db9-280336c41458
- AzureProductSubscriptionName: C1-SIT3-NA
- Tenant Id: bacbd475-b9da-43fa-958c-ed27e9d42d0e
- Object Id: 9eb2ec85-3769-4654-8c23-50dd2a453e02
- SubscriptionType: Unified
- RequestTypeId: AZURE_TECHNICAL
- SupportPlanDisplayName: Unified Support - Advanced
- GrantPermission: True
- ShareMemoryDump: False
- HDInsightConsent: False
- DatabricksConsent: True

- Location: westus
- ResourceUri: /subscriptions/8cc9b872-5ea9-4c4d-8db9-280336c41458/resourceGroups/cai-databricks/providers/Microsoft.Databricks/workspaces/cai-databricks-ods-c1-sit3

**Note: See External Reference in Admin Tab for full details.


<End:Agent_Additional_Properties_Do_Not_Edit>",https://docs.microsoft.com/en-us/azure/databricks/kb/execution/maximum-execution-context,databricks_ms_azure_kb,ms_kb,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Maximum execution context or notebook attachment limit reached Article 03/11/2022 2 minutes to read 3 contributors In this article Problem Cause Solution Problem Notebook or job execution stops and returns either of the following errors: Console Copy Run result unavailable: job failed with error message
Context ExecutionContextId(1731742567765160237) is disconnected.
 Console Copy Can’t attach this notebook because the cluster has reached the attached notebook limit. Detach a notebook and retry.
 Cause When you attach a notebook to a cluster, Azure Databricks creates an execution context. If there are too many notebooks attached to a cluster or too many jobs are created, at some point the cluster reaches its maximum threshold limit of 145 execution contexts, and Azure Databricks returns an error. Solution Configure context auto-eviction, which allows Azure Databricks to remove (evict) idle execution contexts. Additionally, from the pipeline and ETL design perspective, you can avoid this issue by using: Fewer notebooks to reduce the number of execution contexts that are created. A job cluster instead of an interactive cluster. If the use case permits, submit notebooks or jars as jobs.",Maximum execution context or notebook attachment limit reached
90,[ARR] [Sev C] SR-2206210030001213-Policy issue in databricks cluster,"1. issue 
The customer wanted to know how to use cluster policy. After I provided the suggestion and had a call with cx. By Cluster policy settings, cx could restrict user who have the manage permission for not seeing Worker type and Driver type when edit the cluster.
The cx want to know if it is possible restrict to hide the spark config and permission by policy.

2. Ask
 (1)Solution to this problem if you have.
(2) If there other way to hide spark config and permission?
※If it is possible could you assign the APAC or IST time engineer?
※Customer want the update by 6/30 14:00 IST. 

3. What has done :

We provided the document, however there is nothing mention about his request.
-""Best practices: Cluster policies""
https://docs.microsoft.com/en-us/azure/databricks/administration-guide/clusters/policies-best-practices

-""Policy definitions""
https://docs.microsoft.com/en-us/azure/databricks/administration-guide/clusters/policies#policy-definitions

I've tested in my environment.",https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes,databricks_ms_azure_docs,ms_azure,"Docs Azure Azure Databricks Docs Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents Secret scopes Article 07/12/2022 6 minutes to read 6 contributors In this article Overview Create an Azure Key Vault-backed secret scope Create a Databricks-backed secret scope List secret scopes Delete a secret scope Managing secrets begins with creating a secret scope. A secret scope is collection of secrets identified by a name. A workspace is limited to a maximum of 100 secret scopes. Note Databricks recommends aligning secret scopes to roles or applications rather than individuals. Overview There are two types of secret scope: Azure Key Vault-backed and Databricks-backed. Azure Key Vault-backed scopes To reference secrets stored in an Azure Key Vault, you can create a secret scope backed by Azure Key Vault. You can then leverage all of the secrets in the corresponding Key Vault instance from that secret scope. Because the Azure Key Vault-backed secret scope is a read-only interface to the Key Vault, the PutSecret and DeleteSecret Secrets API 2.0 operations are not allowed. To manage secrets in Azure Key Vault, you must use the Azure SetSecret REST API or Azure portal UI. Databricks-backed scopes A Databricks-backed secret scope is stored in (backed by) an encrypted database owned and managed by Azure Databricks. The secret scope name: Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters. The names are considered non-sensitive and are readable by all users in the workspace. You create a Databricks-backed secret scope using the Databricks CLI (version 0.7.1 and above). Alternatively, you can use the Secrets API 2.0. Scope permissions Scopes are created with permissions controlled by ACLs. By default, scopes are created with MANAGE permission for the user who created the scope (the “creator”), which lets the creator read secrets in the scope, write secrets to the scope, and change ACLs for the scope. If your account has the Premium Plan, you can assign granular permissions at any time after you create the scope. For details, see Secret access control. You can also override the default and explicitly grant MANAGE permission to all users when you create the scope. In fact, you must do this if your account does not have the Premium Plan. Best practices As a team lead, you might want to create different scopes for Azure Synapse Analytics and Azure Blob storage credentials and then provide different subgroups in your team access to those scopes. You should consider how to achieve this using the different scope types: If you use a Databricks-backed scope and add the secrets in those two scopes, they will be different secrets (Azure Synapse Analytics in scope 1, and Azure Blob storage in scope 2). If you use an Azure Key Vault-backed scope with each scope referencing a different Azure Key Vault and add your secrets to those two Azure Key Vaults, they will be different sets of secrets (Azure Synapse Analytics ones in scope 1, and Azure Blob storage in scope 2). These will work like Databricks-backed scopes. If you use two Azure Key Vault-backed scopes with both scopes referencing the same Azure Key Vault and add your secrets to that Azure Key Vault, all Azure Synapse Analytics and Azure Blob storage secrets will be available. Since ACLs are at the scope level, all members across the two subgroups will see all secrets. This arrangement does not satisfy your use case of restricting access to a set of secrets to each group. Create an Azure Key Vault-backed secret scope You can create an Azure Key Vault-backed secret scope using the UI or using the Databricks CLI. Create an Azure Key Vault-backed secret scope using the UI Verify that you have Contributor permission on the Azure Key Vault instance that you want to use to back the secret scope. If you do not have a Key Vault instance, follow the instructions in Quickstart: Create a Key Vault using the Azure portal. Go to https://<databricks-instance>#secrets/createScope. This URL is case sensitive; scope in createScope must be uppercase. Enter the name of the secret scope. Secret scope names are case insensitive. Use the Manage Principal drop-down to specify whether All Users have MANAGE permission for this secret scope or only the Creator of the secret scope (that is to say, you). MANAGE permission allows users to read and write to this secret scope, and, in the case of accounts on the Premium Plan, to change permissions for the scope. Your account must have the Premium Plan for you to be able to select Creator. This is the recommended approach: grant MANAGE permission to the Creator when you create the secret scope, and then assign more granular access permissions after you have tested the scope. For an example workflow, see Secret workflow example. If your account has the Standard Plan, you must set the MANAGE permission to the “All Users” group. If you select Creator here, you will see an error message when you try to save the scope. For more information about the MANAGE permission, see Secret access control. Enter the DNS Name (for example, https://databrickskv.vault.azure.net/) and Resource ID, for example: Copy /subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourcegroups/databricks-rg/providers/Microsoft.KeyVault/vaults/databricksKV
 These properties are available from the Properties tab of an Azure Key Vault in your Azure portal. Click the Create button. Use the Databricks CLI databricks secrets list-scopes command to verify that the scope was created successfully. For an example of using secrets when accessing Azure Blob storage, see Mounting cloud object storage on Azure Databricks. Create an Azure Key Vault-backed secret scope using the Databricks CLI Install the CLI and configure it to use an Azure Active Directory (Azure AD) token for authentication. Important You need an Azure AD user token to create an Azure Key Vault-backed secret scope with the Databricks CLI. You cannot use an Azure Databricks personal access token or an Azure AD application token that belongs to a service principal. If the key vault exists in a different tenant than the Azure Databricks workspace, the Azure AD user who creates the secret scope must have permission to create service principals in the key vault’s tenant. Otherwise, the following error occurs: Copy Unable to grant read/list permission to Databricks service principal to KeyVault 'https://xxxxx.vault.azure.net/': Status code 403, {""odata.error"":{""code"":""Authorization_RequestDenied"",""message"":{""lang"":""en"",""value"":""Insufficient privileges to complete the operation.""},""requestId"":""XXXXX"",""date"":""YYYY-MM-DDTHH:MM:SS""}}
 Create the Azure Key Vault scope: Bash Copy databricks secrets create-scope --scope <scope-name> --scope-backend-type AZURE_KEYVAULT --resource-id <azure-keyvault-resource-id> --dns-name <azure-keyvault-dns-name>
 By default, scopes are created with MANAGE permission for the user who created the scope. If your account does not have the Premium Plan, you must override that default and explicitly grant the MANAGE permission to the users (all users) group when you create the scope: Bash Copy  databricks secrets create-scope --scope <scope-name> --scope-backend-type AZURE_KEYVAULT --resource-id <azure-keyvault-resource-id> --dns-name <azure-keyvault-dns-name> --initial-manage-principal users
 If your account in on the Premium Plan, you can change permissions at any time after you create the scope. For details, see Secret access control. Once you have created a Databricks-backed secret scope, you can add secrets. For an example of using secrets when accessing Azure Blob storage, see Mounting cloud object storage on Azure Databricks. Create a Databricks-backed secret scope Secret scope names are case insensitive. To create a scope using the Databricks CLI: Bash Copy databricks secrets create-scope --scope <scope-name>
 By default, scopes are created with MANAGE permission for the user who created the scope. If your account does not have the Premium Plan, you must override that default and explicitly grant the MANAGE permission to “users” (all users) when you create the scope: Bash Copy databricks secrets create-scope --scope <scope-name> --initial-manage-principal users
 You can also create a Databricks-backed secret scope using the Secrets API Put secret operation. If your account has the Premium Plan, you can change permissions at any time after you create the scope. For details, see Secret access control. Once you have created a Databricks-backed secret scope, you can add secrets. List secret scopes To list the existing scopes in a workspace using the CLI: Bash Copy databricks secrets list-scopes
 You can also list existing scopes using the Secrets API List secrets operation. Delete a secret scope Deleting a secret scope deletes all secrets and ACLs applied to the scope. To delete a scope using the CLI: Bash Copy databricks secrets delete-scope --scope <scope-name>
 You can also delete a secret scope using the Secrets API Delete secret scope operation.",Secret scopes
91,Unable to save Git credentials on Git integration tab,Unable to save Git credentials on Git integration tab for user burak_meric@gap.com . We are getting 'Failed to save. Please try again' error message. The same is working fine for my user. Attached screenshot,https://docs.microsoft.com/en-us/azure/databricks/notebooks/github-version-control,databricks_ms_azure_docs,ms_azure,"Azure Azure Databricks Azure Azure Databricks Read in English Save Table of contents Read in English Save Feedback Edit Twitter LinkedIn Facebook Email Table of contents GitHub version control Article 06/15/2022 6 minutes to read 6 contributors In this article Enable and disable Git versioning Configure version control Work with notebook revisions GitHub Enterprise Troubleshooting Note Databricks recommends that you use Git integration with Databricks Repos to sync your work in Azure Databricks with a remote Git repository. This article describes how to set up version control for notebooks using GitHub through the UI. You can also use the Databricks CLI or Workspace API 2.0 to import and export notebooks and manage notebook versions using GitHub tools. Enable and disable Git versioning By default version control is enabled. To toggle this setting, see Manage the ability to version notebooks in Git. If Git versioning is disabled, the Git Integration tab is not available in the User Settings screen. Configure version control To configure version control, you create access credentials in your version control provider, then add those credentials to Azure Databricks. Get an access token In GitHub, follow these steps to create a personal access token that allows access to your repositories: In the upper-right corner of any page, click your profile photo, then click Settings. Click Developer settings. Click the Personal access tokens tab. Click the Generate new token button. Enter a token description. Select the repo permission, and click the Generate token button. Copy the token to your clipboard. You enter this token in Azure Databricks in the next step. See the GitHub documentation to learn more about how to create personal access tokens. Save your access token to Azure Databricks In Azure Databricks, click Settings at the lower left of your screen and click User Settings. Click the Git Integration tab. If you have previously entered credentials, click the Change settings button. In the Git provider drop-down, select GitHub. Paste your token into the Token field. Enter your GitHub username or email into the Git provider username or email field and click Save. Work with notebook revisions You work with notebook revisions in the history panel. Open the history panel by clicking Revision history at the top right of the notebook. Note You cannot modify a notebook while the history panel is open. Link a notebook to GitHub Click Revision history at the top right of the notebook. The Git status bar displays Git: Not linked. Click Git: Not linked. The Git Preferences dialog appears. The first time you open your notebook, the Status is Unlink, because the notebook is not in GitHub. In the Status field, click Link. In the Link field, paste the URL of the GitHub repository. Click the Branch drop-down and select a branch or type the name of a new branch. In the Path in Git Repo field, specify where in the repository to store your file. Python notebooks have the suggested default file extension .py. If you use .ipynb, your notebook will save in iPython notebook format. If the file already exists on GitHub, you can directly copy and paste the URL of the file. Click Save to finish linking your notebook. If this file did not previously exist, a prompt with the option Save this file to your GitHub repo displays. Type a message and click Save. Save a notebook to GitHub While the changes that you make to your notebook are saved automatically to the Azure Databricks revision history, changes do not automatically persist to GitHub. Click Revision history at the top right of the notebook to open the history Panel. Click Save Now to save your notebook to GitHub. The Save Notebook Revision dialog appears. Optionally, enter a message to describe your change. Make sure that Also commit to Git is selected. Click Save. Revert or update a notebook to a version from GitHub Once you link a notebook, Azure Databricks syncs your history with Git every time you re-open the history panel. Versions that sync to Git have commit hashes as part of the entry. Click Revision history at the top right of the notebook to open the history Panel. Choose an entry in the history panel. Azure Databricks displays that version. Click Restore this version. Click Confirm to confirm that you want to restore that version. Unlink a notebook Click Revision history at the top right of the notebook to open the history Panel. The Git status bar displays Git: Synced. Click Git: Synced. In the Git Preferences dialog, click Unlink. Click Save. Click Confirm to confirm that you want to unlink the notebook from version control. Branch support You can work on any branch of your repository and create new branches inside Azure Databricks. Create a branch Click Revision history at the top right of the notebook to open the history Panel. Click the Git status bar to open the GitHub panel. Click the Branch dropdown. Enter a branch name. Select the Create Branch option at the bottom of the dropdown. The parent branch is indicated. You always branch from your current selected branch. Create a pull request Click Revision history at the top right of the notebook to open the history Panel. Click the Git status bar to open the GitHub panel. Click Create PR. GitHub opens to a pull request page for the branch. Rebase a branch You can also rebase your branch inside Azure Databricks. The Rebase link displays if new commits are available in the parent branch. Only rebasing on top of the default branch of the parent repository is supported. For example, assume that you are working on databricks/reference-apps. You fork it into your own account (for example, brkyvz) and start working on a branch called my-branch. If a new update is pushed to databricks:master, then the Rebase button displays, and you will be able to pull the changes into your branch brkyvz:my-branch. Rebasing works a little differently in Azure Databricks. Assume the following branch structure: After a rebase, the branch structure will look like: What’s different here is that Commits C5 and C6 will not apply on top of C4. They will appear as local changes in your notebook. Any merge conflict will show up as follows: You can then commit to GitHub once again using the Save Now button. What happens if someone branched off from my branch that I just rebased? If your branch (for example, branch-a) was the base for another branch (branch-b), and you rebase, you need not worry! Once a user also rebases branch-b, everything will work out. The best practice in this situation is to use separate branches for separate notebooks. Best practices for code reviews Azure Databricks supports Git branching. You can link a notebook to any branch in a repository. Azure Databricks recommends using a separate branch for each notebook. During development, you can link a notebook to a fork of a repository or to a non-default branch in the main repository. To integrate your changes upstream, you can use the Create PR link in the Git Preferences dialog in Azure Databricks to create a GitHub pull request. The Create PR link displays only if you’re not working on the default branch of the parent repository. GitHub Enterprise Important This feature is in Private Preview. To try it, reach out to your Azure Databricks contact. You can also use the Workspace API 2.0 to programmatically create notebooks and manage the code base in GitHub Enterprise Server. Troubleshooting If you receive errors related to syncing GitHub history, verify the following: You can only link a notebook to an initialized Git repository that isn’t empty. Test the URL in a web browser. The GitHub personal access token must be active. To use a private GitHub repository, you must have permission to read the repository. If a notebook is linked to a GitHub branch that is renamed, the change is not automaticaly reflected in Azure Databricks. You must re-link the notebook to the branch manually.",GitHub version control
92,Delta table with partition unable to Handle Insert overwite,"We a have target table which  is partitioned and getting loaded with Insert Overwrite databricks sql command as describe in below link.
https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-dml-insert-into.html

The table should be refreshed in a manner that existing data get overwritten and new data get inserted and the data loaded before into table not coming as source feed on current day should remain as is in the table. 
 But as our daily run is going on , this is not working as above  everyday the  previous day data is getting completely removed and the new set of record getting inserted.
Example how Data loaded:
28th : 521979
29th :521672
30th: 521027
after today's load Table has only 521027, but if Insert overwrite works as expected it should have been 521979 among which 521027 would be refreshed today.

The table creation script as well as insert overwrite are attached along with the dbc file. please help us find out the right approach to implement Insert overwrite  here so that we can keep the history of each day load.
I am available for call anytime after 10:30CET",https://docs.gcp.databricks.com/delta/delta-batch.html,databricks_gcp_docs,gcp_docs,"Table batch reads and writes Delta Lake supports most of the options provided by Apache Spark DataFrame read and write APIs for performing batch reads and writes on tables. For information on Delta Lake SQL commands, see Delta Lake statements. Create a table Delta Lake supports creating two types of tables—tables defined in the metastore and tables defined by path. You can create tables in the following ways. SQL DDL commands: You can use standard SQL DDL commands supported in Apache Spark (for example, CREATE TABLE and REPLACE TABLE) to create Delta tables. CREATE TABLE IF NOT EXISTS default.people10m (
  id INT,
  firstName STRING,
  middleName STRING,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  ssn STRING,
  salary INT
) USING DELTA

CREATE OR REPLACE TABLE default.people10m (
  id INT,
  firstName STRING,
  middleName STRING,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  ssn STRING,
  salary INT
) USING DELTA
 Note In Databricks Runtime 8.0 and above, Delta Lake is the default format and you don’t need USING DELTA. In Databricks Runtime 7.0 and above, SQL also supports a creating table at a path without creating an entry in the Hive metastore. -- Create or replace table with path
CREATE OR REPLACE TABLE delta.`/tmp/delta/people10m` (
  id INT,
  firstName STRING,
  middleName STRING,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  ssn STRING,
  salary INT
) USING DELTA
 DataFrameWriter API: If you want to simultaneously create a table and insert data into it from Spark DataFrames or Datasets, you can use the Spark DataFrameWriter (Scala or Java and Python). # Create table in the metastore using DataFrame's schema and write data to it
df.write.format(""delta"").saveAsTable(""default.people10m"")

# Create or replace partitioned table with path using DataFrame's schema and write/overwrite data to it
df.write.format(""delta"").mode(""overwrite"").save(""/tmp/delta/people10m"")
 // Create table in the metastore using DataFrame's schema and write data to it
df.write.format(""delta"").saveAsTable(""default.people10m"")

// Create table with path using DataFrame's schema and write data to it
df.write.format(""delta"").mode(""overwrite"").save(""/tmp/delta/people10m"")
 In Databricks Runtime 8.0 and above, Delta Lake is the default format and you don’t need to specify USING DELTA, format(""delta""), or using(""delta""). In Databricks Runtime 7.0 and above, you can also create Delta tables using the Spark DataFrameWriterV2 API. DeltaTableBuilder API: You can also use the DeltaTableBuilder API in Delta Lake to create tables. Compared to the DataFrameWriter APIs, this API makes it easier to specify additional information like column comments, table properties, and generated columns. Preview This feature is in Public Preview. Note This feature is available on Databricks Runtime 8.3 and above. # Create table in the metastore
DeltaTable.createIfNotExists(spark) \
  .tableName(""default.people10m"") \
  .addColumn(""id"", ""INT"") \
  .addColumn(""firstName"", ""STRING"") \
  .addColumn(""middleName"", ""STRING"") \
  .addColumn(""lastName"", ""STRING"", comment = ""surname"") \
  .addColumn(""gender"", ""STRING"") \
  .addColumn(""birthDate"", ""TIMESTAMP"") \
  .addColumn(""ssn"", ""STRING"") \
  .addColumn(""salary"", ""INT"") \
  .execute()

# Create or replace table with path and add properties
DeltaTable.createOrReplace(spark) \
  .addColumn(""id"", ""INT"") \
  .addColumn(""firstName"", ""STRING"") \
  .addColumn(""middleName"", ""STRING"") \
  .addColumn(""lastName"", ""STRING"", comment = ""surname"") \
  .addColumn(""gender"", ""STRING"") \
  .addColumn(""birthDate"", ""TIMESTAMP"") \
  .addColumn(""ssn"", ""STRING"") \
  .addColumn(""salary"", ""INT"") \
  .property(""description"", ""table with people data"") \
  .location(""/tmp/delta/people10m"") \
  .execute()
 // Create table in the metastore
DeltaTable.createOrReplace(spark)
  .tableName(""default.people10m"")
  .addColumn(""id"", ""INT"")
  .addColumn(""firstName"", ""STRING"")
  .addColumn(""middleName"", ""STRING"")
  .addColumn(
    DeltaTable.columnBuilder(""lastName"")
      .dataType(""STRING"")
      .comment(""surname"")
      .build())
  .addColumn(""lastName"", ""STRING"", comment = ""surname"")
  .addColumn(""gender"", ""STRING"")
  .addColumn(""birthDate"", ""TIMESTAMP"")
  .addColumn(""ssn"", ""STRING"")
  .addColumn(""salary"", ""INT"")
  .execute()

// Create or replace table with path and add properties
DeltaTable.createOrReplace(spark)
  .addColumn(""id"", ""INT"")
  .addColumn(""firstName"", ""STRING"")
  .addColumn(""middleName"", ""STRING"")
  .addColumn(
    DeltaTable.columnBuilder(""lastName"")
      .dataType(""STRING"")
      .comment(""surname"")
      .build())
  .addColumn(""lastName"", ""STRING"", comment = ""surname"")
  .addColumn(""gender"", ""STRING"")
  .addColumn(""birthDate"", ""TIMESTAMP"")
  .addColumn(""ssn"", ""STRING"")
  .addColumn(""salary"", ""INT"")
  .property(""description"", ""table with people data"")
  .location(""/tmp/delta/people10m"")
  .execute()
 See the API documentation for details. See also Create a table. Partition data You can partition data to speed up queries or DML that have predicates involving the partition columns. To partition data when you create a Delta table, specify a partition by columns. The following example partitions by gender. -- Create table in the metastore
CREATE TABLE default.people10m (
  id INT,
  firstName STRING,
  middleName STRING,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  ssn STRING,
  salary INT
)
USING DELTA
PARTITIONED BY (gender)
 df.write.format(""delta"").partitionBy(""gender"").saveAsTable(""default.people10m"")

DeltaTable.create(spark) \
  .tableName(""default.people10m"") \
  .addColumn(""id"", ""INT"") \
  .addColumn(""firstName"", ""STRING"") \
  .addColumn(""middleName"", ""STRING"") \
  .addColumn(""lastName"", ""STRING"", comment = ""surname"") \
  .addColumn(""gender"", ""STRING"") \
  .addColumn(""birthDate"", ""TIMESTAMP"") \
  .addColumn(""ssn"", ""STRING"") \
  .addColumn(""salary"", ""INT"") \
  .partitionedBy(""gender"") \
  .execute()
 df.write.format(""delta"").partitionBy(""gender"").saveAsTable(""default.people10m"")

DeltaTable.createOrReplace(spark)
  .tableName(""default.people10m"")
  .addColumn(""id"", ""INT"")
  .addColumn(""firstName"", ""STRING"")
  .addColumn(""middleName"", ""STRING"")
  .addColumn(
    DeltaTable.columnBuilder(""lastName"")
      .dataType(""STRING"")
      .comment(""surname"")
      .build())
  .addColumn(""lastName"", ""STRING"", comment = ""surname"")
  .addColumn(""gender"", ""STRING"")
  .addColumn(""birthDate"", ""TIMESTAMP"")
  .addColumn(""ssn"", ""STRING"")
  .addColumn(""salary"", ""INT"")
  .partitionedBy(""gender"")
  .execute()
 To determine whether a table contains a specific partition, use the statement SELECT COUNT(*) > 0 FROM <table-name> WHERE <partition-column> = <value>. If the partition exists, true is returned. For example: SELECT COUNT(*) > 0 AS `Partition exists` FROM default.people10m WHERE gender = ""M""
 display(spark.sql(""SELECT COUNT(*) > 0 AS `Partition exists` FROM default.people10m WHERE gender = 'M'""))
 display(spark.sql(""SELECT COUNT(*) > 0 AS `Partition exists` FROM default.people10m WHERE gender = 'M'""))
 Control data location For tables defined in the metastore, you can optionally specify the LOCATION as a path. Tables created with a specified LOCATION are considered unmanaged by the metastore. Unlike a managed table, where no path is specified, an unmanaged table’s files are not deleted when you DROP the table. When you run CREATE TABLE with a LOCATION that already contains data stored using Delta Lake, Delta Lake does the following: If you specify only the table name and location, for example: CREATE TABLE default.people10m
USING DELTA
LOCATION '/tmp/delta/people10m'
 the table in the metastore automatically inherits the schema, partitioning, and table properties of the existing data. This functionality can be used to “import” data into the metastore. If you specify any configuration (schema, partitioning, or table properties), Delta Lake verifies that the specification exactly matches the configuration of the existing data. Important If the specified configuration does not exactly match the configuration of the data, Delta Lake throws an exception that describes the discrepancy. Note The metastore is not the source of truth about the latest information of a Delta table. In fact, the table definition in the metastore may not contain all the metadata like schema and properties. It contains the location of the table, and the table’s transaction log at the location is the source of truth. If you query the metastore from a system that is not aware of this Delta-specific customization, you may see incomplete or stale table information. Use generated columns Preview This feature is in Public Preview. Note This feature is available on Databricks Runtime 8.3 and above. Delta Lake supports generated columns which are a special type of columns whose values are automatically generated based on a user-specified function over other columns in the Delta table. When you write to a table with generated columns and you do not explicitly provide values for them, Delta Lake automatically computes the values. For example, you can automatically generate a date column (for partitioning the table by date) from the timestamp column; any writes into the table need only specify the data for the timestamp column. However, if you explicitly provide values for them, the values must satisfy the constraint (<value> <=> <generation expression>) IS TRUE or the write will fail with an error. Important Tables created with generated columns have a higher table writer protocol version than the default. See Table protocol versioning to understand table protocol versioning and what it means to have a higher version of a table protocol version. The following example shows how to create a table with generated columns: CREATE TABLE default.people10m (
  id INT,
  firstName STRING,
  middleName STRING,
  lastName STRING,
  gender STRING,
  birthDate TIMESTAMP,
  dateOfBirth DATE GENERATED ALWAYS AS (CAST(birthDate AS DATE)),
  ssn STRING,
  salary INT
)
USING DELTA
PARTITIONED BY (gender)
 DeltaTable.create(spark) \
  .tableName(""default.people10m"") \
  .addColumn(""id"", ""INT"") \
  .addColumn(""firstName"", ""STRING"") \
  .addColumn(""middleName"", ""STRING"") \
  .addColumn(""lastName"", ""STRING"", comment = ""surname"") \
  .addColumn(""gender"", ""STRING"") \
  .addColumn(""birthDate"", ""TIMESTAMP"") \
  .addColumn(""dateOfBirth"", DateType(), generatedAlwaysAs=""CAST(birthDate AS DATE)"") \
  .addColumn(""ssn"", ""STRING"") \
  .addColumn(""salary"", ""INT"") \
  .partitionedBy(""gender"") \
  .execute()
 DeltaTable.create(spark)
  .tableName(""default.people10m"")
  .addColumn(""id"", ""INT"")
  .addColumn(""firstName"", ""STRING"")
  .addColumn(""middleName"", ""STRING"")
  .addColumn(
    DeltaTable.columnBuilder(""lastName"")
      .dataType(""STRING"")
      .comment(""surname"")
      .build())
  .addColumn(""lastName"", ""STRING"", comment = ""surname"")
  .addColumn(""gender"", ""STRING"")
  .addColumn(""birthDate"", ""TIMESTAMP"")
  .addColumn(
    DeltaTable.columnBuilder(""dateOfBirth"")
     .dataType(DateType)
     .generatedAlwaysAs(""CAST(dateOfBirth AS DATE)"")
     .build())
  .addColumn(""ssn"", ""STRING"")
  .addColumn(""salary"", ""INT"")
  .partitionedBy(""gender"")
  .execute()
 Generated columns are stored as if they were normal columns. That is, they occupy storage. The following restrictions apply to generated columns: A generation expression can use any SQL functions in Spark that always return the same result when given the same argument values, except the following types of functions: User-defined functions. Aggregate functions. Window functions. Functions returning multiple rows. For Databricks Runtime 9.1 and above, MERGE operations support generated columns when you set spark.databricks.delta.schema.autoMerge.enabled to true. In Databricks Runtime 8.4 and above with Photon support, Delta Lake may be able to generate partition filters for a query whenever a partition column is defined by one of the following expressions: CAST(col AS DATE) and the type of col is TIMESTAMP. YEAR(col) and the type of col is TIMESTAMP. Two partition columns defined by YEAR(col), MONTH(col) and the type of col is TIMESTAMP. Three partition columns defined by YEAR(col), MONTH(col), DAY(col) and the type of col is TIMESTAMP. Four partition columns defined by YEAR(col), MONTH(col), DAY(col), HOUR(col) and the type of col is TIMESTAMP. SUBSTRING(col, pos, len) and the type of col is STRING DATE_FORMAT(col, format) and the type of col is TIMESTAMP. If a partition column is defined by one of the preceding expressions, and a query filters data using the underlying base column of a generation expression, Delta Lake looks at the relationship between the base column and the generated column, and populates partition filters based on the generated partition column if possible. For example, given the following table: CREATE TABLE events(
  eventId BIGINT,
  data STRING,
  eventType STRING,
  eventTime TIMESTAMP,
  eventDate date GENERATED ALWAYS AS (CAST(eventTime AS DATE))
)
USING DELTA
PARTITIONED BY (eventType, eventDate)
 If you then run the following query: SELECT * FROM events
WHERE eventTime >= ""2020-10-01 00:00:00"" <= ""2020-10-01 12:00:00""
 Delta Lake automatically generates a partition filter so that the preceding query only reads the data in partition date=2020-10-01 even if a partition filter is not specified. As another example, given the following table: CREATE TABLE events(
  eventId BIGINT,
  data STRING,
  eventType STRING,
  eventTime TIMESTAMP,
  year INT GENERATED ALWAYS AS (YEAR(eventTime)),
  month INT GENERATED ALWAYS AS (MONTH(eventTime)),
  day INT GENERATED ALWAYS AS (DAY(eventTime))
)
USING DELTA
PARTITIONED BY (eventType, year, month, day)
 If you then run the following query: SELECT * FROM events
WHERE eventTime >= ""2020-10-01 00:00:00"" <= ""2020-10-01 12:00:00""
 Delta Lake automatically generates a partition filter so that the preceding query only reads the data in partition year=2020/month=10/day=01 even if a partition filter is not specified. You can use an EXPLAIN clause and check the provided plan to see whether Delta Lake automatically generates any partition filters. Use special characters in column names By default, special characters such as spaces and any of the characters ,;{}()\n\t= are not supported in table column names. To include these special characters in a table’s column name, enable column mapping. Read a table You can load a Delta table as a DataFrame by specifying a table name or a path: SELECT * FROM default.people10m   -- query table in the metastore

SELECT * FROM delta.`/tmp/delta/people10m`  -- query table by path
 spark.table(""default.people10m"")    # query table in the metastore

spark.read.format(""delta"").load(""/tmp/delta/people10m"")  # query table by path
 spark.table(""default.people10m"")      // query table in the metastore

spark.read.format(""delta"").load(""/tmp/delta/people10m"")  // create table by path

import io.delta.implicits._
spark.read.delta(""/tmp/delta/people10m"")
 The DataFrame returned automatically reads the most recent snapshot of the table for any query; you never need to run REFRESH TABLE. Delta Lake automatically uses partitioning and statistics to read the minimum amount of data when there are applicable predicates in the query. Query an older snapshot of a table (time travel) In this section: Syntax Examples Data retention Delta Lake time travel allows you to query an older snapshot of a Delta table. Time travel has many use cases, including: Re-creating analyses, reports, or outputs (for example, the output of a machine learning model). This could be useful for debugging or auditing, especially in regulated industries. Writing complex temporal queries. Fixing mistakes in your data. Providing snapshot isolation for a set of queries for fast changing tables. This section describes the supported methods for querying older versions of tables, data retention concerns, and provides examples. Syntax This section shows how to query an older version of a Delta table. In this section: SQL AS OF syntax Example DataFrameReader options @ syntax SQL AS OF syntax SELECT * FROM table_name TIMESTAMP AS OF timestamp_expression
SELECT * FROM table_name VERSION AS OF version
 where timestamp_expression can be any one of: '2018-10-18T22:15:12.013Z', that is, a string that can be cast to a timestamp cast('2018-10-18 13:36:32 CEST' as timestamp) '2018-10-18', that is, a date string In Databricks Runtime 6.6 and above: current_timestamp() - interval 12 hours date_sub(current_date(), 1) Any other expression that is or can be cast to a timestamp version is a long value that can be obtained from the output of DESCRIBE HISTORY table_spec. Neither timestamp_expression nor version can be subqueries. Example SELECT * FROM default.people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'
SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123
 DataFrameReader options DataFrameReader options allow you to create a DataFrame from a Delta table that is fixed to a specific version of the table. df1 = spark.read.format(""delta"").option(""timestampAsOf"", timestamp_string).load(""/tmp/delta/people10m"")
df2 = spark.read.format(""delta"").option(""versionAsOf"", version).load(""/tmp/delta/people10m"")
 For timestamp_string, only date or timestamp strings are accepted. For example, ""2019-01-01"" and ""2019-01-01T00:00:00.000Z"". A common pattern is to use the latest state of the Delta table throughout the execution of a Databricks job to update downstream applications. Because Delta tables auto update, a DataFrame loaded from a Delta table may return different results across invocations if the underlying data is updated. By using time travel, you can fix the data returned by the DataFrame across invocations: latest_version = spark.sql(""SELECT max(version) FROM (DESCRIBE HISTORY delta.`/tmp/delta/people10m`)"").collect()
df = spark.read.format(""delta"").option(""versionAsOf"", latest_version[0][0]).load(""/tmp/delta/people10m"")
 @ syntax You may have a parametrized pipeline, where the input path of your pipeline is a parameter of your job. After the execution of your job, you may want to reproduce the output some time in the future. In this case, you can use the @ syntax to specify the timestamp or version. The timestamp must be in yyyyMMddHHmmssSSS format. You can specify a version after @ by prepending a v to the version. For example, to query version 123 for the table people10m, specify people10m@v123. SELECT * FROM default.people10m@20190101000000000
SELECT * FROM default.people10m@v123
 spark.read.format(""delta"").load(""/tmp/delta/people10m@20190101000000000"") # table on 2019-01-01 00:00:00.000
spark.read.format(""delta"").load(""/tmp/delta/people10m@v123"")              # table on version 123
 Examples Fix accidental deletes to a table for the user 111: INSERT INTO my_table
  SELECT * FROM my_table TIMESTAMP AS OF date_sub(current_date(), 1)
  WHERE userId = 111
 Fix accidental incorrect updates to a table: MERGE INTO my_table target
  USING my_table TIMESTAMP AS OF date_sub(current_date(), 1) source
  ON source.userId = target.userId
  WHEN MATCHED THEN UPDATE SET *
 Query the number of new customers added over the last week. SELECT count(distinct userId) - (
  SELECT count(distinct userId)
  FROM my_table TIMESTAMP AS OF date_sub(current_date(), 7))
 Data retention To time travel to a previous version, you must retain both the log and the data files for that version. The data files backing a Delta table are never deleted automatically; data files are deleted only when you run VACUUM. VACUUM does not delete Delta log files; log files are automatically cleaned up after checkpoints are written. By default you can time travel to a Delta table up to 30 days old unless you have: Run VACUUM on your Delta table. Changed the data or log file retention periods using the following table properties: delta.logRetentionDuration = ""interval <interval>"": controls how long the history for a table is kept. The default is interval 30 days. Each time a checkpoint is written, Databricks automatically cleans up log entries older than the retention interval. If you set this config to a large enough value, many log entries are retained. This should not impact performance as operations against the log are constant time. Operations on history are parallel but will become more expensive as the log size increases. delta.deletedFileRetentionDuration = ""interval <interval>"": controls how long ago a file must have been deleted before being a candidate for VACUUM. The default is interval 7 days. To access 30 days of historical data even if you run VACUUM on the Delta table, set delta.deletedFileRetentionDuration = ""interval 30 days"". This setting may cause your storage costs to go up. Write to a table Append To atomically add new data to an existing Delta table, use append mode: INSERT INTO default.people10m SELECT * FROM morePeople
 df.write.format(""delta"").mode(""append"").save(""/tmp/delta/people10m"")
df.write.format(""delta"").mode(""append"").saveAsTable(""default.people10m"")
 df.write.format(""delta"").mode(""append"").save(""/tmp/delta/people10m"")
df.write.format(""delta"").mode(""append"").saveAsTable(""default.people10m"")

import io.delta.implicits._
df.write.mode(""append"").delta(""/tmp/delta/people10m"")
 Overwrite To atomically replace all the data in a table, use overwrite mode: INSERT OVERWRITE TABLE default.people10m SELECT * FROM morePeople
 df.write.format(""delta"").mode(""overwrite"").save(""/tmp/delta/people10m"")
df.write.format(""delta"").mode(""overwrite"").saveAsTable(""default.people10m"")
 df.write.format(""delta"").mode(""overwrite"").save(""/tmp/delta/people10m"")
df.write.format(""delta"").mode(""overwrite"").saveAsTable(""default.people10m"")

import io.delta.implicits._
df.write.mode(""overwrite"").delta(""/tmp/delta/people10m"")
 Using DataFrames, you can also selectively overwrite only the data that matches an arbitrary expression. This feature is available in Databricks Runtime 9.1 LTS and above. The following command atomically replaces events in January in the target table, which is partitioned by start_date, with the data in df: df.write \
  .format(""delta"") \
  .mode(""overwrite"") \
  .option(""replaceWhere"", ""start_date >= '2017-01-01' AND end_date <= '2017-01-31'"") \
  .save(""/tmp/delta/events"")
 df.write
  .format(""delta"")
  .mode(""overwrite"")
  .option(""replaceWhere"", ""start_date >= '2017-01-01' AND end_date <= '2017-01-31'"")
  .save(""/tmp/delta/events"")
 This sample code writes out the data in df, validates that it all matches the predicate, and performs an atomic replacement. If you want to write out data that doesn’t all match the predicate, to replace the matching rows in the target table, you can disable the constraint check by setting spark.databricks.delta.replaceWhere.constraintCheck.enabled to false: spark.conf.set(""spark.databricks.delta.replaceWhere.constraintCheck.enabled"", False)
 spark.conf.set(""spark.databricks.delta.replaceWhere.constraintCheck.enabled"", false)
 In Databricks Runtime 9.0 and below, replaceWhere overwrites data matching a predicate over partition columns only. The following command atomically replaces the month in January in the target table, which is partitioned by date, with the data in df: df.write \
  .format(""delta"") \
  .mode(""overwrite"") \
  .option(""replaceWhere"", ""birthDate >= '2017-01-01' AND birthDate <= '2017-01-31'"") \
  .save(""/tmp/delta/people10m"")
 df.write
  .format(""delta"")
  .mode(""overwrite"")
  .option(""replaceWhere"", ""birthDate >= '2017-01-01' AND birthDate <= '2017-01-31'"")
  .save(""/tmp/delta/people10m"")
 In Databricks Runtime 9.1 and above, if you want to fall back to the old behavior, you can disable the spark.databricks.delta.replaceWhere.dataColumns.enabled flag: spark.conf.set(""spark.databricks.delta.replaceWhere.dataColumns.enabled"", False)
 spark.conf.set(""spark.databricks.delta.replaceWhere.dataColumns.enabled"", false)
 For Delta Lake support for updating tables, see Table deletes, updates, and merges. Limit rows written in a file You can use the SQL session configuration spark.sql.files.maxRecordsPerFile to specify the maximum number of records to write to a single file for a Delta Lake table. Specifying a value of zero or a negative value represents no limit. In Databricks Runtime 10.5 and above, you can also use the DataFrameWriter option maxRecordsPerFile when using the DataFrame APIs to write to a Delta Lake table. When maxRecordsPerFile is specified, the value of the SQL session configuration spark.sql.files.maxRecordsPerFile is ignored. df.write.format(""delta"") \
  .mode(""append"") \
  .option(""maxRecordsPerFile"", ""10000"") \
  .save(""/tmp/delta/people10m"")
 df.write.format(""delta"")
  .mode(""append"")
  .option(""maxRecordsPerFile"", ""10000"")
  .save(""/tmp/delta/people10m"")
 Idempotent writes Sometimes a job that writes data to a Delta table is restarted due to various reasons (for example, job encounters a failure). The failed job may or may not have written the data to Delta table before terminating. In the case where the data is written to the Delta table, the restarted job writes the same data to the Delta table which results in duplicate data. To address this, Delta tables support the following DataFrameWriter options to make the writes idempotent: txnAppId: A unique string that you can pass on each DataFrame write. For example, this can be the name of the job. txnVersion: A monotonically increasing number that acts as transaction version. This number needs to be unique for data that is being written to the Delta table(s). For example, this can be the epoch seconds of the instant when the query is attempted for the first time. Any subsequent restarts of the same job needs to have the same value for txnVersion. The above combination of options needs to be unique for each new data that is being ingested into the Delta table and the txnVersion needs to be higher than the last data that was ingested into the Delta table. For example: Last successfully written data contains option values as dailyETL:23423 (txnAppId:txnVersion). Next write of data should have txnAppId = dailyETL and txnVersion as at least 23424 (one more than the last written data txnVersion). Any attempt to write data with txnAppId = dailyETL and txnVersion as 23422 or less is ignored because the txnVersion is less than the last recorded txnVersion in the table. Attempt to write data with txnAppId:txnVersion as anotherETL:23424 is successful writing data to the table as it contains a different txnAppId compared to the same option value in last ingested data. Warning This solution assumes that the data being written to Delta table(s) in multiple retries of the job is same. If a write attempt in a Delta table succeeds but due to some downstream failure there is a second write attempt with same txn options but different data, then that second write attempt will be ignored. This can cause unexpected results. Example app_id = ... # A unique string that is used as an application ID.
version = ... # A monotonically increasing number that acts as transaction version.

dataFrame.write.format(...).option(""txnVersion"", version).option(""txnAppId"", app_id).save(...)
 val appId = ... // A unique string that is used as an application ID.
version = ... // A monotonically increasing number that acts as transaction version.

dataFrame.write.format(...).option(""txnVersion"", version).option(""txnAppId"", appId).save(...)
 Set user-defined commit metadata You can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option userMetadata or the SparkSession configuration spark.databricks.delta.commitInfo.userMetadata. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the history operation. SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data
INSERT OVERWRITE default.people10m SELECT * FROM morePeople
 df.write.format(""delta"") \
  .mode(""overwrite"") \
  .option(""userMetadata"", ""overwritten-for-fixing-incorrect-data"") \
  .save(""/tmp/delta/people10m"")
 df.write.format(""delta"")
  .mode(""overwrite"")
  .option(""userMetadata"", ""overwritten-for-fixing-incorrect-data"")
  .save(""/tmp/delta/people10m"")
 Schema validation Delta Lake automatically validates that the schema of the DataFrame being written is compatible with the schema of the table. Delta Lake uses the following rules to determine whether a write from a DataFrame to a table is compatible: All DataFrame columns must exist in the target table. If there are columns in the DataFrame not present in the table, an exception is raised. Columns present in the table but not in the DataFrame are set to null. DataFrame column data types must match the column data types in the target table. If they don’t match, an exception is raised. DataFrame column names cannot differ only by case. This means that you cannot have columns such as “Foo” and “foo” defined in the same table. While you can use Spark in case sensitive or insensitive (default) mode, Parquet is case sensitive when storing and returning column information. Delta Lake is case-preserving but insensitive when storing the schema and has this restriction to avoid potential mistakes, data corruption, or loss issues. Delta Lake support DDL to add new columns explicitly and the ability to update schema automatically. If you specify other options, such as partitionBy, in combination with append mode, Delta Lake validates that they match and throws an error for any mismatch. When partitionBy is not present, appends automatically follow the partitioning of the existing data. Note In Databricks Runtime 7.0 and above, INSERT syntax provides schema enforcement and supports schema evolution. If a column’s data type cannot be safely cast to your Delta Lake table’s data type, then a runtime exception is thrown. If schema evolution is enabled, new columns can exist as the last columns of your schema (or nested columns) for the schema to evolve. For more information about enforcing and evolving schemas in Delta Lake, watch this YouTube video (55 minutes). &nbsp; Update table schema Delta Lake lets you update the schema of a table. The following types of changes are supported: Adding new columns (at arbitrary positions) Reordering existing columns Renaming existing columns You can make these changes explicitly using DDL or implicitly using DML. Important When you update a Delta table schema, streams that read from that table terminate. If you want the stream to continue you must restart it. For recommended methods, see Production considerations for Structured Streaming applications on Databricks. Explicitly update schema You can use the following DDL to explicitly change the schema of a table. Add columns ALTER TABLE table_name ADD COLUMNS (col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name], ...)
 By default, nullability is true. To add a column to a nested field, use: ALTER TABLE table_name ADD COLUMNS (col_name.nested_col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name], ...)
 Example If the schema before running ALTER TABLE boxes ADD COLUMNS (colB.nested STRING AFTER field1) is: - root
| - colA
| - colB
| +-field1
| +-field2
 the schema after is: - root
| - colA
| - colB
| +-field1
| +-nested
| +-field2
 Note Adding nested columns is supported only for structs. Arrays and maps are not supported. Change column comment or ordering ALTER TABLE table_name ALTER [COLUMN] col_name col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name]
 To change a column in a nested field, use: ALTER TABLE table_name ALTER [COLUMN] col_name.nested_col_name nested_col_name data_type [COMMENT col_comment] [FIRST|AFTER colA_name]
 Example If the schema before running ALTER TABLE boxes CHANGE COLUMN colB.field2 field2 STRING FIRST is: - root
| - colA
| - colB
| +-field1
| +-field2
 the schema after is: - root
| - colA
| - colB
| +-field2
| +-field1
 Replace columns ALTER TABLE table_name REPLACE COLUMNS (col_name1 col_type1 [COMMENT col_comment1], ...)
 Example When running the following DDL: ALTER TABLE boxes REPLACE COLUMNS (colC STRING, colB STRUCT<field2:STRING, nested:STRING, field1:STRING>, colA STRING)
 if the schema before is: - root
| - colA
| - colB
| +-field1
| +-field2
 the schema after is: - root
| - colC
| - colB
| +-field2
| +-nested
| +-field1
| - colA
 Rename columns Preview This feature is in Public Preview. Note This feature is available in Databricks Runtime 10.2 and above. To rename columns without rewriting any of the columns’ existing data, you must enable column mapping for the table. See Delta column mapping. To rename a column: ALTER TABLE table_name RENAME COLUMN old_col_name TO new_col_name
 To rename a nested field: ALTER TABLE table_name RENAME COLUMN col_name.old_nested_field TO new_nested_field
 Example When you run the following command: ALTER TABLE boxes RENAME COLUMN colB.field1 TO field001
 If the schema before is: - root
| - colA
| - colB
| +-field1
| +-field2
 Then the schema after is: - root
| - colA
| - colB
| +-field001
| +-field2
 See Delta column mapping. Drop columns Preview This feature is in Public Preview. Note This feature is available in Databricks Runtime 11.0 and above. To drop columns as a metadata-only operation without rewriting any data files, you must enable column mapping for the table. See Delta column mapping. Important Dropping a column from metadata does not delete the underlying data for the column in files. To purge the dropped column data, you can use REORG TABLE to rewrite files. You can then use VACUUM to physically delete the files that contain the dropped column data. To drop a column: ALTER TABLE table_name DROP COLUMN col_name
 To drop multiple columns: ALTER TABLE table_name DROP COLUMNS (col_name_1, col_name_2)
 Change column type or name You can change a column’s type or name or drop a column by rewriting the table. To do this, use the overwriteSchema option: Change a column type spark.read.table(...) \
  .withColumn(""birthDate"", col(""birthDate"").cast(""date"")) \
  .write \
  .format(""delta"") \
  .mode(""overwrite"")
  .option(""overwriteSchema"", ""true"") \
  .saveAsTable(...)
 Change a column name spark.read.table(...) \
  .withColumnRenamed(""dateOfBirth"", ""birthDate"") \
  .write \
  .format(""delta"") \
  .mode(""overwrite"") \
  .option(""overwriteSchema"", ""true"") \
  .saveAsTable(...)
 Automatic schema update Delta Lake can automatically update the schema of a table as part of a DML transaction (either appending or overwriting), and make the schema compatible with the data being written. Add columns Columns that are present in the DataFrame but missing from the table are automatically added as part of a write transaction when: write or writeStream have .option(""mergeSchema"", ""true"") spark.databricks.delta.schema.autoMerge.enabled is true When both options are specified, the option from the DataFrameWriter takes precedence. The added columns are appended to the end of the struct they are present in. Case is preserved when appending a new column. Note mergeSchema is not supported when table access control is enabled (as it elevates a request that requires MODIFY to one that requires ALL PRIVILEGES). mergeSchema cannot be used with INSERT INTO or .write.insertInto(). NullType columns Because Parquet doesn’t support NullType, NullType columns are dropped from the DataFrame when writing into Delta tables, but are still stored in the schema. When a different data type is received for that column, Delta Lake merges the schema to the new data type. If Delta Lake receives a NullType for an existing column, the old schema is retained and the new column is dropped during the write. NullType in streaming is not supported. Since you must set schemas when using streaming this should be very rare. NullType is also not accepted for complex types such as ArrayType and MapType. Replace table schema By default, overwriting the data in a table does not overwrite the schema. When overwriting a table using mode(""overwrite"") without replaceWhere, you may still want to overwrite the schema of the data being written. You replace the schema and partitioning of the table by setting the overwriteSchema option to true: df.write.option(""overwriteSchema"", ""true"")
 Views on tables Delta Lake supports the creation of views on top of Delta tables just like you might with a data source table. These views integrate with table access control to allow for column and row level security. The core challenge when you operate with views is resolving the schemas. If you alter a Delta table schema, you must recreate derivative views to account for any additions to the schema. For instance, if you add a new column to a Delta table, you must make sure that this column is available in the appropriate views built on top of that base table. Table properties You can store your own metadata as a table property using TBLPROPERTIES in CREATE and ALTER. You can then SHOW that metadata. For example: ALTER TABLE default.people10m SET TBLPROPERTIES ('department' = 'accounting', 'delta.appendOnly' = 'true');

-- Show the table's properties.
SHOW TBLPROPERTIES default.people10m;

-- Show just the 'department' table property.
SHOW TBLPROPERTIES default.people10m ('department');
 TBLPROPERTIES are stored as part of Delta table metadata. You cannot define new TBLPROPERTIES in a CREATE statement if a Delta table already exists in a given location. In addition, to tailor behavior and performance, Delta Lake supports certain Delta table properties: Block deletes and updates in a Delta table: delta.appendOnly=true. Configure the time travel retention properties: delta.logRetentionDuration=<interval-string> and delta.deletedFileRetentionDuration=<interval-string>. For details, see Data retention. Configure the number of columns for which statistics are collected: delta.dataSkippingNumIndexedCols=n. This property indicates to the writer that statistics are to be collected only for the first n columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out. Note Modifying a Delta table property is a write operation that will conflict with other concurrent write operations, causing them to fail. We recommend that you modify a table property only when there are no concurrent write operations on the table. You can also set delta.-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property delta.appendOnly=true, set the Spark configuration spark.databricks.delta.properties.defaults.appendOnly to true. For example: spark.sql(""SET spark.databricks.delta.properties.defaults.appendOnly = true"")
 spark.conf.set(""spark.databricks.delta.properties.defaults.appendOnly"", ""true"")
 spark.conf.set(""spark.databricks.delta.properties.defaults.appendOnly"", ""true"")
 See also the Delta table properties reference. Table metadata Delta Lake has rich features for exploring table metadata. It supports SHOW [PARTITIONS | COLUMNS] and DESCRIBE TABLE. See Databricks Runtime 7.x and above: SHOW PARTITIONS, SHOW COLUMNS, DESCRIBE TABLE Databricks Runtime 5.5 LTS and 6.x: _, _, _ It also provides the following unique commands: DESCRIBE DETAIL DESCRIBE HISTORY DESCRIBE DETAIL Provides information about schema, partitioning, table size, and so on. For details, see Retrieve Delta table details. DESCRIBE HISTORY Provides provenance information, including the operation, user, and so on, and operation metrics for each write to a table. Table history is retained for 30 days. For details, see Retrieve Delta table history. The Explore and create tables with the Data tab provides a visual view of this detailed table information and history for Delta tables. In addition to the table schema and sample data, you can click the History tab to see the table history that displays with DESCRIBE HISTORY. Configure storage credentials Delta Lake uses Hadoop FileSystem APIs to access the storage systems. The credentails for storage systems usually can be set through Hadoop configurations. Delta Lake provides multiple ways to set Hadoop configurations similar to Apache Spark. Spark configurations When you start a Spark application on a cluster, you can set the Spark configurations in the form of spark.hadoop.* to pass your custom Hadoop configurations. For example, Setting a value for spark.hadoop.a.b.c will pass the value as a Hadoop configuration a.b.c, and Delta Lake will use it to access Hadoop FileSystem APIs. See __ for more details. SQL session configurations Spark SQL will pass all of the current SQL session configurations to Delta Lake, and Delta Lake will use them to access Hadoop FileSystem APIs. For example, SET a.b.c=x.y.z will tell Delta Lake to pass the value x.y.z as a Hadoop configuration a.b.c, and Delta Lake will use it to access Hadoop FileSystem APIs. DataFrame options Besides setting Hadoop file system configurations through the Spark (cluster) configurations or SQL session configurations, Delta supports reading Hadoop file system configurations from DataFrameReader and DataFrameWriter options (that is, option keys that start with the fs. prefix) when the table is read or written, by using DataFrameReader.load(path) or DataFrameWriter.save(path). Note This feature is available in Databricks Runtime 10.1 and above. For example, you can pass your storage credentails through DataFrame options:     spark.conf.set(""google.cloud.auth.service.account.enable"", ""true"")
df1 = spark.read.format(""delta"") \
  .option(""fs.gs.auth.service.account.email"", ""<client-email-1>"") \
      .option(""fs.gs.project.id"", ""<project-id-1>"") \
      .option(""fs.gs.auth.service.account.private.key"", ""<private-key-1>"") \
      .option(""fs.gs.auth.service.account.private.key.id"", ""<private-key-id-1>"") \
  .read(""..."")
df2 = spark.read.format(""delta"") \
  .option(""fs.gs.auth.service.account.email"", ""<client-email-2>"") \
      .option(""fs.gs.project.id"", ""<project-id-2>"") \
      .option(""fs.gs.auth.service.account.private.key"", ""<private-key-2>"") \
      .option(""fs.gs.auth.service.account.private.key.id"", ""<private-key-id-2>"") \
  .read(""..."")
df1.union(df2).write.format(""delta"") \
  .mode(""overwrite"") \
  .option(""fs.gs.auth.service.account.email"", ""<client-email-3>"") \
      .option(""fs.gs.project.id"", ""<project-id-3>"") \
      .option(""fs.gs.auth.service.account.private.key"", ""<private-key-3>"") \
      .option(""fs.gs.auth.service.account.private.key.id"", ""<private-key-id-3>"") \
  .save(""..."")
     spark.conf.set(""google.cloud.auth.service.account.enable"", ""true"")
val df1 = spark.read.format(""delta"")
  .option(""fs.gs.auth.service.account.email"", ""<client-email-1>"")
      .option(""fs.gs.project.id"", ""<project-id-1>"")
      .option(""fs.gs.auth.service.account.private.key"", ""<private-key-1>"")
      .option(""fs.gs.auth.service.account.private.key.id"", ""<private-key-id-1>"")
  .read(""..."")
val df2 = spark.read.format(""delta"")
  .option(""fs.gs.auth.service.account.email"", ""<client-email-2>"")
      .option(""fs.gs.project.id"", ""<project-id-2>"")
      .option(""fs.gs.auth.service.account.private.key"", ""<private-key-2>"")
      .option(""fs.gs.auth.service.account.private.key.id"", ""<private-key-id-2>"")
  .read(""..."")
df1.union(df2).write.format(""delta"")
  .mode(""overwrite"")
  .option(""fs.gs.auth.service.account.email"", ""<client-email-3>"")
      .option(""fs.gs.project.id"", ""<project-id-3>"")
      .option(""fs.gs.auth.service.account.private.key"", ""<private-key-3>"")
      .option(""fs.gs.auth.service.account.private.key.id"", ""<private-key-id-3>"")
  .save(""..."")
 You can find the details of the Hadoop file system configurations for your storage in Data sources. Notebook For an example of the various Delta table metadata commands, see the end of the following notebook: Delta Lake batch commands notebook Open notebook in new tab Copy link for import",Table batch reads and writes
93,Delta table with partition unable to Handle Insert overwite,"We a have target table which  is partitioned and getting loaded with Insert Overwrite databricks sql command as describe in below link.
https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-dml-insert-into.html

The table should be refreshed in a manner that existing data get overwritten and new data get inserted and the data loaded before into table not coming as source feed on current day should remain as is in the table. 
 But as our daily run is going on , this is not working as above  everyday the  previous day data is getting completely removed and the new set of record getting inserted.
Example how Data loaded:
28th : 521979
29th :521672
30th: 521027
after today's load Table has only 521027, but if Insert overwrite works as expected it should have been 521979 among which 521027 would be refreshed today.

The table creation script as well as insert overwrite are attached along with the dbc file. please help us find out the right approach to implement Insert overwrite  here so that we can keep the history of each day load.
I am available for call anytime after 10:30CET",https://docs.gcp.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-syntax-dml-insert-into.html,databricks_gcp_docs,gcp_docs,"INSERT INTO Inserts new rows into a table and optionally truncates the table or partitions. You specify the inserted rows by value expressions or the result of a query. Syntax INSERT { OVERWRITE | INTO } [ TABLE ] table_name
    [ PARTITION clause ]
    [ ( column_name [, ...] ) ]
    query
 Note When you INSERT INTO a Delta table schema enforcement and evolution is supported. If a column’s data type cannot be safely cast to a Delta table’s data type, a runtime exception is thrown. If schema evolution is enabled, new columns can exist as the last columns of your schema (or nested columns) for the schema to evolve. Parameters INTO or OVERWRITE If you specify OVERWRITE the following applies: Without a partition_spec the table is truncated before inserting the first row. Otherwise all partitions matching the partition_spec are truncated before inserting the first row. If you specify INTO all rows inserted are additive to the existing rows. table_name Identifies the table to be inserted to. The name must not include a temporal specification. PARTITION clause An optional parameter that specifies a target partition for the insert. You may also only partially specify the partition. ( column_name [, …] ) An optional permutation of all the columns in the table. You can use this clause to map the columns if the columns returned by the query does not line up with the natural order of the column. query A query that produces the rows to be inserted. You must match the number of columns returned by the query with the columns in the table excluding partitioning columns with assigned values in the PARTITION clause. If a data type cannot be safely cast to the matching column data type, a runtime exception is thrown. If schema evolution is enabled, new columns can exist as the last columns of your schema (or nested columns) for the schema to evolve. Dynamic partition inserts In PARTITION clause, the partition column values are optional. When the partition specification part_spec is not completely provided, such inserts are called dynamic partition inserts or multi-partition inserts. When the values are not specified, these columns are referred to as dynamic partition columns; otherwise, they are static partition columns. For example, the partition spec (p1 = 3, p2, p3) has a static partition column (p1) and two dynamic partition columns (p2 and p3). In PARTITION clause, static partition keys must come before the dynamic partition keys. This means all partition columns having constant values must appear before other partition columns that do not have an assigned constant value. The partition values of dynamic partition columns are determined during the execution. The dynamic partition columns must be specified last in both part_spec and the input result set (of the row value lists or the select query). They are resolved by position, instead of by names. Thus, the orders must be exactly matched. The DataFrameWriter APIs do not have an interface to specify partition values. Therefore, the insertInto() API is always using dynamic partition mode. Examples In this section: INSERT INTO Insert with a column list Insert with both a partition spec and a column list INSERT OVERWRITE INSERT INTO Single row insert using a VALUES clause > CREATE TABLE students (name VARCHAR(64), address VARCHAR(64), student_id INT)
  PARTITIONED BY (student_id);

> INSERT INTO students VALUES
    ('Amy Smith', '123 Park Ave, San Jose', 111111);

> SELECT * FROM students;
      name               address student_id
 --------- --------------------- ----------
 Amy Smith 123 Park Ave,San Jose     111111
 Multi-row insert using a VALUES clause > INSERT INTO students VALUES
    ('Bob Brown', '456 Taylor St, Cupertino', 222222),
    ('Cathy Johnson', '789 Race Ave, Palo Alto', 333333);

> SELECT * FROM students;
          name                  address student_id
 ------------- ------------------------ ----------
     Amy Smith   123 Park Ave, San Jose     111111
     Bob Brown 456 Taylor St, Cupertino     222222
 Cathy Johnson  789 Race Ave, Palo Alto     333333
 Insert using a subquery -- Assuming the persons table has already been created and populated.
> SELECT * FROM persons;
          name                   address       ssn
 ------------- ------------------------- ---------
 Dora Williams 134 Forest Ave, Melo Park 123456789
   Eddie Davis   245 Market St, Milpitas 345678901

> INSERT INTO students PARTITION (student_id = 444444)
    SELECT name, address FROM persons WHERE name = ""Dora Williams"";

> SELECT * FROM students;
          name                   address student_id
 ------------- ------------------------- ----------
     Amy Smith    123 Park Ave, San Jose     111111
     Bob Brown  456 Taylor St, Cupertino     222222
 Cathy Johnson   789 Race Ave, Palo Alto     333333
 Dora Williams 134 Forest Ave, Melo Park     444444
 Insert using a TABLE clause -- Assuming the visiting_students table has already been created and populated.
> SELECT * FROM visiting_students;
          name               address student_id
 ------------- --------------------- ----------
 Fleur Laurent 345 Copper St, London     777777
 Gordon Martin  779 Lake Ave, Oxford     888888

> INSERT INTO students TABLE visiting_students;

> SELECT * FROM students;
          name                   address student_id
 ------------- ------------------------- ----------
     Amy Smith     123 Park Ave,San Jose     111111
     Bob Brown  456 Taylor St, Cupertino     222222
 Cathy Johnson   789 Race Ave, Palo Alto     333333
 Dora Williams 134 Forest Ave, Melo Park     444444
 Fleur Laurent     345 Copper St, London     777777
 Gordon Martin      779 Lake Ave, Oxford     888888
 Insert into a directory > CREATE TABLE students (name VARCHAR(64), address VARCHAR(64), student_id INT)
    PARTITIONED BY (student_id)
    LOCATION ""/mnt/user1/students"";

> INSERT INTO delta.`/mnt/user1/students` VALUES
    ('Amy Smith', '123 Park Ave, San Jose', 111111);
> SELECT * FROM students;
          name                   address student_id
 ------------- ------------------------- ----------
     Amy Smith    123 Park Ave, San Jose     111111
 Insert with a column list > INSERT INTO students (address, name, student_id) VALUES
    ('Hangzhou, China', 'Kent Yao', 11215016);
> SELECT * FROM students WHERE name = 'Kent Yao';
      name                address student_id
 --------- ---------------------- ----------
 Kent Yao         Hangzhou, China   11215016
 Insert with both a partition spec and a column list > INSERT INTO students PARTITION (student_id = 11215017) (address, name) VALUES
    ('Hangzhou, China', 'Kent Yao Jr.');
> SELECT * FROM students WHERE student_id = 11215017;
         name                address student_id
 ------------ ---------------------- ----------
 Kent Yao Jr.        Hangzhou, China   11215017
 INSERT OVERWRITE Insert using a VALUES clause -- Assuming the students table has already been created and populated.
> SELECT * FROM students;
          name                   address student_id
 ------------- ------------------------- ----------
     Amy Smith    123 Park Ave, San Jose     111111
     Bob Brown  456 Taylor St, Cupertino     222222
 Cathy Johnson   789 Race Ave, Palo Alto     333333
 Dora Williams 134 Forest Ave, Melo Park     444444
 Fleur Laurent     345 Copper St, London     777777
 Gordon Martin      779 Lake Ave, Oxford     888888
   Helen Davis 469 Mission St, San Diego     999999
    Jason Wang     908 Bird St, Saratoga     121212

> INSERT OVERWRITE students VALUES
    ('Ashua Hill', '456 Erica Ct, Cupertino', 111111),
    ('Brian Reed', '723 Kern Ave, Palo Alto', 222222);

> SELECT * FROM students;
       name                 address student_id
 ---------- ----------------------- ----------
 Ashua Hill 456 Erica Ct, Cupertino     111111
 Brian Reed 723 Kern Ave, Palo Alto     222222
 Insert using a subquery -- Assuming the persons table has already been created and populated.
> SELECT * FROM persons;
          name                   address       ssn
 ------------- ------------------------- ---------
 Dora Williams 134 Forest Ave, Melo Park 123456789
   Eddie Davis    245 Market St,Milpitas 345678901

> INSERT OVERWRITE students PARTITION (student_id = 222222)
    SELECT name, address FROM persons WHERE name = ""Dora Williams"";

> SELECT * FROM students;
          name                   address student_id
 ------------- ------------------------- ----------
    Ashua Hill   456 Erica Ct, Cupertino     111111
 Dora Williams 134 Forest Ave, Melo Park     222222
 Insert using a TABLE clause -- Assuming the visiting_students table has already been created and populated.
> SELECT * FROM visiting_students;
          name               address student_id
 ------------- --------------------- ----------
 Fleur Laurent 345 Copper St, London     777777
 Gordon Martin  779 Lake Ave, Oxford     888888

> INSERT OVERWRITE students TABLE visiting_students;

> SELECT * FROM students;
          name               address student_id
 ------------- --------------------- ----------
 Fleur Laurent 345 Copper St, London     777777
 Gordon Martin  779 Lake Ave, Oxford     888888
 Insert overwrite a directory > CREATE TABLE students (name VARCHAR(64), address VARCHAR(64), student_id INT)
    PARTITIONED BY (student_id)
    LOCATION ""/mnt/user1/students"";

> INSERT OVERWRITE delta.`/mnt/user1/students` VALUES
    ('Amy Smith', '123 Park Ave, San Jose', 111111);
> SELECT * FROM students;
          name                   address student_id
 ------------- ------------------------- ----------
     Amy Smith    123 Park Ave, San Jose     111111
 Related articles COPY DELETE MERGE PARTITION query UPDATE INSERT OVERWRITE DIRECTORY INSERT OVERWRITE DIRECTORY with Hive format",INSERT INTO
